{"2024-02-27T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2402.17764v1","updated":"2024-02-27T18:56:19Z","published":"2024-02-27T18:56:19Z","title":"The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits","summary":"  Recent research, such as BitNet, is paving the way for a new era of 1-bit\nLarge Language Models (LLMs). In this work, we introduce a 1-bit LLM variant,\nnamely BitNet b1.58, in which every single parameter (or weight) of the LLM is\nternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16)\nTransformer LLM with the same model size and training tokens in terms of both\nperplexity and end-task performance, while being significantly more\ncost-effective in terms of latency, memory, throughput, and energy consumption.\nMore profoundly, the 1.58-bit LLM defines a new scaling law and recipe for\ntraining new generations of LLMs that are both high-performance and\ncost-effective. Furthermore, it enables a new computation paradigm and opens\nthe door for designing specific hardware optimized for 1-bit LLMs.\n","authors":["Shuming Ma","Hongyu Wang","Lingxiao Ma","Lei Wang","Wenhui Wang","Shaohan Huang","Li Dong","Ruiping Wang","Jilong Xue","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2402.17764v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2402.17762v1","updated":"2024-02-27T18:55:17Z","published":"2024-02-27T18:55:17Z","title":"Massive Activations in Large Language Models","summary":"  We observe an empirical phenomenon in Large Language Models (LLMs) -- very\nfew activations exhibit significantly larger values than others (e.g., 100,000\ntimes larger). We call them massive activations. First, we demonstrate the\nwidespread existence of massive activations across various LLMs and\ncharacterize their locations. Second, we find their values largely stay\nconstant regardless of the input, and they function as indispensable bias terms\nin LLMs. Third, these massive activations lead to the concentration of\nattention probabilities to their corresponding tokens, and further, implicit\nbias terms in the self-attention output. Last, we also study massive\nactivations in Vision Transformers.\n","authors":["Mingjie Sun","Xinlei Chen","J. Zico Kolter","Zhuang Liu"],"pdf_url":"https://arxiv.org/pdf/2402.17762v1.pdf","comment":"Website at\n  https://eric-mingjie.github.io/massive-activations/index.html"},{"id":"http://arxiv.org/abs/2402.17759v1","updated":"2024-02-27T18:52:19Z","published":"2024-02-27T18:52:19Z","title":"Towards Optimal Learning of Language Models","summary":"  This work studies the general principles of improving the learning of\nlanguage models (LMs), which aims at reducing the necessary training steps for\nachieving superior performance. Specifically, we present a theory for the\noptimal learning of LMs. We first propose an objective that optimizes LM\nlearning by maximizing the data compression ratio in an\n\"LM-training-as-lossless-compression\" view. Then, we derive a theorem, named\nLearning Law, to reveal the properties of the dynamics in the optimal learning\nprocess under our objective. The theorem is then validated by experiments on a\nlinear classification and a real-world language modeling task. Finally, we\nempirically verify that the optimal learning of LMs essentially stems from the\nimprovement of the coefficients in the scaling law of LMs, indicating great\npromise and significance for designing practical learning acceleration methods.\nOur code can be found at https://aka.ms/LearningLaw.\n","authors":["Yuxian Gu","Li Dong","Yaru Hao","Qingxiu Dong","Minlie Huang","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2402.17759v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.17492v2","updated":"2024-02-27T18:42:42Z","published":"2023-06-30T09:07:37Z","title":"Preference Ranking Optimization for Human Alignment","summary":"  Large language models (LLMs) often contain misleading content, emphasizing\nthe need to align them with human values to ensure secure AI systems.\nReinforcement learning from human feedback (RLHF) has been employed to achieve\nthis alignment. However, it encompasses two main drawbacks: (1) RLHF exhibits\ncomplexity, instability, and sensitivity to hyperparameters in contrast to SFT.\n(2) Despite massive trial-and-error, multiple sampling is reduced to pair-wise\ncontrast, thus lacking contrasts from a macro perspective. In this paper, we\npropose Preference Ranking Optimization (PRO) as an efficient SFT algorithm to\ndirectly fine-tune LLMs for human alignment. PRO extends the pair-wise contrast\nto accommodate preference rankings of any length. By iteratively contrasting\ncandidates, PRO instructs the LLM to prioritize the best response while\nprogressively ranking the rest responses. In this manner, PRO effectively\ntransforms human alignment into aligning the probability ranking of n responses\ngenerated by LLM with the preference ranking of humans towards these responses.\nExperiments have shown that PRO outperforms baseline algorithms, achieving\ncomparable results to ChatGPT and human responses through automatic-based,\nreward-based, GPT-4, and human evaluations.\n","authors":["Feifan Song","Bowen Yu","Minghao Li","Haiyang Yu","Fei Huang","Yongbin Li","Houfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2306.17492v2.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2402.17753v1","updated":"2024-02-27T18:42:31Z","published":"2024-02-27T18:42:31Z","title":"Evaluating Very Long-Term Conversational Memory of LLM Agents","summary":"  Existing works on long-term open-domain dialogues focus on evaluating model\nresponses within contexts spanning no more than five chat sessions. Despite\nadvancements in long-context large language models (LLMs) and retrieval\naugmented generation (RAG) techniques, their efficacy in very long-term\ndialogues remains unexplored. To address this research gap, we introduce a\nmachine-human pipeline to generate high-quality, very long-term dialogues by\nleveraging LLM-based agent architectures and grounding their dialogues on\npersonas and temporal event graphs. Moreover, we equip each agent with the\ncapability of sharing and reacting to images. The generated conversations are\nverified and edited by human annotators for long-range consistency and\ngrounding to the event graphs. Using this pipeline, we collect LoCoMo, a\ndataset of very long-term conversations, each encompassing 300 turns and 9K\ntokens on avg., over up to 35 sessions. Based on LoCoMo, we present a\ncomprehensive evaluation benchmark to measure long-term memory in models,\nencompassing question answering, event summarization, and multi-modal dialogue\ngeneration tasks. Our experimental results indicate that LLMs exhibit\nchallenges in understanding lengthy conversations and comprehending long-range\ntemporal and causal dynamics within dialogues. Employing strategies like\nlong-context LLMs or RAG can offer improvements but these models still\nsubstantially lag behind human performance.\n","authors":["Adyasha Maharana","Dong-Ho Lee","Sergey Tulyakov","Mohit Bansal","Francesco Barbieri","Yuwei Fang"],"pdf_url":"https://arxiv.org/pdf/2402.17753v1.pdf","comment":"19 pages; Project page: https://snap-research.github.io/locomo/"},{"id":"http://arxiv.org/abs/2402.17733v1","updated":"2024-02-27T18:09:36Z","published":"2024-02-27T18:09:36Z","title":"Tower: An Open Multilingual Large Language Model for Translation-Related\n  Tasks","summary":"  While general-purpose large language models (LLMs) demonstrate proficiency on\nmultiple tasks within the domain of translation, approaches based on open LLMs\nare competitive only when specializing on a single task. In this paper, we\npropose a recipe for tailoring LLMs to multiple tasks present in translation\nworkflows. We perform continued pretraining on a multilingual mixture of\nmonolingual and parallel data, creating TowerBase, followed by finetuning on\ninstructions relevant for translation processes, creating TowerInstruct. Our\nfinal model surpasses open alternatives on several tasks relevant to\ntranslation workflows and is competitive with general-purpose closed LLMs. To\nfacilitate future research, we release the Tower models, our specialization\ndataset, an evaluation framework for LLMs focusing on the translation\necosystem, and a collection of model generations, including ours, on our\nbenchmark.\n","authors":["Duarte M. Alves","José Pombal","Nuno M. Guerreiro","Pedro H. Martins","João Alves","Amin Farajian","Ben Peters","Ricardo Rei","Patrick Fernandes","Sweta Agrawal","Pierre Colombo","José G. C. de Souza","André F. T. Martins"],"pdf_url":"https://arxiv.org/pdf/2402.17733v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17717v1","updated":"2024-02-27T17:52:33Z","published":"2024-02-27T17:52:33Z","title":"AmbigNLG: Addressing Task Ambiguity in Instruction for NLG","summary":"  In this study, we introduce AmbigNLG, a new task designed to tackle the\nchallenge of task ambiguity in instructions for Natural Language Generation\n(NLG) tasks. Despite the impressive capabilities of Large Language Models\n(LLMs) in understanding and executing a wide range of tasks through natural\nlanguage interaction, their performance is significantly hindered by the\nambiguity present in real-world instructions. To address this, AmbigNLG seeks\nto identify and mitigate such ambiguities, aiming to refine instructions to\nmatch user expectations better. We introduce a dataset, AmbigSNI-NLG,\nconsisting of 2,500 instances, and develop an ambiguity taxonomy for\ncategorizing and annotating instruction ambiguities. Our approach demonstrates\nsubstantial improvements in text generation quality, highlighting the critical\nrole of clear and specific instructions in enhancing LLM performance in NLG\ntasks.\n","authors":["Ayana Niwa","Hayate Iso"],"pdf_url":"https://arxiv.org/pdf/2402.17717v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2402.17709v1","updated":"2024-02-27T17:41:58Z","published":"2024-02-27T17:41:58Z","title":"Case-Based or Rule-Based: How Do Transformers Do the Math?","summary":"  Despite the impressive performance in a variety of complex tasks, modern\nlarge language models (LLMs) still have trouble dealing with some math problems\nthat are simple and intuitive for humans, such as addition. While we can easily\nlearn basic rules of addition and apply them to new problems of any length,\nLLMs struggle to do the same. Instead, they may rely on similar \"cases\" seen in\nthe training corpus for help. We define these two different reasoning\nmechanisms as \"rule-based reasoning\" and \"case-based reasoning\". Since\nrule-based reasoning is essential for acquiring the systematic generalization\nability, we aim to explore exactly whether transformers use rule-based or\ncase-based reasoning for math problems. Through carefully designed intervention\nexperiments on five math tasks, we confirm that transformers are performing\ncase-based reasoning, no matter whether scratchpad is used, which aligns with\nthe previous observations that transformers use subgraph matching/shortcut\nlearning to reason. To mitigate such problems, we propose a Rule-Following\nFine-Tuning (RFFT) technique to teach transformers to perform rule-based\nreasoning. Specifically, we provide explicit rules in the input and then\ninstruct transformers to recite and follow the rules step by step. Through\nRFFT, we successfully enable LLMs fine-tuned on 1-5 digit addition to\ngeneralize to up to 12-digit addition with over 95% accuracy, which is over 40%\nhigher than scratchpad. The significant improvement demonstrates that teaching\nLLMs to explicitly use rules helps them learn rule-based reasoning and\ngeneralize better in length.\n","authors":["Yi Hu","Xiaojuan Tang","Haotong Yang","Muhan Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.17709v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17700v1","updated":"2024-02-27T17:25:37Z","published":"2024-02-27T17:25:37Z","title":"RAVEL: Evaluating Interpretability Methods on Disentangling Language\n  Model Representations","summary":"  Individual neurons participate in the representation of multiple high-level\nconcepts. To what extent can different interpretability methods successfully\ndisentangle these roles? To help address this question, we introduce RAVEL\n(Resolving Attribute-Value Entanglements in Language Models), a dataset that\nenables tightly controlled, quantitative comparisons between a variety of\nexisting interpretability methods. We use the resulting conceptual framework to\ndefine the new method of Multi-task Distributed Alignment Search (MDAS), which\nallows us to find distributed representations satisfying multiple causal\ncriteria. With Llama2-7B as the target language model, MDAS achieves\nstate-of-the-art results on RAVEL, demonstrating the importance of going beyond\nneuron-level analyses to identify features distributed across activations. We\nrelease our benchmark at https://github.com/explanare/ravel.\n","authors":["Jing Huang","Zhengxuan Wu","Christopher Potts","Mor Geva","Atticus Geiger"],"pdf_url":"https://arxiv.org/pdf/2402.17700v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11525v3","updated":"2024-02-27T17:12:38Z","published":"2024-02-18T09:51:49Z","title":"Advancing Translation Preference Modeling with RLHF: A Step Towards\n  Cost-Effective Solution","summary":"  Faithfulness, expressiveness, and elegance is the constant pursuit in machine\ntranslation. However, traditional metrics like \\textit{BLEU} do not strictly\nalign with human preference of translation quality. In this paper, we explore\nleveraging reinforcement learning with human feedback (\\textit{RLHF}) to\nimprove translation quality. It is non-trivial to collect a large high-quality\ndataset of human comparisons between translations, especially for low-resource\nlanguages. To address this issue, we propose a cost-effective preference\nlearning strategy, optimizing reward models by distinguishing between human and\nmachine translations. In this manner, the reward model learns the deficiencies\nof machine translation compared to human and guides subsequent improvements in\nmachine translation. Experimental results demonstrate that \\textit{RLHF} can\neffectively enhance translation quality and this improvement benefits other\ntranslation directions not trained with \\textit{RLHF}. Further analysis\nindicates that the model's language capabilities play a crucial role in\npreference learning. A reward model with strong language capabilities can more\nsensitively learn the subtle differences in translation quality and align\nbetter with real human translation preferences.\n","authors":["Nuo Xu","Jun Zhao","Can Zu","Sixian Li","Lu Chen","Zhihao Zhang","Rui Zheng","Shihan Dou","Wenjuan Qin","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2402.11525v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.13633v2","updated":"2024-02-27T17:10:30Z","published":"2023-09-24T13:19:38Z","title":"EvalLM: Interactive Evaluation of Large Language Model Prompts on\n  User-Defined Criteria","summary":"  By simply composing prompts, developers can prototype novel generative\napplications with Large Language Models (LLMs). To refine prototypes into\nproducts, however, developers must iteratively revise prompts by evaluating\noutputs to diagnose weaknesses. Formative interviews (N=8) revealed that\ndevelopers invest significant effort in manually evaluating outputs as they\nassess context-specific and subjective criteria. We present EvalLM, an\ninteractive system for iteratively refining prompts by evaluating multiple\noutputs on user-defined criteria. By describing criteria in natural language,\nusers can employ the system's LLM-based evaluator to get an overview of where\nprompts excel or fail, and improve these based on the evaluator's feedback. A\ncomparative study (N=12) showed that EvalLM, when compared to manual\nevaluation, helped participants compose more diverse criteria, examine twice as\nmany outputs, and reach satisfactory prompts with 59% fewer revisions. Beyond\nprompts, our work can be extended to augment model evaluation and alignment in\nspecific application contexts.\n","authors":["Tae Soo Kim","Yoonjoo Lee","Jamin Shin","Young-Ho Kim","Juho Kim"],"pdf_url":"https://arxiv.org/pdf/2309.13633v2.pdf","comment":"Accepted to CHI 2024"},{"id":"http://arxiv.org/abs/2305.13300v4","updated":"2024-02-27T17:08:49Z","published":"2023-05-22T17:57:41Z","title":"Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large\n  Language Models in Knowledge Conflicts","summary":"  By providing external information to large language models (LLMs), tool\naugmentation (including retrieval augmentation) has emerged as a promising\nsolution for addressing the limitations of LLMs' static parametric memory.\nHowever, how receptive are LLMs to such external evidence, especially when the\nevidence conflicts with their parametric memory? We present the first\ncomprehensive and controlled investigation into the behavior of LLMs when\nencountering knowledge conflicts. We propose a systematic framework to elicit\nhigh-quality parametric memory from LLMs and construct the corresponding\ncounter-memory, which enables us to conduct a series of controlled experiments.\nOur investigation reveals seemingly contradicting behaviors of LLMs. On the one\nhand, different from prior wisdom, we find that LLMs can be highly receptive to\nexternal evidence even when that conflicts with their parametric memory, given\nthat the external evidence is coherent and convincing. On the other hand, LLMs\nalso demonstrate a strong confirmation bias when the external evidence contains\nsome information that is consistent with their parametric memory, despite being\npresented with conflicting evidence at the same time. These results pose\nimportant implications that are worth careful consideration for the further\ndevelopment and deployment of tool- and retrieval-augmented LLMs. Resources are\navailable at https://github.com/OSU-NLP-Group/LLM-Knowledge-Conflict.\n","authors":["Jian Xie","Kai Zhang","Jiangjie Chen","Renze Lou","Yu Su"],"pdf_url":"https://arxiv.org/pdf/2305.13300v4.pdf","comment":"ICLR 2024 (Spotlight)"},{"id":"http://arxiv.org/abs/2402.17682v1","updated":"2024-02-27T16:56:30Z","published":"2024-02-27T16:56:30Z","title":"NextLevelBERT: Investigating Masked Language Modeling with Higher-Level\n  Representations for Long Documents","summary":"  While (large) language models have significantly improved over the last\nyears, they still struggle to sensibly process long sequences found, e.g., in\nbooks, due to the quadratic scaling of the underlying attention mechanism. To\naddress this, we propose NextLevelBERT, a Masked Language Model operating not\non tokens, but on higher-level semantic representations in the form of text\nembeddings. We pretrain NextLevelBERT to predict the vector representation of\nentire masked text chunks and evaluate the effectiveness of the resulting\ndocument vectors on three task types: 1) Semantic Textual Similarity via\nzero-shot document embeddings, 2) Long document classification, 3)\nMultiple-choice question answering. We find that next level Masked Language\nModeling is an effective technique to tackle long-document use cases and can\noutperform much larger embedding models as long as the required level of detail\nis not too high. We make model and code available.\n","authors":["Tamara Czinczoll","Christoph Hönes","Maximilian Schall","Gerard de Melo"],"pdf_url":"https://arxiv.org/pdf/2402.17682v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.14556v2","updated":"2024-02-27T16:48:17Z","published":"2024-01-25T22:50:48Z","title":"Looking Right is Sometimes Right: Investigating the Capabilities of\n  Decoder-only LLMs for Sequence Labeling","summary":"  Pre-trained language models based on masked language modeling (MLM) excel in\nnatural language understanding (NLU) tasks. While fine-tuned MLM-based encoders\nconsistently outperform causal language modeling decoders of comparable size,\nrecent decoder-only large language models (LLMs) perform on par with smaller\nMLM-based encoders. Although their performance improves with scale, LLMs fall\nshort of achieving state-of-the-art results in information extraction (IE)\ntasks, many of which are formulated as sequence labeling (SL). We hypothesize\nthat LLMs' poor SL performance stems from causal masking, which prevents the\nmodel from attending to tokens on the right of the current token. Yet, how\nexactly and to what extent LLMs' performance on SL can be improved remains\nunclear. We explore techniques for improving the SL performance of open LLMs on\nIE tasks by applying layer-wise removal of the causal mask (CM) during LLM\nfine-tuning. This approach yields performance gains competitive with\nstate-of-the-art SL models, matching or outperforming the results of CM removal\nfrom all blocks. Our findings hold for diverse SL tasks, demonstrating that\nopen LLMs with layer-dependent CM removal outperform strong MLM-based encoders\nand even instruction-tuned LLMs.\n","authors":["David Dukić","Jan Šnajder"],"pdf_url":"https://arxiv.org/pdf/2401.14556v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03797v2","updated":"2024-02-27T16:35:56Z","published":"2024-01-08T10:27:25Z","title":"Anatomy of Neural Language Models","summary":"  The fields of generative AI and transfer learning have experienced remarkable\nadvancements in recent years especially in the domain of Natural Language\nProcessing (NLP). Transformers have been at the heart of these advancements\nwhere the cutting-edge transformer-based Language Models (LMs) have led to new\nstate-of-the-art results in a wide spectrum of applications. While the number\nof research works involving neural LMs is exponentially increasing, their vast\nmajority are high-level and far from self-contained. Consequently, a deep\nunderstanding of the literature in this area is a tough task especially in the\nabsence of a unified mathematical framework explaining the main types of neural\nLMs. We address the aforementioned problem in this tutorial where the objective\nis to explain neural LMs in a detailed, simplified and unambiguous mathematical\nframework accompanied by clear graphical illustrations. Concrete examples on\nwidely used models like BERT and GPT2 are explored. Finally, since transformers\npretrained on language-modeling-like tasks have been widely adopted in computer\nvision and time series applications, we briefly explore some examples of such\nsolutions in order to enable readers to understand how transformers work in the\naforementioned domains and compare this use with the original one in NLP.\n","authors":["Majd Saleh","Stéphane Paquelet"],"pdf_url":"https://arxiv.org/pdf/2401.03797v2.pdf","comment":"36 Pages; 25 Figures; some typos and notation errors are corrected in\n  this version"},{"id":"http://arxiv.org/abs/2402.17649v1","updated":"2024-02-27T16:19:37Z","published":"2024-02-27T16:19:37Z","title":"Beyond prompt brittleness: Evaluating the reliability and consistency of\n  political worldviews in LLMs","summary":"  Due to the widespread use of large language models (LLMs) in ubiquitous\nsystems, we need to understand whether they embed a specific worldview and what\nthese views reflect. Recent studies report that, prompted with political\nquestionnaires, LLMs show left-liberal leanings. However, it is as yet unclear\nwhether these leanings are reliable (robust to prompt variations) and whether\nthe leaning is consistent across policies and political leaning. We propose a\nseries of tests which assess the reliability and consistency of LLMs' stances\non political statements based on a dataset of voting-advice questionnaires\ncollected from seven EU countries and annotated for policy domains. We study\nLLMs ranging in size from 7B to 70B parameters and find that their reliability\nincreases with parameter count. Larger models show overall stronger alignment\nwith left-leaning parties but differ among policy programs: They evince a\n(left-wing) positive stance towards environment protection, social welfare but\nalso (right-wing) law and order, with no consistent preferences in foreign\npolicy, migration, and economy.\n","authors":["Tanise Ceron","Neele Falk","Ana Barić","Dmitry Nikolaev","Sebastian Padó"],"pdf_url":"https://arxiv.org/pdf/2402.17649v1.pdf","comment":"10 pages, under review"},{"id":"http://arxiv.org/abs/2402.17645v1","updated":"2024-02-27T16:15:28Z","published":"2024-02-27T16:15:28Z","title":"SongComposer: A Large Language Model for Lyric and Melody Composition in\n  Song Generation","summary":"  We present SongComposer, an innovative LLM designed for song composition. It\ncould understand and generate melodies and lyrics in symbolic song\nrepresentations, by leveraging the capability of LLM. Existing music-related\nLLM treated the music as quantized audio signals, while such implicit encoding\nleads to inefficient encoding and poor flexibility. In contrast, we resort to\nsymbolic song representation, the mature and efficient way humans designed for\nmusic, and enable LLM to explicitly compose songs like humans. In practice, we\ndesign a novel tuple design to format lyric and three note attributes (pitch,\nduration, and rest duration) in the melody, which guarantees the correct LLM\nunderstanding of musical symbols and realizes precise alignment between lyrics\nand melody. To impart basic music understanding to LLM, we carefully collected\nSongCompose-PT, a large-scale song pretraining dataset that includes lyrics,\nmelodies, and paired lyrics-melodies in either Chinese or English. After\nadequate pre-training, 10K carefully crafted QA pairs are used to empower the\nLLM with the instruction-following capability and solve diverse tasks. With\nextensive experiments, SongComposer demonstrates superior performance in\nlyric-to-melody generation, melody-to-lyric generation, song continuation, and\ntext-to-song creation, outperforming advanced LLMs like GPT-4.\n","authors":["Shuangrui Ding","Zihan Liu","Xiaoyi Dong","Pan Zhang","Rui Qian","Conghui He","Dahua Lin","Jiaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2402.17645v1.pdf","comment":"project page: https://pjlab-songcomposer.github.io/ code:\n  https://github.com/pjlab-songcomposer/songcomposer"},{"id":"http://arxiv.org/abs/2402.17644v1","updated":"2024-02-27T16:15:03Z","published":"2024-02-27T16:15:03Z","title":"Are LLMs Capable of Data-based Statistical and Causal Reasoning?\n  Benchmarking Advanced Quantitative Reasoning with Data","summary":"  Quantitative reasoning is a critical skill to analyze data, yet the\nassessment of such ability remains limited. To address this gap, we introduce\nthe Quantitative Reasoning with Data (QRData) benchmark, aiming to evaluate\nLarge Language Models' capability in statistical and causal reasoning with\nreal-world data. The benchmark comprises a carefully constructed dataset of 411\nquestions accompanied by data sheets from textbooks, online learning materials,\nand academic papers. To compare models' quantitative reasoning abilities on\ndata and text, we enrich the benchmark with an auxiliary set of 290 text-only\nquestions, namely QRText. We evaluate natural language reasoning, program-based\nreasoning, and agent reasoning methods including Chain-of-Thought,\nProgram-of-Thoughts, ReAct, and code interpreter assistants on diverse models.\nThe strongest model GPT-4 achieves an accuracy of 58%, which has a large room\nfor improvement. Among open-source models, Deepseek-coder-instruct, a code LLM\npretrained on 2T tokens, gets the highest accuracy of 37%. Analysis reveals\nthat models encounter difficulties in data analysis and causal reasoning, and\nstruggle in using causal knowledge and provided data simultaneously. Code and\ndata are in https://github.com/xxxiaol/QRData.\n","authors":["Xiao Liu","Zirui Wu","Xueqing Wu","Pan Lu","Kai-Wei Chang","Yansong Feng"],"pdf_url":"https://arxiv.org/pdf/2402.17644v1.pdf","comment":"Project website: https://xxxiaol.github.io/QRData/"},{"id":"http://arxiv.org/abs/2402.17641v1","updated":"2024-02-27T16:11:05Z","published":"2024-02-27T16:11:05Z","title":"Variational Learning is Effective for Large Deep Networks","summary":"  We give extensive empirical evidence against the common belief that\nvariational learning is ineffective for large neural networks. We show that an\noptimizer called Improved Variational Online Newton (IVON) consistently matches\nor outperforms Adam for training large networks such as GPT-2 and ResNets from\nscratch. IVON's computational costs are nearly identical to Adam but its\npredictive uncertainty is better. We show several new use cases of IVON where\nwe improve fine-tuning and model merging in Large Language Models, accurately\npredict generalization error, and faithfully estimate sensitivity to data. We\nfind overwhelming evidence in support of effectiveness of variational learning.\n","authors":["Yuesong Shen","Nico Daheim","Bai Cong","Peter Nickl","Gian Maria Marconi","Clement Bazan","Rio Yokota","Iryna Gurevych","Daniel Cremers","Mohammad Emtiyaz Khan","Thomas Möllenhoff"],"pdf_url":"https://arxiv.org/pdf/2402.17641v1.pdf","comment":"The first two authors contributed equally. Code is available here:\n  https://github.com/team-approx-bayes/ivon"},{"id":"http://arxiv.org/abs/2402.17633v1","updated":"2024-02-27T15:59:37Z","published":"2024-02-27T15:59:37Z","title":"From Text Segmentation to Smart Chaptering: A Novel Benchmark for\n  Structuring Video Transcriptions","summary":"  Text segmentation is a fundamental task in natural language processing, where\ndocuments are split into contiguous sections. However, prior research in this\narea has been constrained by limited datasets, which are either small in scale,\nsynthesized, or only contain well-structured documents. In this paper, we\naddress these limitations by introducing a novel benchmark YTSeg focusing on\nspoken content that is inherently more unstructured and both topically and\nstructurally diverse. As part of this work, we introduce an efficient\nhierarchical segmentation model MiniSeg, that outperforms state-of-the-art\nbaselines. Lastly, we expand the notion of text segmentation to a more\npractical \"smart chaptering\" task that involves the segmentation of\nunstructured content, the generation of meaningful segment titles, and a\npotential real-time application of the models.\n","authors":["Fabian Retkowski","Alexander Waibel"],"pdf_url":"https://arxiv.org/pdf/2402.17633v1.pdf","comment":"Accepted to EACL 2024"},{"id":"http://arxiv.org/abs/2402.17630v1","updated":"2024-02-27T15:57:11Z","published":"2024-02-27T15:57:11Z","title":"Fine-Grained Natural Language Inference Based Faithfulness Evaluation\n  for Diverse Summarisation Tasks","summary":"  We study existing approaches to leverage off-the-shelf Natural Language\nInference (NLI) models for the evaluation of summary faithfulness and argue\nthat these are sub-optimal due to the granularity level considered for premises\nand hypotheses. That is, the smaller content unit considered as hypothesis is a\nsentence and premises are made up of a fixed number of document sentences. We\npropose a novel approach, namely InFusE, that uses a variable premise size and\nsimplifies summary sentences into shorter hypotheses. Departing from previous\nstudies which focus on single short document summarisation, we analyse NLI\nbased faithfulness evaluation for diverse summarisation tasks. We introduce\nDiverSumm, a new benchmark comprising long form summarisation (long documents\nand summaries) and diverse summarisation tasks (e.g., meeting and\nmulti-document summarisation). In experiments, InFusE obtains superior\nperformance across the different summarisation tasks. Our code and data are\navailable at https://github.com/HJZnlp/infuse.\n","authors":["Huajian Zhang","Yumo Xu","Laura Perez-Beltrachini"],"pdf_url":"https://arxiv.org/pdf/2402.17630v1.pdf","comment":"EACL 2024"},{"id":"http://arxiv.org/abs/2402.17613v1","updated":"2024-02-27T15:42:33Z","published":"2024-02-27T15:42:33Z","title":"Neural Automated Writing Evaluation with Corrective Feedback","summary":"  The utilization of technology in second language learning and teaching has\nbecome ubiquitous. For the assessment of writing specifically, automated\nwriting evaluation (AWE) and grammatical error correction (GEC) have become\nimmensely popular and effective methods for enhancing writing proficiency and\ndelivering instant and individualized feedback to learners. By leveraging the\npower of natural language processing (NLP) and machine learning algorithms, AWE\nand GEC systems have been developed separately to provide language learners\nwith automated corrective feedback and more accurate and unbiased scoring that\nwould otherwise be subject to examiners. In this paper, we propose an\nintegrated system for automated writing evaluation with corrective feedback as\na means of bridging the gap between AWE and GEC results for second language\nlearners. This system enables language learners to simulate the essay writing\ntests: a student writes and submits an essay, and the system returns the\nassessment of the writing along with suggested grammatical error corrections.\nGiven that automated scoring and grammatical correction are more efficient and\ncost-effective than human grading, this integrated system would also alleviate\nthe burden of manually correcting innumerable essays.\n","authors":["Izia Xiaoxiao Wang","Xihan Wu","Edith Coates","Min Zeng","Jiexin Kuang","Siliang Liu","Mengyang Qiu","Jungyeul Park"],"pdf_url":"https://arxiv.org/pdf/2402.17613v1.pdf","comment":"Submitted to the system demonstration track at NAACL-HLT 2024"},{"id":"http://arxiv.org/abs/2402.17608v1","updated":"2024-02-27T15:34:15Z","published":"2024-02-27T15:34:15Z","title":"Linguistic Knowledge Can Enhance Encoder-Decoder Models (If You Let It)","summary":"  In this paper, we explore the impact of augmenting pre-trained\nEncoder-Decoder models, specifically T5, with linguistic knowledge for the\nprediction of a target task. In particular, we investigate whether fine-tuning\na T5 model on an intermediate task that predicts structural linguistic\nproperties of sentences modifies its performance in the target task of\npredicting sentence-level complexity. Our study encompasses diverse experiments\nconducted on Italian and English datasets, employing both monolingual and\nmultilingual T5 models at various sizes. Results obtained for both languages\nand in cross-lingual configurations show that linguistically motivated\nintermediate fine-tuning has generally a positive impact on target task\nperformance, especially when applied to smaller models and in scenarios with\nlimited data availability.\n","authors":["Alessio Miaschi","Felice Dell'Orletta","Giulia Venturi"],"pdf_url":"https://arxiv.org/pdf/2402.17608v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2402.16819v2","updated":"2024-02-27T15:22:57Z","published":"2024-02-26T18:43:45Z","title":"Nemotron-4 15B Technical Report","summary":"  We introduce Nemotron-4 15B, a 15-billion-parameter large multilingual\nlanguage model trained on 8 trillion text tokens. Nemotron-4 15B demonstrates\nstrong performance when assessed on English, multilingual, and coding tasks: it\noutperforms all existing similarly-sized open models on 4 out of 7 downstream\nevaluation areas and achieves competitive performance to the leading open\nmodels in the remaining ones. Specifically, Nemotron-4 15B exhibits the best\nmultilingual capabilities of all similarly-sized models, even outperforming\nmodels over four times larger and those explicitly specialized for multilingual\ntasks.\n","authors":["Jupinder Parmar","Shrimai Prabhumoye","Joseph Jennings","Mostofa Patwary","Sandeep Subramanian","Dan Su","Chen Zhu","Deepak Narayanan","Aastha Jhunjhunwala","Ayush Dattagupta","Vibhu Jawa","Jiwei Liu","Ameya Mahabaleshwarkar","Osvald Nitski","Annika Brundyn","James Maki","Miguel Martinez","Jiaxuan You","John Kamalu","Patrick LeGresley","Denys Fridman","Jared Casper","Ashwath Aithal","Oleksii Kuchaiev","Mohammad Shoeybi","Jonathan Cohen","Bryan Catanzaro"],"pdf_url":"https://arxiv.org/pdf/2402.16819v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17583v1","updated":"2024-02-27T15:14:19Z","published":"2024-02-27T15:14:19Z","title":"FaultProfIT: Hierarchical Fault Profiling of Incident Tickets in\n  Large-scale Cloud Systems","summary":"  Postmortem analysis is essential in the management of incidents within cloud\nsystems, which provides valuable insights to improve system's reliability and\nrobustness. At CloudA, fault pattern profiling is performed during the\npostmortem phase, which involves the classification of incidents' faults into\nunique categories, referred to as fault pattern. By aggregating and analyzing\nthese fault patterns, engineers can discern common faults, vulnerable\ncomponents and emerging fault trends. However, this process is currently\nconducted by manual labeling, which has inherent drawbacks. On the one hand,\nthe sheer volume of incidents means only the most severe ones are analyzed,\ncausing a skewed overview of fault patterns. On the other hand, the complexity\nof the task demands extensive domain knowledge, which leads to errors and\ninconsistencies. To address these limitations, we propose an automated\napproach, named FaultProfIT, for Fault pattern Profiling of Incident Tickets.\nIt leverages hierarchy-guided contrastive learning to train a hierarchy-aware\nincident encoder and predicts fault patterns with enhanced incident\nrepresentations. We evaluate FaultProfIT using the production incidents from\nCloudA. The results demonstrate that FaultProfIT outperforms state-of-the-art\nmethods. Our ablation study and analysis also verify the effectiveness of\nhierarchy-guided contrastive learning. Additionally, we have deployed\nFaultProfIT at CloudA for six months. To date, FaultProfIT has analyzed 10,000+\nincidents from 30+ cloud services, successfully revealing several fault trends\nthat have informed system improvements.\n","authors":["Junjie Huang","Jinyang Liu","Zhuangbin Chen","Zhihan Jiang","Yichen LI","Jiazhen Gu","Cong Feng","Zengyin Yang","Yongqiang Yang","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2402.17583v1.pdf","comment":"Accepted by Proceedings of the 46th International Conference on\n  Software Engineering: Software Engineering in Practice (ICSE SEIP 2024)"},{"id":"http://arxiv.org/abs/2402.17574v1","updated":"2024-02-27T15:09:20Z","published":"2024-02-27T15:09:20Z","title":"Agent-Pro: Learning to Evolve via Policy-Level Reflection and\n  Optimization","summary":"  Large Language Models exhibit robust problem-solving capabilities for diverse\ntasks. However, most LLM-based agents are designed as specific task solvers\nwith sophisticated prompt engineering, rather than agents capable of learning\nand evolving through interactions. These task solvers necessitate manually\ncrafted prompts to inform task rules and regulate LLM behaviors, inherently\nincapacitating to address complex dynamic scenarios e.g., large interactive\ngames. In light of this, we propose Agent-Pro: an LLM-based Agent with\nPolicy-level Reflection and Optimization that can learn a wealth of expertise\nfrom interactive experiences and progressively elevate its behavioral policy.\nSpecifically, it involves a dynamic belief generation and reflection process\nfor policy evolution. Rather than action-level reflection, Agent-Pro\niteratively reflects on past trajectories and beliefs, fine-tuning its\nirrational beliefs for a better policy. Moreover, a depth-first search is\nemployed for policy optimization, ensuring continual enhancement in policy\npayoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em,\noutperforming vanilla LLM and specialized models. Our results show Agent-Pro\ncan learn and evolve in complex and dynamic scenes, which also benefits\nnumerous LLM-based applications.\n","authors":["Wenqi Zhang","Ke Tang","Hai Wu","Mengna Wang","Yongliang Shen","Guiyang Hou","Zeqi Tan","Peng Li","Yueting Zhuang","Weiming Lu"],"pdf_url":"https://arxiv.org/pdf/2402.17574v1.pdf","comment":"LLM-based Agent"},{"id":"http://arxiv.org/abs/2402.10573v2","updated":"2024-02-27T15:08:02Z","published":"2024-02-16T11:02:29Z","title":"LinkNER: Linking Local Named Entity Recognition Models to Large Language\n  Models using Uncertainty","summary":"  Named Entity Recognition (NER) serves as a fundamental task in natural\nlanguage understanding, bearing direct implications for web content analysis,\nsearch engines, and information retrieval systems. Fine-tuned NER models\nexhibit satisfactory performance on standard NER benchmarks. However, due to\nlimited fine-tuning data and lack of knowledge, it performs poorly on unseen\nentity recognition. As a result, the usability and reliability of NER models in\nweb-related applications are compromised. Instead, Large Language Models (LLMs)\nlike GPT-4 possess extensive external knowledge, but research indicates that\nthey lack specialty for NER tasks. Furthermore, non-public and large-scale\nweights make tuning LLMs difficult. To address these challenges, we propose a\nframework that combines small fine-tuned models with LLMs (LinkNER) and an\nuncertainty-based linking strategy called RDC that enables fine-tuned models to\ncomplement black-box LLMs, achieving better performance. We experiment with\nboth standard NER test sets and noisy social media datasets. LinkNER enhances\nNER task performance, notably surpassing SOTA models in robustness tests. We\nalso quantitatively analyze the influence of key components like uncertainty\nestimation methods, LLMs, and in-context learning on diverse NER tasks,\noffering specific web-related recommendations.\n","authors":["Zhen Zhang","Yuhua Zhao","Hang Gao","Mengting Hu"],"pdf_url":"https://arxiv.org/pdf/2402.10573v2.pdf","comment":"Accepted by WebConf (WWW'2024)"},{"id":"http://arxiv.org/abs/2402.17564v1","updated":"2024-02-27T15:05:32Z","published":"2024-02-27T15:05:32Z","title":"Unleashing the Potential of Large Language Models as Prompt Optimizers:\n  An Analogical Analysis with Gradient-based Model Optimizers","summary":"  Automatic prompt optimization is an important approach to improving the\nperformance of large language models (LLMs). Recent research demonstrates the\npotential of using LLMs as prompt optimizers, which can generate improved task\nprompts via iterative refinement. In this paper, we propose a novel perspective\nto investigate the design of LLM-based prompt optimizers, by drawing an analogy\nwith gradient-based model optimizers. To connect these two approaches, we\nidentify two pivotal factors in model parameter learning: update direction and\nupdate method. Focused on the two aspects, we borrow the theoretical framework\nand learning methods from gradient-based optimization to design improved\nstrategies for LLM-based prompt optimizers. By systematically analyzing a rich\nset of improvement strategies, we further develop a capable Gradient-inspired\nLLM-based Prompt Optimizer called GPO. At each step, it first retrieves\nrelevant prompts from the optimization trajectory as the update direction.\nThen, it utilizes the generation-based refinement strategy to perform the\nupdate, while controlling the edit distance through a cosine-based decay\nstrategy. Extensive experiments demonstrate the effectiveness and efficiency of\nGPO. In particular, GPO brings an additional improvement of up to 56.8% on\nBig-Bench Hard and 55.3% on MMLU compared to baseline methods.\n","authors":["Xinyu Tang","Xiaolei Wang","Wayne Xin Zhao","Siyuan Lu","Yaliang Li","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2402.17564v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.12822v3","updated":"2024-02-27T14:49:43Z","published":"2023-02-24T18:58:06Z","title":"Automatic Prompt Augmentation and Selection with Chain-of-Thought from\n  Labeled Data","summary":"  Chain-of-thought (CoT) advances the reasoning abilities of large language\nmodels (LLMs) and achieves superior performance in complex reasoning tasks.\nHowever, most CoT studies rely on carefully designed human-annotated rational\nchains to prompt LLMs, posing challenges for real-world applications where\nlabeled data is available without rational chains. This paper proposes a new\nstrategy, Automate-CoT (Automatic Prompt Augmentation and Selection with\nChain-of-Thought), that can bypass human engineering of CoT by automatically\naugmenting rational chains from a small labeled dataset, and then pruning\nlow-quality chains to construct a candidate pool of machine-generated rationale\nchains based on the labels. Finally, it selects the optimal combination of\nseveral rationale chains from the pool for CoT prompting by employing a\nvariance-reduced policy gradient strategy to estimate the significance of each\nexample. Automate-CoT enables a quick adaptation of the CoT technique to\ndifferent tasks. Experimental results demonstrate the effectiveness of our\nmethod, where competitive results are achieved on arithmetic reasoning (+2.7%),\ncommonsense reasoning (+3.4%), symbolic reasoning (+3.2%), and non-reasoning\ntasks (+2.5%). The code is available at\nhttps://github.com/SHUMKASHUN/Automate-CoT.\n","authors":["KaShun Shum","Shizhe Diao","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2302.12822v3.pdf","comment":"EMNLP 2023"},{"id":"http://arxiv.org/abs/2402.17553v1","updated":"2024-02-27T14:47:53Z","published":"2024-02-27T14:47:53Z","title":"OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist\n  Autonomous Agents for Desktop and Web","summary":"  For decades, human-computer interaction has fundamentally been manual. Even\ntoday, almost all productive work done on the computer necessitates human input\nat every step. Autonomous virtual agents represent an exciting step in\nautomating many of these menial tasks. Virtual agents would empower users with\nlimited technical proficiency to harness the full possibilities of computer\nsystems. They could also enable the efficient streamlining of numerous computer\ntasks, ranging from calendar management to complex travel bookings, with\nminimal human intervention. In this paper, we introduce OmniACT, the\nfirst-of-a-kind dataset and benchmark for assessing an agent's capability to\ngenerate executable programs to accomplish computer tasks. Our scope extends\nbeyond traditional web automation, covering a diverse range of desktop\napplications. The dataset consists of fundamental tasks such as \"Play the next\nsong\", as well as longer horizon tasks such as \"Send an email to John Doe\nmentioning the time and place to meet\". Specifically, given a pair of screen\nimage and a visually-grounded natural language task, the goal is to generate a\nscript capable of fully executing the task. We run several strong baseline\nlanguage model agents on our benchmark. The strongest baseline, GPT-4, performs\nthe best on our benchmark However, its performance level still reaches only 15%\nof the human proficiency in generating executable scripts capable of completing\nthe task, demonstrating the challenge of our task for conventional web agents.\nOur benchmark provides a platform to measure and evaluate the progress of\nlanguage model agents in automating computer tasks and motivates future work\ntowards building multimodal models that bridge large language models and the\nvisual grounding of computer screens.\n","authors":["Raghav Kapoor","Yash Parag Butala","Melisa Russak","Jing Yu Koh","Kiran Kamble","Waseem Alshikh","Ruslan Salakhutdinov"],"pdf_url":"https://arxiv.org/pdf/2402.17553v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17546v1","updated":"2024-02-27T14:38:47Z","published":"2024-02-27T14:38:47Z","title":"COCOA: CBT-based Conversational Counseling Agent using Memory\n  Specialized in Cognitive Distortions and Dynamic Prompt","summary":"  The demand for conversational agents that provide mental health care is\nconsistently increasing. In this work, we develop a psychological counseling\nagent, referred to as CoCoA, that applies Cognitive Behavioral Therapy (CBT)\ntechniques to identify and address cognitive distortions inherent in the\nclient's statements. Specifically, we construct a memory system to efficiently\nmanage information necessary for counseling while extracting high-level\ninsights about the client from their utterances. Additionally, to ensure that\nthe counseling agent generates appropriate responses, we introduce dynamic\nprompting to flexibly apply CBT techniques and facilitate the appropriate\nretrieval of information. We conducted dialogues between CoCoA and characters\nfrom Character.ai, creating a dataset for evaluation. Then, we asked GPT to\nevaluate the constructed counseling dataset, and our model demonstrated a\nstatistically significant difference from other models.\n","authors":["Suyeon Lee","Jieun Kang","Harim Kim","Kyoung-Mee Chung","Dongha Lee","Jinyoung Yeo"],"pdf_url":"https://arxiv.org/pdf/2402.17546v1.pdf","comment":"4 pages, 2 figures"},{"id":"http://arxiv.org/abs/2310.02174v3","updated":"2024-02-27T14:17:53Z","published":"2023-10-03T16:08:41Z","title":"Ask Again, Then Fail: Large Language Models' Vacillations in Judgement","summary":"  With the emergence of generative conversational large language models (LLMs)\nlike ChatGPT, serving as virtual assistants in various fields, the stability\nand reliability of their responses have become crucial. However, during usage,\nit has been observed that these models tend to waver in their judgements when\nconfronted with follow-up questions from users expressing skepticism or\ndisagreement. In this work, we draw inspiration from questioning strategies in\neducation and propose a \\textsc{Follow-up Questioning Mechanism} along with two\nevaluation metrics to assess the judgement consistency of LLMs before and after\nexposure to disturbances. We evaluate the judgement consistency of ChatGPT,\nPaLM2-Bison, and Vicuna-13B under this mechanism across eight reasoning\nbenchmarks. Empirical results show that even when the initial answers are\ncorrect, judgement consistency sharply decreases when LLMs face disturbances\nsuch as questioning, negation, or misleading. Additionally, we study these\nmodels' judgement consistency under various settings (sampling temperature and\nprompts) to validate this issue further, observing the impact of prompt tone\nand conducting an in-depth error analysis for deeper behavioral insights.\nFurthermore, we also explore several prompting methods to mitigate this issue\nand demonstrate their\neffectiveness\\footnote{\\url{https://github.com/NUSTM/LLMs-Waver-In-Judgements}}.\n","authors":["Qiming Xie","Zengzhi Wang","Yi Feng","Rui Xia"],"pdf_url":"https://arxiv.org/pdf/2310.02174v3.pdf","comment":"Update mitigation results of fine-tuning the model on synthesized\n  high-quality preference data with DPO algorithm"},{"id":"http://arxiv.org/abs/2402.17532v1","updated":"2024-02-27T14:16:19Z","published":"2024-02-27T14:16:19Z","title":"Retrieval is Accurate Generation","summary":"  Standard language models generate text by selecting tokens from a fixed,\nfinite, and standalone vocabulary. We introduce a novel method that selects\ncontext-aware phrases from a collection of supporting documents. One of the\nmost significant challenges for this paradigm shift is determining the training\noracles, because a string of text can be segmented in various ways and each\nsegment can be retrieved from numerous possible documents. To address this, we\npropose to initialize the training oracles using linguistic heuristics and,\nmore importantly, bootstrap the oracles through iterative self-reinforcement.\nExtensive experiments show that our model not only outperforms standard\nlanguage models on a variety of knowledge-intensive tasks but also demonstrates\nimproved generation quality in open-ended text generation. For instance,\ncompared to the standard language model counterpart, our model raises the\naccuracy from 23.47% to 36.27% on OpenbookQA, and improves the MAUVE score from\n42.61% to 81.58% in open-ended text generation. Remarkably, our model also\nachieves the best performance and the lowest latency among several\nretrieval-augmented baselines. In conclusion, we assert that retrieval is more\naccurate generation and hope that our work will encourage further research on\nthis new paradigm shift.\n","authors":["Bowen Cao","Deng Cai","Leyang Cui","Xuxin Cheng","Wei Bi","Yuexian Zou","Shuming Shi"],"pdf_url":"https://arxiv.org/pdf/2402.17532v1.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2402.17531v1","updated":"2024-02-27T14:14:23Z","published":"2024-02-27T14:14:23Z","title":"Nissist: An Incident Mitigation Copilot based on Troubleshooting Guides","summary":"  Effective incident management is pivotal for the smooth operation of\nenterprises-level cloud services. In order to expedite incident mitigation,\nservice teams compile troubleshooting knowledge into Troubleshooting Guides\n(TSGs) accessible to on-call engineers (OCEs). While automated pipelines are\nenabled to resolve the most frequent and easy incidents, there still exist\ncomplex incidents that require OCEs' intervention. However, TSGs are often\nunstructured and incomplete, which requires manual interpretation by OCEs,\nleading to on-call fatigue and decreased productivity, especially among\nnew-hire OCEs. In this work, we propose Nissist which leverages TSGs and\nincident mitigation histories to provide proactive suggestions, reducing human\nintervention. Leveraging Large Language Models (LLM), Nissist extracts insights\nfrom unstructured TSGs and historical incident mitigation discussions, forming\na comprehensive knowledge base. Its multi-agent system design enhances\nproficiency in precisely discerning user queries, retrieving relevant\ninformation, and delivering systematic plans consecutively. Through our user\ncase and experiment, we demonstrate that Nissist significant reduce Time to\nMitigate (TTM) in incident mitigation, alleviating operational burdens on OCEs\nand improving service reliability. Our demo is available at\nhttps://aka.ms/nissist_demo.\n","authors":["Kaikai An","Fangkai Yang","Liqun Li","Zhixing Ren","Hao Huang","Lu Wang","Pu Zhao","Yu Kang","Hua Ding","Qingwei Lin","Saravan Rajmohan","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.17531v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2402.17527v1","updated":"2024-02-27T14:11:32Z","published":"2024-02-27T14:11:32Z","title":"Predict the Next Word: <Humans exhibit uncertainty in this task and\n  language models _____>","summary":"  Language models (LMs) are statistical models trained to assign probability to\nhuman-generated text. As such, it is reasonable to question whether they\napproximate linguistic variability exhibited by humans well. This form of\nstatistical assessment is difficult to perform at the passage level, for it\nrequires acceptability judgements (i.e., human evaluation) or a robust\nautomated proxy (which is non-trivial). At the word level, however, given some\ncontext, samples from an LM can be assessed via exact matching against a\nprerecorded dataset of alternative single-word continuations of the available\ncontext. We exploit this fact and evaluate the LM's ability to reproduce\nvariability that humans (in particular, a population of English speakers)\nexhibit in the 'next word prediction' task. This can be seen as assessing a\nform of calibration, which, in the context of text classification, Baan et al.\n(2022) termed calibration to human uncertainty. We assess GPT2, BLOOM and\nChatGPT and find that they exhibit fairly low calibration to human uncertainty.\nWe also verify the failure of expected calibration error (ECE) to reflect this,\nand as such, advise the community against relying on it in this setting.\n","authors":["Evgenia Ilia","Wilker Aziz"],"pdf_url":"https://arxiv.org/pdf/2402.17527v1.pdf","comment":"22 pages, EACL 2024"},{"id":"http://arxiv.org/abs/2304.13689v2","updated":"2024-02-27T13:57:08Z","published":"2023-04-26T17:15:39Z","title":"HeySQuAD: A Spoken Question Answering Dataset","summary":"  Spoken question answering (SQA) systems are critical for digital assistants\nand other real-world use cases, but evaluating their performance is a challenge\ndue to the importance of human-spoken questions. This study presents a new\nlarge-scale community-shared SQA dataset called HeySQuAD, which includes 76k\nhuman-spoken questions, 97k machine-generated questions, and their\ncorresponding textual answers from the SQuAD QA dataset. Our goal is to measure\nthe ability of machines to accurately understand noisy spoken questions and\nprovide reliable answers. Through extensive testing, we demonstrate that\ntraining with transcribed human-spoken and original SQuAD questions leads to a\nsignificant improvement (12.51%) in answering human-spoken questions compared\nto training with only the original SQuAD textual questions. Moreover,\nevaluating with a higher-quality transcription can lead to a further\nimprovement of 2.03%. This research has significant implications for the\ndevelopment of SQA systems and their ability to meet the needs of users in\nreal-world scenarios.\n","authors":["Yijing Wu","SaiKrishna Rallabandi","Ravisutha Srinivasamurthy","Parag Pravin Dakle","Alolika Gon","Preethi Raghavan"],"pdf_url":"https://arxiv.org/pdf/2304.13689v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17512v1","updated":"2024-02-27T13:54:48Z","published":"2024-02-27T13:54:48Z","title":"Latent Attention for Linear Time Transformers","summary":"  The time complexity of the standard attention mechanism in a transformer\nscales quadratically with the length of the sequence. We introduce a method to\nreduce this to linear scaling with time, based on defining attention via latent\nvectors. The method is readily usable as a drop-in replacement for the standard\nattention mechanism. Our \"Latte Transformer\" model can be implemented for both\nbidirectional and unidirectional tasks, with the causal version allowing a\nrecurrent implementation which is memory and time-efficient during inference of\nlanguage generation tasks. Whilst next token prediction scales linearly with\nthe sequence length for a standard transformer, a Latte Transformer requires\nconstant time to compute the next token. The empirical performance of our\nmethod is comparable to standard attention, yet allows scaling to context\nwindows much larger than practical in standard attention.\n","authors":["Rares Dolga","Marius Cobzarenco","David Barber"],"pdf_url":"https://arxiv.org/pdf/2402.17512v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14872v2","updated":"2024-02-27T13:49:22Z","published":"2024-02-21T15:13:50Z","title":"Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts\n  Against Open-source LLMs","summary":"  Large Language Models (LLMs), used in creative writing, code generation, and\ntranslation, generate text based on input sequences but are vulnerable to\njailbreak attacks, where crafted prompts induce harmful outputs. Most jailbreak\nprompt methods use a combination of jailbreak templates followed by questions\nto ask to create jailbreak prompts. However, existing jailbreak prompt designs\ngenerally suffer from excessive semantic differences, resulting in an inability\nto resist defenses that use simple semantic metrics as thresholds. Jailbreak\nprompts are semantically more varied than the original questions used for\nqueries. In this paper, we introduce a Semantic Mirror Jailbreak (SMJ) approach\nthat bypasses LLMs by generating jailbreak prompts that are semantically\nsimilar to the original question. We model the search for jailbreak prompts\nthat satisfy both semantic similarity and jailbreak validity as a\nmulti-objective optimization problem and employ a standardized set of genetic\nalgorithms for generating eligible prompts. Compared to the baseline\nAutoDAN-GA, SMJ achieves attack success rates (ASR) that are at most 35.4%\nhigher without ONION defense and 85.2% higher with ONION defense. SMJ's better\nperformance in all three semantic meaningfulness metrics of Jailbreak Prompt,\nSimilarity, and Outlier, also means that SMJ is resistant to defenses that use\nthose metrics as thresholds.\n","authors":["Xiaoxia Li","Siyuan Liang","Jiyi Zhang","Han Fang","Aishan Liu","Ee-Chien Chang"],"pdf_url":"https://arxiv.org/pdf/2402.14872v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17509v1","updated":"2024-02-27T13:49:12Z","published":"2024-02-27T13:49:12Z","title":"Extreme Miscalibration and the Illusion of Adversarial Robustness","summary":"  Deep learning-based Natural Language Processing (NLP) models are vulnerable\nto adversarial attacks, where small perturbations can cause a model to\nmisclassify. Adversarial Training (AT) is often used to increase model\nrobustness. However, we have discovered an intriguing phenomenon: deliberately\nor accidentally miscalibrating models masks gradients in a way that interferes\nwith adversarial attack search methods, giving rise to an apparent increase in\nrobustness. We show that this observed gain in robustness is an illusion of\nrobustness (IOR), and demonstrate how an adversary can perform various forms of\ntest-time temperature calibration to nullify the aforementioned interference\nand allow the adversarial attack to find adversarial examples. Hence, we urge\nthe NLP community to incorporate test-time temperature scaling into their\nrobustness evaluations to ensure that any observed gains are genuine. Finally,\nwe show how the temperature can be scaled during \\textit{training} to improve\ngenuine robustness.\n","authors":["Vyas Raina","Samson Tan","Volkan Cevher","Aditya Rawal","Sheng Zha","George Karypis"],"pdf_url":"https://arxiv.org/pdf/2402.17509v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17505v1","updated":"2024-02-27T13:44:09Z","published":"2024-02-27T13:44:09Z","title":"BASES: Large-scale Web Search User Simulation with Large Language Model\n  based Agents","summary":"  Due to the excellent capacities of large language models (LLMs), it becomes\nfeasible to develop LLM-based agents for reliable user simulation. Considering\nthe scarcity and limit (e.g., privacy issues) of real user data, in this paper,\nwe conduct large-scale user simulation for web search, to improve the analysis\nand modeling of user search behavior. Specially, we propose BASES, a novel user\nsimulation framework with LLM-based agents, designed to facilitate\ncomprehensive simulations of web search user behaviors. Our simulation\nframework can generate unique user profiles at scale, which subsequently leads\nto diverse search behaviors. To demonstrate the effectiveness of BASES, we\nconduct evaluation experiments based on two human benchmarks in both Chinese\nand English, demonstrating that BASES can effectively simulate large-scale\nhuman-like search behaviors. To further accommodate the research on web search,\nwe develop WARRIORS, a new large-scale dataset encompassing web search user\nbehaviors, including both Chinese and English versions, which can greatly\nbolster research in the field of information retrieval. Our code and data will\nbe publicly released soon.\n","authors":["Ruiyang Ren","Peng Qiu","Yingqi Qu","Jing Liu","Wayne Xin Zhao","Hua Wu","Ji-Rong Wen","Haifeng Wang"],"pdf_url":"https://arxiv.org/pdf/2402.17505v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.18338v2","updated":"2024-02-27T13:24:06Z","published":"2023-10-21T15:23:20Z","title":"Small Language Models Fine-tuned to Coordinate Larger Language Models\n  improve Complex Reasoning","summary":"  Large Language Models (LLMs) prompted to generate chain-of-thought (CoT)\nexhibit impressive reasoning capabilities. Recent attempts at prompt\ndecomposition toward solving complex, multi-step reasoning problems depend on\nthe ability of the LLM to simultaneously decompose and solve the problem. A\nsignificant disadvantage is that foundational LLMs are typically not available\nfor fine-tuning, making adaptation computationally prohibitive. We believe (and\ndemonstrate) that problem decomposition and solution generation are distinct\ncapabilites, better addressed in separate modules, than by one monolithic LLM.\nWe introduce DaSLaM, which uses a decomposition generator to decompose complex\nproblems into subproblems that require fewer reasoning steps. These subproblems\nare answered by a solver. We use a relatively small (13B parameters) LM as the\ndecomposition generator, which we train using policy gradient optimization to\ninteract with a solver LM (regarded as black-box) and guide it through\nsubproblems, thereby rendering our method solver-agnostic. Evaluation on\nmultiple different reasoning datasets reveal that with our method, a 175\nbillion parameter LM (text-davinci-003) can produce competitive or even better\nperformance, compared to its orders-of-magnitude larger successor, GPT-4.\nAdditionally, we show that DaSLaM is not limited by the solver's capabilities\nas a function of scale; e.g., solver LMs with diverse sizes give significant\nperformance improvement with our solver-agnostic decomposition technique.\nExhaustive ablation studies evince the superiority of our modular finetuning\ntechnique over exorbitantly large decomposer LLMs, based on prompting alone.\n","authors":["Gurusha Juneja","Subhabrata Dutta","Soumen Chakrabarti","Sunny Manchanda","Tanmoy Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2310.18338v2.pdf","comment":"EMNLP 2023 (Typos corrected)"},{"id":"http://arxiv.org/abs/2402.17497v1","updated":"2024-02-27T13:22:51Z","published":"2024-02-27T13:22:51Z","title":"REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain\n  Question Answering","summary":"  Considering the limited internal parametric knowledge, retrieval-augmented\ngeneration (RAG) has been widely used to extend the knowledge scope of large\nlanguage models (LLMs). Despite the extensive efforts on RAG research, in\nexisting methods, LLMs cannot precisely assess the relevance of retrieved\ndocuments, thus likely leading to misleading or even incorrect utilization of\nexternal knowledge (i.e., retrieved documents). To address this issue, in this\npaper, we propose REAR, a RElevance-Aware Retrieval-augmented approach for\nopen-domain question answering (QA). As the key motivation, we aim to enhance\nthe self-awareness of source relevance for LLMs, so as to adaptively utilize\nexternal knowledge in RAG systems. Specially, we develop a new architecture for\nLLM based RAG system, by incorporating a specially designed rank head that\nprecisely assesses the relevance of retrieved documents. Furthermore, we\npropose an improved training method based on bi-granularity relevance fusion\nand noise-resistant training. By combining the improvements in both\narchitecture and training, our proposed REAR can better utilize external\nknowledge by effectively perceiving the relevance of retrieved documents.\nExperiments on four open-domain QA tasks show that REAR significantly\noutperforms previous a number of competitive RAG approaches. Our code and data\ncan be accessed at https://github.com/RUCAIBox/REAR.\n","authors":["Yuhao Wang","Ruiyang Ren","Junyi Li","Wayne Xin Zhao","Jing Liu","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2402.17497v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17496v1","updated":"2024-02-27T13:22:47Z","published":"2024-02-27T13:22:47Z","title":"Emotional Voice Messages (EMOVOME) database: emotion recognition in\n  spontaneous voice messages","summary":"  Emotional Voice Messages (EMOVOME) is a spontaneous speech dataset containing\n999 audio messages from real conversations on a messaging app from 100 Spanish\nspeakers, gender balanced. Voice messages were produced in-the-wild conditions\nbefore participants were recruited, avoiding any conscious bias due to\nlaboratory environment. Audios were labeled in valence and arousal dimensions\nby three non-experts and two experts, which were then combined to obtain a\nfinal label per dimension. The experts also provided an extra label\ncorresponding to seven emotion categories. To set a baseline for future\ninvestigations using EMOVOME, we implemented emotion recognition models using\nboth speech and audio transcriptions. For speech, we used the standard eGeMAPS\nfeature set and support vector machines, obtaining 49.27% and 44.71% unweighted\naccuracy for valence and arousal respectively. For text, we fine-tuned a\nmultilingual BERT model and achieved 61.15% and 47.43% unweighted accuracy for\nvalence and arousal respectively. This database will significantly contribute\nto research on emotion recognition in the wild, while also providing a unique\nnatural and freely accessible resource for Spanish.\n","authors":["Lucía Gómez Zaragozá","Rocío del Amor","Elena Parra Vargas","Valery Naranjo","Mariano Alcañiz Raya","Javier Marín-Morales"],"pdf_url":"https://arxiv.org/pdf/2402.17496v1.pdf","comment":"10 pages, 6 figures, submitted to Scientific Data"},{"id":"http://arxiv.org/abs/2402.17493v1","updated":"2024-02-27T13:18:00Z","published":"2024-02-27T13:18:00Z","title":"Prescribing Large Language Models for Perioperative Care: What's The\n  Right Dose for Pre-trained Models?","summary":"  Postoperative risk predictions can inform effective perioperative care\nmanagement and planning. We aimed to assess whether clinical large language\nmodels (LLMs) can predict postoperative risks using clinical texts with various\ntraining strategies. The main cohort involved 84,875 records from Barnes Jewish\nHospital (BJH) system between 2018 and 2021. Methods were replicated on Beth\nIsrael Deaconess's MIMIC dataset. Both studies had mean duration of follow-up\nbased on the length of postoperative ICU stay less than 7 days. For the BJH\ndataset, outcomes included 30-day mortality, pulmonary embolism (PE) and\npneumonia. Three domain adaptation and finetuning strategies were implemented\nfor BioGPT, ClinicalBERT and BioClinicalBERT: self-supervised objectives;\nincorporating labels with semi-supervised fine-tuning; and foundational\nmodelling through multi-task learning. Model performance was compared using the\narea under the receiver operating characteristic curve (AUROC) and the area\nunder the precision recall curve (AUPRC) for classification tasks, and mean\nsquared error (MSE) and R2 for regression tasks. Pre-trained LLMs outperformed\ntraditional word embeddings, with absolute maximal gains of 38.3% for AUROC and\n14% for AUPRC. Adapting models further improved performance: (1)\nself-supervised finetuning by 3.2% for AUROC and 1.5% for AUPRC; (2)\nsemi-supervised finetuning by 1.8% for AUROC and 2% for AUPRC, compared to\nself-supervised finetuning; (3) foundational modelling by 3.6% for AUROC and\n2.6% for AUPRC, compared to self-supervised finetuning. Pre-trained clinical\nLLMs offer opportunities for postoperative risk predictions in unforeseen data,\nwith peaks in foundational models indicating the potential of task-agnostic\nlearning towards the generalizability of LLMs in perioperative care.\n","authors":["Bing Xue","Charles Alba","Joanna Abraham","Thomas Kannampallil","Chenyang Lu"],"pdf_url":"https://arxiv.org/pdf/2402.17493v1.pdf","comment":"Supplemental file available at: http://tinyurl.com/mszmjna9; models\n  publicly available at:\n  https://huggingface.co/cja5553/BJH-perioperative-notes-bioGPT AND\n  https://huggingface.co/cja5553/BJH-perioperative-notes-bioGPT"},{"id":"http://arxiv.org/abs/2210.17264v2","updated":"2024-02-27T13:14:51Z","published":"2022-10-31T12:44:53Z","title":"Cross-lingual Text-To-Speech with Flow-based Voice Conversion for\n  Improved Pronunciation","summary":"  This paper presents a method for end-to-end cross-lingual text-to-speech\n(TTS) which aims to preserve the target language's pronunciation regardless of\nthe original speaker's language. The model used is based on a non-attentive\nTacotron architecture, where the decoder has been replaced with a normalizing\nflow network conditioned on the speaker identity, allowing both TTS and voice\nconversion (VC) to be performed by the same model due to the inherent\nlinguistic content and speaker identity disentanglement. When used in a\ncross-lingual setting, acoustic features are initially produced with a native\nspeaker of the target language and then voice conversion is applied by the same\nmodel in order to convert these features to the target speaker's voice. We\nverify through objective and subjective evaluations that our method can have\nbenefits compared to baseline cross-lingual synthesis. By including speakers\naveraging 7.5 minutes of speech, we also present positive results on\nlow-resource scenarios.\n","authors":["Nikolaos Ellinas","Georgios Vamvoukakis","Konstantinos Markopoulos","Georgia Maniati","Panos Kakoulidis","June Sig Sung","Inchul Hwang","Spyros Raptis","Aimilios Chalamandaris","Pirros Tsiakoulis"],"pdf_url":"https://arxiv.org/pdf/2210.17264v2.pdf","comment":"Fundamental changes to the model described and experimental procedure"},{"id":"http://arxiv.org/abs/2402.03848v2","updated":"2024-02-27T13:14:28Z","published":"2024-02-06T09:50:08Z","title":"ANLS* -- A Universal Document Processing Metric for Generative Large\n  Language Models","summary":"  Traditionally, discriminative models have been the predominant choice for\ntasks like document classification and information extraction. These models\nmake predictions that fall into a limited number of predefined classes,\nfacilitating a binary true or false evaluation and enabling the direct\ncalculation of metrics such as the F1 score. However, recent advancements in\ngenerative large language models (GLLMs) have prompted a shift in the field due\nto their enhanced zero-shot capabilities, which eliminate the need for a\ndownstream dataset and computationally expensive fine-tuning. However,\nevaluating GLLMs presents a challenge as the binary true or false evaluation\nused for discriminative models is not applicable to the predictions made by\nGLLMs. This paper introduces a new metric for generative models called ANLS*\nfor evaluating a wide variety of tasks, including information extraction and\nclassification tasks. The ANLS* metric extends existing ANLS metrics as a\ndrop-in-replacement and is still compatible with previously reported ANLS\nscores. An evaluation of 7 different datasets and 3 different GLLMs using the\nANLS* metric is also provided, demonstrating the importance of the proposed\nmetric. We also benchmark a novel approach to generate prompts for documents,\ncalled SFT, against other prompting techniques such as LATIN. In 15 out of 21\ncases, SFT outperforms other techniques and improves the state-of-the-art,\nsometimes by as much as 15 percentage points.\n  Sources are available at https://github.com/deepopinion/anls_star_metric\n","authors":["David Peer","Philemon Schöpf","Volckmar Nebendahl","Alexander Rietzler","Sebastian Stabinger"],"pdf_url":"https://arxiv.org/pdf/2402.03848v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08894v2","updated":"2024-02-27T13:04:44Z","published":"2023-11-15T11:56:56Z","title":"Few-shot Transfer Learning for Knowledge Base Question Answering: Fusing\n  Supervised Models with In-Context Learning","summary":"  Existing Knowledge Base Question Answering (KBQA) architectures are hungry\nfor annotated data, which make them costly and time-consuming to deploy. We\nintroduce the problem of few-shot transfer learning for KBQA, where the target\ndomain offers only a few labeled examples, but a large labeled training dataset\nis available in a source domain. We propose a novel KBQA architecture called\nFuSIC-KBQA that performs KB-retrieval using multiple source-trained retrievers,\nre-ranks using an LLM and uses this as input for LLM few-shot in-context\nlearning to generate logical forms, which are further refined using\nexecution-guided feedback. Experiments over four source-target KBQA pairs of\nvarying complexity show that FuSIC-KBQA significantly outperforms adaptations\nof SoTA KBQA models for this setting. Additional experiments in the in-domain\nsetting show that FuSIC-KBQA also outperforms SoTA KBQA models when training\ndata is limited.\n","authors":["Mayur Patidar","Riya Sawhney","Avinash Singh","Biswajit Chatterjee"," Mausam","Indrajit Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2311.08894v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17478v1","updated":"2024-02-27T13:02:19Z","published":"2024-02-27T13:02:19Z","title":"Can GPT-4 Identify Propaganda? Annotation and Detection of Propaganda\n  Spans in News Articles","summary":"  The use of propaganda has spiked on mainstream and social media, aiming to\nmanipulate or mislead users. While efforts to automatically detect propaganda\ntechniques in textual, visual, or multimodal content have increased, most of\nthem primarily focus on English content. The majority of the recent initiatives\ntargeting medium to low-resource languages produced relatively small annotated\ndatasets, with a skewed distribution, posing challenges for the development of\nsophisticated propaganda detection models. To address this challenge, we\ncarefully develop the largest propaganda dataset to date, ArPro, comprised of\n8K paragraphs from newspaper articles, labeled at the text span level following\na taxonomy of 23 propagandistic techniques. Furthermore, our work offers the\nfirst attempt to understand the performance of large language models (LLMs),\nusing GPT-4, for fine-grained propaganda detection from text. Results showed\nthat GPT-4's performance degrades as the task moves from simply classifying a\nparagraph as propagandistic or not, to the fine-grained task of detecting\npropaganda techniques and their manifestation in text. Compared to models\nfine-tuned on the dataset for propaganda detection at different classification\ngranularities, GPT-4 is still far behind. Finally, we evaluate GPT-4 on a\ndataset consisting of six other languages for span detection, and results\nsuggest that the model struggles with the task across languages. Our dataset\nand resources will be released to the community.\n","authors":["Maram Hasanain","Fatema Ahmed","Firoj Alam"],"pdf_url":"https://arxiv.org/pdf/2402.17478v1.pdf","comment":"Accepted as a full paper at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2402.16458v2","updated":"2024-02-27T12:52:24Z","published":"2024-02-26T10:02:29Z","title":"ID-XCB: Data-independent Debiasing for Fair and Accurate\n  Transformer-based Cyberbullying Detection","summary":"  Swear words are a common proxy to collect datasets with cyberbullying\nincidents. Our focus is on measuring and mitigating biases derived from\nspurious associations between swear words and incidents occurring as a result\nof such data collection strategies. After demonstrating and quantifying these\nbiases, we introduce ID-XCB, the first data-independent debiasing technique\nthat combines adversarial training, bias constraints and debias fine-tuning\napproach aimed at alleviating model attention to bias-inducing words without\nimpacting overall model performance. We explore ID-XCB on two popular\nsession-based cyberbullying datasets along with comprehensive ablation and\ngeneralisation studies. We show that ID-XCB learns robust cyberbullying\ndetection capabilities while mitigating biases, outperforming state-of-the-art\ndebiasing methods in both performance and bias mitigation. Our quantitative and\nqualitative analyses demonstrate its generalisability to unseen data.\n","authors":["Peiling Yi","Arkaitz Zubiaga"],"pdf_url":"https://arxiv.org/pdf/2402.16458v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.11033v3","updated":"2024-02-27T12:51:48Z","published":"2024-01-19T21:21:02Z","title":"FAIR Enough: How Can We Develop and Assess a FAIR-Compliant Dataset for\n  Large Language Models' Training?","summary":"  The rapid evolution of Large Language Models (LLMs) underscores the critical\nimportance of ethical considerations and data integrity in AI development,\nemphasizing the role of FAIR (Findable, Accessible, Interoperable, Reusable)\ndata principles. While these principles have long been a cornerstone of ethical\ndata stewardship, their application in LLM training data is less prevalent, an\nissue our research aims to address. Our study begins with a review of existing\nliterature, highlighting the significance of FAIR principles in data management\nfor model training. Building on this foundation, we introduce a novel framework\nthat incorporates FAIR principles into the LLM training process. A key aspect\nof this approach is a comprehensive checklist, designed to assist researchers\nand developers in consistently applying FAIR data principles throughout the\nmodel development lifecycle. The practicality and effectiveness of our\nframework are demonstrated through a case study that involves creating a\nFAIR-compliant dataset to detect and reduce biases. This case study not only\nvalidates the usefulness of our framework but also establishes new benchmarks\nfor more equitable, transparent, and ethical practices in LLM training. We\noffer this framework to the community as a means to promote technologically\nadvanced, ethically sound, and socially responsible AI models.\n","authors":["Shaina Raza","Shardul Ghuge","Chen Ding","Elham Dolatabadi","Deval Pandya"],"pdf_url":"https://arxiv.org/pdf/2401.11033v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17463v1","updated":"2024-02-27T12:39:23Z","published":"2024-02-27T12:39:23Z","title":"Training-Free Long-Context Scaling of Large Language Models","summary":"  The ability of Large Language Models (LLMs) to process and generate coherent\ntext is markedly weakened when the number of input tokens exceeds their\npretraining length. Given the expensive overhead of finetuning large-scale\nmodels with longer sequences, we propose Dual Chunk Attention (DCA), which\nenables Llama2 70B to support context windows of more than 100k tokens without\ncontinual training. By decomposing the attention computation for long sequences\ninto chunk-based modules, DCA manages to effectively capture the relative\npositional information of tokens within the same chunk (Intra-Chunk) and across\ndistinct chunks (Inter-Chunk), as well as integrates seamlessly with Flash\nAttention. In addition to its impressive extrapolation capability, DCA achieves\nperformance on practical long-context tasks that is comparable to or even\nbetter than that of finetuned models. When compared with proprietary models,\nour training-free 70B model attains 94% of the performance of gpt-3.5-16k,\nindicating it is a viable open-source alternative. All code and data used in\nthis work are released at \\url{https://github.com/HKUNLP/ChunkLlama}.\n","authors":["Chenxin An","Fei Huang","Jun Zhang","Shansan Gong","Xipeng Qiu","Chang Zhou","Lingpeng Kong"],"pdf_url":"https://arxiv.org/pdf/2402.17463v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17456v1","updated":"2024-02-27T12:27:51Z","published":"2024-02-27T12:27:51Z","title":"A Piece of Theatre: Investigating How Teachers Design LLM Chatbots to\n  Assist Adolescent Cyberbullying Education","summary":"  Cyberbullying harms teenagers' mental health, and teaching them upstanding\nintervention is crucial. Wizard-of-Oz studies show chatbots can scale up\npersonalized and interactive cyberbullying education, but implementing such\nchatbots is a challenging and delicate task. We created a no-code chatbot\ndesign tool for K-12 teachers. Using large language models and prompt chaining,\nour tool allows teachers to prototype bespoke dialogue flows and chatbot\nutterances. In offering this tool, we explore teachers' distinctive needs when\ndesigning chatbots to assist their teaching, and how chatbot design tools might\nbetter support them. Our findings reveal that teachers welcome the tool\nenthusiastically. Moreover, they see themselves as playwrights guiding both the\nstudents' and the chatbot's behaviors, while allowing for some improvisation.\nTheir goal is to enable students to rehearse both desirable and undesirable\nreactions to cyberbullying in a safe environment. We discuss the design\nopportunities LLM-Chains offer for empowering teachers and the research\nopportunities this work opens up.\n","authors":["Michael A. Hedderich","Natalie N. Bazarova","Wenting Zou","Ryun Shim","Xinda Ma","Qian Yang"],"pdf_url":"https://arxiv.org/pdf/2402.17456v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.08532v2","updated":"2024-02-27T12:13:46Z","published":"2023-09-15T16:50:09Z","title":"Connecting Large Language Models with Evolutionary Algorithms Yields\n  Powerful Prompt Optimizers","summary":"  Large Language Models (LLMs) excel in various tasks, but they rely on\ncarefully crafted prompts that often demand substantial human effort. To\nautomate this process, in this paper, we propose a novel framework for discrete\nprompt optimization, called EvoPrompt, which borrows the idea of evolutionary\nalgorithms (EAs) as they exhibit good performance and fast convergence. To\nenable EAs to work on discrete prompts, which are natural language expressions\nthat need to be coherent and human-readable, we connect LLMs with EAs. This\napproach allows us to simultaneously leverage the powerful language processing\ncapabilities of LLMs and the efficient optimization performance of EAs.\nSpecifically, abstaining from any gradients or parameters, EvoPrompt starts\nfrom a population of prompts and iteratively generates new prompts with LLMs\nbased on the evolutionary operators, improving the population based on the\ndevelopment set. We optimize prompts for both closed- and open-source LLMs\nincluding GPT-3.5 and Alpaca, on 31 datasets covering language understanding,\ngeneration tasks, as well as BIG-Bench Hard (BBH) tasks. EvoPrompt\nsignificantly outperforms human-engineered prompts and existing methods for\nautomatic prompt generation (e.g., up to 25% on BBH). Furthermore, EvoPrompt\ndemonstrates that connecting LLMs with EAs creates synergies, which could\ninspire further research on the combination of LLMs and conventional\nalgorithms.\n","authors":["Qingyan Guo","Rui Wang","Junliang Guo","Bei Li","Kaitao Song","Xu Tan","Guoqing Liu","Jiang Bian","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2309.08532v2.pdf","comment":"International Conference on Learning Representations (ICLR) 2024"},{"id":"http://arxiv.org/abs/2402.17447v1","updated":"2024-02-27T12:03:56Z","published":"2024-02-27T12:03:56Z","title":"Deep Learning Based Named Entity Recognition Models for Recipes","summary":"  Food touches our lives through various endeavors, including flavor,\nnourishment, health, and sustainability. Recipes are cultural capsules\ntransmitted across generations via unstructured text. Automated protocols for\nrecognizing named entities, the building blocks of recipe text, are of immense\nvalue for various applications ranging from information extraction to novel\nrecipe generation. Named entity recognition is a technique for extracting\ninformation from unstructured or semi-structured data with known labels.\nStarting with manually-annotated data of 6,611 ingredient phrases, we created\nan augmented dataset of 26,445 phrases cumulatively. Simultaneously, we\nsystematically cleaned and analyzed ingredient phrases from RecipeDB, the\ngold-standard recipe data repository, and annotated them using the Stanford\nNER. Based on the analysis, we sampled a subset of 88,526 phrases using a\nclustering-based approach while preserving the diversity to create the\nmachine-annotated dataset. A thorough investigation of NER approaches on these\nthree datasets involving statistical, fine-tuning of deep learning-based\nlanguage models and few-shot prompting on large language models (LLMs) provides\ndeep insights. We conclude that few-shot prompting on LLMs has abysmal\nperformance, whereas the fine-tuned spaCy-transformer emerges as the best model\nwith macro-F1 scores of 95.9%, 96.04%, and 95.71% for the manually-annotated,\naugmented, and machine-annotated datasets, respectively.\n","authors":["Mansi Goel","Ayush Agarwal","Shubham Agrawal","Janak Kapuriya","Akhil Vamshi Konam","Rishabh Gupta","Shrey Rastogi"," Niharika","Ganesh Bagler"],"pdf_url":"https://arxiv.org/pdf/2402.17447v1.pdf","comment":"13 pages, 6 main figures and 2 in appendices, and 3 main tables;\n  Accepted for publication in LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2402.17437v1","updated":"2024-02-27T11:50:05Z","published":"2024-02-27T11:50:05Z","title":"Exploiting Emotion-Semantic Correlations for Empathetic Response\n  Generation","summary":"  Empathetic response generation aims to generate empathetic responses by\nunderstanding the speaker's emotional feelings from the language of dialogue.\nRecent methods capture emotional words in the language of communicators and\nconstruct them as static vectors to perceive nuanced emotions. However,\nlinguistic research has shown that emotional words in language are dynamic and\nhave correlations with other grammar semantic roles, i.e., words with semantic\nmeanings, in grammar. Previous methods overlook these two characteristics,\nwhich easily lead to misunderstandings of emotions and neglect of key\nsemantics. To address this issue, we propose a dynamical Emotion-Semantic\nCorrelation Model (ESCM) for empathetic dialogue generation tasks. ESCM\nconstructs dynamic emotion-semantic vectors through the interaction of context\nand emotions. We introduce dependency trees to reflect the correlations between\nemotions and semantics. Based on dynamic emotion-semantic vectors and\ndependency trees, we propose a dynamic correlation graph convolutional network\nto guide the model in learning context meanings in dialogue and generating\nempathetic responses. Experimental results on the EMPATHETIC-DIALOGUES dataset\nshow that ESCM understands semantics and emotions more accurately and expresses\nfluent and informative empathetic responses. Our analysis results also indicate\nthat the correlations between emotions and semantics are frequently used in\ndialogues, which is of great significance for empathetic perception and\nexpression.\n","authors":["Zhou Yang","Zhaochun Ren","Yufeng Wang","Xiaofei Zhu","Zhihao Chen","Tiecheng Cai","Yunbing Wu","Yisong Su","Sibo Ju","Xiangwen Liao"],"pdf_url":"https://arxiv.org/pdf/2402.17437v1.pdf","comment":"12 pages, 3 figures, Findings of EMNLP 2023"},{"id":"http://arxiv.org/abs/2402.17433v1","updated":"2024-02-27T11:45:21Z","published":"2024-02-27T11:45:21Z","title":"Enhancing EEG-to-Text Decoding through Transferable Representations from\n  Pre-trained Contrastive EEG-Text Masked Autoencoder","summary":"  Reconstructing natural language from non-invasive electroencephalography\n(EEG) holds great promise as a language decoding technology for brain-computer\ninterfaces (BCIs). However, EEG-based language decoding is still in its nascent\nstages, facing several technical issues such as: 1) Absence of a hybrid\nstrategy that can effectively integrate cross-modality (between EEG and text)\nself-learning with intra-modality self-reconstruction of EEG features or\ntextual sequences; 2) Under-utilization of large language models (LLMs) to\nenhance EEG-based language decoding. To address above issues, we propose the\nContrastive EEG-Text Masked Autoencoder (CET-MAE), a novel model that\norchestrates compound self-supervised learning across and within EEG and text\nthrough a dedicated multi-stream encoder. Furthermore, we develop a framework\ncalled E2T-PTR (EEG-to-Text decoding using Pretrained Transferable\nRepresentations), which leverages pre-trained modules alongside the EEG stream\nfrom CET-MAE and further enables an LLM (specifically BART) to decode text from\nEEG sequences. Comprehensive experiments conducted on the popular text-evoked\nEEG database, ZuCo, demonstrate the superiority of E2T-PTR, which outperforms\nthe state-of-the-art in ROUGE-1 F1 and BLEU-4 scores by 8.34% and 32.21%,\nrespectively. These results indicate significant advancements in the field and\nunderscores the proposed framework's potential to enable more powerful and\nwidespread BCI applications.\n","authors":["Jiaqi Wang","Zhenxi Song","Zhengyu Ma","Xipeng Qiu","Min Zhang","Zhiguo Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.17433v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09216v2","updated":"2024-02-27T11:27:27Z","published":"2024-02-14T14:53:56Z","title":"Scaling the Authoring of AutoTutors with Large Language Models","summary":"  Large Language Models (LLMs) have found several use cases in education,\nranging from automatic question generation to essay evaluation. In this paper,\nwe explore the potential of using Large Language Models (LLMs) to author\nIntelligent Tutoring Systems. A common pitfall of LLMs is their straying from\ndesired pedagogical strategies such as leaking the answer to the student, and\nin general, providing no guarantees. We posit that while LLMs with certain\nguardrails can take the place of subject experts, the overall pedagogical\ndesign still needs to be handcrafted for the best learning results. Based on\nthis principle, we create a sample end-to-end tutoring system named MWPTutor,\nwhich uses LLMs to fill in the state space of a pre-defined finite state\ntransducer. This approach retains the structure and the pedagogy of traditional\ntutoring systems that has been developed over the years by learning scientists\nbut brings in additional flexibility of LLM-based approaches. Through a human\nevaluation study on two datasets based on math word problems, we show that our\nhybrid approach achieves a better overall tutoring score than an instructed,\nbut otherwise free-form, GPT-4. MWPTutor is completely modular and opens up the\nscope for the community to improve its performance by improving individual\nmodules or using different teaching strategies that it can follow\n","authors":["Sankalan Pal Chowdhury","Vilém Zouhar","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2402.09216v2.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2402.16063v2","updated":"2024-02-27T11:07:23Z","published":"2024-02-25T11:24:41Z","title":"Citation-Enhanced Generation for LLM-based Chatbot","summary":"  Large language models (LLMs) exhibit powerful general intelligence across\ndiverse scenarios, including their integration into chatbots. However, a vital\nchallenge of LLM-based chatbots is that they may produce hallucinated content\nin responses, which significantly limits their applicability. Various efforts\nhave been made to alleviate hallucination, such as retrieval augmented\ngeneration and reinforcement learning with human feedback, but most of them\nrequire additional training and data annotation. In this paper, we propose a\nnovel post-hoc Citation-Enhanced Generation (CEG) approach combined with\nretrieval argumentation. Unlike previous studies that focus on preventing\nhallucinations during generation, our method addresses this issue in a post-hoc\nway. It incorporates a retrieval module to search for supporting documents\nrelevant to the generated content, and employs a natural language\ninference-based citation generation module. Once the statements in the\ngenerated content lack of reference, our model can regenerate responses until\nall statements are supported by citations. Note that our method is a\ntraining-free plug-and-play plugin that is capable of various LLMs. Experiments\non various hallucination-related datasets show our framework outperforms\nstate-of-the-art methods in both hallucination detection and response\nregeneration on three benchmarks. Our codes and dataset will be publicly\navailable.\n","authors":["Weitao Li","Junkai Li","Weizhi Ma","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2402.16063v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16823v2","updated":"2024-02-27T11:03:10Z","published":"2024-02-26T18:48:27Z","title":"Language Agents as Optimizable Graphs","summary":"  Various human-designed prompt engineering techniques have been proposed to\nimprove problem solvers based on Large Language Models (LLMs), yielding many\ndisparate code bases. We unify these approaches by describing LLM-based agents\nas computational graphs. The nodes implement functions to process multimodal\ndata or query LLMs, and the edges describe the information flow between\noperations. Graphs can be recursively combined into larger composite graphs\nrepresenting hierarchies of inter-agent collaboration (where edges connect\noperations of different agents). Our novel automatic graph optimizers (1)\nrefine node-level LLM prompts (node optimization) and (2) improve agent\norchestration by changing graph connectivity (edge optimization). Experiments\ndemonstrate that our framework can be used to efficiently develop, integrate,\nand automatically improve various LLM agents. The code can be found at\nhttps://github.com/metauto-ai/gptswarm.\n","authors":["Mingchen Zhuge","Wenyi Wang","Louis Kirsch","Francesco Faccio","Dmitrii Khizbullin","Jürgen Schmidhuber"],"pdf_url":"https://arxiv.org/pdf/2402.16823v2.pdf","comment":"Project Website: https://gptswarm.org ; Github Repo:\n  https://github.com/metauto-ai/gptswarm ; Replace to fix typos"},{"id":"http://arxiv.org/abs/2402.17411v1","updated":"2024-02-27T11:02:12Z","published":"2024-02-27T11:02:12Z","title":"Consistency Matters: Explore LLMs Consistency From a Black-Box\n  Perspective","summary":"  Nowadays both commercial and open-source academic LLM have become the\nmainstream models of NLP. However, there is still a lack of research on LLM\nconsistency, meaning that throughout the various stages of LLM research and\ndeployment, its internal parameters and capabilities should remain unchanged.\nThis issue exists in both the industrial and academic sectors. The solution to\nthis problem is often time-consuming and labor-intensive, and there is also an\nadditional cost of secondary deployment, resulting in economic and time losses.\nTo fill this gap, we build an LLM consistency task dataset and design several\nbaselines. Additionally, we choose models of diverse scales for the main\nexperiments. Specifically, in the LightGBM experiment, we used traditional NLG\nmetrics (i.e., ROUGE, BLEU, METEOR) as the features needed for model training.\nThe final result exceeds the manual evaluation and GPT3.5 as well as other\nmodels in the main experiment, achieving the best performance. In the end, we\nuse the best performing LightGBM model as the base model to build the\nevaluation tool, which can effectively assist in the deployment of business\nmodels. Our code and tool demo are available at\nhttps://github.com/heavenhellchen/Consistency.git\n","authors":["Fufangchen Zhao","Guoqiang Jin","Jiaheng Huang","Rui Zhao","Fei Tan"],"pdf_url":"https://arxiv.org/pdf/2402.17411v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17407v1","updated":"2024-02-27T10:57:07Z","published":"2024-02-27T10:57:07Z","title":"A Neural Rewriting System to Solve Algorithmic Problems","summary":"  Modern neural network architectures still struggle to learn algorithmic\nprocedures that require to systematically apply compositional rules to solve\nout-of-distribution problem instances. In this work, we propose an original\napproach to learn algorithmic tasks inspired by rewriting systems, a classic\nframework in symbolic artificial intelligence. We show that a rewriting system\ncan be implemented as a neural architecture composed by specialized modules:\nthe Selector identifies the target sub-expression to process, the Solver\nsimplifies the sub-expression by computing the corresponding result, and the\nCombiner produces a new version of the original expression by replacing the\nsub-expression with the solution provided. We evaluate our model on three types\nof algorithmic tasks that require simplifying symbolic formulas involving\nlists, arithmetic, and algebraic expressions. We test the extrapolation\ncapabilities of the proposed architecture using formulas involving a higher\nnumber of operands and nesting levels than those seen during training, and we\nbenchmark its performance against the Neural Data Router, a recent model\nspecialized for systematic generalization, and a state-of-the-art large\nlanguage model (GPT-4) probed with advanced prompting strategies.\n","authors":["Flavio Petruzzellis","Alberto Testolin","Alessandro Sperduti"],"pdf_url":"https://arxiv.org/pdf/2402.17407v1.pdf","comment":"Preprint. Work in progress"},{"id":"http://arxiv.org/abs/2402.17400v1","updated":"2024-02-27T10:47:24Z","published":"2024-02-27T10:47:24Z","title":"Investigating Continual Pretraining in Large Language Models: Insights\n  and Implications","summary":"  This paper studies the evolving domain of Continual Learning (CL) in large\nlanguage models (LLMs), with a focus on developing strategies for efficient and\nsustainable training. Our primary emphasis is on continual domain-adaptive\npretraining, a process designed to equip LLMs with the ability to integrate new\ninformation from various domains while retaining previously learned knowledge\nand enhancing cross-domain knowledge transfer without relying on\ndomain-specific identification. Unlike previous studies, which mostly\nconcentrate on a limited selection of tasks or domains and primarily aim to\naddress the issue of forgetting, our research evaluates the adaptability and\ncapabilities of LLMs to changing data landscapes in practical scenarios. To\nthis end, we introduce a new benchmark designed to measure the adaptability of\nLLMs to these evolving data environments, offering a comprehensive framework\nfor evaluation. We examine the impact of model size on learning efficacy and\nforgetting, as well as how the progression and similarity of emerging domains\naffect the knowledge transfer within these models. Our findings uncover several\nkey insights: (i) when the sequence of domains shows semantic similarity,\ncontinual pretraining enables LLMs to better specialize in the current domain\ncompared to stand-alone fine-tuning, (ii) training across a diverse range of\ndomains enhances both backward and forward knowledge transfer, and (iii)\nsmaller models are particularly sensitive to continual pretraining, showing the\nmost significant rates of both forgetting and learning. We posit that our\nresearch marks a shift towards establishing a more realistic benchmark for\ninvestigating CL in LLMs, and has the potential to play a key role in guiding\nthe direction of future research in the field.\n","authors":["Çağatay Yıldız","Nishaanth Kanna Ravichandran","Prishruit Punia","Matthias Bethge","Beyza Ermis"],"pdf_url":"https://arxiv.org/pdf/2402.17400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17396v1","updated":"2024-02-27T10:44:52Z","published":"2024-02-27T10:44:52Z","title":"Benchmarking GPT-4 on Algorithmic Problems: A Systematic Evaluation of\n  Prompting Strategies","summary":"  Large Language Models (LLMs) have revolutionized the field of Natural\nLanguage Processing thanks to their ability to reuse knowledge acquired on\nmassive text corpora on a wide variety of downstream tasks, with minimal (if\nany) tuning steps. At the same time, it has been repeatedly shown that LLMs\nlack systematic generalization, which allows to extrapolate the learned\nstatistical regularities outside the training distribution. In this work, we\noffer a systematic benchmarking of GPT-4, one of the most advanced LLMs\navailable, on three algorithmic tasks characterized by the possibility to\ncontrol the problem difficulty with two parameters. We compare the performance\nof GPT-4 with that of its predecessor (GPT-3.5) and with a variant of the\nTransformer-Encoder architecture recently introduced to solve similar tasks,\nthe Neural Data Router. We find that the deployment of advanced prompting\ntechniques allows GPT-4 to reach superior accuracy on all tasks, demonstrating\nthat state-of-the-art LLMs constitute a very strong baseline also in\nchallenging tasks that require systematic generalization.\n","authors":["Flavio Petruzzellis","Alberto Testolin","Alessandro Sperduti"],"pdf_url":"https://arxiv.org/pdf/2402.17396v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2402.17392v1","updated":"2024-02-27T10:38:37Z","published":"2024-02-27T10:38:37Z","title":"Spot the bot: Coarse-Grained Partition of Semantic Paths for Bots and\n  Humans","summary":"  Nowadays, technology is rapidly advancing: bots are writing comments,\narticles, and reviews. Due to this fact, it is crucial to know if the text was\nwritten by a human or by a bot. This paper focuses on comparing structures of\nthe coarse-grained partitions of semantic paths for human-written and\nbot-generated texts. We compare the clusterizations of datasets of n-grams from\nliterary texts and texts generated by several bots. The hypothesis is that the\nstructures and clusterizations are different. Our research supports the\nhypothesis. As the semantic structure may be different for different languages,\nwe investigate Russian, English, German, and Vietnamese languages.\n","authors":["Vasilii A. Gromov","Alexandra S. Kogan"],"pdf_url":"https://arxiv.org/pdf/2402.17392v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17389v1","updated":"2024-02-27T10:31:00Z","published":"2024-02-27T10:31:00Z","title":"FairBelief - Assessing Harmful Beliefs in Language Models","summary":"  Language Models (LMs) have been shown to inherit undesired biases that might\nhurt minorities and underrepresented groups if such systems were integrated\ninto real-world applications without careful fairness auditing. This paper\nproposes FairBelief, an analytical approach to capture and assess beliefs,\ni.e., propositions that an LM may embed with different degrees of confidence\nand that covertly influence its predictions. With FairBelief, we leverage\nprompting to study the behavior of several state-of-the-art LMs across\ndifferent previously neglected axes, such as model scale and likelihood,\nassessing predictions on a fairness dataset specifically designed to quantify\nLMs' outputs' hurtfulness. Finally, we conclude with an in-depth qualitative\nassessment of the beliefs emitted by the models. We apply FairBelief to English\nLMs, revealing that, although these architectures enable high performances on\ndiverse natural language processing tasks, they show hurtful beliefs about\nspecific genders. Interestingly, training procedure and dataset, model scale,\nand architecture induce beliefs of different degrees of hurtfulness.\n","authors":["Mattia Setzu","Marta Marchiori Manerba","Pasquale Minervini","Debora Nozza"],"pdf_url":"https://arxiv.org/pdf/2402.17389v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17377v1","updated":"2024-02-27T10:14:57Z","published":"2024-02-27T10:14:57Z","title":"KoDialogBench: Evaluating Conversational Understanding of Language\n  Models with Korean Dialogue Benchmark","summary":"  As language models are often deployed as chatbot assistants, it becomes a\nvirtue for models to engage in conversations in a user's first language. While\nthese models are trained on a wide range of languages, a comprehensive\nevaluation of their proficiency in low-resource languages such as Korean has\nbeen lacking. In this work, we introduce KoDialogBench, a benchmark designed to\nassess language models' conversational capabilities in Korean. To this end, we\ncollect native Korean dialogues on daily topics from public sources, or\ntranslate dialogues from other languages. We then structure these conversations\ninto diverse test datasets, spanning from dialogue comprehension to response\nselection tasks. Leveraging the proposed benchmark, we conduct extensive\nevaluations and analyses of various language models to measure a foundational\nunderstanding of Korean dialogues. Experimental results indicate that there\nexists significant room for improvement in models' conversation skills.\nFurthermore, our in-depth comparisons across different language models\nhighlight the effectiveness of recent training techniques in enhancing\nconversational proficiency. We anticipate that KoDialogBench will promote the\nprogress towards conversation-aware Korean language models.\n","authors":["Seongbo Jang","Seonghyeon Lee","Hwanjo Yu"],"pdf_url":"https://arxiv.org/pdf/2402.17377v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2402.17371v1","updated":"2024-02-27T10:09:40Z","published":"2024-02-27T10:09:40Z","title":"A Dataset for Metaphor Detection in Early Medieval Hebrew Poetry","summary":"  There is a large volume of late antique and medieval Hebrew texts. They\nrepresent a crucial linguistic and cultural bridge between Biblical and modern\nHebrew. Poetry is prominent in these texts and one of its main haracteristics\nis the frequent use of metaphor. Distinguishing figurative and literal language\nuse is a major task for scholars of the Humanities, especially in the fields of\nliterature, linguistics, and hermeneutics. This paper presents a new,\nchallenging dataset of late antique and medieval Hebrew poetry with expert\nannotations of metaphor, as well as some baseline results, which we hope will\nfacilitate further research in this area.\n","authors":["Michael Toker","Oren Mishali","Ophir Münz-Manor","Benny Kimelfeld","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2402.17371v1.pdf","comment":"EACL 2024. Project webpage: https://tokeron.github.io/metaphor/"},{"id":"http://arxiv.org/abs/2312.06185v3","updated":"2024-02-27T10:05:04Z","published":"2023-12-11T07:56:25Z","title":"KnowGPT: Knowledge Injection for Large Language Models","summary":"  Generative Large Language Models (LLMs), such as ChatGPT, offer interactive\nAPIs that can answer common questions at a human-expert level. However, these\nmodels often give inaccurate or incorrect responses when faced with questions\nrequiring domain-specific or professional-specific knowledge not covered in\ntheir training corpus. Furthermore, many state-of-the-art LLMs are not\nopen-source, making it challenging to inject knowledge with model APIs only. In\nthis work, we introduce KnowGPT, a black-box knowledge injection framework for\nLLMs in question answering. KnowGPT leverages deep reinforcement learning (RL)\nto extract relevant knowledge from Knowledge Graphs (KGs) and use Multi-Armed\nBandit (MAB) to construct the most suitable prompt for each question. Our\nextensive experiments on three benchmark datasets showcase that KnowGPT\nsignificantly enhances the existing methods. Notably, KnowGPT achieves an\naverage improvement of 23.7% over ChatGPT and an average improvement of 2.9%\nover GPT-4. Additionally, KnowGPT attains a 91.6% accuracy on the OpenbookQA\nofficial leaderboard, which is comparable to human-level performance.\n","authors":["Qinggang Zhang","Junnan Dong","Hao Chen","Daochen Zha","Zailiang Yu","Xiao Huang"],"pdf_url":"https://arxiv.org/pdf/2312.06185v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17358v1","updated":"2024-02-27T09:52:27Z","published":"2024-02-27T09:52:27Z","title":"SoFA: Shielded On-the-fly Alignment via Priority Rule Following","summary":"  The alignment problem in Large Language Models (LLMs) involves adapting them\nto the broad spectrum of human values. This requirement challenges existing\nalignment methods due to diversity of preferences and regulatory standards.\nThis paper introduces a novel alignment paradigm, priority rule following,\nwhich defines rules as the primary control mechanism in each dialog,\nprioritizing them over user instructions. Our preliminary analysis reveals that\neven the advanced LLMs, such as GPT-4, exhibit shortcomings in understanding\nand prioritizing the rules. Therefore, we present PriorityDistill, a\nsemi-automated approach for distilling priority following signals from LLM\nsimulations to ensure robust rule integration and adherence. Our experiments\nshow that this method not only effectively minimizes misalignments utilizing\nonly one general rule but also adapts smoothly to various unseen rules,\nensuring they are shielded from hijacking and that the model responds\nappropriately.\n","authors":["Xinyu Lu","Bowen Yu","Yaojie Lu","Hongyu Lin","Haiyang Yu","Le Sun","Xianpei Han","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2402.17358v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17355v1","updated":"2024-02-27T09:47:36Z","published":"2024-02-27T09:47:36Z","title":"RECOST: External Knowledge Guided Data-efficient Instruction Tuning","summary":"  In the current landscape of large language models (LLMs), the process of\ninstruction tuning serves as an essential step. Considering the high computing\npower overhead, data-efficient instruction tuning was proposed to reduce the\ntraining data size in this process, aiming at selecting high-quality\ninstructional data. Nevertheless, we argue that most current data-efficient\ninstruction-tuning methods are highly dependent on the quality of the original\ninstruction-tuning dataset. When it comes to datasets synthesized by LLMs, a\ncommon scenario in this field, dirty samples will even be selected with a\nhigher probability than other samples. To address these challenges, we utilized\nexternal knowledge (relevant examples or paragraphs) to evaluate those samples\nsynthesized by LLMs with an in-context-based relative predictive entropy. Based\non the new metric, we proposed a framework, dubbed as \\textbf{RECOST}, which\nintegrates external-knowledge-base re-ranking and diversity-consistent sampling\ninto a single pipeline. Through extensive experiments on several synthetic\ndatasets (Alpaca and Alpaca-gpt4), we demonstrate the effectiveness of our\nmethod and achieve even better results with only \\textbf{1\\%} of the full\ndataset.\n","authors":["Qi Zhang","Yiming Zhang","Haobo Wang","Junbo Zhao"],"pdf_url":"https://arxiv.org/pdf/2402.17355v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17333v1","updated":"2024-02-27T09:10:28Z","published":"2024-02-27T09:10:28Z","title":"Unsupervised multiple choices question answering via universal corpus","summary":"  Unsupervised question answering is a promising yet challenging task, which\nalleviates the burden of building large-scale annotated data in a new domain.\nIt motivates us to study the unsupervised multiple-choice question answering\n(MCQA) problem. In this paper, we propose a novel framework designed to\ngenerate synthetic MCQA data barely based on contexts from the universal domain\nwithout relying on any form of manual annotation. Possible answers are\nextracted and used to produce related questions, then we leverage both named\nentities (NE) and knowledge graphs to discover plausible distractors to form\ncomplete synthetic samples. Experiments on multiple MCQA datasets demonstrate\nthe effectiveness of our method.\n","authors":["Qin Zhang","Hao Ge","Xiaojun Chen","Meng Fang"],"pdf_url":"https://arxiv.org/pdf/2402.17333v1.pdf","comment":"5 pages, 1 figures, published to ICASSP 2024"},{"id":"http://arxiv.org/abs/2402.17311v1","updated":"2024-02-27T08:33:31Z","published":"2024-02-27T08:33:31Z","title":"SKT5SciSumm - A Hybrid Generative Approach for Multi-Document Scientific\n  Summarization","summary":"  Summarization for scientific text has shown significant benefits both for the\nresearch community and human society. Given the fact that the nature of\nscientific text is distinctive and the input of the multi-document\nsummarization task is substantially long, the task requires sufficient\nembedding generation and text truncation without losing important information.\nTo tackle these issues, in this paper, we propose SKT5SciSumm - a hybrid\nframework for multi-document scientific summarization (MDSS). We leverage the\nSentence-Transformer version of Scientific Paper Embeddings using\nCitation-Informed Transformers (SPECTER) to encode and represent textual\nsentences, allowing for efficient extractive summarization using k-means\nclustering. We employ the T5 family of models to generate abstractive summaries\nusing extracted sentences. SKT5SciSumm achieves state-of-the-art performance on\nthe Multi-XScience dataset. Through extensive experiments and evaluation, we\nshowcase the benefits of our model by using less complicated models to achieve\nremarkable results, thereby highlighting its potential in advancing the field\nof multi-document summarization for scientific text.\n","authors":["Huy Quoc To","Hung-Nghiep Tran","Andr'e Greiner-Petter","Felix Beierle","Akiko Aizawa"],"pdf_url":"https://arxiv.org/pdf/2402.17311v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17304v1","updated":"2024-02-27T08:27:15Z","published":"2024-02-27T08:27:15Z","title":"Probing Multimodal Large Language Models for Global and Local Semantic\n  Representation","summary":"  The success of large language models has inspired researchers to transfer\ntheir exceptional representing ability to other modalities. Several recent\nworks leverage image-caption alignment datasets to train multimodal large\nlanguage models (MLLMs), which achieve state-of-the-art performance on\nimage-to-text tasks. However, there are very few studies exploring whether\nMLLMs truly understand the complete image information, i.e., global\ninformation, or if they can only capture some local object information. In this\nstudy, we find that the intermediate layers of models can encode more global\nsemantic information, whose representation vectors perform better on\nvisual-language entailment tasks, rather than the topmost layers. We further\nprobe models for local semantic representation through object detection tasks.\nAnd we draw a conclusion that the topmost layers may excessively focus on local\ninformation, leading to a diminished ability to encode global information.\n","authors":["Mingxu Tao","Quzhe Huang","Kun Xu","Liwei Chen","Yansong Feng","Dongyan Zhao"],"pdf_url":"https://arxiv.org/pdf/2402.17304v1.pdf","comment":"Accepted by LREC-COLING 2024 as a short paper"},{"id":"http://arxiv.org/abs/2402.17302v1","updated":"2024-02-27T08:24:32Z","published":"2024-02-27T08:24:32Z","title":"Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in\n  Indonesian and Sundanese","summary":"  Large Language Models (LLMs) are increasingly being used to generate\nsynthetic data for training and evaluating models. However, it is unclear\nwhether they can generate a good quality of question answering (QA) dataset\nthat incorporates knowledge and cultural nuance embedded in a language,\nespecially for low-resource languages. In this study, we investigate the\neffectiveness of using LLMs in generating culturally relevant commonsense QA\ndatasets for Indonesian and Sundanese languages. To do so, we create datasets\nfor these languages using various methods involving both LLMs and human\nannotators. Our experiments show that the current best-performing LLM, GPT-4\nTurbo, is capable of generating questions with adequate knowledge in Indonesian\nbut not in Sundanese, highlighting the performance discrepancy between medium-\nand lower-resource languages. We also benchmark various LLMs on our generated\ndatasets and find that they perform better on the LLM-generated datasets\ncompared to those created by humans.\n","authors":["Rifki Afina Putri","Faiz Ghifari Haznitrama","Dea Adhista","Alice Oh"],"pdf_url":"https://arxiv.org/pdf/2402.17302v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16311v2","updated":"2024-02-27T08:21:35Z","published":"2024-02-26T05:30:48Z","title":"Cross-domain Chinese Sentence Pattern Parsing","summary":"  Sentence Pattern Structure (SPS) parsing is a syntactic analysis method\nprimarily employed in language teaching.Existing SPS parsers rely heavily on\ntextbook corpora for training, lacking cross-domain capability.To overcome this\nconstraint, this paper proposes an innovative approach leveraging large\nlanguage models (LLMs) within a self-training framework. Partial syntactic\nrules from a source domain are combined with target domain sentences to\ndynamically generate training data, enhancing the adaptability of the parser to\ndiverse domains.Experiments conducted on textbook and news domains demonstrate\nthe effectiveness of the proposed method, outperforming rule-based baselines by\n1.68 points on F1 metrics.\n","authors":["Jingsi Yu","Cunliang Kong","Liner Yang","Meishan Zhang","Lin Zhu","Yujie Wang","Haozhe Lin","Maosong Sun","Erhong Yang"],"pdf_url":"https://arxiv.org/pdf/2402.16311v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03563v2","updated":"2024-02-27T07:37:08Z","published":"2024-02-05T22:22:49Z","title":"Distinguishing the Knowable from the Unknowable with Language Models","summary":"  We study the feasibility of identifying epistemic uncertainty (reflecting a\nlack of knowledge), as opposed to aleatoric uncertainty (reflecting entropy in\nthe underlying distribution), in the outputs of large language models (LLMs)\nover free-form text. In the absence of ground-truth probabilities, we explore a\nsetting where, in order to (approximately) disentangle a given LLM's\nuncertainty, a significantly larger model stands in as a proxy for the ground\ntruth. We show that small linear probes trained on the embeddings of frozen,\npretrained models accurately predict when larger models will be more confident\nat the token level and that probes trained on one text domain generalize to\nothers. Going further, we propose a fully unsupervised method that achieves\nnon-trivial accuracy on the same task. Taken together, we interpret these\nresults as evidence that LLMs naturally contain internal representations of\ndifferent types of uncertainty that could potentially be leveraged to devise\nmore informative indicators of model confidence in diverse practical settings.\n","authors":["Gustaf Ahdritz","Tian Qin","Nikhil Vyas","Boaz Barak","Benjamin L. Edelman"],"pdf_url":"https://arxiv.org/pdf/2402.03563v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13516v2","updated":"2024-02-27T07:27:07Z","published":"2024-02-21T03:58:49Z","title":"ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity\n  within Large Language Models","summary":"  Activation sparsity refers to the existence of considerable\nweakly-contributed elements among activation outputs. As a prevalent property\nof the models using the ReLU activation function, it has been proven a\npromising paradigm to boost model inference efficiency. Nevertheless, most\nlarge language models (LLMs) adopt activation functions without intrinsic\nactivation sparsity (e.g., GELU and Swish). Some recent efforts have explored\nintroducing ReLU or its variants as the substitutive activation function to\nhelp LLMs achieve activation sparsity and inference acceleration, but few can\nsimultaneously obtain high sparsity and comparable model performance. This\npaper introduces an effective sparsification method named \"ProSparse\" to push\nLLMs for higher activation sparsity without decreasing model performance.\nSpecifically, after substituting the activation function of LLMs with ReLU,\nProSparse adopts progressive sparsity regularization with a factor smoothly\nincreasing along sine curves in multiple stages. This can enhance activation\nsparsity and alleviate performance degradation by avoiding radical shifts in\nactivation distribution. With ProSparse, we obtain high sparsity of 89.32% and\n88.80% for LLaMA2-7B and LLaMA2-13B, respectively, achieving comparable\nperformance to their original Swish-activated versions. Our inference\nacceleration experiments further demonstrate the practical acceleration brought\nby higher activation sparsity.\n","authors":["Chenyang Song","Xu Han","Zhengyan Zhang","Shengding Hu","Xiyu Shi","Kuai Li","Chen Chen","Zhiyuan Liu","Guangli Li","Tao Yang","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2402.13516v2.pdf","comment":"16 pages, 3 figures, 7 tables"},{"id":"http://arxiv.org/abs/2402.17263v1","updated":"2024-02-27T07:14:12Z","published":"2024-02-27T07:14:12Z","title":"Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning","summary":"  Parameter-efficient fine-tuning (PEFT) is a popular method for tailoring\npre-trained large language models (LLMs), especially as the models' scale and\nthe diversity of tasks increase. Low-rank adaptation (LoRA) is based on the\nidea that the adaptation process is intrinsically low-dimensional, i.e.,\nsignificant model changes can be represented with relatively few parameters.\nHowever, decreasing the rank encounters challenges with generalization errors\nfor specific tasks when compared to full-parameter fine-tuning. We present\nMELoRA, a mini-ensemble low-rank adapters that uses fewer trainable parameters\nwhile maintaining a higher rank, thereby offering improved performance\npotential. The core idea is to freeze original pretrained weights and train a\ngroup of mini LoRAs with only a small number of parameters. This can capture a\nsignificant degree of diversity among mini LoRAs, thus promoting better\ngeneralization ability. We conduct a theoretical analysis and empirical studies\non various NLP tasks. Our experimental results show that, compared to LoRA,\nMELoRA achieves better performance with 8 times fewer trainable parameters on\nnatural language understanding tasks and 36 times fewer trainable parameters on\ninstruction following tasks, which demonstrates the effectiveness of MELoRA.\n","authors":["Pengjie Ren","Chengshun Shi","Shiguang Wu","Mengqi Zhang","Zhaochun Ren","Maarten de Rijke","Zhumin Chen","Jiahuan Pei"],"pdf_url":"https://arxiv.org/pdf/2402.17263v1.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2402.17262v1","updated":"2024-02-27T07:11:59Z","published":"2024-02-27T07:11:59Z","title":"Speak Out of Turn: Safety Vulnerability of Large Language Models in\n  Multi-turn Dialogue","summary":"  Large Language Models (LLMs) have been demonstrated to generate illegal or\nunethical responses, particularly when subjected to \"jailbreak.\" Research on\njailbreak has highlighted the safety issues of LLMs. However, prior studies\nhave predominantly focused on single-turn dialogue, ignoring the potential\ncomplexities and risks presented by multi-turn dialogue, a crucial mode through\nwhich humans derive information from LLMs. In this paper, we argue that humans\ncould exploit multi-turn dialogue to induce LLMs into generating harmful\ninformation. LLMs may not intend to reject cautionary or borderline unsafe\nqueries, even if each turn is closely served for one malicious purpose in a\nmulti-turn dialogue. Therefore, by decomposing an unsafe query into several\nsub-queries for multi-turn dialogue, we induced LLMs to answer harmful\nsub-questions incrementally, culminating in an overall harmful response. Our\nexperiments, conducted across a wide range of LLMs, indicate current\ninadequacies in the safety mechanisms of LLMs in multi-turn dialogue. Our\nfindings expose vulnerabilities of LLMs in complex scenarios involving\nmulti-turn dialogue, presenting new challenges for the safety of LLMs.\n","authors":["Zhenhong Zhou","Jiuyang Xiang","Haopeng Chen","Quan Liu","Zherui Li","Sen Su"],"pdf_url":"https://arxiv.org/pdf/2402.17262v1.pdf","comment":"working in progress 23pages, 18 figures"},{"id":"http://arxiv.org/abs/2402.17256v1","updated":"2024-02-27T07:02:10Z","published":"2024-02-27T07:02:10Z","title":"Beyond the Known: Investigating LLMs Performance on Out-of-Domain Intent\n  Detection","summary":"  Out-of-domain (OOD) intent detection aims to examine whether the user's query\nfalls outside the predefined domain of the system, which is crucial for the\nproper functioning of task-oriented dialogue (TOD) systems. Previous methods\naddress it by fine-tuning discriminative models. Recently, some studies have\nbeen exploring the application of large language models (LLMs) represented by\nChatGPT to various downstream tasks, but it is still unclear for their ability\non OOD detection task.This paper conducts a comprehensive evaluation of LLMs\nunder various experimental settings, and then outline the strengths and\nweaknesses of LLMs. We find that LLMs exhibit strong zero-shot and few-shot\ncapabilities, but is still at a disadvantage compared to models fine-tuned with\nfull resource. More deeply, through a series of additional analysis\nexperiments, we discuss and summarize the challenges faced by LLMs and provide\nguidance for future work including injecting domain knowledge, strengthening\nknowledge transfer from IND(In-domain) to OOD, and understanding long\ninstructions.\n","authors":["Pei Wang","Keqing He","Yejie Wang","Xiaoshuai Song","Yutao Mou","Jingang Wang","Yunsen Xian","Xunliang Cai","Weiran Xu"],"pdf_url":"https://arxiv.org/pdf/2402.17256v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.12675v2","updated":"2024-02-27T06:57:36Z","published":"2023-05-22T03:28:47Z","title":"A Frustratingly Simple Decoding Method for Neural Text Generation","summary":"  We introduce a frustratingly simple, super efficient and surprisingly\neffective decoding method, which we call Frustratingly Simple Decoding (FSD),\nfor neural text generation. The idea behind FSD is straightforward: we build an\nanti-LM based on previously generated text and use this anti-LM to penalize\nfuture generation of what has been generated. The anti-LM can be implemented as\nsimple as an n-gram language model or a vectorized variant. In this way, FSD\nintroduces no extra model parameters and negligible computational overhead (FSD\ncan be as fast as greedy search). Despite the simplicity, FSD is surprisingly\neffective; Experiments show that FSD can outperform the canonical methods to\ndate (i.e., nucleus sampling) as well as several strong baselines that were\nproposed recently.\n","authors":["Haoran Yang","Deng Cai","Huayang Li","Wei Bi","Wai Lam","Shuming Shi"],"pdf_url":"https://arxiv.org/pdf/2305.12675v2.pdf","comment":"LREC-Coling 2024"},{"id":"http://arxiv.org/abs/2402.12563v2","updated":"2024-02-27T06:32:35Z","published":"2024-02-19T21:38:02Z","title":"Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of\n  Large Language Models","summary":"  The recent success of Large Language Models (LLMs) has catalyzed an\nincreasing interest in their self-correction capabilities. This paper presents\na comprehensive investigation into the intrinsic self-correction of LLMs,\nattempting to address the ongoing debate about its feasibility. Our research\nhas identified an important latent factor - the \"confidence\" of LLMs - during\nthe self-correction process. Overlooking this factor may cause the models to\nover-criticize themselves, resulting in unreliable conclusions regarding the\nefficacy of self-correction. We have experimentally observed that LLMs possess\nthe capability to understand the \"confidence\" in their own responses. It\nmotivates us to develop an \"If-or-Else\" (IoE) prompting framework, designed to\nguide LLMs in assessing their own \"confidence\", facilitating intrinsic\nself-corrections. We conduct extensive experiments and demonstrate that our\nIoE-based Prompt can achieve a consistent improvement regarding the accuracy of\nself-corrected responses over the initial answers. Our study not only sheds\nlight on the underlying factors affecting self-correction in LLMs, but also\nintroduces a practical framework that utilizes the IoE prompting principle to\nefficiently improve self-correction capabilities with \"confidence\". The code is\navailable at https://github.com/MBZUAI-CLeaR/IoE-Prompting.git.\n","authors":["Loka Li","Guangyi Chen","Yusheng Su","Zhenhao Chen","Yixuan Zhang","Eric Xing","Kun Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.12563v2.pdf","comment":"12 figures, 9 tables"},{"id":"http://arxiv.org/abs/2402.16040v2","updated":"2024-02-27T06:25:25Z","published":"2024-02-25T09:41:50Z","title":"EHRNoteQA: A Patient-Specific Question Answering Benchmark for\n  Evaluating Large Language Models in Clinical Settings","summary":"  This study introduces EHRNoteQA, a novel patient-specific question answering\nbenchmark tailored for evaluating Large Language Models (LLMs) in clinical\nenvironments. Based on MIMIC-IV Electronic Health Record (EHR), a team of three\nmedical professionals has curated the dataset comprising 962 unique questions,\neach linked to a specific patient's EHR clinical notes. What makes EHRNoteQA\ndistinct from existing EHR-based benchmarks is as follows: Firstly, it is the\nfirst dataset to adopt a multi-choice question answering format, a design\nchoice that effectively evaluates LLMs with reliable scores in the context of\nautomatic evaluation, compared to other formats. Secondly, it requires an\nanalysis of multiple clinical notes to answer a single question, reflecting the\ncomplex nature of real-world clinical decision-making where clinicians review\nextensive records of patient histories. Our comprehensive evaluation on various\nlarge language models showed that their scores on EHRNoteQA correlate more\nclosely with their performance in addressing real-world medical questions\nevaluated by clinicians than their scores from other LLM benchmarks. This\nunderscores the significance of EHRNoteQA in evaluating LLMs for medical\napplications and highlights its crucial role in facilitating the integration of\nLLMs into healthcare systems. The dataset will be made available to the public\nunder PhysioNet credential access, promoting further research in this vital\nfield.\n","authors":["Sunjun Kweon","Jiyoun Kim","Heeyoung Kwak","Dongchul Cha","Hangyul Yoon","Kwanghyun Kim","Seunghyun Won","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2402.16040v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2402.17237v1","updated":"2024-02-27T06:11:54Z","published":"2024-02-27T06:11:54Z","title":"Image-Text Matching with Multi-View Attention","summary":"  Existing two-stream models for image-text matching show good performance\nwhile ensuring retrieval speed and have received extensive attention from\nindustry and academia. These methods use a single representation to encode\nimage and text separately and get a matching score with cosine similarity or\nthe inner product of vectors. However, the performance of the two-stream model\nis often sub-optimal. On the one hand, a single representation is challenging\nto cover complex content comprehensively. On the other hand, in this framework\nof lack of interaction, it is challenging to match multiple meanings which\nleads to information being ignored. To address the problems mentioned above and\nfacilitate the performance of the two-stream model, we propose a multi-view\nattention approach for two-stream image-text matching MVAM\n(\\textbf{M}ulti-\\textbf{V}iew \\textbf{A}ttention \\textbf{M}odel). It first\nlearns multiple image and text representations by diverse attention heads with\ndifferent view codes. And then concatenate these representations into one for\nmatching. A diversity objective is also used to promote diversity between\nattention heads. With this method, models are able to encode images and text\nfrom different views and attend to more key points. So we can get\nrepresentations that contain more information. When doing retrieval tasks, the\nmatching scores between images and texts can be calculated from different\naspects, leading to better matching performance. Experiment results on MSCOCO\nand Flickr30K show that our proposed model brings improvements over existing\nmodels. Further case studies show that different attention heads can focus on\ndifferent contents and finally obtain a more comprehensive representation.\n","authors":["Rui Cheng","Wanqing Cui"],"pdf_url":"https://arxiv.org/pdf/2402.17237v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17231v1","updated":"2024-02-27T05:50:35Z","published":"2024-02-27T05:50:35Z","title":"MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical\n  Reasoning","summary":"  Tool-augmented Large Language Models (TALM) are known to enhance the skillset\nof large language models (LLM), thereby, leading to their improved reasoning\nabilities across many tasks. While, TALMs have been successfully employed in\ndifferent question-answering benchmarks, their efficacy on complex mathematical\nreasoning benchmarks, and the potential complimentary benefits offered by tools\nfor knowledge retrieval and mathematical equation solving, are open research\nquestions. In this work, we present MATHSENSEI, a tool-augmented large language\nmodel for mathematical reasoning. Augmented with tools for knowledge retrieval\n(Bing Web Search), program execution (Python), and symbolic equation solving\n(Wolfram-Alpha), we study the complimentary benefits of these tools through\nevaluations on mathematical reasoning datasets. We perform exhaustive ablations\non MATH,a popular dataset for evaluating mathematical reasoning on diverse\nmathematical disciplines. We also conduct experiments involving well-known tool\nplanners to study the impact of tool sequencing on the model performance.\nMATHSENSEI achieves 13.5% better accuracy over gpt-3.5-turbo with\nchain-of-thought on the MATH dataset. We further observe that TALMs are not as\neffective for simpler math word problems (in GSM-8k), and the benefit increases\nas the complexity and required knowledge increases (progressively over AQuA,\nMMLU-Math, and higher level complex questions in MATH). The code and data are\navailable at https://github.com/Debrup-61/MathSensei.\n","authors":["Debrup Das","Debopriyo Banerjee","Somak Aditya","Ashish Kulkarni"],"pdf_url":"https://arxiv.org/pdf/2402.17231v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11462v4","updated":"2024-02-27T05:42:31Z","published":"2023-12-18T18:59:46Z","title":"Cascade Speculative Drafting for Even Faster LLM Inference","summary":"  Introduced to enhance the efficiency of large language model (LLM) inference,\nspeculative decoding operates by having a smaller model generate a draft. A\nlarger target model then reviews this draft to align with its output, and any\nacceptance by the target model results in a reduction of the number of the\ntarget model runs, ultimately improving efficiency. However, the drafting\nprocess in speculative decoding includes slow autoregressive generation and\nallocates equal time to generating tokens, irrespective of their importance.\nThese inefficiencies collectively contribute to the suboptimal performance of\nspeculative decoding. To further improve LLM inference, we introduce Cascade\nSpeculative Drafting (CS Drafting), a speculative execution algorithm that\nincorporates two types of cascades. The Vertical Cascade eliminates\nautoregressive generation from neural models, while the Horizontal Cascade\noptimizes time allocation in drafting for improved efficiency. Combining both\ncascades, CS Drafting achieves up to an 81 percent additional speedup over\nspeculative decoding in our experiments, while maintaining the same output\ndistribution as the target model. Our code is publicly available at\nhttps://github.com/lfsszd/CS-Drafting.\n","authors":["Ziyi Chen","Xiaocong Yang","Jiacheng Lin","Chenkai Sun","Kevin Chen-Chuan Chang","Jie Huang"],"pdf_url":"https://arxiv.org/pdf/2312.11462v4.pdf","comment":"Preprint in progress"},{"id":"http://arxiv.org/abs/2402.17226v1","updated":"2024-02-27T05:37:10Z","published":"2024-02-27T05:37:10Z","title":"Reasoning in Conversation: Solving Subjective Tasks through Dialogue\n  Simulation for Large Language Models","summary":"  Large Language Models (LLMs) have achieved remarkable performance in\nobjective tasks such as open-domain question answering and mathematical\nreasoning, which can often be solved through recalling learned factual\nknowledge or chain-of-thought style reasoning. However, we find that the\nperformance of LLMs in subjective tasks is still unsatisfactory, such as\nmetaphor recognition, dark humor detection, etc. Compared to objective tasks,\nsubjective tasks focus more on interpretation or emotional response rather than\na universally accepted reasoning pathway. Based on the characteristics of the\ntasks and the strong dialogue-generation capabilities of LLMs, we propose RiC\n(Reasoning in Conversation), a method that focuses on solving subjective tasks\nthrough dialogue simulation. The motivation of RiC is to mine useful contextual\ninformation by simulating dialogues instead of supplying chain-of-thought style\nrationales, thereby offering potential useful knowledge behind dialogues for\ngiving the final answers. We evaluate both API-based and open-source LLMs\nincluding GPT-4, ChatGPT, and OpenChat across twelve tasks. Experimental\nresults show that RiC can yield significant improvement compared with various\nbaselines.\n","authors":["Xiaolong Wang","Yile Wang","Yuanchi Zhang","Fuwen Luo","Peng Li","Maosong Sun","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2402.17226v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15159v2","updated":"2024-02-27T05:23:35Z","published":"2024-02-23T07:43:26Z","title":"Machine Unlearning of Pre-trained Large Language Models","summary":"  This study investigates the concept of the `right to be forgotten' within the\ncontext of large language models (LLMs). We explore machine unlearning as a\npivotal solution, with a focus on pre-trained models--a notably\nunder-researched area. Our research delineates a comprehensive framework for\nmachine unlearning in pre-trained LLMs, encompassing a critical analysis of\nseven diverse unlearning methods. Through rigorous evaluation using curated\ndatasets from arXiv, books, and GitHub, we establish a robust benchmark for\nunlearning performance, demonstrating that these methods are over $10^5$ times\nmore computationally efficient than retraining. Our results show that\nintegrating gradient ascent with gradient descent on in-distribution data\nimproves hyperparameter robustness. We also provide detailed guidelines for\nefficient hyperparameter tuning in the unlearning process. Our findings advance\nthe discourse on ethical AI practices, offering substantive insights into the\nmechanics of machine unlearning for pre-trained LLMs and underscoring the\npotential for responsible AI development.\n","authors":["Jin Yao","Eli Chien","Minxin Du","Xinyao Niu","Tianhao Wang","Zezhou Cheng","Xiang Yue"],"pdf_url":"https://arxiv.org/pdf/2402.15159v2.pdf","comment":"Code is available at https://github.com/yaojin17/Unlearning_LLM"},{"id":"http://arxiv.org/abs/2402.17205v1","updated":"2024-02-27T04:55:03Z","published":"2024-02-27T04:55:03Z","title":"Measuring Vision-Language STEM Skills of Neural Models","summary":"  We introduce a new challenge to test the STEM skills of neural models. The\nproblems in the real world often require solutions, combining knowledge from\nSTEM (science, technology, engineering, and math). Unlike existing datasets,\nour dataset requires the understanding of multimodal vision-language\ninformation of STEM. Our dataset features one of the largest and most\ncomprehensive datasets for the challenge. It includes 448 skills and 1,073,146\nquestions spanning all STEM subjects. Compared to existing datasets that often\nfocus on examining expert-level ability, our dataset includes fundamental\nskills and questions designed based on the K-12 curriculum. We also add\nstate-of-the-art foundation models such as CLIP and GPT-3.5-Turbo to our\nbenchmark. Results show that the recent model advances only help master a very\nlimited number of lower grade-level skills (2.5% in the third grade) in our\ndataset. In fact, these models are still well below (averaging 54.7%) the\nperformance of elementary students, not to mention near expert-level\nperformance. To understand and increase the performance on our dataset, we\nteach the models on a training split of our dataset. Even though we observe\nimproved performance, the model performance remains relatively low compared to\naverage elementary students. To solve STEM problems, we will need novel\nalgorithmic innovations from the community.\n","authors":["Jianhao Shen","Ye Yuan","Srbuhi Mirzoyan","Ming Zhang","Chenguang Wang"],"pdf_url":"https://arxiv.org/pdf/2402.17205v1.pdf","comment":"Accepted in ICLR 2024"},{"id":"http://arxiv.org/abs/2402.16107v2","updated":"2024-02-27T04:48:36Z","published":"2024-02-25T15:11:58Z","title":"FuseChat: Knowledge Fusion of Chat Models","summary":"  While training large language models (LLMs) from scratch can indeed lead to\nmodels with distinct capabilities and strengths, this approach incurs\nsubstantial costs and may lead to potential redundancy in competencies. An\nalternative strategy is to combine existing LLMs into a more robust LLM,\nthereby diminishing the necessity for expensive pre-training. However, due to\nthe diverse architectures of LLMs, direct parameter blending proves to be\nunfeasible. Recently, \\textsc{FuseLLM} introduced the concept of knowledge\nfusion to transfer the collective knowledge of multiple structurally varied\nLLMs into a target LLM through lightweight continual training. In this report,\nwe extend the scalability and flexibility of the \\textsc{FuseLLM} framework to\nrealize the fusion of chat LLMs, resulting in \\textsc{FuseChat}.\n\\textsc{FuseChat} comprises two main stages. Firstly, we undertake knowledge\nfusion for structurally and scale-varied source LLMs to derive multiple target\nLLMs of identical structure and size via lightweight fine-tuning. Then, these\ntarget LLMs are merged within the parameter space, wherein we propose a novel\nmethod for determining the merging weights based on the variation ratio of\nparameter matrices before and after fine-tuning. We validate our approach using\nthree prominent chat LLMs with diverse architectures and scales, namely\n\\texttt{NH2-Mixtral-8x7B}, \\texttt{NH2-Solar-10.7B}, and\n\\texttt{OpenChat-3.5-7B}. Experimental results spanning various chat domains\ndemonstrate the superiority of \\texttt{\\textsc{FuseChat}-7B} across a broad\nspectrum of chat LLMs at 7B and 34B scales, even surpassing \\texttt{GPT-3.5\n(March)} and approaching \\texttt{Mixtral-8x7B-Instruct}. Our code, model\nweights, and data are openly accessible at\n\\url{https://github.com/fanqiwan/FuseLLM}.\n","authors":["Fanqi Wan","Ziyi Yang","Longguang Zhong","Xiaojun Quan","Xinting Huang","Wei Bi"],"pdf_url":"https://arxiv.org/pdf/2402.16107v2.pdf","comment":"Technical Report, work in progress"},{"id":"http://arxiv.org/abs/2402.04249v2","updated":"2024-02-27T04:43:08Z","published":"2024-02-06T18:59:08Z","title":"HarmBench: A Standardized Evaluation Framework for Automated Red Teaming\n  and Robust Refusal","summary":"  Automated red teaming holds substantial promise for uncovering and mitigating\nthe risks associated with the malicious use of large language models (LLMs),\nyet the field lacks a standardized evaluation framework to rigorously assess\nnew methods. To address this issue, we introduce HarmBench, a standardized\nevaluation framework for automated red teaming. We identify several desirable\nproperties previously unaccounted for in red teaming evaluations and\nsystematically design HarmBench to meet these criteria. Using HarmBench, we\nconduct a large-scale comparison of 18 red teaming methods and 33 target LLMs\nand defenses, yielding novel insights. We also introduce a highly efficient\nadversarial training method that greatly enhances LLM robustness across a wide\nrange of attacks, demonstrating how HarmBench enables codevelopment of attacks\nand defenses. We open source HarmBench at\nhttps://github.com/centerforaisafety/HarmBench.\n","authors":["Mantas Mazeika","Long Phan","Xuwang Yin","Andy Zou","Zifan Wang","Norman Mu","Elham Sakhaee","Nathaniel Li","Steven Basart","Bo Li","David Forsyth","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2402.04249v2.pdf","comment":"Website: https://www.harmbench.org"},{"id":"http://arxiv.org/abs/2402.17193v1","updated":"2024-02-27T04:18:49Z","published":"2024-02-27T04:18:49Z","title":"When Scaling Meets LLM Finetuning: The Effect of Data, Model and\n  Finetuning Method","summary":"  While large language models (LLMs) often adopt finetuning to unlock their\ncapabilities for downstream applications, our understanding on the inductive\nbiases (especially the scaling properties) of different finetuning methods is\nstill limited. To fill this gap, we conduct systematic experiments studying\nwhether and how different scaling factors, including LLM model size,\npretraining data size, new finetuning parameter size and finetuning data size,\naffect the finetuning performance. We consider two types of finetuning --\nfull-model tuning (FMT) and parameter efficient tuning (PET, including prompt\ntuning and LoRA), and explore their scaling behaviors in the data-limited\nregime where the LLM model size substantially outweighs the finetuning data\nsize. Based on two sets of pretrained bilingual LLMs from 1B to 16B and\nexperiments on bilingual machine translation and multilingual summarization\nbenchmarks, we find that 1) LLM finetuning follows a powerbased multiplicative\njoint scaling law between finetuning data size and each other scaling factor;\n2) LLM finetuning benefits more from LLM model scaling than pretraining data\nscaling, and PET parameter scaling is generally ineffective; and 3) the optimal\nfinetuning method is highly task- and finetuning data-dependent. We hope our\nfindings could shed light on understanding, selecting and developing LLM\nfinetuning methods.\n","authors":["Biao Zhang","Zhongtao Liu","Colin Cherry","Orhan Firat"],"pdf_url":"https://arxiv.org/pdf/2402.17193v1.pdf","comment":"ICLR24"},{"id":"http://arxiv.org/abs/2402.17189v1","updated":"2024-02-27T04:08:59Z","published":"2024-02-27T04:08:59Z","title":"An Effective Mixture-Of-Experts Approach For Code-Switching Speech\n  Recognition Leveraging Encoder Disentanglement","summary":"  With the massive developments of end-to-end (E2E) neural networks, recent\nyears have witnessed unprecedented breakthroughs in automatic speech\nrecognition (ASR). However, the codeswitching phenomenon remains a major\nobstacle that hinders ASR from perfection, as the lack of labeled data and the\nvariations between languages often lead to degradation of ASR performance. In\nthis paper, we focus exclusively on improving the acoustic encoder of E2E ASR\nto tackle the challenge caused by the codeswitching phenomenon. Our main\ncontributions are threefold: First, we introduce a novel disentanglement loss\nto enable the lower-layer of the encoder to capture inter-lingual acoustic\ninformation while mitigating linguistic confusion at the higher-layer of the\nencoder. Second, through comprehensive experiments, we verify that our proposed\nmethod outperforms the prior-art methods using pretrained dual-encoders,\nmeanwhile having access only to the codeswitching corpus and consuming half of\nthe parameterization. Third, the apparent differentiation of the encoders'\noutput features also corroborates the complementarity between the\ndisentanglement loss and the mixture-of-experts (MoE) architecture.\n","authors":["Tzu-Ting Yang","Hsin-Wei Wang","Yi-Cheng Wang","Chi-Han Lin","Berlin Chen"],"pdf_url":"https://arxiv.org/pdf/2402.17189v1.pdf","comment":"ICASSP 2024"},{"id":"http://arxiv.org/abs/2402.17184v1","updated":"2024-02-27T03:40:44Z","published":"2024-02-27T03:40:44Z","title":"Extreme Encoder Output Frame Rate Reduction: Improving Computational\n  Latencies of Large End-to-End Models","summary":"  The accuracy of end-to-end (E2E) automatic speech recognition (ASR) models\ncontinues to improve as they are scaled to larger sizes, with some now reaching\nbillions of parameters. Widespread deployment and adoption of these models,\nhowever, requires computationally efficient strategies for decoding. In the\npresent work, we study one such strategy: applying multiple frame reduction\nlayers in the encoder to compress encoder outputs into a small number of output\nframes. While similar techniques have been investigated in previous work, we\nachieve dramatically more reduction than has previously been demonstrated\nthrough the use of multiple funnel reduction layers. Through ablations, we\nstudy the impact of various architectural choices in the encoder to identify\nthe most effective strategies. We demonstrate that we can generate one encoder\noutput frame for every 2.56 sec of input speech, without significantly\naffecting word error rate on a large-scale voice search task, while improving\nencoder and decoder latencies by 48% and 92% respectively, relative to a strong\nbut computationally expensive baseline.\n","authors":["Rohit Prabhavalkar","Zhong Meng","Weiran Wang","Adam Stooke","Xingyu Cai","Yanzhang He","Arun Narayanan","Dongseong Hwang","Tara N. Sainath","Pedro J. Moreno"],"pdf_url":"https://arxiv.org/pdf/2402.17184v1.pdf","comment":"Accepted to 2024 IEEE International Conference on Acoustics, Speech,\n  and Signal Processing (ICASSP 2024)"},{"id":"http://arxiv.org/abs/2311.09675v2","updated":"2024-02-27T03:37:03Z","published":"2023-11-16T08:42:26Z","title":"Where Do People Tell Stories Online? Story Detection Across Online\n  Communities","summary":"  Story detection in online communities is a challenging task as stories are\nscattered across communities and interwoven with non-storytelling spans within\na single text. We address this challenge by building and releasing the\nStorySeeker toolkit, including a richly annotated dataset of 502 Reddit posts\nand comments, a detailed codebook adapted to the social media context, and\nmodels to predict storytelling at the document and span level. Our dataset is\nsampled from hundreds of popular English-language Reddit communities ranging\nacross 33 topic categories, and it contains fine-grained expert annotations,\nincluding binary story labels, story spans, and event spans. We evaluate a\nrange of detection methods using our data, and we identify the distinctive\ntextual features of online storytelling, focusing on storytelling span\ndetection, which we introduce as a new task. We illuminate distributional\ncharacteristics of storytelling on a large community-centric social media\nplatform, and we also conduct a case study on r/ChangeMyView, where\nstorytelling is used as one of many persuasive strategies, illustrating that\nour data and models can be used for both inter- and intra-community research.\nFinally, we discuss implications of our tools and analyses for narratology and\nthe study of online communities.\n","authors":["Maria Antoniak","Joel Mire","Maarten Sap","Elliott Ash","Andrew Piper"],"pdf_url":"https://arxiv.org/pdf/2311.09675v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12026v2","updated":"2024-02-27T03:22:49Z","published":"2024-02-19T10:34:48Z","title":"Acquiring Clean Language Models from Backdoor Poisoned Datasets by\n  Downscaling Frequency Space","summary":"  Despite the notable success of language models (LMs) in various natural\nlanguage processing (NLP) tasks, the reliability of LMs is susceptible to\nbackdoor attacks. Prior research attempts to mitigate backdoor learning while\ntraining the LMs on the poisoned dataset, yet struggles against complex\nbackdoor attacks in real-world scenarios. In this paper, we investigate the\nlearning mechanisms of backdoor LMs in the frequency space by Fourier analysis.\nOur findings indicate that the backdoor mapping presented on the poisoned\ndatasets exhibits a more discernible inclination towards lower frequency\ncompared to clean mapping, resulting in the faster convergence of backdoor\nmapping. To alleviate this dilemma, we propose Multi-Scale Low-Rank Adaptation\n(MuScleLoRA), which deploys multiple radial scalings in the frequency space\nwith low-rank adaptation to the target model and further aligns the gradients\nwhen updating parameters. Through downscaling in the frequency space,\nMuScleLoRA encourages the model to prioritize the learning of relatively\nhigh-frequency clean mapping, consequently mitigating backdoor learning.\nExperimental results demonstrate that MuScleLoRA outperforms baselines\nsignificantly. Notably, MuScleLoRA reduces the average success rate of diverse\nbackdoor attacks to below 15\\% across multiple datasets and generalizes to\nvarious backbone LMs, including BERT, RoBERTa, and Llama2. The codes are\navailable at https://github.com/ZrW00/MuScleLoRA.\n","authors":["Zongru Wu","Zhuosheng Zhang","Pengzhou Cheng","Gongshen Liu"],"pdf_url":"https://arxiv.org/pdf/2402.12026v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17168v1","updated":"2024-02-27T03:03:06Z","published":"2024-02-27T03:03:06Z","title":"Benchmarking Data Science Agents","summary":"  In the era of data-driven decision-making, the complexity of data analysis\nnecessitates advanced expertise and tools of data science, presenting\nsignificant challenges even for specialists. Large Language Models (LLMs) have\nemerged as promising aids as data science agents, assisting humans in data\nanalysis and processing. Yet their practical efficacy remains constrained by\nthe varied demands of real-world applications and complicated analytical\nprocess. In this paper, we introduce DSEval -- a novel evaluation paradigm, as\nwell as a series of innovative benchmarks tailored for assessing the\nperformance of these agents throughout the entire data science lifecycle.\nIncorporating a novel bootstrapped annotation method, we streamline dataset\npreparation, improve the evaluation coverage, and expand benchmarking\ncomprehensiveness. Our findings uncover prevalent obstacles and provide\ncritical insights to inform future advancements in the field.\n","authors":["Yuge Zhang","Qiyang Jiang","Xingyu Han","Nan Chen","Yuqing Yang","Kan Ren"],"pdf_url":"https://arxiv.org/pdf/2402.17168v1.pdf","comment":"Source code and data are available at\n  https://github.com/MetaCopilot/dseval"},{"id":"http://arxiv.org/abs/2305.12077v2","updated":"2024-02-27T02:51:16Z","published":"2023-05-20T03:32:48Z","title":"Few-Shot Dialogue Summarization via Skeleton-Assisted Prompt Transfer in\n  Prompt Tuning","summary":"  In real-world scenarios, labeled samples for dialogue summarization are\nusually limited (i.e., few-shot) due to high annotation costs for high-quality\ndialogue summaries. To efficiently learn from few-shot samples, previous works\nhave utilized massive annotated data from other downstream tasks and then\nperformed prompt transfer in prompt tuning so as to enable cross-task knowledge\ntransfer. However, existing general-purpose prompt transfer techniques lack\nconsideration for dialogue-specific information. In this paper, we focus on\nimproving the prompt transfer from dialogue state tracking to dialogue\nsummarization and propose Skeleton-Assisted Prompt Transfer (SAPT), which\nleverages skeleton generation as extra supervision that functions as a medium\nconnecting the distinct source and target task and resulting in the model's\nbetter consumption of dialogue state information. To automatically extract\ndialogue skeletons as supervised training data for skeleton generation, we\ndesign a novel approach with perturbation-based probes requiring neither\nannotation effort nor domain knowledge. Training the model on such skeletons\ncan also help preserve model capability during prompt transfer. Our method\nsignificantly outperforms existing baselines. In-depth analyses demonstrate the\neffectiveness of our method in facilitating cross-task knowledge transfer in\nfew-shot dialogue summarization.\n","authors":["Kaige Xie","Tong Yu","Haoliang Wang","Junda Wu","Handong Zhao","Ruiyi Zhang","Kanak Mahadik","Ani Nenkova","Mark Riedl"],"pdf_url":"https://arxiv.org/pdf/2305.12077v2.pdf","comment":"Accepted to EACL 2024"},{"id":"http://arxiv.org/abs/2402.17151v1","updated":"2024-02-27T02:36:43Z","published":"2024-02-27T02:36:43Z","title":"Clustering Document Parts: Detecting and Characterizing Influence\n  Campaigns From Documents","summary":"  We propose a novel clustering pipeline to detect and characterize influence\ncampaigns from documents. This approach clusters parts of document, detects\nclusters that likely reflect an influence campaign, and then identifies\ndocuments linked to an influence campaign via their association with the\nhigh-influence clusters. Our approach outperforms both the direct\ndocument-level classification and the direct document-level clustering approach\nin predicting if a document is part of an influence campaign. We propose\nvarious novel techniques to enhance our pipeline, including using an existing\nevent factuality prediction system to obtain document parts, and aggregating\nmultiple clustering experiments to improve the performance of both cluster and\ndocument classification. Classifying documents on the top of clustering not\nonly accurately extracts the parts of the documents that are relevant to\ninfluence campaigns, but also capture influence campaigns as a coordinated and\nholistic phenomenon. Our approach makes possible more fine-grained and\ninterpretable characterizations of influence campaigns from documents.\n","authors":["Zhengxiang Wang","Owen Rambow"],"pdf_url":"https://arxiv.org/pdf/2402.17151v1.pdf","comment":"12 pages, 2 figures, 5 tables"},{"id":"http://arxiv.org/abs/2402.15180v2","updated":"2024-02-27T01:39:20Z","published":"2024-02-23T08:22:24Z","title":"Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks\n  with Self-Refinement","summary":"  Caution: This paper includes offensive words that could potentially cause\nunpleasantness. Language models (LMs) are vulnerable to exploitation for\nadversarial misuse. Training LMs for safety alignment is extensive and makes it\nhard to respond to fast-developing attacks immediately, such as jailbreaks. We\npropose self-refine with formatting that achieves outstanding safety even in\nnon-safety-aligned LMs and evaluate our method alongside several defense\nbaselines, demonstrating that it is the safest training-free method against\njailbreak attacks. Additionally, we proposed a formatting method that improves\nthe efficiency of the self-refine process while reducing attack success rates\nin fewer iterations. We've also observed that non-safety-aligned LMs outperform\nsafety-aligned LMs in safety tasks by giving more helpful and safe responses.\nIn conclusion, our findings can achieve less safety risk with fewer\ncomputational costs, allowing non-safety LM to be easily utilized in real-world\nservice.\n","authors":["Heegyu Kim","Sehyun Yuk","Hyunsouk Cho"],"pdf_url":"https://arxiv.org/pdf/2402.15180v2.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2402.17124v1","updated":"2024-02-27T01:37:23Z","published":"2024-02-27T01:37:23Z","title":"Fact-and-Reflection (FaR) Improves Confidence Calibration of Large\n  Language Models","summary":"  For a LLM to be trustworthy, its confidence level should be well-calibrated\nwith its actual performance. While it is now common sense that LLM performances\nare greatly impacted by prompts, the confidence calibration in prompting LLMs\nhas yet to be thoroughly explored. In this paper, we explore how different\nprompting strategies influence LLM confidence calibration and how it could be\nimproved. We conduct extensive experiments on six prompting methods in the\nquestion-answering context and we observe that, while these methods help\nimprove the expected LLM calibration, they also trigger LLMs to be\nover-confident when responding to some instances. Inspired by human cognition,\nwe propose Fact-and-Reflection (FaR) prompting, which improves the LLM\ncalibration in two steps. First, FaR elicits the known \"facts\" that are\nrelevant to the input prompt from the LLM. And then it asks the model to\n\"reflect\" over them to generate the final answer. Experiments show that FaR\nprompting achieves significantly better calibration; it lowers the Expected\nCalibration Error by 23.5% on our multi-purpose QA tasks. Notably, FaR\nprompting even elicits the capability of verbally expressing concerns in less\nconfident scenarios, which helps trigger retrieval augmentation for solving\nthese harder instances.\n","authors":["Xinran Zhao","Hongming Zhang","Xiaoman Pan","Wenlin Yao","Dong Yu","Tongshuang Wu","Jianshu Chen"],"pdf_url":"https://arxiv.org/pdf/2402.17124v1.pdf","comment":"17 pages, 10 figures"},{"id":"http://arxiv.org/abs/2402.17119v1","updated":"2024-02-27T01:25:52Z","published":"2024-02-27T01:25:52Z","title":"Creating Suspenseful Stories: Iterative Planning with Large Language\n  Models","summary":"  Automated story generation has been one of the long-standing challenges in\nNLP. Among all dimensions of stories, suspense is very common in human-written\nstories but relatively under-explored in AI-generated stories. While recent\nadvances in large language models (LLMs) have greatly promoted language\ngeneration in general, state-of-the-art LLMs are still unreliable when it comes\nto suspenseful story generation. We propose a novel iterative-prompting-based\nplanning method that is grounded in two theoretical foundations of story\nsuspense from cognitive psychology and narratology. This theory-grounded method\nworks in a fully zero-shot manner and does not rely on any supervised story\ncorpora. To the best of our knowledge, this paper is the first attempt at\nsuspenseful story generation with LLMs. Extensive human evaluations of the\ngenerated suspenseful stories demonstrate the effectiveness of our method.\n","authors":["Kaige Xie","Mark Riedl"],"pdf_url":"https://arxiv.org/pdf/2402.17119v1.pdf","comment":"Accepted to EACL 2024"},{"id":"http://arxiv.org/abs/2402.17097v1","updated":"2024-02-27T00:22:18Z","published":"2024-02-27T00:22:18Z","title":"Re-Ex: Revising after Explanation Reduces the Factual Errors in LLM\n  Responses","summary":"  Mitigating hallucination issues is one of the main challenges of LLMs we need\nto overcome, in order to reliably use them in real-world scenarios. Recently,\nvarious methods are proposed to check the factual errors in the LLM-generated\ntexts and revise them accordingly, to reduce the hallucination issue. In this\npaper, we propose Re-Ex, a method of revising LLM-generated texts, which\nintroduces a novel step dubbed as the factual error explanation step. Re-Ex\nrevises the initial response of LLMs using 3-steps: first, external tools are\nused to get the evidences on the factual errors in the response; second, LLMs\nare instructed to explain the problematic parts of the response based on the\nevidences gathered in the first step; finally, LLMs revise the response using\nthe explanation obtained in the second step. In addition to the explanation\nstep, we propose new prompting techniques to reduce the amount of tokens and\nwall-clock time required for the response revision process. Compared with\nexisting methods including Factool, CoVE, and RARR, Re-Ex provides better\nrevision performance with less time and fewer tokens in multiple benchmarks.\n","authors":["Juyeon Kim","Jeongeun Lee","Yoonho Chang","Chanyeol Choi","Junseong Kim","Jy-yong Sohn"],"pdf_url":"https://arxiv.org/pdf/2402.17097v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2109.02473v4","updated":"2024-02-27T00:18:22Z","published":"2021-08-30T00:48:48Z","title":"A Robust Cybersecurity Topic Classification Tool","summary":"  In this research, we use user defined labels from three internet text sources\n(Reddit, Stackexchange, Arxiv) to train 21 different machine learning models\nfor the topic classification task of detecting cybersecurity discussions in\nnatural text. We analyze the false positive and false negative rates of each of\nthe 21 model's in a cross validation experiment. Then we present a\nCybersecurity Topic Classification (CTC) tool, which takes the majority vote of\nthe 21 trained machine learning models as the decision mechanism for detecting\ncybersecurity related text. We also show that the majority vote mechanism of\nthe CTC tool provides lower false negative and false positive rates on average\nthan any of the 21 individual models. We show that the CTC tool is scalable to\nthe hundreds of thousands of documents with a wall clock time on the order of\nhours.\n","authors":["Elijah Pelofske","Lorie M. Liebrock","Vincent Urias"],"pdf_url":"https://arxiv.org/pdf/2109.02473v4.pdf","comment":"Improved formatting"},{"id":"http://arxiv.org/abs/2402.03299v2","updated":"2024-02-27T00:09:00Z","published":"2024-02-05T18:54:43Z","title":"GUARD: Role-playing to Generate Natural-language Jailbreakings to Test\n  Guideline Adherence of Large Language Models","summary":"  The discovery of \"jailbreaks\" to bypass safety filters of Large Language\nModels (LLMs) and harmful responses have encouraged the community to implement\nsafety measures. One major safety measure is to proactively test the LLMs with\njailbreaks prior to the release. Therefore, such testing will require a method\nthat can generate jailbreaks massively and efficiently. In this paper, we\nfollow a novel yet intuitive strategy to generate jailbreaks in the style of\nthe human generation. We propose a role-playing system that assigns four\ndifferent roles to the user LLMs to collaborate on new jailbreaks. Furthermore,\nwe collect existing jailbreaks and split them into different independent\ncharacteristics using clustering frequency and semantic patterns sentence by\nsentence. We organize these characteristics into a knowledge graph, making them\nmore accessible and easier to retrieve. Our system of different roles will\nleverage this knowledge graph to generate new jailbreaks, which have proved\neffective in inducing LLMs to generate unethical or guideline-violating\nresponses. In addition, we also pioneer a setting in our system that will\nautomatically follow the government-issued guidelines to generate jailbreaks to\ntest whether LLMs follow the guidelines accordingly. We refer to our system as\nGUARD (Guideline Upholding through Adaptive Role-play Diagnostics). We have\nempirically validated the effectiveness of GUARD on three cutting-edge\nopen-sourced LLMs (Vicuna-13B, LongChat-7B, and Llama-2-7B), as well as a\nwidely-utilized commercial LLM (ChatGPT). Moreover, our work extends to the\nrealm of vision language models (MiniGPT-v2 and Gemini Vision Pro), showcasing\nGUARD's versatility and contributing valuable insights for the development of\nsafer, more reliable LLM-based applications across diverse modalities.\n","authors":["Haibo Jin","Ruoxi Chen","Andy Zhou","Jinyin Chen","Yang Zhang","Haohan Wang"],"pdf_url":"https://arxiv.org/pdf/2402.03299v2.pdf","comment":"22 papges"},{"id":"http://arxiv.org/abs/2402.08496v3","updated":"2024-02-27T00:05:28Z","published":"2024-02-13T14:51:45Z","title":"A Systematic Review of Data-to-Text NLG","summary":"  This systematic review undertakes a comprehensive analysis of current\nresearch on data-to-text generation, identifying gaps, challenges, and future\ndirections within the field. Relevant literature in this field on datasets,\nevaluation metrics, application areas, multilingualism, language models, and\nhallucination mitigation methods is reviewed. Various methods for producing\nhigh-quality text are explored, addressing the challenge of hallucinations in\ndata-to-text generation. These methods include re-ranking, traditional and\nneural pipeline architecture, planning architectures, data cleaning, controlled\ngeneration, and modification of models and training techniques. Their\neffectiveness and limitations are assessed, highlighting the need for\nuniversally applicable strategies to mitigate hallucinations. The review also\nexamines the usage, popularity, and impact of datasets, alongside evaluation\nmetrics, with an emphasis on both automatic and human assessment. Additionally,\nthe evolution of data-to-text models, particularly the widespread adoption of\ntransformer models, is discussed. Despite advancements in text quality, the\nreview emphasizes the importance of research in low-resourced languages and the\nengineering of datasets in these languages to promote inclusivity. Finally,\nseveral application domains of data-to-text are highlighted, emphasizing their\nrelevance in such domains. Overall, this review serves as a guiding framework\nfor fostering innovation and advancing data-to-text generation.\n","authors":["Chinonso Cynthia Osuji","Thiago Castro Ferreira","Brian Davis"],"pdf_url":"https://arxiv.org/pdf/2402.08496v3.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2402.17768v1","updated":"2024-02-27T18:59:18Z","published":"2024-02-27T18:59:18Z","title":"Diffusion Meets DAgger: Supercharging Eye-in-hand Imitation Learning","summary":"  A common failure mode for policies trained with imitation is compounding\nexecution errors at test time. When the learned policy encounters states that\nwere not present in the expert demonstrations, the policy fails, leading to\ndegenerate behavior. The Dataset Aggregation, or DAgger approach to this\nproblem simply collects more data to cover these failure states. However, in\npractice, this is often prohibitively expensive. In this work, we propose\nDiffusion Meets DAgger (DMD), a method to reap the benefits of DAgger without\nthe cost for eye-in-hand imitation learning problems. Instead of collecting new\nsamples to cover out-of-distribution states, DMD uses recent advances in\ndiffusion models to create these samples with diffusion models. This leads to\nrobust performance from few demonstrations. In experiments conducted for\nnon-prehensile pushing on a Franka Research 3, we show that DMD can achieve a\nsuccess rate of 80% with as few as 8 expert demonstrations, where naive\nbehavior cloning reaches only 20%. DMD also outperform competing NeRF-based\naugmentation schemes by 50%.\n","authors":["Xiaoyu Zhang","Matthew Chang","Pranav Kumar","Saurabh Gupta"],"pdf_url":"https://arxiv.org/pdf/2402.17768v1.pdf","comment":"for project website with video, see\n  https://sites.google.com/view/diffusion-meets-dagger"},{"id":"http://arxiv.org/abs/2308.00566v2","updated":"2024-02-27T18:59:14Z","published":"2023-07-31T17:59:08Z","title":"Stochastic positional embeddings improve masked image modeling","summary":"  Masked Image Modeling (MIM) is a promising self-supervised learning approach\nthat enables learning from unlabeled images. Despite its recent success,\nlearning good representations through MIM remains challenging because it\nrequires predicting the right semantic content in accurate locations. For\nexample, given an incomplete picture of a dog, we can guess that there is a\ntail, but we cannot determine its exact location. In this work, we propose to\nincorporate location uncertainty into MIM by using stochastic positional\nembeddings (StoP). Specifically, we condition the model on stochastic masked\ntoken positions drawn from a Gaussian distribution. StoP reduces overfitting to\nlocation features and guides the model toward learning features that are more\nrobust to location uncertainties. Quantitatively, StoP improves downstream MIM\nperformance on a variety of downstream tasks, including $+1.7\\%$ on ImageNet\nlinear probing using ViT-B, and $+2.5\\%$ for ViT-H using $1\\%$ of the data.\n","authors":["Amir Bar","Florian Bordes","Assaf Shocher","Mahmoud Assran","Pascal Vincent","Nicolas Ballas","Trevor Darrell","Amir Globerson","Yann LeCun"],"pdf_url":"https://arxiv.org/pdf/2308.00566v2.pdf","comment":"Code and models available in https://github.com/amirbar/StoP"},{"id":"http://arxiv.org/abs/2402.17767v1","updated":"2024-02-27T18:58:54Z","published":"2024-02-27T18:58:54Z","title":"Opening Cabinets and Drawers in the Real World using a Commodity Mobile\n  Manipulator","summary":"  Pulling open cabinets and drawers presents many difficult technical\nchallenges in perception (inferring articulation parameters for objects from\nonboard sensors), planning (producing motion plans that conform to tight task\nconstraints), and control (making and maintaining contact while applying forces\non the environment). In this work, we build an end-to-end system that enables a\ncommodity mobile manipulator (Stretch RE2) to pull open cabinets and drawers in\ndiverse previously unseen real world environments. We conduct 4 days of real\nworld testing of this system spanning 31 different objects from across 13\ndifferent real world environments. Our system achieves a success rate of 61% on\nopening novel cabinets and drawers in unseen environments zero-shot. An\nanalysis of the failure modes suggests that errors in perception are the most\nsignificant challenge for our system. We will open source code and models for\nothers to replicate and build upon our system.\n","authors":["Arjun Gupta","Michelle Zhang","Rishik Sathua","Saurabh Gupta"],"pdf_url":"https://arxiv.org/pdf/2402.17767v1.pdf","comment":"Project webpage:\n  https://arjung128.github.io/opening-cabinets-and-drawers"},{"id":"http://arxiv.org/abs/2402.17766v1","updated":"2024-02-27T18:57:12Z","published":"2024-02-27T18:57:12Z","title":"ShapeLLM: Universal 3D Object Understanding for Embodied Interaction","summary":"  This paper presents ShapeLLM, the first 3D Multimodal Large Language Model\n(LLM) designed for embodied interaction, exploring a universal 3D object\nunderstanding with 3D point clouds and languages. ShapeLLM is built upon an\nimproved 3D encoder by extending ReCon to ReCon++ that benefits from multi-view\nimage distillation for enhanced geometry understanding. By utilizing ReCon++ as\nthe 3D point cloud input encoder for LLMs, ShapeLLM is trained on constructed\ninstruction-following data and tested on our newly human-curated evaluation\nbenchmark, 3D MM-Vet. ReCon++ and ShapeLLM achieve state-of-the-art performance\nin 3D geometry understanding and language-unified 3D interaction tasks, such as\nembodied visual grounding.\n","authors":["Zekun Qi","Runpei Dong","Shaochen Zhang","Haoran Geng","Chunrui Han","Zheng Ge","Li Yi","Kaisheng Ma"],"pdf_url":"https://arxiv.org/pdf/2402.17766v1.pdf","comment":"Tech report"},{"id":"http://arxiv.org/abs/2402.17758v1","updated":"2024-02-27T18:51:52Z","published":"2024-02-27T18:51:52Z","title":"ADL4D: Towards A Contextually Rich Dataset for 4D Activities of Daily\n  Living","summary":"  Hand-Object Interactions (HOIs) are conditioned on spatial and temporal\ncontexts like surrounding objects, pre- vious actions, and future intents (for\nexample, grasping and handover actions vary greatly based on objects proximity\nand trajectory obstruction). However, existing datasets for 4D HOI (3D HOI over\ntime) are limited to one subject inter- acting with one object only. This\nrestricts the generalization of learning-based HOI methods trained on those\ndatasets. We introduce ADL4D, a dataset of up to two subjects inter- acting\nwith different sets of objects performing Activities of Daily Living (ADL) like\nbreakfast or lunch preparation ac- tivities. The transition between multiple\nobjects to complete a certain task over time introduces a unique context\nlacking in existing datasets. Our dataset consists of 75 sequences with a total\nof 1.1M RGB-D frames, hand and object poses, and per-hand fine-grained action\nannotations. We develop an automatic system for multi-view multi-hand 3D pose\nan- notation capable of tracking hand poses over time. We inte- grate and test\nit against publicly available datasets. Finally, we evaluate our dataset on the\ntasks of Hand Mesh Recov- ery (HMR) and Hand Action Segmentation (HAS).\n","authors":["Marsil Zakour","Partha Pratim Nath","Ludwig Lohmer","Emre Faik Gökçe","Martin Piccolrovazzi","Constantin Patsch","Yuankai Wu","Rahul Chaudhari","Eckehard Steinbach"],"pdf_url":"https://arxiv.org/pdf/2402.17758v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.02189v3","updated":"2024-02-27T18:38:50Z","published":"2023-11-03T18:44:21Z","title":"FairSeg: A Large-Scale Medical Image Segmentation Dataset for Fairness\n  Learning Using Segment Anything Model with Fair Error-Bound Scaling","summary":"  Fairness in artificial intelligence models has gained significantly more\nattention in recent years, especially in the area of medicine, as fairness in\nmedical models is critical to people's well-being and lives. High-quality\nmedical fairness datasets are needed to promote fairness learning research.\nExisting medical fairness datasets are all for classification tasks, and no\nfairness datasets are available for medical segmentation, while medical\nsegmentation is an equally important clinical task as classifications, which\ncan provide detailed spatial information on organ abnormalities ready to be\nassessed by clinicians. In this paper, we propose the first fairness dataset\nfor medical segmentation named Harvard-FairSeg with 10,000 subject samples. In\naddition, we propose a fair error-bound scaling approach to reweight the loss\nfunction with the upper error-bound in each identity group, using the segment\nanything model (SAM). We anticipate that the segmentation performance equity\ncan be improved by explicitly tackling the hard cases with high training errors\nin each identity group. To facilitate fair comparisons, we utilize a novel\nequity-scaled segmentation performance metric to compare segmentation metrics\nin the context of fairness, such as the equity-scaled Dice coefficient. Through\ncomprehensive experiments, we demonstrate that our fair error-bound scaling\napproach either has superior or comparable fairness performance to the\nstate-of-the-art fairness learning models. The dataset and code are publicly\naccessible via https://ophai.hms.harvard.edu/harvard-fairseg10k.\n","authors":["Yu Tian","Min Shi","Yan Luo","Ava Kouhana","Tobias Elze","Mengyu Wang"],"pdf_url":"https://arxiv.org/pdf/2311.02189v3.pdf","comment":"ICLR 2024; Codes available at\n  https://github.com/Harvard-Ophthalmology-AI-Lab/FairSeg"},{"id":"http://arxiv.org/abs/2402.16825v2","updated":"2024-02-27T18:30:07Z","published":"2024-02-26T18:51:15Z","title":"Weighted Monte Carlo augmented spherical Fourier-Bessel convolutional\n  layers for 3D abdominal organ segmentation","summary":"  Filter-decomposition-based group equivariant convolutional neural networks\nshow promising stability and data efficiency for 3D image feature extraction.\nHowever, the existing filter-decomposition-based 3D group equivariant neural\nnetworks rely on parameter-sharing designs and are mostly limited to rotation\ntransform groups, where the chosen spherical harmonic filter bases consider\nonly angular orthogonality. These limitations hamper its application to deep\nneural network architectures for medical image segmentation. To address these\nissues, this paper describes a non-parameter-sharing affine group equivariant\nneural network for 3D medical image segmentation based on an adaptive\naggregation of Monte Carlo augmented spherical Fourier Bessel filter bases. The\nefficiency and flexibility of the adopted non-parameter strategy enable for the\nfirst time an efficient implementation of 3D affine group equivariant\nconvolutional neural networks for volumetric data. The introduced spherical\nBessel Fourier filter basis combines both angular and radial orthogonality for\nbetter feature extraction. The 3D image segmentation experiments on two\nabdominal image sets, BTCV and the NIH Pancreas datasets, show that the\nproposed methods excel the state-of-the-art 3D neural networks with high\ntraining stability and data efficiency. The code will be available at\nhttps://github.com/ZhaoWenzhao/WVMS.\n","authors":["Wenzhao Zhao","Steffen Albert","Barbara D. Wichtmann","Angelika Maurer","Ulrike Attenberger","Frank G. Zöllner","Jürgen Hesser"],"pdf_url":"https://arxiv.org/pdf/2402.16825v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17745v1","updated":"2024-02-27T18:29:07Z","published":"2024-02-27T18:29:07Z","title":"LoDIP: Low light phase retrieval with deep image prior","summary":"  Phase retrieval (PR) is a fundamental challenge in scientific imaging,\nenabling nanoscale techniques like coherent diffractive imaging (CDI). Imaging\nat low radiation doses becomes important in applications where samples are\nsusceptible to radiation damage. However, most PR methods struggle in low dose\nscenario due to the presence of very high shot noise. Advancements in the\noptical data acquisition setup, exemplified by in-situ CDI, have shown\npotential for low-dose imaging. But these depend on a time series of\nmeasurements, rendering them unsuitable for single-image applications.\nSimilarly, on the computational front, data-driven phase retrieval techniques\nare not readily adaptable to the single-image context. Deep learning based\nsingle-image methods, such as deep image prior, have been effective for various\nimaging tasks but have exhibited limited success when applied to PR. In this\nwork, we propose LoDIP which combines the in-situ CDI setup with the power of\nimplicit neural priors to tackle the problem of single-image low-dose phase\nretrieval. Quantitative evaluations demonstrate the superior performance of\nLoDIP on this task as well as applicability to real experimental scenarios.\n","authors":["Raunak Manekar","Elisa Negrini","Minh Pham","Daniel Jacobs","Jaideep Srivastava"],"pdf_url":"https://arxiv.org/pdf/2402.17745v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17744v1","updated":"2024-02-27T18:25:16Z","published":"2024-02-27T18:25:16Z","title":"Analyzing Regional Organization of the Human Hippocampus in 3D-PLI Using\n  Contrastive Learning and Geometric Unfolding","summary":"  Understanding the cortical organization of the human brain requires\ninterpretable descriptors for distinct structural and functional imaging data.\n3D polarized light imaging (3D-PLI) is an imaging modality for visualizing\nfiber architecture in postmortem brains with high resolution that also captures\nthe presence of cell bodies, for example, to identify hippocampal subfields.\nThe rich texture in 3D-PLI images, however, makes this modality particularly\ndifficult to analyze and best practices for characterizing architectonic\npatterns still need to be established. In this work, we demonstrate a novel\nmethod to analyze the regional organization of the human hippocampus in 3D-PLI\nby combining recent advances in unfolding methods with deep texture features\nobtained using a self-supervised contrastive learning approach. We identify\nclusters in the representations that correspond well with classical\ndescriptions of hippocampal subfields, lending validity to the developed\nmethodology.\n","authors":["Alexander Oberstrass","Jordan DeKraker","Nicola Palomero-Gallagher","Sascha E. A. Muenzing","Alan C. Evans","Markus Axer","Katrin Amunts","Timo Dickscheid"],"pdf_url":"https://arxiv.org/pdf/2402.17744v1.pdf","comment":"Accepted to ISBI 2024"},{"id":"http://arxiv.org/abs/2310.04311v2","updated":"2024-02-27T18:10:02Z","published":"2023-10-06T15:17:45Z","title":"Distributed Deep Joint Source-Channel Coding with Decoder-Only Side\n  Information","summary":"  We consider low-latency image transmission over a noisy wireless channel when\ncorrelated side information is present only at the receiver side (the Wyner-Ziv\nscenario). In particular, we are interested in developing practical schemes\nusing a data-driven joint source-channel coding (JSCC) approach, which has been\npreviously shown to outperform conventional separation-based approaches in the\npractical finite blocklength regimes, and to provide graceful degradation with\nchannel quality. We propose a novel neural network architecture that\nincorporates the decoder-only side information at multiple stages at the\nreceiver side. Our results demonstrate that the proposed method succeeds in\nintegrating the side information, yielding improved performance at all channel\nconditions in terms of the various quality measures considered here, especially\nat low channel signal-to-noise ratios (SNRs) and small bandwidth ratios (BRs).\nWe have made the source code of the proposed method public to enable further\nresearch, and the reproducibility of the results.\n","authors":["Selim F. Yilmaz","Ezgi Ozyilkan","Deniz Gunduz","Elza Erkip"],"pdf_url":"https://arxiv.org/pdf/2310.04311v2.pdf","comment":"To appear in IEEE International Conference on Machine Learning for\n  Communication and Networking (ICMLCN) 2024"},{"id":"http://arxiv.org/abs/2402.17729v1","updated":"2024-02-27T18:01:59Z","published":"2024-02-27T18:01:59Z","title":"Towards Fairness-Aware Adversarial Learning","summary":"  Although adversarial training (AT) has proven effective in enhancing the\nmodel's robustness, the recently revealed issue of fairness in robustness has\nnot been well addressed, i.e. the robust accuracy varies significantly among\ndifferent categories. In this paper, instead of uniformly evaluating the\nmodel's average class performance, we delve into the issue of robust fairness,\nby considering the worst-case distribution across various classes. We propose a\nnovel learning paradigm, named Fairness-Aware Adversarial Learning (FAAL). As a\ngeneralization of conventional AT, we re-define the problem of adversarial\ntraining as a min-max-max framework, to ensure both robustness and fairness of\nthe trained model. Specifically, by taking advantage of distributional robust\noptimization, our method aims to find the worst distribution among different\ncategories, and the solution is guaranteed to obtain the upper bound\nperformance with high probability. In particular, FAAL can fine-tune an unfair\nrobust model to be fair within only two epochs, without compromising the\noverall clean and robust accuracies. Extensive experiments on various image\ndatasets validate the superior performance and efficiency of the proposed FAAL\ncompared to other state-of-the-art methods.\n","authors":["Yanghao Zhang","Tianle Zhang","Ronghui Mu","Xiaowei Huang","Wenjie Ruan"],"pdf_url":"https://arxiv.org/pdf/2402.17729v1.pdf","comment":"This work will appear in the CVPR 2024 conference proceedings"},{"id":"http://arxiv.org/abs/2402.17726v1","updated":"2024-02-27T17:58:09Z","published":"2024-02-27T17:58:09Z","title":"VRP-SAM: SAM with Visual Reference Prompt","summary":"  In this paper, we propose a novel Visual Reference Prompt (VRP) encoder that\nempowers the Segment Anything Model (SAM) to utilize annotated reference images\nas prompts for segmentation, creating the VRP-SAM model. In essence, VRP-SAM\ncan utilize annotated reference images to comprehend specific objects and\nperform segmentation of specific objects in target image. It is note that the\nVRP encoder can support a variety of annotation formats for reference images,\nincluding \\textbf{point}, \\textbf{box}, \\textbf{scribble}, and \\textbf{mask}.\nVRP-SAM achieves a breakthrough within the SAM framework by extending its\nversatility and applicability while preserving SAM's inherent strengths, thus\nenhancing user-friendliness. To enhance the generalization ability of VRP-SAM,\nthe VRP encoder adopts a meta-learning strategy. To validate the effectiveness\nof VRP-SAM, we conducted extensive empirical studies on the Pascal and COCO\ndatasets. Remarkably, VRP-SAM achieved state-of-the-art performance in visual\nreference segmentation with minimal learnable parameters. Furthermore, VRP-SAM\ndemonstrates strong generalization capabilities, allowing it to perform\nsegmentation of unseen objects and enabling cross-domain segmentation.\n","authors":["Yanpeng Sun","Jiahui Chen","Shan Zhang","Xinyu Zhang","Qiang Chen","Gang Zhang","Errui Ding","Jingdong Wang","Zechao Li"],"pdf_url":"https://arxiv.org/pdf/2402.17726v1.pdf","comment":"Accepted by CVPR 2024; It is not the camera-ready version"},{"id":"http://arxiv.org/abs/2402.17725v1","updated":"2024-02-27T17:58:05Z","published":"2024-02-27T17:58:05Z","title":"MedContext: Learning Contextual Cues for Efficient Volumetric Medical\n  Segmentation","summary":"  Volumetric medical segmentation is a critical component of 3D medical image\nanalysis that delineates different semantic regions. Deep neural networks have\nsignificantly improved volumetric medical segmentation, but they generally\nrequire large-scale annotated data to achieve better performance, which can be\nexpensive and prohibitive to obtain. To address this limitation, existing works\ntypically perform transfer learning or design dedicated pretraining-finetuning\nstages to learn representative features. However, the mismatch between the\nsource and target domain can make it challenging to learn optimal\nrepresentation for volumetric data, while the multi-stage training demands\nhigher compute as well as careful selection of stage-specific design choices.\nIn contrast, we propose a universal training framework called MedContext that\nis architecture-agnostic and can be incorporated into any existing training\nframework for 3D medical segmentation. Our approach effectively learns self\nsupervised contextual cues jointly with the supervised voxel segmentation task\nwithout requiring large-scale annotated volumetric medical data or dedicated\npretraining-finetuning stages. The proposed approach induces contextual\nknowledge in the network by learning to reconstruct the missing organ or parts\nof an organ in the output segmentation space. The effectiveness of MedContext\nis validated across multiple 3D medical datasets and four state-of-the-art\nmodel architectures. Our approach demonstrates consistent gains in segmentation\nperformance across datasets and different architectures even in few-shot data\nscenarios. Our code and pretrained models are available at\nhttps://github.com/hananshafi/MedContext\n","authors":["Hanan Gani","Muzammal Naseer","Fahad Khan","Salman Khan"],"pdf_url":"https://arxiv.org/pdf/2402.17725v1.pdf","comment":"Code available at https://github.com/hananshafi/MedContext"},{"id":"http://arxiv.org/abs/2402.17723v1","updated":"2024-02-27T17:57:04Z","published":"2024-02-27T17:57:04Z","title":"Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion\n  Latent Aligners","summary":"  Video and audio content creation serves as the core technique for the movie\nindustry and professional users. Recently, existing diffusion-based methods\ntackle video and audio generation separately, which hinders the technique\ntransfer from academia to industry. In this work, we aim at filling the gap,\nwith a carefully designed optimization-based framework for cross-visual-audio\nand joint-visual-audio generation. We observe the powerful generation ability\nof off-the-shelf video or audio generation models. Thus, instead of training\nthe giant models from scratch, we propose to bridge the existing strong models\nwith a shared latent representation space. Specifically, we propose a\nmultimodality latent aligner with the pre-trained ImageBind model. Our latent\naligner shares a similar core as the classifier guidance that guides the\ndiffusion denoising process during inference time. Through carefully designed\noptimization strategy and loss functions, we show the superior performance of\nour method on joint video-audio generation, visual-steered audio generation,\nand audio-steered visual generation tasks. The project website can be found at\nhttps://yzxing87.github.io/Seeing-and-Hearing/\n","authors":["Yazhou Xing","Yingqing He","Zeyue Tian","Xintao Wang","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2402.17723v1.pdf","comment":"Accepted to CVPR 2024. Project website:\n  https://yzxing87.github.io/Seeing-and-Hearing/"},{"id":"http://arxiv.org/abs/2402.17706v1","updated":"2024-02-27T17:36:01Z","published":"2024-02-27T17:36:01Z","title":"Adaptive quantization with mixed-precision based on low-cost proxy","summary":"  It is critical to deploy complicated neural network models on hardware with\nlimited resources. This paper proposes a novel model quantization method, named\nthe Low-Cost Proxy-Based Adaptive Mixed-Precision Model Quantization (LCPAQ),\nwhich contains three key modules. The hardware-aware module is designed by\nconsidering the hardware limitations, while an adaptive mixed-precision\nquantization module is developed to evaluate the quantization sensitivity by\nusing the Hessian matrix and Pareto frontier techniques. Integer linear\nprogramming is used to fine-tune the quantization across different layers. Then\nthe low-cost proxy neural architecture search module efficiently explores the\nideal quantization hyperparameters. Experiments on the ImageNet demonstrate\nthat the proposed LCPAQ achieves comparable or superior quantization accuracy\nto existing mixed-precision models. Notably, LCPAQ achieves 1/200 of the search\ntime compared with existing methods, which provides a shortcut in practical\nquantization use for resource-limited devices.\n","authors":["Junzhe Chen","Qiao Yang","Senmao Tian","Shunli Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.17706v1.pdf","comment":"accepted by icassp2024"},{"id":"http://arxiv.org/abs/2306.08649v2","updated":"2024-02-27T17:34:43Z","published":"2023-06-14T17:28:46Z","title":"OCAtari: Object-Centric Atari 2600 Reinforcement Learning Environments","summary":"  Cognitive science and psychology suggest that object-centric representations\nof complex scenes are a promising step towards enabling efficient abstract\nreasoning from low-level perceptual features. Yet, most deep reinforcement\nlearning approaches only rely on pixel-based representations that do not\ncapture the compositional properties of natural scenes. For this, we need\nenvironments and datasets that allow us to work and evaluate object-centric\napproaches. In our work, we extend the Atari Learning Environments, the\nmost-used evaluation framework for deep RL approaches, by introducing OCAtari,\nthat performs resource-efficient extractions of the object-centric states for\nthese games. Our framework allows for object discovery, object representation\nlearning, as well as object-centric RL. We evaluate OCAtari's detection\ncapabilities and resource efficiency. Our source code is available at\ngithub.com/k4ntz/OC_Atari.\n","authors":["Quentin Delfosse","Jannis Blüml","Bjarne Gregori","Sebastian Sztwiertnia","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2306.08649v2.pdf","comment":"26 pages, 8 main paper pages, 36 appendix pages. In main paper: 4\n  figures, 3 tables"},{"id":"http://arxiv.org/abs/2306.00003v2","updated":"2024-02-27T17:12:18Z","published":"2023-05-25T18:22:12Z","title":"Detecting Heart Disease from Multi-View Ultrasound Images via Supervised\n  Attention Multiple Instance Learning","summary":"  Aortic stenosis (AS) is a degenerative valve condition that causes\nsubstantial morbidity and mortality. This condition is under-diagnosed and\nunder-treated. In clinical practice, AS is diagnosed with expert review of\ntransthoracic echocardiography, which produces dozens of ultrasound images of\nthe heart. Only some of these views show the aortic valve. To automate\nscreening for AS, deep networks must learn to mimic a human expert's ability to\nidentify views of the aortic valve then aggregate across these relevant images\nto produce a study-level diagnosis. We find previous approaches to AS detection\nyield insufficient accuracy due to relying on inflexible averages across\nimages. We further find that off-the-shelf attention-based multiple instance\nlearning (MIL) performs poorly. We contribute a new end-to-end MIL approach\nwith two key methodological innovations. First, a supervised attention\ntechnique guides the learned attention mechanism to favor relevant views.\nSecond, a novel self-supervised pretraining strategy applies contrastive\nlearning on the representation of the whole study instead of individual images\nas commonly done in prior literature. Experiments on an open-access dataset and\nan external validation set show that our approach yields higher accuracy while\nreducing model size.\n","authors":["Zhe Huang","Benjamin S. Wessler","Michael C. Hughes"],"pdf_url":"https://arxiv.org/pdf/2306.00003v2.pdf","comment":"Echocardiogram; multiple-instance learning; self-supervised learning;\n  semi-supervised learning; medical imaging"},{"id":"http://arxiv.org/abs/2402.15852v2","updated":"2024-02-27T17:00:03Z","published":"2024-02-24T16:39:16Z","title":"NaVid: Video-based VLM Plans the Next Step for Vision-and-Language\n  Navigation","summary":"  Vision-and-Language Navigation (VLN) stands as a key research problem of\nEmbodied AI, aiming at enabling agents to navigate in unseen environments\nfollowing linguistic instructions. In this field, generalization is a\nlong-standing challenge, either to out-of-distribution scenes or from Sim to\nReal. In this paper, we propose NaVid, a video-based large vision language\nmodel (VLM), to mitigate such a generalization gap. NaVid makes the first\nendeavour to showcase the capability of VLMs to achieve state-of-the-art level\nnavigation performance without any maps, odometer and depth inputs. Following\nhuman instruction, NaVid only requires an on-the-fly video stream from a\nmonocular RGB camera equipped on the robot to output the next-step action. Our\nformulation mimics how humans navigate and naturally gets rid of the problems\nintroduced by odometer noises, and the Sim2Real gaps from map or depth inputs.\nMoreover, our video-based approach can effectively encode the historical\nobservations of robots as spatio-temporal contexts for decision-making and\ninstruction following. We train NaVid with 550k navigation samples collected\nfrom VLN-CE trajectories, including action-planning and instruction-reasoning\nsamples, along with 665k large-scale web data. Extensive experiments show that\nNaVid achieves SOTA performance in simulation environments and the real world,\ndemonstrating superior cross-dataset and Sim2Real transfer. We thus believe our\nproposed VLM approach plans the next step for not only the navigation agents\nbut also this research field.\n","authors":["Jiazhao Zhang","Kunyu Wang","Rongtao Xu","Gengze Zhou","Yicong Hong","Xiaomeng Fang","Qi Wu","Zhizheng Zhang","Wang He"],"pdf_url":"https://arxiv.org/pdf/2402.15852v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17680v1","updated":"2024-02-27T16:54:08Z","published":"2024-02-27T16:54:08Z","title":"MCF-VC: Mitigate Catastrophic Forgetting in Class-Incremental Learning\n  for Multimodal Video Captioning","summary":"  To address the problem of catastrophic forgetting due to the invisibility of\nold categories in sequential input, existing work based on relatively simple\ncategorization tasks has made some progress. In contrast, video captioning is a\nmore complex task in multimodal scenario, which has not been explored in the\nfield of incremental learning. After identifying this stability-plasticity\nproblem when analyzing video with sequential input, we originally propose a\nmethod to Mitigate Catastrophic Forgetting in class-incremental learning for\nmultimodal Video Captioning (MCF-VC). As for effectively maintaining good\nperformance on old tasks at the macro level, we design Fine-grained Sensitivity\nSelection (FgSS) based on the Mask of Linear's Parameters and Fisher\nSensitivity to pick useful knowledge from old tasks. Further, in order to\nbetter constrain the knowledge characteristics of old and new tasks at the\nspecific feature level, we have created the Two-stage Knowledge Distillation\n(TsKD), which is able to learn the new task well while weighing the old task.\nSpecifically, we design two distillation losses, which constrain the cross\nmodal semantic information of semantic attention feature map and the textual\ninformation of the final outputs respectively, so that the inter-model and\nintra-model stylized knowledge of the old class is retained while learning the\nnew class. In order to illustrate the ability of our model to resist\nforgetting, we designed a metric CIDER_t to detect the stage forgetting rate.\nOur experiments on the public dataset MSR-VTT show that the proposed method\nsignificantly resists the forgetting of previous tasks without replaying old\nsamples, and performs well on the new task.\n","authors":["Huiyu Xiong","Lanxiao Wang","Heqian Qiu","Taijin Zhao","Benliu Qiu","Hongliang Li"],"pdf_url":"https://arxiv.org/pdf/2402.17680v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2402.17678v1","updated":"2024-02-27T16:53:16Z","published":"2024-02-27T16:53:16Z","title":"CAD-SIGNet: CAD Language Inference from Point Clouds using Layer-wise\n  Sketch Instance Guided Attention","summary":"  Reverse engineering in the realm of Computer-Aided Design (CAD) has been a\nlongstanding aspiration, though not yet entirely realized. Its primary aim is\nto uncover the CAD process behind a physical object given its 3D scan. We\npropose CAD-SIGNet, an end-to-end trainable and auto-regressive architecture to\nrecover the design history of a CAD model represented as a sequence of\nsketch-and-extrusion from an input point cloud. Our model learns\nvisual-language representations by layer-wise cross-attention between point\ncloud and CAD language embedding. In particular, a new Sketch instance Guided\nAttention (SGA) module is proposed in order to reconstruct the fine-grained\ndetails of the sketches. Thanks to its auto-regressive nature, CAD-SIGNet not\nonly reconstructs a unique full design history of the corresponding CAD model\ngiven an input point cloud but also provides multiple plausible design choices.\nThis allows for an interactive reverse engineering scenario by providing\ndesigners with multiple next-step choices along with the design process.\nExtensive experiments on publicly available CAD datasets showcase the\neffectiveness of our approach against existing baseline models in two settings,\nnamely, full design history recovery and conditional auto-completion from point\nclouds.\n","authors":["Mohammad Sadil Khan","Elona Dupont","Sk Aziz Ali","Kseniya Cherenkova","Anis Kacem","Djamila Aouada"],"pdf_url":"https://arxiv.org/pdf/2402.17678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17672v1","updated":"2024-02-27T16:46:21Z","published":"2024-02-27T16:46:21Z","title":"SDF2Net: Shallow to Deep Feature Fusion Network for PolSAR Image\n  Classification","summary":"  Polarimetric synthetic aperture radar (PolSAR) images encompass valuable\ninformation that can facilitate extensive land cover interpretation and\ngenerate diverse output products. Extracting meaningful features from PolSAR\ndata poses challenges distinct from those encountered in optical imagery. Deep\nlearning (DL) methods offer effective solutions for overcoming these challenges\nin PolSAR feature extraction. Convolutional neural networks (CNNs) play a\ncrucial role in capturing PolSAR image characteristics by leveraging kernel\ncapabilities to consider local information and the complex-valued nature of\nPolSAR data. In this study, a novel three-branch fusion of complex-valued CNN,\nnamed the Shallow to Deep Feature Fusion Network (SDF2Net), is proposed for\nPolSAR image classification. To validate the performance of the proposed\nmethod, classification results are compared against multiple state-of-the-art\napproaches using the airborne synthetic aperture radar (AIRSAR) datasets of\nFlevoland and San Francisco, as well as the ESAR Oberpfaffenhofen dataset. The\nresults indicate that the proposed approach demonstrates improvements in\noverallaccuracy, with a 1.3% and 0.8% enhancement for the AIRSAR datasets and a\n0.5% improvement for the ESAR dataset. Analyses conducted on the Flevoland data\nunderscore the effectiveness of the SDF2Net model, revealing a promising\noverall accuracy of 96.01% even with only a 1% sampling ratio.\n","authors":["Mohammed Q. Alkhatib","M. Sami Zitouni","Mina Al-Saad","Nour Aburaed","Hussain Al-Ahmad"],"pdf_url":"https://arxiv.org/pdf/2402.17672v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.01195v3","updated":"2024-02-27T16:40:01Z","published":"2023-06-01T23:20:47Z","title":"Consistency-guided Prompt Learning for Vision-Language Models","summary":"  We propose Consistency-guided Prompt learning (CoPrompt), a new fine-tuning\nmethod for vision-language models. Our approach improves the generalization of\nlarge foundation models when fine-tuned on downstream tasks in a few-shot\nsetting. The basic idea of CoPrompt is to enforce a consistency constraint in\nthe prediction of the trainable and pre-trained models to prevent overfitting\non the downstream task. Additionally, we introduce the following two components\ninto our consistency constraint to further boost the performance: enforcing\nconsistency on two perturbed inputs and combining two dominant paradigms of\ntuning, prompting and adapter. Enforcing consistency on perturbed input serves\nto further regularize the consistency constraint, thereby improving\ngeneralization. Moreover, the integration of adapters and prompts not only\nenhances performance on downstream tasks but also offers increased tuning\nflexibility in both input and output spaces. This facilitates more effective\nadaptation to downstream tasks in a few-shot learning setting. Experiments show\nthat CoPrompt outperforms existing methods on a range of evaluation suites,\nincluding base-to-novel generalization, domain generalization, and\ncross-dataset evaluation. On generalization, CoPrompt improves the\nstate-of-the-art on zero-shot tasks and the overall harmonic mean over 11\ndatasets. Detailed ablation studies show the effectiveness of each of the\ncomponents in CoPrompt. We make our code available at\nhttps://github.com/ShuvenduRoy/CoPrompt.\n","authors":["Shuvendu Roy","Ali Etemad"],"pdf_url":"https://arxiv.org/pdf/2306.01195v3.pdf","comment":"Published as a conference paper at ICLR 2024"},{"id":"http://arxiv.org/abs/2402.14611v2","updated":"2024-02-27T16:39:35Z","published":"2024-02-22T15:02:13Z","title":"Overcoming Dimensional Collapse in Self-supervised Contrastive Learning\n  for Medical Image Segmentation","summary":"  Self-supervised learning (SSL) approaches have achieved great success when\nthe amount of labeled data is limited. Within SSL, models learn robust feature\nrepresentations by solving pretext tasks. One such pretext task is contrastive\nlearning, which involves forming pairs of similar and dissimilar input samples,\nguiding the model to distinguish between them. In this work, we investigate the\napplication of contrastive learning to the domain of medical image analysis.\nOur findings reveal that MoCo v2, a state-of-the-art contrastive learning\nmethod, encounters dimensional collapse when applied to medical images. This is\nattributed to the high degree of inter-image similarity shared between the\nmedical images. To address this, we propose two key contributions: local\nfeature learning and feature decorrelation. Local feature learning improves the\nability of the model to focus on the local regions of the image, while feature\ndecorrelation removes the linear dependence among the features. Our\nexperimental findings demonstrate that our contributions significantly enhance\nthe model's performance in the downstream task of medical segmentation, both in\nthe linear evaluation and full fine-tuning settings. This work illustrates the\nimportance of effectively adapting SSL techniques to the characteristics of\nmedical imaging tasks. The source code will be made publicly available at:\nhttps://github.com/CAMMA-public/med-moco\n","authors":["Jamshid Hassanpour","Vinkle Srivastav","Didier Mutter","Nicolas Padoy"],"pdf_url":"https://arxiv.org/pdf/2402.14611v2.pdf","comment":"Accepted at at ISBI-2024 (https://biomedicalimaging.org/2024/). 4\n  pages, 2 figures, 2 tables"},{"id":"http://arxiv.org/abs/2402.17664v1","updated":"2024-02-27T16:35:07Z","published":"2024-02-27T16:35:07Z","title":"Bayesian Differentiable Physics for Cloth Digitalization","summary":"  We propose a new method for cloth digitalization. Deviating from existing\nmethods which learn from data captured under relatively casual settings, we\npropose to learn from data captured in strictly tested measuring protocols, and\nfind plausible physical parameters of the cloths. However, such data is\ncurrently absent, so we first propose a new dataset with accurate cloth\nmeasurements. Further, the data size is considerably smaller than the ones in\ncurrent deep learning, due to the nature of the data capture process. To learn\nfrom small data, we propose a new Bayesian differentiable cloth model to\nestimate the complex material heterogeneity of real cloths. It can provide\nhighly accurate digitalization from very limited data samples. Through\nexhaustive evaluation and comparison, we show our method is accurate in cloth\ndigitalization, efficient in learning from limited data samples, and general in\ncapturing material variations. Code and data are available\nhttps://github.com/realcrane/Bayesian-Differentiable-Physics-for-Cloth-Digitalization\n","authors":["Deshan Gong","Ningtao Mao","He Wang"],"pdf_url":"https://arxiv.org/pdf/2402.17664v1.pdf","comment":"9 pages, 8 figures, to be published in CVPR"},{"id":"http://arxiv.org/abs/2402.17653v1","updated":"2024-02-27T16:23:11Z","published":"2024-02-27T16:23:11Z","title":"Mitigating Distributional Shift in Semantic Segmentation via Uncertainty\n  Estimation from Unlabelled Data","summary":"  Knowing when a trained segmentation model is encountering data that is\ndifferent to its training data is important. Understanding and mitigating the\neffects of this play an important part in their application from a performance\nand assurance perspective - this being a safety concern in applications such as\nautonomous vehicles (AVs). This work presents a segmentation network that can\ndetect errors caused by challenging test domains without any additional\nannotation in a single forward pass. As annotation costs limit the diversity of\nlabelled datasets, we use easy-to-obtain, uncurated and unlabelled data to\nlearn to perform uncertainty estimation by selectively enforcing consistency\nover data augmentation. To this end, a novel segmentation benchmark based on\nthe SAX Dataset is used, which includes labelled test data spanning three\nautonomous-driving domains, ranging in appearance from dense urban to off-road.\nThe proposed method, named Gamma-SSL, consistently outperforms uncertainty\nestimation and Out-of-Distribution (OoD) techniques on this difficult benchmark\n- by up to 10.7% in area under the receiver operating characteristic (ROC)\ncurve and 19.2% in area under the precision-recall (PR) curve in the most\nchallenging of the three scenarios.\n","authors":["David S. W. Williams","Daniele De Martini","Matthew Gadd","Paul Newman"],"pdf_url":"https://arxiv.org/pdf/2402.17653v1.pdf","comment":"Accepted for publication in IEEE Transactions on Robotics (T-RO)"},{"id":"http://arxiv.org/abs/2302.02887v2","updated":"2024-02-27T15:59:39Z","published":"2023-02-06T15:53:34Z","title":"UVDoc: Neural Grid-based Document Unwarping","summary":"  Restoring the original, flat appearance of a printed document from casual\nphotographs of bent and wrinkled pages is a common everyday problem. In this\npaper we propose a novel method for grid-based single-image document unwarping.\nOur method performs geometric distortion correction via a fully convolutional\ndeep neural network that learns to predict the 3D grid mesh of the document and\nthe corresponding 2D unwarping grid in a dual-task fashion, implicitly encoding\nthe coupling between the shape of a 3D piece of paper and its 2D image. In\norder to allow unwarping models to train on data that is more realistic in\nappearance than the commonly used synthetic Doc3D dataset, we create and\npublish our own dataset, called UVDoc, which combines pseudo-photorealistic\ndocument images with physically accurate 3D shape and unwarping function\nannotations. Our dataset is labeled with all the information necessary to train\nour unwarping network, without having to engineer separate loss functions that\ncan deal with the lack of ground-truth typically found in document in the wild\ndatasets. We perform an in-depth evaluation that demonstrates that with the\ninclusion of our novel pseudo-photorealistic dataset, our relatively small\nnetwork architecture achieves state-of-the-art results on the DocUNet\nbenchmark. We show that the pseudo-photorealistic nature of our UVDoc dataset\nallows for new and better evaluation methods, such as lighting-corrected\nMS-SSIM. We provide a novel benchmark dataset that facilitates such\nevaluations, and propose a metric that quantifies line straightness after\nunwarping. Our code, results and UVDoc dataset are available at\nhttps://github.com/tanguymagne/UVDoc.\n","authors":["Floor Verhoeven","Tanguy Magne","Olga Sorkine-Hornung"],"pdf_url":"https://arxiv.org/pdf/2302.02887v2.pdf","comment":"14 pages, published in SIGGRAPH Asia 2023 Conference Papers"},{"id":"http://arxiv.org/abs/2402.17624v1","updated":"2024-02-27T15:52:59Z","published":"2024-02-27T15:52:59Z","title":"CustomSketching: Sketch Concept Extraction for Sketch-based Image\n  Synthesis and Editing","summary":"  Personalization techniques for large text-to-image (T2I) models allow users\nto incorporate new concepts from reference images. However, existing methods\nprimarily rely on textual descriptions, leading to limited control over\ncustomized images and failing to support fine-grained and local editing (e.g.,\nshape, pose, and details). In this paper, we identify sketches as an intuitive\nand versatile representation that can facilitate such control, e.g., contour\nlines capturing shape information and flow lines representing texture. This\nmotivates us to explore a novel task of sketch concept extraction: given one or\nmore sketch-image pairs, we aim to extract a special sketch concept that\nbridges the correspondence between the images and sketches, thus enabling\nsketch-based image synthesis and editing at a fine-grained level. To accomplish\nthis, we introduce CustomSketching, a two-stage framework for extracting novel\nsketch concepts. Considering that an object can often be depicted by a contour\nfor general shapes and additional strokes for internal details, we introduce a\ndual-sketch representation to reduce the inherent ambiguity in sketch\ndepiction. We employ a shape loss and a regularization loss to balance fidelity\nand editability during optimization. Through extensive experiments, a user\nstudy, and several applications, we show our method is effective and superior\nto the adapted baselines.\n","authors":["Chufeng Xiao","Hongbo Fu"],"pdf_url":"https://arxiv.org/pdf/2402.17624v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17622v1","updated":"2024-02-27T15:49:54Z","published":"2024-02-27T15:49:54Z","title":"Masked Gamma-SSL: Learning Uncertainty Estimation via Masked Image\n  Modeling","summary":"  This work proposes a semantic segmentation network that produces high-quality\nuncertainty estimates in a single forward pass. We exploit general\nrepresentations from foundation models and unlabelled datasets through a Masked\nImage Modeling (MIM) approach, which is robust to augmentation hyper-parameters\nand simpler than previous techniques. For neural networks used in\nsafety-critical applications, bias in the training data can lead to errors;\ntherefore it is crucial to understand a network's limitations at run time and\nact accordingly. To this end, we test our proposed method on a number of test\ndomains including the SAX Segmentation benchmark, which includes labelled test\ndata from dense urban, rural and off-road driving domains. The proposed method\nconsistently outperforms uncertainty estimation and Out-of-Distribution (OoD)\ntechniques on this difficult benchmark.\n","authors":["David S. W. Williams","Matthew Gadd","Paul Newman","Daniele De Martini"],"pdf_url":"https://arxiv.org/pdf/2402.17622v1.pdf","comment":"Accepted for publication at 2024 IEEE International Conference on\n  Robotics and Automation (ICRA)"},{"id":"http://arxiv.org/abs/2402.17614v1","updated":"2024-02-27T15:43:53Z","published":"2024-02-27T15:43:53Z","title":"Adapt Before Comparison: A New Perspective on Cross-Domain Few-Shot\n  Segmentation","summary":"  Few-shot segmentation performance declines substantially when facing images\nfrom a domain different than the training domain, effectively limiting\nreal-world use cases. To alleviate this, recently cross-domain few-shot\nsegmentation (CD-FSS) has emerged. Works that address this task mainly\nattempted to learn segmentation on a source domain in a manner that generalizes\nacross domains. Surprisingly, we can outperform these approaches while\neliminating the training stage and removing their main segmentation network. We\nshow test-time task-adaption is the key for successful CD-FSS instead.\nTask-adaption is achieved by appending small networks to the feature pyramid of\na conventionally classification-pretrained backbone. To avoid overfitting to\nthe few labeled samples in supervised fine-tuning, consistency across augmented\nviews of input images serves as guidance while learning the parameters of the\nattached layers. Despite our self-restriction not to use any images other than\nthe few labeled samples at test time, we achieve new state-of-the-art\nperformance in CD-FSS, evidencing the need to rethink approaches for the task.\n","authors":["Jonas Herzog"],"pdf_url":"https://arxiv.org/pdf/2402.17614v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17611v1","updated":"2024-02-27T15:37:15Z","published":"2024-02-27T15:37:15Z","title":"A Large-scale Evaluation of Pretraining Paradigms for the Detection of\n  Defects in Electroluminescence Solar Cell Images","summary":"  Pretraining has been shown to improve performance in many domains, including\nsemantic segmentation, especially in domains with limited labelled data. In\nthis work, we perform a large-scale evaluation and benchmarking of various\npretraining methods for Solar Cell Defect Detection (SCDD) in\nelectroluminescence images, a field with limited labelled datasets. We cover\nsupervised training with semantic segmentation, semi-supervised learning, and\ntwo self-supervised techniques. We also experiment with both in-distribution\nand out-of-distribution (OOD) pretraining and observe how this affects\ndownstream performance. The results suggest that supervised training on a large\nOOD dataset (COCO), self-supervised pretraining on a large OOD dataset\n(ImageNet), and semi-supervised pretraining (CCT) all yield statistically\nequivalent performance for mean Intersection over Union (mIoU). We achieve a\nnew state-of-the-art for SCDD and demonstrate that certain pretraining schemes\nresult in superior performance on underrepresented classes. Additionally, we\nprovide a large-scale unlabelled EL image dataset of $22000$ images, and a\n$642$-image labelled semantic segmentation EL dataset, for further research in\ndeveloping self- and semi-supervised training techniques in this domain.\n","authors":["David Torpey","Lawrence Pratt","Richard Klein"],"pdf_url":"https://arxiv.org/pdf/2402.17611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06823v3","updated":"2024-02-27T15:33:52Z","published":"2023-10-10T17:53:36Z","title":"NECO: NEural Collapse Based Out-of-distribution detection","summary":"  Detecting out-of-distribution (OOD) data is a critical challenge in machine\nlearning due to model overconfidence, often without awareness of their\nepistemological limits. We hypothesize that ``neural collapse'', a phenomenon\naffecting in-distribution data for models trained beyond loss convergence, also\ninfluences OOD data. To benefit from this interplay, we introduce NECO, a novel\npost-hoc method for OOD detection, which leverages the geometric properties of\n``neural collapse'' and of principal component spaces to identify OOD data. Our\nextensive experiments demonstrate that NECO achieves state-of-the-art results\non both small and large-scale OOD detection tasks while exhibiting strong\ngeneralization capabilities across different network architectures.\nFurthermore, we provide a theoretical explanation for the effectiveness of our\nmethod in OOD detection. Code is available at https://gitlab.com/drti/neco\n","authors":["Mouïn Ben Ammar","Nacim Belkhir","Sebastian Popescu","Antoine Manzanera","Gianni Franchi"],"pdf_url":"https://arxiv.org/pdf/2310.06823v3.pdf","comment":"Accepted to ICLR2024"},{"id":"http://arxiv.org/abs/2312.07920v2","updated":"2024-02-27T15:26:51Z","published":"2023-12-13T06:30:51Z","title":"DrivingGaussian: Composite Gaussian Splatting for Surrounding Dynamic\n  Autonomous Driving Scenes","summary":"  We present DrivingGaussian, an efficient and effective framework for\nsurrounding dynamic autonomous driving scenes. For complex scenes with moving\nobjects, we first sequentially and progressively model the static background of\nthe entire scene with incremental static 3D Gaussians. We then leverage a\ncomposite dynamic Gaussian graph to handle multiple moving objects,\nindividually reconstructing each object and restoring their accurate positions\nand occlusion relationships within the scene. We further use a LiDAR prior for\nGaussian Splatting to reconstruct scenes with greater details and maintain\npanoramic consistency. DrivingGaussian outperforms existing methods in driving\nscene reconstruction and enables photorealistic surround-view synthesis with\nhigh-fidelity and multi-camera consistency. Our project page is at:\nhttps://github.com/VDIGPKU/DrivingGaussian.\n","authors":["Xiaoyu Zhou","Zhiwei Lin","Xiaojun Shan","Yongtao Wang","Deqing Sun","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2312.07920v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17589v1","updated":"2024-02-27T15:22:20Z","published":"2024-02-27T15:22:20Z","title":"PLReMix: Combating Noisy Labels with Pseudo-Label Relaxed Contrastive\n  Representation Learning","summary":"  Recently, the application of Contrastive Representation Learning (CRL) in\nlearning with noisy labels (LNL) has shown promising advancements due to its\nremarkable ability to learn well-distributed representations for better\ndistinguishing noisy labels. However, CRL is mainly used as a pre-training\ntechnique, leading to a complicated multi-stage training pipeline. We also\nobserved that trivially combining CRL with supervised LNL methods decreases\nperformance. Using different images from the same class as negative pairs in\nCRL creates optimization conflicts between CRL and the supervised loss. To\naddress these two issues, we propose an end-to-end PLReMix framework that\navoids the complicated pipeline by introducing a Pseudo-Label Relaxed (PLR)\ncontrastive loss to alleviate the conflicts between losses. This PLR loss\nconstructs a reliable negative set of each sample by filtering out its\ninappropriate negative pairs that overlap at the top k indices of prediction\nprobabilities, leading to more compact semantic clusters than vanilla CRL.\nFurthermore, a two-dimensional Gaussian Mixture Model (GMM) is adopted to\ndistinguish clean and noisy samples by leveraging semantic information and\nmodel outputs simultaneously, which is expanded on the previously widely used\none-dimensional form. The PLR loss and a semi-supervised loss are\nsimultaneously applied to train on the GMM divided clean and noisy samples.\nExperiments on multiple benchmark datasets demonstrate the effectiveness of the\nproposed method. Our proposed PLR loss is scalable, which can be easily\nintegrated into other LNL methods and boost their performance. Codes will be\navailable.\n","authors":["Xiaoyu Liu","Beitong Zhou","Cheng Cheng"],"pdf_url":"https://arxiv.org/pdf/2402.17589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16564v3","updated":"2024-02-27T15:08:57Z","published":"2023-03-29T09:47:35Z","title":"Implicit Visual Bias Mitigation by Posterior Estimate Sharpening of a\n  Bayesian Neural Network","summary":"  The fairness of a deep neural network is strongly affected by dataset bias\nand spurious correlations, both of which are usually present in modern\nfeature-rich and complex visual datasets. Due to the difficulty and variability\nof the task, no single de-biasing method has been universally successful. In\nparticular, implicit methods not requiring explicit knowledge of bias variables\nare especially relevant for real-world applications. We propose a novel\nimplicit mitigation method using a Bayesian neural network, allowing us to\nleverage the relationship between epistemic uncertainties and the presence of\nbias or spurious correlations in a sample. Our proposed posterior estimate\nsharpening procedure encourages the network to focus on core features that do\nnot contribute to high uncertainties. Experimental results on three benchmark\ndatasets demonstrate that Bayesian networks with sharpened posterior estimates\nperform comparably to prior existing methods and show potential worthy of\nfurther exploration.\n","authors":["Rebecca S Stone","Nishant Ravikumar","Andrew J Bulpitt","David C Hogg"],"pdf_url":"https://arxiv.org/pdf/2303.16564v3.pdf","comment":"We are revising this paper with significant changes"},{"id":"http://arxiv.org/abs/2402.17563v1","updated":"2024-02-27T15:05:13Z","published":"2024-02-27T15:05:13Z","title":"Structure-Guided Adversarial Training of Diffusion Models","summary":"  Diffusion models have demonstrated exceptional efficacy in various generative\napplications. While existing models focus on minimizing a weighted sum of\ndenoising score matching losses for data distribution modeling, their training\nprimarily emphasizes instance-level optimization, overlooking valuable\nstructural information within each mini-batch, indicative of pair-wise\nrelationships among samples. To address this limitation, we introduce\nStructure-guided Adversarial training of Diffusion Models (SADM). In this\npioneering approach, we compel the model to learn manifold structures between\nsamples in each training batch. To ensure the model captures authentic manifold\nstructures in the data distribution, we advocate adversarial training of the\ndiffusion generator against a novel structure discriminator in a minimax game,\ndistinguishing real manifold structures from the generated ones. SADM\nsubstantially improves existing diffusion transformers (DiT) and outperforms\nexisting methods in image generation and cross-domain fine-tuning tasks across\n12 datasets, establishing a new state-of-the-art FID of 1.58 and 2.11 on\nImageNet for class-conditional image generation at resolutions of 256x256 and\n512x512, respectively.\n","authors":["Ling Yang","Haotian Qian","Zhilong Zhang","Jingwei Liu","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2402.17563v1.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2402.17562v1","updated":"2024-02-27T15:02:17Z","published":"2024-02-27T15:02:17Z","title":"An Empirical Study of the Generalization Ability of Lidar 3D Object\n  Detectors to Unseen Domains","summary":"  3D Object Detectors (3D-OD) are crucial for understanding the environment in\nmany robotic tasks, especially autonomous driving. Including 3D information via\nLidar sensors improves accuracy greatly. However, such detectors perform poorly\non domains they were not trained on, i.e. different locations, sensors,\nweather, etc., limiting their reliability in safety-critical applications.\nThere exist methods to adapt 3D-ODs to these domains; however, these methods\ntreat 3D-ODs as a black box, neglecting underlying architectural decisions and\nsource-domain training strategies. Instead, we dive deep into the details of\n3D-ODs, focusing our efforts on fundamental factors that influence robustness\nprior to domain adaptation.\n  We systematically investigate four design choices (and the interplay between\nthem) often overlooked in 3D-OD robustness and domain adaptation: architecture,\nvoxel encoding, data augmentations, and anchor strategies. We assess their\nimpact on the robustness of nine state-of-the-art 3D-ODs across six benchmarks\nencompassing three types of domain gaps - sensor type, weather, and location.\n  Our main findings are: (1) transformer backbones with local point features\nare more robust than 3D CNNs, (2) test-time anchor size adjustment is crucial\nfor adaptation across geographical locations, significantly boosting scores\nwithout retraining, (3) source-domain augmentations allow the model to\ngeneralize to low-resolution sensors, and (4) surprisingly, robustness to bad\nweather is improved when training directly on more clean weather data than on\ntraining with bad weather data. We outline our main conclusions and findings to\nprovide practical guidance on developing more robust 3D-ODs.\n","authors":["George Eskandar","Chongzhe Zhang","Abhishek Kaushik","Karim Guirguis","Mohamed Sayed","Bin Yang"],"pdf_url":"https://arxiv.org/pdf/2402.17562v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17561v1","updated":"2024-02-27T14:59:48Z","published":"2024-02-27T14:59:48Z","title":"PHNet: Patch-based Normalization for Portrait Harmonization","summary":"  A common problem for composite images is the incompatibility of their\nforeground and background components. Image harmonization aims to solve this\nproblem, making the whole image look more authentic and coherent. Most existing\nsolutions predict lookup tables (LUTs) or reconstruct images, utilizing various\nattributes of composite images. Recent approaches have primarily focused on\nemploying global transformations like normalization and color curve rendering\nto achieve visual consistency, and they often overlook the importance of local\nvisual coherence. We present a patch-based harmonization network consisting of\nnovel Patch-based normalization (PN) blocks and a feature extractor based on\nstatistical color transfer. Extensive experiments demonstrate the network's\nhigh generalization capability for different domains. Our network achieves\nstate-of-the-art results on the iHarmony4 dataset. Also, we created a new human\nportrait harmonization dataset based on FFHQ and checked the proposed method to\nshow the generalization ability by achieving the best metrics on it. The\nbenchmark experiments confirm that the suggested patch-based normalization\nblock and feature extractor effectively improve the network's capability to\nharmonize portraits. Our code and model baselines are publicly available.\n","authors":["Karen Efremyan","Elizaveta Petrova","Evgeny Kaskov","Alexander Kapitanov"],"pdf_url":"https://arxiv.org/pdf/2402.17561v1.pdf","comment":"Image harmonization, Patch-based normalization, Portrait\n  harmonization"},{"id":"http://arxiv.org/abs/2402.11996v2","updated":"2024-02-27T14:53:53Z","published":"2024-02-19T09:41:57Z","title":"ISCUTE: Instance Segmentation of Cables Using Text Embedding","summary":"  In the field of robotics and automation, conventional object recognition and\ninstance segmentation methods face a formidable challenge when it comes to\nperceiving Deformable Linear Objects (DLOs) like wires, cables, and flexible\ntubes. This challenge arises primarily from the lack of distinct attributes\nsuch as shape, color, and texture, which calls for tailored solutions to\nachieve precise identification. In this work, we propose a foundation\nmodel-based DLO instance segmentation technique that is text-promptable and\nuser-friendly. Specifically, our approach combines the text-conditioned\nsemantic segmentation capabilities of CLIPSeg model with the zero-shot\ngeneralization capabilities of Segment Anything Model (SAM). We show that our\nmethod exceeds SOTA performance on DLO instance segmentation, achieving a mIoU\nof $91.21\\%$. We also introduce a rich and diverse DLO-specific dataset for\ninstance segmentation.\n","authors":["Shir Kozlovsky","Omkar Joglekar","Dotan Di Castro"],"pdf_url":"https://arxiv.org/pdf/2402.11996v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17555v1","updated":"2024-02-27T14:51:56Z","published":"2024-02-27T14:51:56Z","title":"Scribble Hides Class: Promoting Scribble-Based Weakly-Supervised\n  Semantic Segmentation with Its Class Label","summary":"  Scribble-based weakly-supervised semantic segmentation using sparse scribble\nsupervision is gaining traction as it reduces annotation costs when compared to\nfully annotated alternatives. Existing methods primarily generate pseudo-labels\nby diffusing labeled pixels to unlabeled ones with local cues for supervision.\nHowever, this diffusion process fails to exploit global semantics and\nclass-specific cues, which are important for semantic segmentation. In this\nstudy, we propose a class-driven scribble promotion network, which utilizes\nboth scribble annotations and pseudo-labels informed by image-level classes and\nglobal semantics for supervision. Directly adopting pseudo-labels might\nmisguide the segmentation model, thus we design a localization rectification\nmodule to correct foreground representations in the feature space. To further\ncombine the advantages of both supervisions, we also introduce a distance\nentropy loss for uncertainty reduction, which adapts per-pixel confidence\nweights according to the reliable region determined by the scribble and\npseudo-label's boundary. Experiments on the ScribbleSup dataset with different\nqualities of scribble annotations outperform all the previous methods,\ndemonstrating the superiority and robustness of our method.The code is\navailable at\nhttps://github.com/Zxl19990529/Class-driven-Scribble-Promotion-Network.\n","authors":["Xinliang Zhang","Lei Zhu","Hangzhou He","Lujia Jin","Yanye Lu"],"pdf_url":"https://arxiv.org/pdf/2402.17555v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17553v1","updated":"2024-02-27T14:47:53Z","published":"2024-02-27T14:47:53Z","title":"OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist\n  Autonomous Agents for Desktop and Web","summary":"  For decades, human-computer interaction has fundamentally been manual. Even\ntoday, almost all productive work done on the computer necessitates human input\nat every step. Autonomous virtual agents represent an exciting step in\nautomating many of these menial tasks. Virtual agents would empower users with\nlimited technical proficiency to harness the full possibilities of computer\nsystems. They could also enable the efficient streamlining of numerous computer\ntasks, ranging from calendar management to complex travel bookings, with\nminimal human intervention. In this paper, we introduce OmniACT, the\nfirst-of-a-kind dataset and benchmark for assessing an agent's capability to\ngenerate executable programs to accomplish computer tasks. Our scope extends\nbeyond traditional web automation, covering a diverse range of desktop\napplications. The dataset consists of fundamental tasks such as \"Play the next\nsong\", as well as longer horizon tasks such as \"Send an email to John Doe\nmentioning the time and place to meet\". Specifically, given a pair of screen\nimage and a visually-grounded natural language task, the goal is to generate a\nscript capable of fully executing the task. We run several strong baseline\nlanguage model agents on our benchmark. The strongest baseline, GPT-4, performs\nthe best on our benchmark However, its performance level still reaches only 15%\nof the human proficiency in generating executable scripts capable of completing\nthe task, demonstrating the challenge of our task for conventional web agents.\nOur benchmark provides a platform to measure and evaluate the progress of\nlanguage model agents in automating computer tasks and motivates future work\ntowards building multimodal models that bridge large language models and the\nvisual grounding of computer screens.\n","authors":["Raghav Kapoor","Yash Parag Butala","Melisa Russak","Jing Yu Koh","Kiran Kamble","Waseem Alshikh","Ruslan Salakhutdinov"],"pdf_url":"https://arxiv.org/pdf/2402.17553v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17544v1","updated":"2024-02-27T14:34:14Z","published":"2024-02-27T14:34:14Z","title":"Adapting Learned Image Codecs to Screen Content via Adjustable\n  Transformations","summary":"  As learned image codecs (LICs) become more prevalent, their low coding\nefficiency for out-of-distribution data becomes a bottleneck for some\napplications. To improve the performance of LICs for screen content (SC) images\nwithout breaking backwards compatibility, we propose to introduce parameterized\nand invertible linear transformations into the coding pipeline without changing\nthe underlying baseline codec's operation flow. We design two neural networks\nto act as prefilters and postfilters in our setup to increase the coding\nefficiency and help with the recovery from coding artifacts. Our end-to-end\ntrained solution achieves up to 10% bitrate savings on SC compression compared\nto the baseline LICs while introducing only 1% extra parameters.\n","authors":["H. Burak Dogaroglu","A. Burakhan Koyuncu","Atanas Boev","Elena Alshina","Eckehard Steinbach"],"pdf_url":"https://arxiv.org/pdf/2402.17544v1.pdf","comment":"7 pages, 6 figures, 2 tables"},{"id":"http://arxiv.org/abs/2310.04671v3","updated":"2024-02-27T14:22:09Z","published":"2023-10-07T03:16:30Z","title":"Visual Abductive Reasoning Meets Driving Hazard Prediction","summary":"  This paper addresses the problem of predicting hazards that drivers may\nencounter while driving a car. We formulate it as a task of anticipating\nimpending accidents using a single input image captured by car dashcams. Unlike\nexisting approaches to driving hazard prediction that rely on computational\nsimulations or anomaly detection from videos, this study focuses on high-level\ninference from static images. The problem needs predicting and reasoning about\nfuture events based on uncertain observations, which falls under visual\nabductive reasoning. To enable research in this understudied area, a new\ndataset named the DHPR (Driving Hazard Prediction and Reasoning) dataset is\ncreated. The dataset consists of 15K dashcam images of street scenes, and each\nimage is associated with a tuple containing car speed, a hypothesized hazard\ndescription, and visual entities present in the scene. These are annotated by\nhuman annotators, who identify risky scenes and provide descriptions of\npotential accidents that could occur a few seconds later. We present several\nbaseline methods and evaluate their performance on our dataset, identifying\nremaining issues and discussing future directions. This study contributes to\nthe field by introducing a novel problem formulation and dataset, enabling\nresearchers to explore the potential of multi-modal AI for driving hazard\nprediction.\n","authors":["Korawat Charoenpitaks","Van-Quang Nguyen","Masanori Suganuma","Masahiro Takahashi","Ryoma Niihara","Takayuki Okatani"],"pdf_url":"https://arxiv.org/pdf/2310.04671v3.pdf","comment":"Main Paper: 10 pages, Supplementary Materials: 28 pages"},{"id":"http://arxiv.org/abs/2402.17535v1","updated":"2024-02-27T14:21:56Z","published":"2024-02-27T14:21:56Z","title":"Multimodal Learned Sparse Retrieval with Probabilistic Expansion Control","summary":"  Learned sparse retrieval (LSR) is a family of neural methods that encode\nqueries and documents into sparse lexical vectors that can be indexed and\nretrieved efficiently with an inverted index. We explore the application of LSR\nto the multi-modal domain, with a focus on text-image retrieval. While LSR has\nseen success in text retrieval, its application in multimodal retrieval remains\nunderexplored. Current approaches like LexLIP and STAIR require complex\nmulti-step training on massive datasets. Our proposed approach efficiently\ntransforms dense vectors from a frozen dense model into sparse lexical vectors.\nWe address issues of high dimension co-activation and semantic deviation\nthrough a new training algorithm, using Bernoulli random variables to control\nquery expansion. Experiments with two dense models (BLIP, ALBEF) and two\ndatasets (MSCOCO, Flickr30k) show that our proposed algorithm effectively\nreduces co-activation and semantic deviation. Our best-performing sparsified\nmodel outperforms state-of-the-art text-image LSR models with a shorter\ntraining time and lower GPU memory requirements. Our approach offers an\neffective solution for training LSR retrieval models in multimodal settings.\nOur code and model checkpoints are available at\ngithub.com/thongnt99/lsr-multimodal\n","authors":["Thong Nguyen","Mariya Hendriksen","Andrew Yates","Maarten de Rijke"],"pdf_url":"https://arxiv.org/pdf/2402.17535v1.pdf","comment":"17 pages, accepted as a full paper at ECIR 2024"},{"id":"http://arxiv.org/abs/2402.17533v1","updated":"2024-02-27T14:16:39Z","published":"2024-02-27T14:16:39Z","title":"Black-box Adversarial Attacks Against Image Quality Assessment Models","summary":"  The goal of No-Reference Image Quality Assessment (NR-IQA) is to predict the\nperceptual quality of an image in line with its subjective evaluation. To put\nthe NR-IQA models into practice, it is essential to study their potential\nloopholes for model refinement. This paper makes the first attempt to explore\nthe black-box adversarial attacks on NR-IQA models. Specifically, we first\nformulate the attack problem as maximizing the deviation between the estimated\nquality scores of original and perturbed images, while restricting the\nperturbed image distortions for visual quality preservation. Under such\nformulation, we then design a Bi-directional loss function to mislead the\nestimated quality scores of adversarial examples towards an opposite direction\nwith maximum deviation. On this basis, we finally develop an efficient and\neffective black-box attack method against NR-IQA models. Extensive experiments\nreveal that all the evaluated NR-IQA models are vulnerable to the proposed\nattack method. And the generated perturbations are not transferable, enabling\nthem to serve the investigation of specialities of disparate IQA models.\n","authors":["Yu Ran","Ao-Xiang Zhang","Mingjie Li","Weixuan Tang","Yuan-Gen Wang"],"pdf_url":"https://arxiv.org/pdf/2402.17533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17525v1","updated":"2024-02-27T14:07:09Z","published":"2024-02-27T14:07:09Z","title":"Diffusion Model-Based Image Editing: A Survey","summary":"  Denoising diffusion models have emerged as a powerful tool for various image\ngeneration and editing tasks, facilitating the synthesis of visual content in\nan unconditional or input-conditional manner. The core idea behind them is\nlearning to reverse the process of gradually adding noise to images, allowing\nthem to generate high-quality samples from a complex distribution. In this\nsurvey, we provide an exhaustive overview of existing methods using diffusion\nmodels for image editing, covering both theoretical and practical aspects in\nthe field. We delve into a thorough analysis and categorization of these works\nfrom multiple perspectives, including learning strategies, user-input\nconditions, and the array of specific editing tasks that can be accomplished.\nIn addition, we pay special attention to image inpainting and outpainting, and\nexplore both earlier traditional context-driven and current multimodal\nconditional methods, offering a comprehensive analysis of their methodologies.\nTo further evaluate the performance of text-guided image editing algorithms, we\npropose a systematic benchmark, EditEval, featuring an innovative metric, LMM\nScore. Finally, we address current limitations and envision some potential\ndirections for future research. The accompanying repository is released at\nhttps://github.com/SiatMMLab/Awesome-Diffusion-Model-Based-Image-Editing-Methods.\n","authors":["Yi Huang","Jiancheng Huang","Yifan Liu","Mingfu Yan","Jiaxi Lv","Jianzhuang Liu","Wei Xiong","He Zhang","Shifeng Chen","Liangliang Cao"],"pdf_url":"https://arxiv.org/pdf/2402.17525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17521v1","updated":"2024-02-27T14:05:05Z","published":"2024-02-27T14:05:05Z","title":"AVS-Net: Point Sampling with Adaptive Voxel Size for 3D Point Cloud\n  Analysis","summary":"  Efficient downsampling plays a crucial role in point cloud learning,\nparticularly for large-scale 3D scenes. Existing downsampling methods either\nrequire a huge computational burden or sacrifice fine-grained geometric\ninformation. This paper presents an advanced sampler that achieves both high\naccuracy and efficiency. The proposed method utilizes voxel-based sampling as a\nfoundation, but effectively addresses the challenges regarding voxel size\ndetermination and the preservation of critical geometric cues. Specifically, we\npropose a Voxel Adaptation Module that adaptively adjusts voxel sizes with the\nreference of point-based downsampling ratio. This ensures the sampling results\nexhibit a favorable distribution for comprehending various 3D objects or\nscenes. Additionally, we introduce a network compatible with arbitrary voxel\nsizes for sampling and feature extraction while maintaining high efficiency.\nOur method achieves state-of-the-art accuracy on the ShapeNetPart and ScanNet\nbenchmarks with promising efficiency. Code will be available at\nhttps://github.com/yhc2021/AVS-Net.\n","authors":["Hongcheng Yang","Dingkang Liang","Dingyuan Zhang","Xingyu Jiang","Zhe Liu","Zhikang Zou","Yingying Zhu"],"pdf_url":"https://arxiv.org/pdf/2402.17521v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2306.14287v2","updated":"2024-02-27T14:01:23Z","published":"2023-06-25T16:29:51Z","title":"Efficient Contextformer: Spatio-Channel Window Attention for Fast\n  Context Modeling in Learned Image Compression","summary":"  Entropy estimation is essential for the performance of learned image\ncompression. It has been demonstrated that a transformer-based entropy model is\nof critical importance for achieving a high compression ratio, however, at the\nexpense of a significant computational effort. In this work, we introduce the\nEfficient Contextformer (eContextformer) - a computationally efficient\ntransformer-based autoregressive context model for learned image compression.\nThe eContextformer efficiently fuses the patch-wise, checkered, and\nchannel-wise grouping techniques for parallel context modeling, and introduces\na shifted window spatio-channel attention mechanism. We explore better training\nstrategies and architectural designs and introduce additional complexity\noptimizations. During decoding, the proposed optimization techniques\ndynamically scale the attention span and cache the previous attention\ncomputations, drastically reducing the model and runtime complexity. Compared\nto the non-parallel approach, our proposal has ~145x lower model complexity and\n~210x faster decoding speed, and achieves higher average bit savings on Kodak,\nCLIC2020, and Tecnick datasets. Additionally, the low complexity of our context\nmodel enables online rate-distortion algorithms, which further improve the\ncompression performance. We achieve up to 17% bitrate savings over the intra\ncoding of Versatile Video Coding (VVC) Test Model (VTM) 16.2 and surpass\nvarious learning-based compression models.\n","authors":["A. Burakhan Koyuncu","Panqi Jia","Atanas Boev","Elena Alshina","Eckehard Steinbach"],"pdf_url":"https://arxiv.org/pdf/2306.14287v2.pdf","comment":"Accepted for IEEE TCSVT (14 pages, 10 figures, 9 tables)"},{"id":"http://arxiv.org/abs/2402.16569v2","updated":"2024-02-27T13:59:32Z","published":"2024-02-26T13:47:32Z","title":"Pretrained Visual Uncertainties","summary":"  Accurate uncertainty estimation is vital to trustworthy machine learning, yet\nuncertainties typically have to be learned for each task anew. This work\nintroduces the first pretrained uncertainty modules for vision models. Similar\nto standard pretraining this enables the zero-shot transfer of uncertainties\nlearned on a large pretraining dataset to specialized downstream datasets. We\nenable our large-scale pretraining on ImageNet-21k by solving a gradient\nconflict in previous uncertainty modules and accelerating the training by up to\n180x. We find that the pretrained uncertainties generalize to unseen datasets.\nIn scrutinizing the learned uncertainties, we find that they capture aleatoric\nuncertainty, disentangled from epistemic components. We demonstrate that this\nenables safe retrieval and uncertainty-aware dataset visualization. To\nencourage applications to further problems and domains, we release all\npretrained checkpoints and code under https://github.com/mkirchhof/url .\n","authors":["Michael Kirchhof","Mark Collier","Seong Joon Oh","Enkelejda Kasneci"],"pdf_url":"https://arxiv.org/pdf/2402.16569v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17514v1","updated":"2024-02-27T13:55:17Z","published":"2024-02-27T13:55:17Z","title":"Robust Unsupervised Crowd Counting and Localization with Adaptive\n  Resolution SAM","summary":"  The existing crowd counting models require extensive training data, which is\ntime-consuming to annotate. To tackle this issue, we propose a simple yet\neffective crowd counting method by utilizing the Segment-Everything-Everywhere\nModel (SEEM), an adaptation of the Segmentation Anything Model (SAM), to\ngenerate pseudo-labels for training crowd counting models. However, our initial\ninvestigation reveals that SEEM's performance in dense crowd scenes is limited,\nprimarily due to the omission of many persons in high-density areas. To\novercome this limitation, we propose an adaptive resolution SEEM to handle the\nscale variations, occlusions, and overlapping of people within crowd scenes.\nAlongside this, we introduce a robust localization method, based on Gaussian\nMixture Models, for predicting the head positions in the predicted people\nmasks. Given the mask and point pseudo-labels, we propose a robust loss\nfunction, which is designed to exclude uncertain regions based on SEEM's\npredictions, thereby enhancing the training process of the counting networks.\nFinally, we propose an iterative method for generating pseudo-labels. This\nmethod aims at improving the quality of the segmentation masks by identifying\nmore tiny persons in high-density regions, which are often missed in the first\npseudo-labeling stage. Overall, our proposed method achieves the best\nunsupervised performance in crowd counting, while also being comparable results\nto some supervised methods. This makes it a highly effective and versatile tool\nfor crowd counting, especially in situations where labeled data is not\navailable.\n","authors":["Jia Wan","Qiangqiang Wu","Wei Lin","Antoni B. Chan"],"pdf_url":"https://arxiv.org/pdf/2402.17514v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08586v3","updated":"2024-02-27T13:53:43Z","published":"2023-10-12T17:59:57Z","title":"PonderV2: Pave the Way for 3D Foundation Model with A Universal\n  Pre-training Paradigm","summary":"  In contrast to numerous NLP and 2D vision foundational models, learning a 3D\nfoundational model poses considerably greater challenges. This is primarily due\nto the inherent data variability and diversity of downstream tasks. In this\npaper, we introduce a novel universal 3D pre-training framework designed to\nfacilitate the acquisition of efficient 3D representation, thereby establishing\na pathway to 3D foundational models. Considering that informative 3D features\nshould encode rich geometry and appearance cues that can be utilized to render\nrealistic images, we propose to learn 3D representations by differentiable\nneural rendering. We train a 3D backbone with a devised volumetric neural\nrenderer by comparing the rendered with the real images. Notably, our approach\nseamlessly integrates the learned 3D encoder into various downstream tasks.\nThese tasks encompass not only high-level challenges such as 3D detection and\nsegmentation but also low-level objectives like 3D reconstruction and image\nsynthesis, spanning both indoor and outdoor scenarios. Besides, we also\nillustrate the capability of pre-training a 2D backbone using the proposed\nmethodology, surpassing conventional pre-training methods by a large margin.\nFor the first time, PonderV2 achieves state-of-the-art performance on 11 indoor\nand outdoor benchmarks, implying its effectiveness. Code and models are\navailable at https://github.com/OpenGVLab/PonderV2.\n","authors":["Haoyi Zhu","Honghui Yang","Xiaoyang Wu","Di Huang","Sha Zhang","Xianglong He","Hengshuang Zhao","Chunhua Shen","Yu Qiao","Tong He","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2310.08586v3.pdf","comment":"arXiv admin note: text overlap with arXiv:2301.00157"},{"id":"http://arxiv.org/abs/2309.14065v5","updated":"2024-02-27T13:50:39Z","published":"2023-09-25T11:57:16Z","title":"AsymFormer: Asymmetrical Cross-Modal Representation Learning for Mobile\n  Platform Real-Time RGB-D Semantic Segmentation","summary":"  In the realm of robotic intelligence, achieving efficient and precise RGB-D\nsemantic segmentation is a key cornerstone. State-of-the-art multimodal\nsemantic segmentation methods, primarily rooted in symmetrical skeleton\nnetworks, find it challenging to harmonize computational efficiency and\nprecision. In this work, we propose AsymFormer, a novel network for real-time\nRGB-D semantic segmentation, which targets the minimization of superfluous\nparameters by optimizing the distribution of computational resources and\nintroduces an asymmetrical backbone to allow for the effective fusion of\nmultimodal features. Furthermore, we explore techniques to bolster network\naccuracy by redefining feature selection and extracting multi-modal\nself-similarity features without a substantial increase in the parameter count,\nthereby ensuring real-time execution on robotic platforms. Additionally, a\nLocal Attention-Guided Feature Selection (LAFS) module is used to selectively\nfuse features from different modalities by leveraging their dependencies.\nSubsequently, a Cross-Modal Attention-Guided Feature Correlation Embedding\n(CMA) module is introduced to further extract cross-modal representations. This\nmethod is evaluated on NYUv2 and SUNRGBD datasets, with AsymFormer\ndemonstrating competitive results with 54.1% mIoU on NYUv2 and 49.1% mIoU on\nSUNRGBD. Notably, AsymFormer achieves an inference speed of 65 FPS and after\nimplementing mixed precision quantization, it attains an impressive inference\nspeed of 79 FPS on RTX3090. This significantly outperforms existing multi-modal\nmethods, thereby demonstrating that AsymFormer can strike a balance between\nhigh accuracy and efficiency for RGB-D semantic segmentation.\n","authors":["Siqi Du","Weixi Wang","Renzhong Guo","Shengjun Tang"],"pdf_url":"https://arxiv.org/pdf/2309.14065v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17510v1","updated":"2024-02-27T13:50:34Z","published":"2024-02-27T13:50:34Z","title":"Demonstrating and Reducing Shortcuts in Vision-Language Representation\n  Learning","summary":"  Vision-language models (VLMs) mainly rely on contrastive training to learn\ngeneral-purpose representations of images and captions. We focus on the\nsituation when one image is associated with several captions, each caption\ncontaining both information shared among all captions and unique information\nper caption about the scene depicted in the image. In such cases, it is unclear\nwhether contrastive losses are sufficient for learning task-optimal\nrepresentations that contain all the information provided by the captions or\nwhether the contrastive learning setup encourages the learning of a simple\nshortcut that minimizes contrastive loss. We introduce synthetic shortcuts for\nvision-language: a training and evaluation framework where we inject synthetic\nshortcuts into image-text data. We show that contrastive VLMs trained from\nscratch or fine-tuned with data containing these synthetic shortcuts mainly\nlearn features that represent the shortcut. Hence, contrastive losses are not\nsufficient to learn task-optimal representations, i.e., representations that\ncontain all task-relevant information shared between the image and associated\ncaptions. We examine two methods to reduce shortcut learning in our training\nand evaluation framework: (i) latent target decoding and (ii) implicit feature\nmodification. We show empirically that both methods improve performance on the\nevaluation task, but only partly reduce shortcut learning when training and\nevaluating with our shortcut learning framework. Hence, we show the difficulty\nand challenge of our shortcut learning framework for contrastive\nvision-language representation learning.\n","authors":["Maurits Bleeker","Mariya Hendriksen","Andrew Yates","Maarten de Rijke"],"pdf_url":"https://arxiv.org/pdf/2402.17510v1.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2402.17507v1","updated":"2024-02-27T13:47:23Z","published":"2024-02-27T13:47:23Z","title":"Interactive Multi-Head Self-Attention with Linear Complexity","summary":"  We propose an efficient interactive method for multi-head self-attention via\ndecomposition. For existing methods using multi-head self-attention, the\nattention operation of each head is computed independently. However, we show\nthat the interactions between cross-heads of the attention matrix enhance the\ninformation flow of the attention operation. Considering that the attention\nmatrix of each head can be seen as a feature of networks, it is beneficial to\nestablish connectivity between them to capture interactions better. However, a\nstraightforward approach to capture the interactions between the cross-heads is\ncomputationally prohibitive as the complexity grows substantially with the high\ndimension of an attention matrix. In this work, we propose an effective method\nto decompose the attention operation into query- and key-less components. This\nwill result in a more manageable size for the attention matrix, specifically\nfor the cross-head interactions. Expensive experimental results show that the\nproposed cross-head interaction approach performs favorably against existing\nefficient attention methods and state-of-the-art backbone models.\n","authors":["Hankyul Kang","Ming-Hsuan Yang","Jongbin Ryu"],"pdf_url":"https://arxiv.org/pdf/2402.17507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.00676v2","updated":"2024-02-27T13:46:11Z","published":"2023-07-02T22:08:24Z","title":"Pay Attention to the Atlas: Atlas-Guided Test-Time Adaptation Method for\n  Robust 3D Medical Image Segmentation","summary":"  Convolutional neural networks (CNNs) often suffer from poor performance when\ntested on target data that differs from the training (source) data\ndistribution, particularly in medical imaging applications where variations in\nimaging protocols across different clinical sites and scanners lead to\ndifferent imaging appearances. However, re-accessing source training data for\nunsupervised domain adaptation or labeling additional test data for model\nfine-tuning can be difficult due to privacy issues and high labeling costs,\nrespectively. To solve this problem, we propose a novel atlas-guided test-time\nadaptation (TTA) method for robust 3D medical image segmentation, called\nAdaAtlas. AdaAtlas only takes one single unlabeled test sample as input and\nadapts the segmentation network by minimizing an atlas-based loss.\nSpecifically, the network is adapted so that its prediction after registration\nis aligned with the learned atlas in the atlas space, which helps to reduce\nanatomical segmentation errors at test time. In addition, different from most\nexisting TTA methods which restrict the adaptation to batch normalization\nblocks in the segmentation network only, we further exploit the use of channel\nand spatial attention blocks for improved adaptability at test time. Extensive\nexperiments on multiple datasets from different sites show that AdaAtlas with\nattention blocks adapted (AdaAtlas-Attention) achieves superior performance\nimprovements, greatly outperforming other competitive TTA methods.\n","authors":["Jingjie Guo","Weitong Zhang","Matthew Sinclair","Daniel Rueckert","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2307.00676v2.pdf","comment":"Accepted by MICCAI BTSD-1001AI workshop. (Oral\n  presentation).https://btsdmiccai.github.io/"},{"id":"http://arxiv.org/abs/2402.17502v1","updated":"2024-02-27T13:41:32Z","published":"2024-02-27T13:41:32Z","title":"FedLPPA: Learning Personalized Prompt and Aggregation for Federated\n  Weakly-supervised Medical Image Segmentation","summary":"  Federated learning (FL) effectively mitigates the data silo challenge brought\nabout by policies and privacy concerns, implicitly harnessing more data for\ndeep model training. However, traditional centralized FL models grapple with\ndiverse multi-center data, especially in the face of significant data\nheterogeneity, notably in medical contexts. In the realm of medical image\nsegmentation, the growing imperative to curtail annotation costs has amplified\nthe importance of weakly-supervised techniques which utilize sparse annotations\nsuch as points, scribbles, etc. A pragmatic FL paradigm shall accommodate\ndiverse annotation formats across different sites, which research topic remains\nunder-investigated. In such context, we propose a novel personalized FL\nframework with learnable prompt and aggregation (FedLPPA) to uniformly leverage\nheterogeneous weak supervision for medical image segmentation. In FedLPPA, a\nlearnable universal knowledge prompt is maintained, complemented by multiple\nlearnable personalized data distribution prompts and prompts representing the\nsupervision sparsity. Integrated with sample features through a dual-attention\nmechanism, those prompts empower each local task decoder to adeptly adjust to\nboth the local distribution and the supervision form. Concurrently, a\ndual-decoder strategy, predicated on prompt similarity, is introduced for\nenhancing the generation of pseudo-labels in weakly-supervised learning,\nalleviating overfitting and noise accumulation inherent to local data, while an\nadaptable aggregation method is employed to customize the task decoder on a\nparameter-wise basis. Extensive experiments on three distinct medical image\nsegmentation tasks involving different modalities underscore the superiority of\nFedLPPA, with its efficacy closely parallels that of fully supervised\ncentralized training. Our code and data will be available.\n","authors":["Li Lin","Yixiang Liu","Jiewei Wu","Pujin Cheng","Zhiyuan Cai","Kenneth K. Y. Wong","Xiaoying Tang"],"pdf_url":"https://arxiv.org/pdf/2402.17502v1.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2306.05812v2","updated":"2024-02-27T13:40:40Z","published":"2023-06-09T11:05:09Z","title":"HRTF upsampling with a generative adversarial network using a gnomonic\n  equiangular projection","summary":"  An individualised head-related transfer function (HRTF) is very important for\ncreating realistic virtual reality (VR) and augmented reality (AR)\nenvironments. However, acoustically measuring high-quality HRTFs requires\nexpensive equipment and an acoustic lab setting. To overcome these limitations\nand to make this measurement more efficient HRTF upsampling has been exploited\nin the past where a high-resolution HRTF is created from a low-resolution one.\nThis paper demonstrates how generative adversarial networks (GANs) can be\napplied to HRTF upsampling. We propose a novel approach that transforms the\nHRTF data for direct use with a convolutional super-resolution generative\nadversarial network (SRGAN). This new approach is benchmarked against three\nbaselines: barycentric upsampling, spherical harmonic (SH) upsampling and an\nHRTF selection approach. Experimental results show that the proposed method\noutperforms all three baselines in terms of log-spectral distortion (LSD) and\nlocalisation performance using perceptual models when the input HRTF is sparse\n(less than 20 measured positions).\n","authors":["Aidan O. T. Hogg","Mads Jenkins","He Liu","Isaac Squires","Samuel J. Cooper","Lorenzo Picinali"],"pdf_url":"https://arxiv.org/pdf/2306.05812v2.pdf","comment":"15 pages, 9 figures, Preprint (Accepted to IEEE/ACM Transactions on\n  Audio, Speech, and Language Processing on the 15 Feb 2024)"},{"id":"http://arxiv.org/abs/2307.12499v3","updated":"2024-02-27T13:39:09Z","published":"2023-07-24T03:10:02Z","title":"AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion\n  Models","summary":"  Unrestricted adversarial attacks present a serious threat to deep learning\nmodels and adversarial defense techniques. They pose severe security problems\nfor deep learning applications because they can effectively bypass defense\nmechanisms. However, previous attack methods often utilize Generative\nAdversarial Networks (GANs), which are not theoretically provable and thus\ngenerate unrealistic examples by incorporating adversarial objectives,\nespecially for large-scale datasets like ImageNet. In this paper, we propose a\nnew method, called AdvDiff, to generate unrestricted adversarial examples with\ndiffusion models. We design two novel adversarial guidance techniques to\nconduct adversarial sampling in the reverse generation process of diffusion\nmodels. These two techniques are effective and stable to generate high-quality,\nrealistic adversarial examples by integrating gradients of the target\nclassifier interpretably. Experimental results on MNIST and ImageNet datasets\ndemonstrate that AdvDiff is effective to generate unrestricted adversarial\nexamples, which outperforms GAN-based methods in terms of attack performance\nand generation quality.\n","authors":["Xuelong Dai","Kaisheng Liang","Bin Xiao"],"pdf_url":"https://arxiv.org/pdf/2307.12499v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11585v2","updated":"2024-02-27T13:33:05Z","published":"2024-02-18T13:24:48Z","title":"PolypNextLSTM: A lightweight and fast polyp video segmentation network\n  using ConvNext and ConvLSTM","summary":"  Commonly employed in polyp segmentation, single image UNet architectures lack\nthe temporal insight clinicians gain from video data in diagnosing polyps. To\nmirror clinical practices more faithfully, our proposed solution,\nPolypNextLSTM, leverages video-based deep learning, harnessing temporal\ninformation for superior segmentation performance with the least parameter\noverhead, making it possibly suitable for edge devices. PolypNextLSTM employs a\nUNet-like structure with ConvNext-Tiny as its backbone, strategically omitting\nthe last two layers to reduce parameter overhead. Our temporal fusion module, a\nConvolutional Long Short Term Memory (ConvLSTM), effectively exploits temporal\nfeatures. Our primary novelty lies in PolypNextLSTM, which stands out as the\nleanest in parameters and the fastest model, surpassing the performance of five\nstate-of-the-art image and video-based deep learning models. The evaluation of\nthe SUN-SEG dataset spans easy-to-detect and hard-to-detect polyp scenarios,\nalong with videos containing challenging artefacts like fast motion and\nocclusion. Comparison against 5 image-based and 5 video-based models\ndemonstrates PolypNextLSTM's superiority, achieving a Dice score of 0.7898 on\nthe hard-to-detect polyp test set, surpassing image-based PraNet (0.7519) and\nvideo-based PNSPlusNet (0.7486). Notably, our model excels in videos featuring\ncomplex artefacts such as ghosting and occlusion. PolypNextLSTM, integrating\npruned ConvNext-Tiny with ConvLSTM for temporal fusion, not only exhibits\nsuperior segmentation performance but also maintains the highest frames per\nspeed among evaluated models. Access code here\nhttps://github.com/mtec-tuhh/PolypNextLSTM\n","authors":["Debayan Bhattacharya","Konrad Reuter","Finn Behrendnt","Lennart Maack","Sarah Grube","Alexander Schlaefer"],"pdf_url":"https://arxiv.org/pdf/2402.11585v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.13841v3","updated":"2024-02-27T13:18:48Z","published":"2021-11-27T07:57:41Z","title":"Adaptive Perturbation for Adversarial Attack","summary":"  In recent years, the security of deep learning models achieves more and more\nattentions with the rapid development of neural networks, which are vulnerable\nto adversarial examples. Almost all existing gradient-based attack methods use\nthe sign function in the generation to meet the requirement of perturbation\nbudget on $L_\\infty$ norm. However, we find that the sign function may be\nimproper for generating adversarial examples since it modifies the exact\ngradient direction. Instead of using the sign function, we propose to directly\nutilize the exact gradient direction with a scaling factor for generating\nadversarial perturbations, which improves the attack success rates of\nadversarial examples even with fewer perturbations. At the same time, we also\ntheoretically prove that this method can achieve better black-box\ntransferability. Moreover, considering that the best scaling factor varies\nacross different images, we propose an adaptive scaling factor generator to\nseek an appropriate scaling factor for each image, which avoids the\ncomputational cost for manually searching the scaling factor. Our method can be\nintegrated with almost all existing gradient-based attack methods to further\nimprove their attack success rates. Extensive experiments on the CIFAR10 and\nImageNet datasets show that our method exhibits higher transferability and\noutperforms the state-of-the-art methods.\n","authors":["Zheng Yuan","Jie Zhang","Zhaoyan Jiang","Liangliang Li","Shiguang Shan"],"pdf_url":"https://arxiv.org/pdf/2111.13841v3.pdf","comment":"Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI). 18 pages, 7 figures, 14 tables"},{"id":"http://arxiv.org/abs/2402.17487v1","updated":"2024-02-27T13:12:18Z","published":"2024-02-27T13:12:18Z","title":"Bit Rate Matching Algorithm Optimization in JPEG-AI Verification Model","summary":"  The research on neural network (NN) based image compression has shown\nsuperior performance compared to classical compression frameworks. Unlike the\nhand-engineered transforms in the classical frameworks, NN-based models learn\nthe non-linear transforms providing more compact bit representations, and\nachieve faster coding speed on parallel devices over their classical\ncounterparts. Those properties evoked the attention of both scientific and\nindustrial communities, resulting in the standardization activity JPEG-AI. The\nverification model for the standardization process of JPEG-AI is already in\ndevelopment and has surpassed the advanced VVC intra codec. To generate\nreconstructed images with the desired bits per pixel and assess the BD-rate\nperformance of both the JPEG-AI verification model and VVC intra, bit rate\nmatching is employed. However, the current state of the JPEG-AI verification\nmodel experiences significant slowdowns during bit rate matching, resulting in\nsuboptimal performance due to an unsuitable model. The proposed methodology\noffers a gradual algorithmic optimization for matching bit rates, resulting in\na fourfold acceleration and over 1% improvement in BD-rate at the base\noperation point. At the high operation point, the acceleration increases up to\nsixfold.\n","authors":["Panqi Jia","A. Burakhan Koyuncu","Jue Mao","Ze Cui","Yi Ma","Tiansheng Guo","Timofey Solovyev","Alexander Karabutov","Yin Zhao","Jing Wang","Elena Alshina","Andre Kaup"],"pdf_url":"https://arxiv.org/pdf/2402.17487v1.pdf","comment":"Accepted at (IEEE) PCS 2024; 6 pages"},{"id":"http://arxiv.org/abs/2402.17486v1","updated":"2024-02-27T13:12:00Z","published":"2024-02-27T13:12:00Z","title":"MGE: A Training-Free and Efficient Model Generation and Enhancement\n  Scheme","summary":"  To provide a foundation for the research of deep learning models, the\nconstruction of model pool is an essential step. This paper proposes a\nTraining-Free and Efficient Model Generation and Enhancement Scheme (MGE). This\nscheme primarily considers two aspects during the model generation process: the\ndistribution of model parameters and model performance. Experiments result\nshows that generated models are comparable to models obtained through normal\ntraining, and even superior in some cases. Moreover, the time consumed in\ngenerating models accounts for only 1\\% of the time required for normal model\ntraining. More importantly, with the enhancement of Evolution-MGE, generated\nmodels exhibits competitive generalization ability in few-shot tasks. And the\nbehavioral dissimilarity of generated models has the potential of adversarial\ndefense.\n","authors":["Xuan Wang","Zeshan Pang","Yuliang Lu","Xuehu Yan"],"pdf_url":"https://arxiv.org/pdf/2402.17486v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17485v1","updated":"2024-02-27T13:10:11Z","published":"2024-02-27T13:10:11Z","title":"EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with\n  Audio2Video Diffusion Model under Weak Conditions","summary":"  In this work, we tackle the challenge of enhancing the realism and\nexpressiveness in talking head video generation by focusing on the dynamic and\nnuanced relationship between audio cues and facial movements. We identify the\nlimitations of traditional techniques that often fail to capture the full\nspectrum of human expressions and the uniqueness of individual facial styles.\nTo address these issues, we propose EMO, a novel framework that utilizes a\ndirect audio-to-video synthesis approach, bypassing the need for intermediate\n3D models or facial landmarks. Our method ensures seamless frame transitions\nand consistent identity preservation throughout the video, resulting in highly\nexpressive and lifelike animations. Experimental results demonsrate that EMO is\nable to produce not only convincing speaking videos but also singing videos in\nvarious styles, significantly outperforming existing state-of-the-art\nmethodologies in terms of expressiveness and realism.\n","authors":["Linrui Tian","Qi Wang","Bang Zhang","Liefeng Bo"],"pdf_url":"https://arxiv.org/pdf/2402.17485v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17483v1","updated":"2024-02-27T13:08:47Z","published":"2024-02-27T13:08:47Z","title":"AlignMiF: Geometry-Aligned Multimodal Implicit Field for LiDAR-Camera\n  Joint Synthesis","summary":"  Neural implicit fields have been a de facto standard in novel view synthesis.\nRecently, there exist some methods exploring fusing multiple modalities within\na single field, aiming to share implicit features from different modalities to\nenhance reconstruction performance. However, these modalities often exhibit\nmisaligned behaviors: optimizing for one modality, such as LiDAR, can adversely\naffect another, like camera performance, and vice versa. In this work, we\nconduct comprehensive analyses on the multimodal implicit field of LiDAR-camera\njoint synthesis, revealing the underlying issue lies in the misalignment of\ndifferent sensors. Furthermore, we introduce AlignMiF, a geometrically aligned\nmultimodal implicit field with two proposed modules: Geometry-Aware Alignment\n(GAA) and Shared Geometry Initialization (SGI). These modules effectively align\nthe coarse geometry across different modalities, significantly enhancing the\nfusion process between LiDAR and camera data. Through extensive experiments\nacross various datasets and scenes, we demonstrate the effectiveness of our\napproach in facilitating better interaction between LiDAR and camera modalities\nwithin a unified neural field. Specifically, our proposed AlignMiF, achieves\nremarkable improvement over recent implicit fusion methods (+2.01 and +3.11\nimage PSNR on the KITTI-360 and Waymo datasets) and consistently surpasses\nsingle modality performance (13.8% and 14.2% reduction in LiDAR Chamfer\nDistance on the respective datasets).\n","authors":["Tao Tang","Guangrun Wang","Yixing Lao","Peng Chen","Jie Liu","Liang Lin","Kaicheng Yu","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2402.17483v1.pdf","comment":"CVPR2024"},{"id":"http://arxiv.org/abs/2402.17482v1","updated":"2024-02-27T13:08:34Z","published":"2024-02-27T13:08:34Z","title":"Automated Classification of Phonetic Segments in Child Speech Using Raw\n  Ultrasound Imaging","summary":"  Speech sound disorder (SSD) is defined as a persistent impairment in speech\nsound production leading to reduced speech intelligibility and hindered verbal\ncommunication. Early recognition and intervention of children with SSD and\ntimely referral to speech and language therapists (SLTs) for treatment are\ncrucial. Automated detection of speech impairment is regarded as an efficient\nmethod for examining and screening large populations. This study focuses on\nadvancing the automatic diagnosis of SSD in early childhood by proposing a\ntechnical solution that integrates ultrasound tongue imaging (UTI) with\ndeep-learning models. The introduced FusionNet model combines UTI data with the\nextracted texture features to classify UTI. The overarching aim is to elevate\nthe accuracy and efficiency of UTI analysis, particularly for classifying\nspeech sounds associated with SSD. This study compared the FusionNet approach\nwith standard deep-learning methodologies, highlighting the excellent\nimprovement results of the FusionNet model in UTI classification and the\npotential of multi-learning in improving UTI classification in speech therapy\nclinics.\n","authors":["Saja Al Ani","Joanne Cleland","Ahmed Zoha"],"pdf_url":"https://arxiv.org/pdf/2402.17482v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17470v1","updated":"2024-02-27T12:52:44Z","published":"2024-02-27T12:52:44Z","title":"Bit Distribution Study and Implementation of Spatial Quality Map in the\n  JPEG-AI Standardization","summary":"  Currently, there is a high demand for neural network-based image compression\ncodecs. These codecs employ non-linear transforms to create compact bit\nrepresentations and facilitate faster coding speeds on devices compared to the\nhand-crafted transforms used in classical frameworks. The scientific and\nindustrial communities are highly interested in these properties, leading to\nthe standardization effort of JPEG-AI. The JPEG-AI verification model has been\nreleased and is currently under development for standardization. Utilizing\nneural networks, it can outperform the classic codec VVC intra by over 10%\nBD-rate operating at base operation point. Researchers attribute this success\nto the flexible bit distribution in the spatial domain, in contrast to VVC\nintra's anchor that is generated with a constant quality point. However, our\nstudy reveals that VVC intra displays a more adaptable bit distribution\nstructure through the implementation of various block sizes. As a result of our\nobservations, we have proposed a spatial bit allocation method to optimize the\nJPEG-AI verification model's bit distribution and enhance the visual quality.\nFurthermore, by applying the VVC bit distribution strategy, the objective\nperformance of JPEG-AI verification mode can be further improved, resulting in\na maximum gain of 0.45 dB in PSNR-Y.\n","authors":["Panqi Jia","Jue Mao","Esin Koyuncu","A. Burakhan Koyuncu","Timofey Solovyev","Alexander Karabutov","Yin Zhao","Elena Alshina","Andre Kaup"],"pdf_url":"https://arxiv.org/pdf/2402.17470v1.pdf","comment":"5 pages, 3 figures, 4 tables"},{"id":"http://arxiv.org/abs/2402.17465v1","updated":"2024-02-27T12:42:07Z","published":"2024-02-27T12:42:07Z","title":"Model X-ray:Detect Backdoored Models via Decision Boundary","summary":"  Deep neural networks (DNNs) have revolutionized various industries, leading\nto the rise of Machine Learning as a Service (MLaaS). In this paradigm,\nwell-trained models are typically deployed through APIs. However, DNNs are\nsusceptible to backdoor attacks, which pose significant risks to their\napplications. This vulnerability necessitates a method for users to ascertain\nwhether an API is compromised before usage. Although many backdoor detection\nmethods have been developed, they often operate under the assumption that the\ndefender has access to specific information such as details of the attack, soft\npredictions from the model API, and even the knowledge of the model parameters,\nlimiting their practicality in MLaaS scenarios. To address it, in this paper,\nwe begin by presenting an intriguing observation: the decision boundary of the\nbackdoored model exhibits a greater degree of closeness than that of the clean\nmodel. Simultaneously, if only one single label is infected, a larger portion\nof the regions will be dominated by the attacked label. Building upon this\nobservation, we propose Model X-ray, a novel backdoor detection approach for\nMLaaS through the analysis of decision boundaries. Model X-ray can not only\nidentify whether the target API is infected by backdoor attacks but also\ndetermine the target attacked label under the all-to-one attack strategy.\nImportantly, it accomplishes this solely by the hard prediction of clean\ninputs, regardless of any assumptions about attacks and prior knowledge of the\ntraining details of the model. Extensive experiments demonstrated that Model\nX-ray can be effective for MLaaS across diverse backdoor attacks, datasets, and\narchitectures.\n","authors":["Yanghao Su","Jie Zhang","Ting Xu","Tianwei Zhang","Weiming Zhang","Nenghai Yu"],"pdf_url":"https://arxiv.org/pdf/2402.17465v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17464v1","updated":"2024-02-27T12:42:06Z","published":"2024-02-27T12:42:06Z","title":"Generative 3D Part Assembly via Part-Whole-Hierarchy Message Passing","summary":"  Generative 3D part assembly involves understanding part relationships and\npredicting their 6-DoF poses for assembling a realistic 3D shape. Prior work\noften focus on the geometry of individual parts, neglecting part-whole\nhierarchies of objects. Leveraging two key observations: 1) super-part poses\nprovide strong hints about part poses, and 2) predicting super-part poses is\neasier due to fewer superparts, we propose a part-whole-hierarchy message\npassing network for efficient 3D part assembly. We first introduce super-parts\nby grouping geometrically similar parts without any semantic labels. Then we\nemploy a part-whole hierarchical encoder, wherein a super-part encoder predicts\nlatent super-part poses based on input parts. Subsequently, we transform the\npoint cloud using the latent poses, feeding it to the part encoder for\naggregating super-part information and reasoning about part relationships to\npredict all part poses. In training, only ground-truth part poses are required.\nDuring inference, the predicted latent poses of super-parts enhance\ninterpretability. Experimental results on the PartNet dataset show that our\nmethod achieves state-of-the-art performance in part and connectivity accuracy\nand enables an interpretable hierarchical part assembly.\n","authors":["Bi'an Du","Xiang Gao","Wei Hu","Renjie Liao"],"pdf_url":"https://arxiv.org/pdf/2402.17464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17454v1","updated":"2024-02-27T12:26:45Z","published":"2024-02-27T12:26:45Z","title":"Segment anything model for head and neck tumor segmentation with CT, PET\n  and MRI multi-modality images","summary":"  Deep learning presents novel opportunities for the auto-segmentation of gross\ntumor volume (GTV) in head and neck cancer (HNC), yet fully automatic methods\nusually necessitate significant manual refinement. This study investigates the\nSegment Anything Model (SAM), recognized for requiring minimal human prompting\nand its zero-shot generalization ability across natural images. We specifically\nexamine MedSAM, a version of SAM fine-tuned with large-scale public medical\nimages. Despite its progress, the integration of multi-modality images (CT,\nPET, MRI) for effective GTV delineation remains a challenge. Focusing on SAM's\napplication in HNC GTV segmentation, we assess its performance in both\nzero-shot and fine-tuned scenarios using single (CT-only) and fused\nmulti-modality images. Our study demonstrates that fine-tuning SAM\nsignificantly enhances its segmentation accuracy, building upon the already\neffective zero-shot results achieved with bounding box prompts. These findings\nopen a promising avenue for semi-automatic HNC GTV segmentation.\n","authors":["Jintao Ren","Mathis Rasmussen","Jasper Nijkamp","Jesper Grau Eriksen","Stine Korreman"],"pdf_url":"https://arxiv.org/pdf/2402.17454v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.14723v2","updated":"2024-02-27T12:19:09Z","published":"2023-07-27T09:23:22Z","title":"EFLNet: Enhancing Feature Learning for Infrared Small Target Detection","summary":"  Single-frame infrared small target detection is considered to be a\nchallenging task, due to the extreme imbalance between target and background,\nbounding box regression is extremely sensitive to infrared small target, and\ntarget information is easy to lose in the high-level semantic layer. In this\narticle, we propose an enhancing feature learning network (EFLNet) to address\nthese problems. First, we notice that there is an extremely imbalance between\nthe target and the background in the infrared image, which makes the model pay\nmore attention to the background features rather than target features. To\naddress this problem, we propose a new adaptive threshold focal loss (ATFL)\nfunction that decouples the target and the background, and utilizes the\nadaptive mechanism to adjust the loss weight to force the model to allocate\nmore attention to target features. Second, we introduce the normalized Gaussian\nWasserstein distance (NWD) to alleviate the difficulty of convergence caused by\nthe extreme sensitivity of the bounding box regression to infrared small\ntarget. Finally, we incorporate a dynamic head mechanism into the network to\nenable adaptive learning of the relative importance of each semantic layer.\nExperimental results demonstrate our method can achieve better performance in\nthe detection performance of infrared small target compared to the\nstate-of-the-art (SOTA) deep-learning-based methods. The source codes and\nbounding box annotated datasets are available at\nhttps://github.com/YangBo0411/infrared-small-target.\n","authors":["Bo Yang","Xinyu Zhang","Jian Zhang","Jun Luo","Mingliang Zhou","Yangjun Pi"],"pdf_url":"https://arxiv.org/pdf/2307.14723v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17438v1","updated":"2024-02-27T11:50:44Z","published":"2024-02-27T11:50:44Z","title":"V2C-Long: Longitudinal Cortex Reconstruction with Spatiotemporal\n  Correspondence","summary":"  Reconstructing the cortex from longitudinal MRI is indispensable for\nanalyzing morphological changes in the human brain. Despite the recent\ndisruption of cortical surface reconstruction with deep learning, challenges\narising from longitudinal data are still persistent. Especially the lack of\nstrong spatiotemporal point correspondence hinders downstream analyses due to\nthe introduced noise. To address this issue, we present V2C-Long, the first\ndedicated deep learning-based cortex reconstruction method for longitudinal\nMRI. In contrast to existing methods, V2C-Long surfaces are directly comparable\nin a cross-sectional and longitudinal manner. We establish strong inherent\nspatiotemporal correspondences via a novel composition of two deep mesh\ndeformation networks and fast aggregation of feature-enhanced within-subject\ntemplates. The results on internal and external test data demonstrate that\nV2C-Long yields cortical surfaces with improved accuracy and consistency\ncompared to previous methods. Finally, this improvement manifests in higher\nsensitivity to regional cortical atrophy in Alzheimer's disease.\n","authors":["Fabian Bongratz","Jan Fecht","Anne-Marie Rickmann","Christian Wachinger"],"pdf_url":"https://arxiv.org/pdf/2402.17438v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2402.17430v1","updated":"2024-02-27T11:43:09Z","published":"2024-02-27T11:43:09Z","title":"Leveraging Enhanced Queries of Point Sets for Vectorized Map\n  Construction","summary":"  In autonomous driving, the high-definition (HD) map plays a crucial role in\nlocalization and planning. Recently, several methods have facilitated\nend-to-end online map construction in DETR-like frameworks. However, little\nattention has been paid to the potential capabilities of exploring the query\nmechanism. This paper introduces MapQR, an end-to-end method with an emphasis\non enhancing query capabilities for constructing online vectorized maps.\nAlthough the map construction is essentially a point set prediction task, MapQR\nutilizes instance queries rather than point queries. These instance queries are\nscattered for the prediction of point sets and subsequently gathered for the\nfinal matching. This query design, called the scatter-and-gather query, shares\ncontent information in the same map element and avoids possible inconsistency\nof content information in point queries. We further exploit prior information\nto enhance an instance query by adding positional information embedded from\ntheir reference points. Together with a simple and effective improvement of a\nBEV encoder, the proposed MapQR achieves the best mean average precision (mAP)\nand maintains good efficiency on both nuScenes and Argoverse 2. In addition,\nintegrating our query design into other models can boost their performance\nsignificantly. The code will be available at https://github.com/HXMap/MapQR.\n","authors":["Zihao Liu","Xiaoyu Zhang","Guangwei Liu","Ji Zhao","Ningyi Xu"],"pdf_url":"https://arxiv.org/pdf/2402.17430v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17427v1","updated":"2024-02-27T11:40:50Z","published":"2024-02-27T11:40:50Z","title":"VastGaussian: Vast 3D Gaussians for Large Scene Reconstruction","summary":"  Existing NeRF-based methods for large scene reconstruction often have\nlimitations in visual quality and rendering speed. While the recent 3D Gaussian\nSplatting works well on small-scale and object-centric scenes, scaling it up to\nlarge scenes poses challenges due to limited video memory, long optimization\ntime, and noticeable appearance variations. To address these challenges, we\npresent VastGaussian, the first method for high-quality reconstruction and\nreal-time rendering on large scenes based on 3D Gaussian Splatting. We propose\na progressive partitioning strategy to divide a large scene into multiple\ncells, where the training cameras and point cloud are properly distributed with\nan airspace-aware visibility criterion. These cells are merged into a complete\nscene after parallel optimization. We also introduce decoupled appearance\nmodeling into the optimization process to reduce appearance variations in the\nrendered images. Our approach outperforms existing NeRF-based methods and\nachieves state-of-the-art results on multiple large scene datasets, enabling\nfast optimization and high-fidelity real-time rendering.\n","authors":["Jiaqi Lin","Zhihao Li","Xiao Tang","Jianzhuang Liu","Shiyong Liu","Jiayue Liu","Yangdi Lu","Xiaofei Wu","Songcen Xu","Youliang Yan","Wenming Yang"],"pdf_url":"https://arxiv.org/pdf/2402.17427v1.pdf","comment":"Accepted to CVPR 2024. Project website:\n  https://vastgaussian.github.io"},{"id":"http://arxiv.org/abs/2401.10731v2","updated":"2024-02-27T11:37:52Z","published":"2024-01-19T14:49:42Z","title":"Removal and Selection: Improving RGB-Infrared Object Detection via\n  Coarse-to-Fine Fusion","summary":"  Object detection in visible (RGB) and infrared (IR) images has been widely\napplied in recent years. Leveraging the complementary characteristics of RGB\nand IR images, the object detector provides reliable and robust object\nlocalization from day to night. Existing fusion strategies directly inject RGB\nand IR images into convolution neural networks, leading to inferior detection\nperformance. Since the RGB and IR features have modality-specific noise, these\nstrategies will worsen the fused features along with the propagation. Inspired\nby the mechanism of human brain processing multimodal information, this work\nintroduces a new coarse-to-fine perspective to purify and fuse two modality\nfeatures. Specifically, following this perspective, we design a Redundant\nSpectrum Removal module to coarsely remove interfering information within each\nmodality and a Dynamic Feature Selection module to finely select the desired\nfeatures for feature fusion. To verify the effectiveness of the coarse-to-fine\nfusion strategy, we construct a new object detector called Removal and\nSelection Detector (RSDet). Extensive experiments on three RGB-IR object\ndetection datasets verify the superior performance of our method.\n","authors":["Tianyi Zhao","Maoxun Yuan","Xingxing Wei"],"pdf_url":"https://arxiv.org/pdf/2401.10731v2.pdf","comment":"9pages, 7figures"},{"id":"http://arxiv.org/abs/2402.17424v1","updated":"2024-02-27T11:32:37Z","published":"2024-02-27T11:32:37Z","title":"ViTaL: An Advanced Framework for Automated Plant Disease Identification\n  in Leaf Images Using Vision Transformers and Linear Projection For Feature\n  Reduction","summary":"  Our paper introduces a robust framework for the automated identification of\ndiseases in plant leaf images. The framework incorporates several key stages to\nenhance disease recognition accuracy. In the pre-processing phase, a thumbnail\nresizing technique is employed to resize images, minimizing the loss of\ncritical image details while ensuring computational efficiency. Normalization\nprocedures are applied to standardize image data before feature extraction.\nFeature extraction is facilitated through a novel framework built upon Vision\nTransformers, a state-of-the-art approach in image analysis. Additionally,\nalternative versions of the framework with an added layer of linear projection\nand blockwise linear projections are explored. This comparative analysis allows\nfor the evaluation of the impact of linear projection on feature extraction and\noverall model performance. To assess the effectiveness of the proposed\nframework, various Convolutional Neural Network (CNN) architectures are\nutilized, enabling a com- prehensive evaluation of linear projection's\ninfluence on key evaluation metrics. The findings demonstrate the efficacy of\nthe proposed framework, with the top- performing model achieving a Hamming loss\nof 0.054. Furthermore, we propose a novel hardware design specifically tailored\nfor scanning diseased leaves in an omnidirectional fashion. The hardware\nimplementation utilizes a Raspberry Pi Compute Module to address low-memory\nconfigurations, ensuring practicality and affordability. This innovative\nhardware solution enhances the overall feasibility and accessibility of the\nproposed automated disease identification system. This research contributes to\nthe field of agriculture by offering valuable insights and tools for the early\ndetection and management of plant diseases, potentially leading to improved\ncrop yields and enhanced food security.\n","authors":["Abhishek Sebastian","Annis Fathima A","Pragna R","Madhan Kumar S","Yaswanth Kannan G","Vinay Murali"],"pdf_url":"https://arxiv.org/pdf/2402.17424v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17420v1","updated":"2024-02-27T11:23:39Z","published":"2024-02-27T11:23:39Z","title":"PANDAS: Prototype-based Novel Class Discovery and Detection","summary":"  Object detectors are typically trained once and for all on a fixed set of\nclasses. However, this closed-world assumption is unrealistic in practice, as\nnew classes will inevitably emerge after the detector is deployed in the wild.\nIn this work, we look at ways to extend a detector trained for a set of base\nclasses so it can i) spot the presence of novel classes, and ii) automatically\nenrich its repertoire to be able to detect those newly discovered classes\ntogether with the base ones. We propose PANDAS, a method for novel class\ndiscovery and detection. It discovers clusters representing novel classes from\nunlabeled data, and represents old and new classes with prototypes. During\ninference, a distance-based classifier uses these prototypes to assign a label\nto each detected object instance. The simplicity of our method makes it widely\napplicable. We experimentally demonstrate the effectiveness of PANDAS on the\nVOC 2012 and COCO-to-LVIS benchmarks. It performs favorably against the state\nof the art for this task while being computationally more affordable.\n","authors":["Tyler L. Hayes","César R. de Souza","Namil Kim","Jiwon Kim","Riccardo Volpi","Diane Larlus"],"pdf_url":"https://arxiv.org/pdf/2402.17420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17417v1","updated":"2024-02-27T11:17:46Z","published":"2024-02-27T11:17:46Z","title":"CARZero: Cross-Attention Alignment for Radiology Zero-Shot\n  Classification","summary":"  The advancement of Zero-Shot Learning in the medical domain has been driven\nforward by using pre-trained models on large-scale image-text pairs, focusing\non image-text alignment. However, existing methods primarily rely on cosine\nsimilarity for alignment, which may not fully capture the complex relationship\nbetween medical images and reports. To address this gap, we introduce a novel\napproach called Cross-Attention Alignment for Radiology Zero-Shot\nClassification (CARZero). Our approach innovatively leverages cross-attention\nmechanisms to process image and report features, creating a Similarity\nRepresentation that more accurately reflects the intricate relationships in\nmedical semantics. This representation is then linearly projected to form an\nimage-text similarity matrix for cross-modality alignment. Additionally,\nrecognizing the pivotal role of prompt selection in zero-shot learning, CARZero\nincorporates a Large Language Model-based prompt alignment strategy. This\nstrategy standardizes diverse diagnostic expressions into a unified format for\nboth training and inference phases, overcoming the challenges of manual prompt\ndesign. Our approach is simple yet effective, demonstrating state-of-the-art\nperformance in zero-shot classification on five official chest radiograph\ndiagnostic test sets, including remarkable results on datasets with long-tail\ndistributions of rare diseases. This achievement is attributed to our new\nimage-text alignment strategy, which effectively addresses the complex\nrelationship between medical images and reports.\n","authors":["Haoran Lai","Qingsong Yao","Zihang Jiang","Rongsheng Wang","Zhiyang He","Xiaodong Tao","S. Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/2402.17417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17414v1","updated":"2024-02-27T11:08:51Z","published":"2024-02-27T11:08:51Z","title":"Neural Video Compression with Feature Modulation","summary":"  The emerging conditional coding-based neural video codec (NVC) shows\nsuperiority over commonly-used residual coding-based codec and the latest NVC\nalready claims to outperform the best traditional codec. However, there still\nexist critical problems blocking the practicality of NVC. In this paper, we\npropose a powerful conditional coding-based NVC that solves two critical\nproblems via feature modulation. The first is how to support a wide quality\nrange in a single model. Previous NVC with this capability only supports about\n3.8 dB PSNR range on average. To tackle this limitation, we modulate the latent\nfeature of the current frame via the learnable quantization scaler. During the\ntraining, we specially design the uniform quantization parameter sampling\nmechanism to improve the harmonization of encoding and quantization. This\nresults in a better learning of the quantization scaler and helps our NVC\nsupport about 11.4 dB PSNR range. The second is how to make NVC still work\nunder a long prediction chain. We expose that the previous SOTA NVC has an\nobvious quality degradation problem when using a large intra-period setting. To\nthis end, we propose modulating the temporal feature with a periodically\nrefreshing mechanism to boost the quality. %Besides solving the above two\nproblems, we also design a single model that can support both RGB and YUV\ncolorspaces. Notably, under single intra-frame setting, our codec can achieve\n29.7\\% bitrate saving over previous SOTA NVC with 16\\% MACs reduction. Our\ncodec serves as a notable landmark in the journey of NVC evolution. The codes\nare at https://github.com/microsoft/DCVC.\n","authors":["Jiahao Li","Bin Li","Yan Lu"],"pdf_url":"https://arxiv.org/pdf/2402.17414v1.pdf","comment":"CVPR 2024. Codes are at https://github.com/microsoft/DCVC"},{"id":"http://arxiv.org/abs/2402.17412v1","updated":"2024-02-27T11:05:34Z","published":"2024-02-27T11:05:34Z","title":"DiffuseKronA: A Parameter Efficient Fine-tuning Method for Personalized\n  Diffusion Model","summary":"  In the realm of subject-driven text-to-image (T2I) generative models, recent\ndevelopments like DreamBooth and BLIP-Diffusion have led to impressive results\nyet encounter limitations due to their intensive fine-tuning demands and\nsubstantial parameter requirements. While the low-rank adaptation (LoRA) module\nwithin DreamBooth offers a reduction in trainable parameters, it introduces a\npronounced sensitivity to hyperparameters, leading to a compromise between\nparameter efficiency and the quality of T2I personalized image synthesis.\nAddressing these constraints, we introduce \\textbf{\\textit{DiffuseKronA}}, a\nnovel Kronecker product-based adaptation module that not only significantly\nreduces the parameter count by 35\\% and 99.947\\% compared to LoRA-DreamBooth\nand the original DreamBooth, respectively, but also enhances the quality of\nimage synthesis. Crucially, \\textit{DiffuseKronA} mitigates the issue of\nhyperparameter sensitivity, delivering consistent high-quality generations\nacross a wide range of hyperparameters, thereby diminishing the necessity for\nextensive fine-tuning. Furthermore, a more controllable decomposition makes\n\\textit{DiffuseKronA} more interpretable and even can achieve up to a 50\\%\nreduction with results comparable to LoRA-Dreambooth. Evaluated against diverse\nand complex input images and text prompts, \\textit{DiffuseKronA} consistently\noutperforms existing models, producing diverse images of higher quality with\nimproved fidelity and a more accurate color distribution of objects, all the\nwhile upholding exceptional parameter efficiency, thus presenting a substantial\nadvancement in the field of T2I generative modeling. Our project page,\nconsisting of links to the code, and pre-trained checkpoints, is available at\n\\href{https://diffusekrona.github.io/}{https://diffusekrona.github.io/}.\n","authors":["Shyam Marjit","Harshit Singh","Nityanand Mathur","Sayak Paul","Chia-Mu Yu","Pin-Yu Chen"],"pdf_url":"https://arxiv.org/pdf/2402.17412v1.pdf","comment":"Project Page:\n  \\href{https://diffusekrona.github.io/}{https://diffusekrona.github.io/}"},{"id":"http://arxiv.org/abs/2402.17410v1","updated":"2024-02-27T11:01:58Z","published":"2024-02-27T11:01:58Z","title":"A novel image space formalism of Fourier domain interpolation neural\n  networks for noise propagation analysis","summary":"  Purpose: To develop an image space formalism of multi-layer convolutional\nneural networks (CNNs) for Fourier domain interpolation in MRI reconstructions\nand analytically estimate noise propagation during CNN inference. Theory and\nMethods: Nonlinear activations in the Fourier domain (also known as k-space)\nusing complex-valued Rectifier Linear Units are expressed as elementwise\nmultiplication with activation masks. This operation is transformed into a\nconvolution in the image space. After network training in k-space, this\napproach provides an algebraic expression for the derivative of the\nreconstructed image with respect to the aliased coil images, which serve as the\ninput tensors to the network in the image space. This allows the variance in\nthe network inference to be estimated analytically and to be used to describe\nnoise characteristics. Monte-Carlo simulations and numerical approaches based\non auto-differentiation were used for validation. The framework was tested on\nretrospectively undersampled invivo brain images. Results: Inferences conducted\nin the image domain are quasi-identical to inferences in the k-space,\nunderlined by corresponding quantitative metrics. Noise variance maps obtained\nfrom the analytical expression correspond with those obtained via Monte-Carlo\nsimulations, as well as via an auto-differentiation approach. The noise\nresilience is well characterized, as in the case of classical Parallel Imaging.\nKomolgorov-Smirnov tests demonstrate Gaussian distributions of voxel magnitudes\nin variance maps obtained via Monte-Carlo simulations. Conclusion: The\nquasi-equivalent image space formalism for neural networks for k-space\ninterpolation enables fast and accurate description of the noise\ncharacteristics during CNN inference, analogous to geometry-factor maps in\ntraditional parallel imaging methods.\n","authors":["Peter Dawood","Felix Breuer","Istvan Homolya","Jannik Stebani","Maximilian Gram","Peter M. Jakob","Moritz Zaiss","Martin Blaimer"],"pdf_url":"https://arxiv.org/pdf/2402.17410v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13641v2","updated":"2024-02-27T11:00:35Z","published":"2024-01-24T18:10:39Z","title":"How Good is ChatGPT at Face Biometrics? A First Look into Recognition,\n  Soft Biometrics, and Explainability","summary":"  Large Language Models (LLMs) such as GPT developed by OpenAI, have already\nshown astonishing results, introducing quick changes in our society. This has\nbeen intensified by the release of ChatGPT which allows anyone to interact in a\nsimple conversational way with LLMs, without any experience in the field\nneeded. As a result, ChatGPT has been rapidly applied to many different tasks\nsuch as code- and song-writer, education, virtual assistants, etc., showing\nimpressive results for tasks for which it was not trained (zero-shot learning).\n  The present study aims to explore the ability of ChatGPT, based on the recent\nGPT-4 multimodal LLM, for the task of face biometrics. In particular, we\nanalyze the ability of ChatGPT to perform tasks such as face verification,\nsoft-biometrics estimation, and explainability of the results. ChatGPT could be\nvery valuable to further increase the explainability and transparency of\nautomatic decisions in human scenarios. Experiments are carried out in order to\nevaluate the performance and robustness of ChatGPT, using popular public\nbenchmarks and comparing the results with state-of-the-art methods in the\nfield. The results achieved in this study show the potential of LLMs such as\nChatGPT for face biometrics, especially to enhance explainability. For\nreproducibility reasons, we release all the code in GitHub.\n","authors":["Ivan DeAndres-Tame","Ruben Tolosana","Ruben Vera-Rodriguez","Aythami Morales","Julian Fierrez","Javier Ortega-Garcia"],"pdf_url":"https://arxiv.org/pdf/2401.13641v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.15862v2","updated":"2024-02-27T10:59:33Z","published":"2022-05-28T11:12:38Z","title":"Snapture -- A Novel Neural Architecture for Combined Static and Dynamic\n  Hand Gesture Recognition","summary":"  As robots are expected to get more involved in people's everyday lives,\nframeworks that enable intuitive user interfaces are in demand. Hand gesture\nrecognition systems provide a natural way of communication and, thus, are an\nintegral part of seamless Human-Robot Interaction (HRI). Recent years have\nwitnessed an immense evolution of computational models powered by deep\nlearning. However, state-of-the-art models fall short in expanding across\ndifferent gesture domains, such as emblems and co-speech. In this paper, we\npropose a novel hybrid hand gesture recognition system. Our architecture\nenables learning both static and dynamic gestures: by capturing a so-called\n\"snapshot\" of the gesture performance at its peak, we integrate the hand pose\nalong with the dynamic movement. Moreover, we present a method for analyzing\nthe motion profile of a gesture to uncover its dynamic characteristics and\nwhich allows regulating a static channel based on the amount of motion. Our\nevaluation demonstrates the superiority of our approach on two gesture\nbenchmarks compared to a CNNLSTM baseline. We also provide an analysis on a\ngesture class basis that unveils the potential of our Snapture architecture for\nperformance improvements. Thanks to its modular implementation, our framework\nallows the integration of other multimodal data like facial expressions and\nhead tracking, which are important cues in HRI scenarios, into one\narchitecture. Thus, our work contributes both to gesture recognition research\nand machine learning applications for non-verbal communication with robots.\n","authors":["Hassan Ali","Doreen Jirak","Stefan Wermter"],"pdf_url":"https://arxiv.org/pdf/2205.15862v2.pdf","comment":"In Cognitive Computation(Accepted:30/06/2023,\n  Published:17/07/2023),20 pages,20 figures,4 tables;Please find the published\n  version/info to cite:\n  https://doi.org/10.1007/s12559-023-10174-z;Repositories:\n  https://zenodo.org/doi/10.5281/zenodo.10679196,\n  https://zenodo.org/doi/10.5281/zenodo.10693816;This work was co-funded by\n  Horizon Europe project TERAIS under Grant agreement number 101079338"},{"id":"http://arxiv.org/abs/2311.15537v2","updated":"2024-02-27T10:59:30Z","published":"2023-11-27T05:00:38Z","title":"SED: A Simple Encoder-Decoder for Open-Vocabulary Semantic Segmentation","summary":"  Open-vocabulary semantic segmentation strives to distinguish pixels into\ndifferent semantic groups from an open set of categories. Most existing methods\nexplore utilizing pre-trained vision-language models, in which the key is to\nadopt the image-level model for pixel-level segmentation task. In this paper,\nwe propose a simple encoder-decoder, named SED, for open-vocabulary semantic\nsegmentation, which comprises a hierarchical encoder-based cost map generation\nand a gradual fusion decoder with category early rejection. The hierarchical\nencoder-based cost map generation employs hierarchical backbone, instead of\nplain transformer, to predict pixel-level image-text cost map. Compared to\nplain transformer, hierarchical backbone better captures local spatial\ninformation and has linear computational complexity with respect to input size.\nOur gradual fusion decoder employs a top-down structure to combine cost map and\nthe feature maps of different backbone levels for segmentation. To accelerate\ninference speed, we introduce a category early rejection scheme in the decoder\nthat rejects many no-existing categories at the early layer of decoder,\nresulting in at most 4.7 times acceleration without accuracy degradation.\nExperiments are performed on multiple open-vocabulary semantic segmentation\ndatasets, which demonstrates the efficacy of our SED method. When using\nConvNeXt-B, our SED method achieves mIoU score of 31.6\\% on ADE20K with 150\ncategories at 82 millisecond ($ms$) per image on a single A6000. We will\nrelease it at \\url{https://github.com/xb534/SED.git}.\n","authors":["Bin Xie","Jiale Cao","Jin Xie","Fahad Shahbaz Khan","Yanwei Pang"],"pdf_url":"https://arxiv.org/pdf/2311.15537v2.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2402.17406v1","updated":"2024-02-27T10:55:07Z","published":"2024-02-27T10:55:07Z","title":"LSPT: Long-term Spatial Prompt Tuning for Visual Representation Learning","summary":"  Visual Prompt Tuning (VPT) techniques have gained prominence for their\ncapacity to adapt pre-trained Vision Transformers (ViTs) to downstream visual\ntasks using specialized learnable tokens termed as prompts. Contemporary VPT\nmethodologies, especially when employed with self-supervised vision\ntransformers, often default to the introduction of new learnable prompts or\ngated prompt tokens predominantly sourced from the model's previous block. A\npivotal oversight in such approaches is their failure to harness the potential\nof long-range previous blocks as sources of prompts within each self-supervised\nViT. To bridge this crucial gap, we introduce Long-term Spatial Prompt Tuning\n(LSPT) - a revolutionary approach to visual representation learning. Drawing\ninspiration from the intricacies of the human brain, LSPT ingeniously\nincorporates long-term gated prompts. This feature serves as temporal coding,\ncurbing the risk of forgetting parameters acquired from earlier blocks. Further\nenhancing its prowess, LSPT brings into play patch tokens, serving as spatial\ncoding. This is strategically designed to perpetually amass class-conscious\nfeatures, thereby fortifying the model's prowess in distinguishing and\nidentifying visual categories. To validate the efficacy of our proposed method,\nwe engaged in rigorous experimentation across 5 FGVC and 19 VTAB-1K benchmarks.\nOur empirical findings underscore the superiority of LSPT, showcasing its\nability to set new benchmarks in visual prompt tuning performance.\n","authors":["Shentong Mo","Yansen Wang","Xufang Luo","Dongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2402.17406v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06454v3","updated":"2024-02-27T10:54:36Z","published":"2023-12-11T15:41:05Z","title":"Point Transformer with Federated Learning for Predicting Breast Cancer\n  HER2 Status from Hematoxylin and Eosin-Stained Whole Slide Images","summary":"  Directly predicting human epidermal growth factor receptor 2 (HER2) status\nfrom widely available hematoxylin and eosin (HE)-stained whole slide images\n(WSIs) can reduce technical costs and expedite treatment selection. Accurately\npredicting HER2 requires large collections of multi-site WSIs. Federated\nlearning enables collaborative training of these WSIs without gigabyte-size\nWSIs transportation and data privacy concerns. However, federated learning\nencounters challenges in addressing label imbalance in multi-site WSIs from the\nreal world. Moreover, existing WSI classification methods cannot simultaneously\nexploit local context information and long-range dependencies in the site-end\nfeature representation of federated learning. To address these issues, we\npresent a point transformer with federated learning for multi-site HER2 status\nprediction from HE-stained WSIs. Our approach incorporates two novel designs.\nWe propose a dynamic label distribution strategy and an auxiliary classifier,\nwhich helps to establish a well-initialized model and mitigate label\ndistribution variations across sites. Additionally, we propose a farthest\ncosine sampling based on cosine distance. It can sample the most distinctive\nfeatures and capture the long-range dependencies. Extensive experiments and\nanalysis show that our method achieves state-of-the-art performance at four\nsites with a total of 2687 WSIs. Furthermore, we demonstrate that our model can\ngeneralize to two unseen sites with 229 WSIs.\n","authors":["Bao Li","Zhenyu Liu","Lizhi Shao","Bensheng Qiu","Hong Bu","Jie Tian"],"pdf_url":"https://arxiv.org/pdf/2312.06454v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17403v1","updated":"2024-02-27T10:49:05Z","published":"2024-02-27T10:49:05Z","title":"Sora Generates Videos with Stunning Geometrical Consistency","summary":"  The recently developed Sora model [1] has exhibited remarkable capabilities\nin video generation, sparking intense discussions regarding its ability to\nsimulate real-world phenomena. Despite its growing popularity, there is a lack\nof established metrics to evaluate its fidelity to real-world physics\nquantitatively. In this paper, we introduce a new benchmark that assesses the\nquality of the generated videos based on their adherence to real-world physics\nprinciples. We employ a method that transforms the generated videos into 3D\nmodels, leveraging the premise that the accuracy of 3D reconstruction is\nheavily contingent on the video quality. From the perspective of 3D\nreconstruction, we use the fidelity of the geometric constraints satisfied by\nthe constructed 3D models as a proxy to gauge the extent to which the generated\nvideos conform to real-world physics rules. Project page:\nhttps://sora-geometrical-consistency.github.io/\n","authors":["Xuanyi Li","Daquan Zhou","Chenxu Zhang","Shaodong Wei","Qibin Hou","Ming-Ming Cheng"],"pdf_url":"https://arxiv.org/pdf/2402.17403v1.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2308.00214v3","updated":"2024-02-27T10:41:58Z","published":"2023-08-01T01:12:29Z","title":"The Impact of Loss Functions and Scene Representations for 3D/2D\n  Registration on Single-view Fluoroscopic X-ray Pose Estimation","summary":"  Many tasks performed in image-guided procedures can be cast as pose\nestimation problems, where specific projections are chosen to reach a target in\n3D space. In this study, we first develop a differentiable projection\n(DiffProj) rendering framework for the efficient computation of Digitally\nReconstructed Radiographs (DRRs) with automatic differentiability from either\nCone-Beam Computerized Tomography (CBCT) or neural scene representations,\nincluding two newly proposed methods, Neural Tuned Tomography (NeTT) and masked\nNeural Radiance Fields (mNeRF). We then perform pose estimation by iterative\ngradient descent using various candidate loss functions, that quantify the\nimage discrepancy of the synthesized DRR with respect to the ground-truth\nfluoroscopic X-ray image. Compared to alternative loss functions, the Mutual\nInformation loss function can significantly improve pose estimation accuracy,\nas it can effectively prevent entrapment in local optima. Using the Mutual\nInformation loss, a comprehensive evaluation of pose estimation performed on a\ntomographic X-ray dataset of 50 patients$'$ skulls shows that utilizing either\ndiscretized (CBCT) or neural (NeTT/mNeRF) scene representations in DiffProj\nleads to comparable performance in DRR appearance and pose estimation (3D angle\nerrors: mean $\\leq$ 3.2{\\deg} and 90% quantile $\\leq$ 3.4{\\deg}), despite the\nlatter often incurring considerable training expenses and time. These findings\ncould be instrumental for selecting appropriate approaches to improve the\nefficiency and effectiveness of fluoroscopic X-ray pose estimation in\nwidespread image-guided interventions.\n","authors":["Chaochao Zhou","Syed Hasib Akhter Faruqui","Abhinav Patel","Ramez N. Abdalla","Michael C. Hurley","Ali Shaibani","Matthew B. Potts","Babak S. Jahromi","Sameer A. Ansari","Donald R. Cantrell"],"pdf_url":"https://arxiv.org/pdf/2308.00214v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17376v1","updated":"2024-02-27T10:13:30Z","published":"2024-02-27T10:13:30Z","title":"Accelerating Diffusion Sampling with Optimized Time Steps","summary":"  Diffusion probabilistic models (DPMs) have shown remarkable performance in\nhigh-resolution image synthesis, but their sampling efficiency is still to be\ndesired due to the typically large number of sampling steps. Recent\nadvancements in high-order numerical ODE solvers for DPMs have enabled the\ngeneration of high-quality images with much fewer sampling steps. While this is\na significant development, most sampling methods still employ uniform time\nsteps, which is not optimal when using a small number of steps. To address this\nissue, we propose a general framework for designing an optimization problem\nthat seeks more appropriate time steps for a specific numerical ODE solver for\nDPMs. This optimization problem aims to minimize the distance between the\nground-truth solution to the ODE and an approximate solution corresponding to\nthe numerical solver. It can be efficiently solved using the constrained trust\nregion method, taking less than $15$ seconds. Our extensive experiments on both\nunconditional and conditional sampling using pixel- and latent-space DPMs\ndemonstrate that, when combined with the state-of-the-art sampling method\nUniPC, our optimized time steps significantly improve image generation\nperformance in terms of FID scores for datasets such as CIFAR-10 and ImageNet,\ncompared to using uniform time steps.\n","authors":["Shuchen Xue","Zhaoqiang Liu","Fei Chen","Shifeng Zhang","Tianyang Hu","Enze Xie","Zhenguo Li"],"pdf_url":"https://arxiv.org/pdf/2402.17376v1.pdf","comment":"Accepted to CVPR 2024. Under camera-ready revision"},{"id":"http://arxiv.org/abs/2402.17372v1","updated":"2024-02-27T10:10:12Z","published":"2024-02-27T10:10:12Z","title":"Coupled Laplacian Eigenmaps for Locally-Aware 3D Rigid Point Cloud\n  Matching","summary":"  Point cloud matching, a crucial technique in computer vision, medical and\nrobotics fields, is primarily concerned with finding correspondences between\npairs of point clouds or voxels. In some practical scenarios, emphasizing local\ndifferences is crucial for accurately identifying a correct match, thereby\nenhancing the overall robustness and reliability of the matching process.\nCommonly used shape descriptors have several limitations and often fail to\nprovide meaningful local insights on the paired geometries. In this work, we\npropose a new technique, based on graph Laplacian eigenmaps, to match point\nclouds by taking into account fine local structures. To deal with the order and\nsign ambiguity of Laplacian eigenmaps, we introduce a new operator, called\nCoupled Laplacian, that allows to easily generate aligned eigenspaces for\nmultiple rigidly-registered geometries. We show that the similarity between\nthose aligned high-dimensional spaces provides a locally meaningful score to\nmatch shapes. We initially evaluate the performance of the proposed technique\nin a point-wise manner, specifically focusing on the task of object anomaly\nlocalization using the MVTec 3D-AD dataset. Additionally, we define a new\nmedical task, called automatic Bone Side Estimation (BSE), which we address\nthrough a global similarity score derived from coupled eigenspaces. In order to\ntest it, we propose a benchmark collecting bone surface structures from various\npublic datasets. Our matching technique, based on Coupled Laplacian,\noutperforms other methods by reaching an impressive accuracy on both tasks. The\ncode to reproduce our experiments is publicly available at\nhttps://github.com/matteo-bastico/CoupledLaplacian and in the Supplementary\nCode.\n","authors":["Matteo Bastico","Etienne Decencière","Laurent Corté","Yannick Tillier","David Ryckelynck"],"pdf_url":"https://arxiv.org/pdf/2402.17372v1.pdf","comment":"This paper has been accepted at Computer Vision and Patter\n  Recognition (CVPR) 2024"},{"id":"http://arxiv.org/abs/2402.17370v1","updated":"2024-02-27T10:09:29Z","published":"2024-02-27T10:09:29Z","title":"An Efficient MLP-based Point-guided Segmentation Network for Ore Images\n  with Ambiguous Boundary","summary":"  The precise segmentation of ore images is critical to the successful\nexecution of the beneficiation process. Due to the homogeneous appearance of\nthe ores, which leads to low contrast and unclear boundaries, accurate\nsegmentation becomes challenging, and recognition becomes problematic. This\npaper proposes a lightweight framework based on Multi-Layer Perceptron (MLP),\nwhich focuses on solving the problem of edge burring. Specifically, we\nintroduce a lightweight backbone better suited for efficiently extracting\nlow-level features. Besides, we design a feature pyramid network consisting of\ntwo MLP structures that balance local and global information thus enhancing\ndetection accuracy. Furthermore, we propose a novel loss function that guides\nthe prediction points to match the instance edge points to achieve clear object\nboundaries. We have conducted extensive experiments to validate the efficacy of\nour proposed method. Our approach achieves a remarkable processing speed of\nover 27 frames per second (FPS) with a model size of only 73 MB. Moreover, our\nmethod delivers a consistently high level of accuracy, with impressive\nperformance scores of 60.4 and 48.9 in~$AP_{50}^{box}$ and~$AP_{50}^{mask}$\nrespectively, as compared to the currently available state-of-the-art\ntechniques, when tested on the ore image dataset. The source code will be\nreleased at \\url{https://github.com/MVME-HBUT/ORENEXT}.\n","authors":["Guodong Sun","Yuting Peng","Le Cheng","Mengya Xu","An Wang","Bo Wu","Hongliang Ren","Yang Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.17370v1.pdf","comment":"10 pages, 8 figures"},{"id":"http://arxiv.org/abs/2402.17364v1","updated":"2024-02-27T09:56:15Z","published":"2024-02-27T09:56:15Z","title":"Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis","summary":"  Recent works in implicit representations, such as Neural Radiance Fields\n(NeRF), have advanced the generation of realistic and animatable head avatars\nfrom video sequences. These implicit methods are still confronted by visual\nartifacts and jitters, since the lack of explicit geometric constraints poses a\nfundamental challenge in accurately modeling complex facial deformations. In\nthis paper, we introduce Dynamic Tetrahedra (DynTet), a novel hybrid\nrepresentation that encodes explicit dynamic meshes by neural networks to\nensure geometric consistency across various motions and viewpoints. DynTet is\nparameterized by the coordinate-based networks which learn signed distance,\ndeformation, and material texture, anchoring the training data into a\npredefined tetrahedra grid. Leveraging Marching Tetrahedra, DynTet efficiently\ndecodes textured meshes with a consistent topology, enabling fast rendering\nthrough a differentiable rasterizer and supervision via a pixel loss. To\nenhance training efficiency, we incorporate classical 3D Morphable Models to\nfacilitate geometry learning and define a canonical space for simplifying\ntexture learning. These advantages are readily achievable owing to the\neffective geometric representation employed in DynTet. Compared with prior\nworks, DynTet demonstrates significant improvements in fidelity, lip\nsynchronization, and real-time performance according to various metrics. Beyond\nproducing stable and visually appealing synthesis videos, our method also\noutputs the dynamic meshes which is promising to enable many emerging\napplications.\n","authors":["Zicheng Zhang","Ruobing Zheng","Ziwen Liu","Congying Han","Tianqi Li","Meng Wang","Tiande Guo","Jingdong Chen","Bonan Li","Ming Yang"],"pdf_url":"https://arxiv.org/pdf/2402.17364v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2402.17360v1","updated":"2024-02-27T09:53:16Z","published":"2024-02-27T09:53:16Z","title":"CAPT: Category-level Articulation Estimation from a Single Point Cloud\n  Using Transformer","summary":"  The ability to estimate joint parameters is essential for various\napplications in robotics and computer vision. In this paper, we propose CAPT:\ncategory-level articulation estimation from a point cloud using Transformer.\nCAPT uses an end-to-end transformer-based architecture for joint parameter and\nstate estimation of articulated objects from a single point cloud. The proposed\nCAPT methods accurately estimate joint parameters and states for various\narticulated objects with high precision and robustness. The paper also\nintroduces a motion loss approach, which improves articulation estimation\nperformance by emphasizing the dynamic features of articulated objects.\nAdditionally, the paper presents a double voting strategy to provide the\nframework with coarse-to-fine parameter estimation. Experimental results on\nseveral category datasets demonstrate that our methods outperform existing\nalternatives for articulation estimation. Our research provides a promising\nsolution for applying Transformer-based architectures in articulated object\nanalysis.\n","authors":["Lian Fu","Ryoichi Ishikawa","Yoshihiro Sato","Takeshi Oishi"],"pdf_url":"https://arxiv.org/pdf/2402.17360v1.pdf","comment":"Accepted to ICRA 2024"},{"id":"http://arxiv.org/abs/2302.01622v4","updated":"2024-02-27T09:42:10Z","published":"2023-02-03T09:49:13Z","title":"Private, fair and accurate: Training large-scale, privacy-preserving AI\n  models in medical imaging","summary":"  Artificial intelligence (AI) models are increasingly used in the medical\ndomain. However, as medical data is highly sensitive, special precautions to\nensure its protection are required. The gold standard for privacy preservation\nis the introduction of differential privacy (DP) to model training. Prior work\nindicates that DP has negative implications on model accuracy and fairness,\nwhich are unacceptable in medicine and represent a main barrier to the\nwidespread use of privacy-preserving techniques. In this work, we evaluated the\neffect of privacy-preserving training of AI models regarding accuracy and\nfairness compared to non-private training. For this, we used two datasets: (1)\nA large dataset (N=193,311) of high quality clinical chest radiographs, and (2)\na dataset (N=1,625) of 3D abdominal computed tomography (CT) images, with the\ntask of classifying the presence of pancreatic ductal adenocarcinoma (PDAC).\nBoth were retrospectively collected and manually labeled by experienced\nradiologists. We then compared non-private deep convolutional neural networks\n(CNNs) and privacy-preserving (DP) models with respect to privacy-utility\ntrade-offs measured as area under the receiver-operator-characteristic curve\n(AUROC), and privacy-fairness trade-offs, measured as Pearson's r or\nStatistical Parity Difference. We found that, while the privacy-preserving\ntrainings yielded lower accuracy, they did largely not amplify discrimination\nagainst age, sex or co-morbidity. Our study shows that -- under the challenging\nrealistic circumstances of a real-life clinical dataset -- the\nprivacy-preserving training of diagnostic deep learning models is possible with\nexcellent diagnostic accuracy and fairness.\n","authors":["Soroosh Tayebi Arasteh","Alexander Ziller","Christiane Kuhl","Marcus Makowski","Sven Nebelung","Rickmer Braren","Daniel Rueckert","Daniel Truhn","Georgios Kaissis"],"pdf_url":"https://arxiv.org/pdf/2302.01622v4.pdf","comment":"Published in Communications Medicine. Nature Portfolio"},{"id":"http://arxiv.org/abs/2402.17351v1","updated":"2024-02-27T09:41:59Z","published":"2024-02-27T09:41:59Z","title":"ICP-Flow: LiDAR Scene Flow Estimation with ICP","summary":"  Scene flow characterizes the 3D motion between two LiDAR scans captured by an\nautonomous vehicle at nearby timesteps. Prevalent methods consider scene flow\nas point-wise unconstrained flow vectors that can be learned by either\nlarge-scale training beforehand or time-consuming optimization at inference.\nHowever, these methods do not take into account that objects in autonomous\ndriving often move rigidly. We incorporate this rigid-motion assumption into\nour design, where the goal is to associate objects over scans and then estimate\nthe locally rigid transformations. We propose ICP-Flow, a learning-free flow\nestimator. The core of our design is the conventional Iterative Closest Point\n(ICP) algorithm, which aligns the objects over time and outputs the\ncorresponding rigid transformations. Crucially, to aid ICP, we propose a\nhistogram-based initialization that discovers the most likely translation, thus\nproviding a good starting point for ICP. The complete scene flow is then\nrecovered from the rigid transformations. We outperform state-of-the-art\nbaselines, including supervised models, on the Waymo dataset and perform\ncompetitively on Argoverse-v2 and nuScenes. Further, we train a feedforward\nneural network, supervised by the pseudo labels from our model, and achieve top\nperformance among all models capable of real-time inference. We validate the\nadvantage of our model on scene flow estimation with longer temporal gaps, up\nto 0.5 seconds where other models fail to deliver meaningful results.\n","authors":["Yancong Lin","Holger Caesar"],"pdf_url":"https://arxiv.org/pdf/2402.17351v1.pdf","comment":"The IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  2024"},{"id":"http://arxiv.org/abs/2402.17339v1","updated":"2024-02-27T09:13:27Z","published":"2024-02-27T09:13:27Z","title":"SocialCVAE: Predicting Pedestrian Trajectory via Interaction Conditioned\n  Latents","summary":"  Pedestrian trajectory prediction is the key technology in many applications\nfor providing insights into human behavior and anticipating human future\nmotions. Most existing empirical models are explicitly formulated by observed\nhuman behaviors using explicable mathematical terms with a deterministic\nnature, while recent work has focused on developing hybrid models combined with\nlearning-based techniques for powerful expressiveness while maintaining\nexplainability. However, the deterministic nature of the learned steering\nbehaviors from the empirical models limits the models' practical performance.\nTo address this issue, this work proposes the social conditional variational\nautoencoder (SocialCVAE) for predicting pedestrian trajectories, which employs\na CVAE to explore behavioral uncertainty in human motion decisions. SocialCVAE\nlearns socially reasonable motion randomness by utilizing a socially\nexplainable interaction energy map as the CVAE's condition, which illustrates\nthe future occupancy of each pedestrian's local neighborhood area. The energy\nmap is generated using an energy-based interaction model, which anticipates the\nenergy cost (i.e., repulsion intensity) of pedestrians' interactions with\nneighbors. Experimental results on two public benchmarks including 25 scenes\ndemonstrate that SocialCVAE significantly improves prediction accuracy compared\nwith the state-of-the-art methods, with up to 16.85% improvement in Average\nDisplacement Error (ADE) and 69.18% improvement in Final Displacement Error\n(FDE).\n","authors":["Wei Xiang","Haoteng Yin","He Wang","Xiaogang Jin"],"pdf_url":"https://arxiv.org/pdf/2402.17339v1.pdf","comment":"Accepted by AAAI'24"},{"id":"http://arxiv.org/abs/2402.17323v1","updated":"2024-02-27T09:01:03Z","published":"2024-02-27T09:01:03Z","title":"SDDGR: Stable Diffusion-based Deep Generative Replay for Class\n  Incremental Object Detection","summary":"  In the field of class incremental learning (CIL), genera- tive replay has\nbecome increasingly prominent as a method to mitigate the catastrophic\nforgetting, alongside the con- tinuous improvements in generative models.\nHowever, its application in class incremental object detection (CIOD) has been\nsignificantly limited, primarily due to the com- plexities of scenes involving\nmultiple labels. In this paper, we propose a novel approach called stable\ndiffusion deep generative replay (SDDGR) for CIOD. Our method utilizes a\ndiffusion-based generative model with pre-trained text- to-diffusion networks\nto generate realistic and diverse syn- thetic images. SDDGR incorporates an\niterative refinement strategy to produce high-quality images encompassing old\nclasses. Additionally, we adopt an L2 knowledge distilla- tion technique to\nimprove the retention of prior knowledge in synthetic images. Furthermore, our\napproach includes pseudo-labeling for old objects within new task images, pre-\nventing misclassification as background elements. Exten- sive experiments on\nthe COCO 2017 dataset demonstrate that SDDGR significantly outperforms existing\nalgorithms, achieving a new state-of-the-art in various CIOD scenarios. The\nsource code will be made available to the public.\n","authors":["Junsu Kim","Hoseong Cho","Jihyeon Kim","Yihalem Yimolal Tiruneh","Seungryul Baek"],"pdf_url":"https://arxiv.org/pdf/2402.17323v1.pdf","comment":"Accepted to CVPR 2024. We will post a camera-ready version later"},{"id":"http://arxiv.org/abs/2402.17319v1","updated":"2024-02-27T08:51:20Z","published":"2024-02-27T08:51:20Z","title":"A Vanilla Multi-Task Framework for Dense Visual Prediction Solution to\n  1st VCL Challenge -- Multi-Task Robustness Track","summary":"  In this report, we present our solution to the multi-task robustness track of\nthe 1st Visual Continual Learning (VCL) Challenge at ICCV 2023 Workshop. We\npropose a vanilla framework named UniNet that seamlessly combines various\nvisual perception algorithms into a multi-task model. Specifically, we choose\nDETR3D, Mask2Former, and BinsFormer for 3D object detection, instance\nsegmentation, and depth estimation tasks, respectively. The final submission is\na single model with InternImage-L backbone, and achieves a 49.6 overall score\n(29.5 Det mAP, 80.3 mTPS, 46.4 Seg mAP, and 7.93 silog) on SHIFT validation\nset. Besides, we provide some interesting observations in our experiments which\nmay facilitate the development of multi-task learning in dense visual\nprediction.\n","authors":["Zehui Chen","Qiuchen Wang","Zhenyu Li","Jiaming Liu","Shanghang Zhang","Feng Zhao"],"pdf_url":"https://arxiv.org/pdf/2402.17319v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2402.17318v1","updated":"2024-02-27T08:50:45Z","published":"2024-02-27T08:50:45Z","title":"Scaling Supervised Local Learning with Augmented Auxiliary Networks","summary":"  Deep neural networks are typically trained using global error signals that\nbackpropagate (BP) end-to-end, which is not only biologically implausible but\nalso suffers from the update locking problem and requires huge memory\nconsumption. Local learning, which updates each layer independently with a\ngradient-isolated auxiliary network, offers a promising alternative to address\nthe above problems. However, existing local learning methods are confronted\nwith a large accuracy gap with the BP counterpart, particularly for large-scale\nnetworks. This is due to the weak coupling between local layers and their\nsubsequent network layers, as there is no gradient communication across layers.\nTo tackle this issue, we put forward an augmented local learning method, dubbed\nAugLocal. AugLocal constructs each hidden layer's auxiliary network by\nuniformly selecting a small subset of layers from its subsequent network layers\nto enhance their synergy. We also propose to linearly reduce the depth of\nauxiliary networks as the hidden layer goes deeper, ensuring sufficient network\ncapacity while reducing the computational cost of auxiliary networks. Our\nextensive experiments on four image classification datasets (i.e., CIFAR-10,\nSVHN, STL-10, and ImageNet) demonstrate that AugLocal can effectively scale up\nto tens of local layers with a comparable accuracy to BP-trained networks while\nreducing GPU memory usage by around 40%. The proposed AugLocal method,\ntherefore, opens up a myriad of opportunities for training high-performance\ndeep neural networks on resource-constrained platforms.Code is available at\nhttps://github.com/ChenxiangMA/AugLocal.\n","authors":["Chenxiang Ma","Jibin Wu","Chenyang Si","Kay Chen Tan"],"pdf_url":"https://arxiv.org/pdf/2402.17318v1.pdf","comment":"Accepted by ICLR 2024"},{"id":"http://arxiv.org/abs/2402.17317v1","updated":"2024-02-27T08:49:30Z","published":"2024-02-27T08:49:30Z","title":"How we won BraTS 2023 Adult Glioma challenge? Just faking it! Enhanced\n  Synthetic Data Augmentation and Model Ensemble for brain tumour segmentation","summary":"  Deep Learning is the state-of-the-art technology for segmenting brain\ntumours. However, this requires a lot of high-quality data, which is difficult\nto obtain, especially in the medical field. Therefore, our solutions address\nthis problem by using unconventional mechanisms for data augmentation.\nGenerative adversarial networks and registration are used to massively increase\nthe amount of available samples for training three different deep learning\nmodels for brain tumour segmentation, the first task of the BraTS2023\nchallenge. The first model is the standard nnU-Net, the second is the Swin\nUNETR and the third is the winning solution of the BraTS 2021 Challenge. The\nentire pipeline is built on the nnU-Net implementation, except for the\ngeneration of the synthetic data. The use of convolutional algorithms and\ntransformers is able to fill each other's knowledge gaps. Using the new metric,\nour best solution achieves the dice results 0.9005, 0.8673, 0.8509 and HD95\n14.940, 14.467, 17.699 (whole tumour, tumour core and enhancing tumour) in the\nvalidation set.\n","authors":["André Ferreira","Naida Solak","Jianning Li","Philipp Dammann","Jens Kleesiek","Victor Alves","Jan Egger"],"pdf_url":"https://arxiv.org/pdf/2402.17317v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17316v1","updated":"2024-02-27T08:47:19Z","published":"2024-02-27T08:47:19Z","title":"Towards Robust and Efficient Cloud-Edge Elastic Model Adaptation via\n  Selective Entropy Distillation","summary":"  The conventional deep learning paradigm often involves training a deep model\non a server and then deploying the model or its distilled ones to\nresource-limited edge devices. Usually, the models shall remain fixed once\ndeployed (at least for some period) due to the potential high cost of model\nadaptation for both the server and edge sides. However, in many real-world\nscenarios, the test environments may change dynamically (known as distribution\nshifts), which often results in degraded performance. Thus, one has to adapt\nthe edge models promptly to attain promising performance. Moreover, with the\nincreasing data collected at the edge, this paradigm also fails to further\nadapt the cloud model for better performance. To address these, we encounter\ntwo primary challenges: 1) the edge model has limited computation power and may\nonly support forward propagation; 2) the data transmission budget between cloud\nand edge devices is limited in latency-sensitive scenarios. In this paper, we\nestablish a Cloud-Edge Elastic Model Adaptation (CEMA) paradigm in which the\nedge models only need to perform forward propagation and the edge models can be\nadapted online. In our CEMA, to reduce the communication burden, we devise two\ncriteria to exclude unnecessary samples from uploading to the cloud, i.e.,\ndynamic unreliable and low-informative sample exclusion. Based on the uploaded\nsamples, we update and distribute the affine parameters of normalization layers\nby distilling from the stronger foundation model to the edge model with a\nsample replay strategy. Extensive experimental results on ImageNet-C and\nImageNet-R verify the effectiveness of our CEMA.\n","authors":["Yaofo Chen","Shuaicheng Niu","Shoukai Xu","Hengjie Song","Yaowei Wang","Mingkui Tan"],"pdf_url":"https://arxiv.org/pdf/2402.17316v1.pdf","comment":"Published in ICLR 2024"},{"id":"http://arxiv.org/abs/2310.06958v4","updated":"2024-02-27T08:34:43Z","published":"2023-10-10T19:21:41Z","title":"Comparing the Robustness of Modern No-Reference Image- and Video-Quality\n  Metrics to Adversarial Attacks","summary":"  Nowadays, neural-network-based image- and video-quality metrics perform\nbetter than traditional methods. However, they also became more vulnerable to\nadversarial attacks that increase metrics' scores without improving visual\nquality. The existing benchmarks of quality metrics compare their performance\nin terms of correlation with subjective quality and calculation time.\nNonetheless, the adversarial robustness of image-quality metrics is also an\narea worth researching. This paper analyses modern metrics' robustness to\ndifferent adversarial attacks. We adapted adversarial attacks from computer\nvision tasks and compared attacks' efficiency against 15 no-reference image-\nand video-quality metrics. Some metrics showed high resistance to adversarial\nattacks, which makes their usage in benchmarks safer than vulnerable metrics.\nThe benchmark accepts submissions of new metrics for researchers who want to\nmake their metrics more robust to attacks or to find such metrics for their\nneeds. The latest results can be found online:\nhttps://videoprocessing.ai/benchmarks/metrics-robustness.html.\n","authors":["Anastasia Antsiferova","Khaled Abud","Aleksandr Gushchin","Ekaterina Shumitskaya","Sergey Lavrushkin","Dmitriy Vatolin"],"pdf_url":"https://arxiv.org/pdf/2310.06958v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17310v1","updated":"2024-02-27T08:33:03Z","published":"2024-02-27T08:33:03Z","title":"Method of Tracking and Analysis of Fluorescent-Labeled Cells Using\n  Automatic Thresholding and Labeling","summary":"  High-throughput screening using cell images is an efficient method for\nscreening new candidates for pharmaceutical drugs. To complete the screening\nprocess, it is essential to have an efficient process for analyzing cell\nimages. This paper presents a new method for efficiently tracking cells and\nquantitatively detecting the signal ratio between cytoplasm and nuclei.\nExisting methods include those that use image processing techniques and those\nthat utilize artificial intelligence (AI). However, these methods do not\nconsider the correspondence of cells between images, or require a significant\namount of new learning data to train AI. Therefore, our method uses automatic\nthresholding and labeling algorithms to compare the position of each cell\nbetween images, and continuously measure and analyze the signal ratio of cells.\nThis paper describes the algorithm of our method. Using the method, we\nexperimented to investigate the effect of the number of opening and closing\noperations during the binarization process on the tracking of the cells.\nThrough the experiment, we determined the appropriate number of opening and\nclosing processes.\n","authors":["Mizuki Fukasawa","Tomokazu Fukuda","Takuya Akashi"],"pdf_url":"https://arxiv.org/pdf/2402.17310v1.pdf","comment":"5 pages, 7 figures"},{"id":"http://arxiv.org/abs/2402.17307v1","updated":"2024-02-27T08:31:39Z","published":"2024-02-27T08:31:39Z","title":"Denoising Diffusion Models for Inpainting of Healthy Brain Tissue","summary":"  This paper is a contribution to the \"BraTS 2023 Local Synthesis of Healthy\nBrain Tissue via Inpainting Challenge\". The task of this challenge is to\ntransform tumor tissue into healthy tissue in brain magnetic resonance (MR)\nimages. This idea originates from the problem that MR images can be evaluated\nusing automatic processing tools, however, many of these tools are optimized\nfor the analysis of healthy tissue. By solving the given inpainting task, we\nenable the automatic analysis of images featuring lesions, and further\ndownstream tasks. Our approach builds on denoising diffusion probabilistic\nmodels. We use a 2D model that is trained using slices in which healthy tissue\nwas cropped out and is learned to be inpainted again. This allows us to use the\nground truth healthy tissue during training. In the sampling stage, we replace\nthe slices containing diseased tissue in the original 3D volume with the slices\ncontaining the healthy tissue inpainting. With our approach, we achieve\ncomparable results to the competing methods. On the validation set our model\nachieves a mean SSIM of 0.7804, a PSNR of 20.3525 and a MSE of 0.0113. In\nfuture we plan to extend our 2D model to a 3D model, allowing to inpaint the\nregion of interest as a whole without losing context information of neighboring\nslices.\n","authors":["Alicia Durrer","Philippe C. Cattin","Julia Wolleb"],"pdf_url":"https://arxiv.org/pdf/2402.17307v1.pdf","comment":"12 pages, 5 figures, MICCAI challenge submission"},{"id":"http://arxiv.org/abs/2402.17298v1","updated":"2024-02-27T08:20:45Z","published":"2024-02-27T08:20:45Z","title":"ArcSin: Adaptive ranged cosine Similarity injected noise for\n  Language-Driven Visual Tasks","summary":"  In this study, we address the challenging task of bridging the modality gap\nbetween learning from language and inference for visual tasks, including Visual\nQuestion Answering (VQA), Image Captioning (IC) and Visual Entailment (VE). We\ntrain models for these tasks in a zero-shot cross-modal transfer setting, a\ndomain where the previous state-of-the-art method relied on the fixed scale\nnoise injection, often compromising the semantic content of the original\nmodality embedding. To combat it, we propose a novel method called Adaptive\nranged cosine Similarity injected noise (ArcSin). First, we introduce an\ninnovative adaptive noise scale that effectively generates the textual elements\nwith more variability while preserving the original text feature's integrity.\nSecond, a similarity pool strategy is employed, expanding the domain\ngeneralization potential by broadening the overall noise scale. This dual\nstrategy effectively widens the scope of the original domain while safeguarding\ncontent integrity. Our empirical results demonstrate that these models closely\nrival those trained on images in terms of performance. Specifically, our method\nexhibits substantial improvements over the previous state-of-the-art, achieving\ngains of 1.9 and 1.1 CIDEr points in S-Cap and M-Cap, respectively.\nAdditionally, we observe increases of 1.5 percentage points (pp), 1.4 pp, and\n1.4 pp in accuracy for VQA, VQA-E, and VE, respectively, pushing the boundaries\nof what is achievable within the constraints of image-trained model benchmarks.\nThe code will be released.\n","authors":["Yang Liu","Xiaomin Yu","Gongyu Zhang","Christos Bergeles","Prokar Dasgupta","Alejandro Granados","Sebastien Ourselin"],"pdf_url":"https://arxiv.org/pdf/2402.17298v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17296v1","updated":"2024-02-27T08:19:51Z","published":"2024-02-27T08:19:51Z","title":"Learning Exposure Correction in Dynamic Scenes","summary":"  Capturing videos with wrong exposure usually produces unsatisfactory visual\neffects. While image exposure correction is a popular topic, the video\ncounterpart is less explored in the literature. Directly applying prior\nimage-based methods to input videos often results in temporal incoherence with\nlow visual quality. Existing research in this area is also limited by the lack\nof high-quality benchmark datasets. To address these issues, we construct the\nfirst real-world paired video dataset, including both underexposure and\noverexposure dynamic scenes. To achieve spatial alignment, we utilize two DSLR\ncameras and a beam splitter to simultaneously capture improper and normal\nexposure videos. In addition, we propose a Video Exposure Correction Network\n(VECNet) based on Retinex theory, which incorporates a two-stream illumination\nlearning mechanism to enhance the overexposure and underexposure factors,\nrespectively. The estimated multi-frame reflectance and dual-path illumination\ncomponents are fused at both feature and image levels, leading to visually\nappealing results. Experimental results demonstrate that the proposed method\noutperforms existing image exposure correction and underexposed video\nenhancement methods. The code and dataset will be available soon.\n","authors":["Jin Liu","Bo Wang","Chuanming Wang","Huiyuan Fu","Huadong Ma"],"pdf_url":"https://arxiv.org/pdf/2402.17296v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.19944v2","updated":"2024-02-27T08:16:32Z","published":"2023-10-30T18:59:32Z","title":"Conditional Unscented Autoencoders for Trajectory Prediction","summary":"  The CVAE is one of the most widely-used models in trajectory prediction for\nAD. It captures the interplay between a driving context and its ground-truth\nfuture into a probabilistic latent space and uses it to produce predictions. In\nthis paper, we challenge key components of the CVAE. We leverage recent\nadvances in the space of the VAE, the foundation of the CVAE, which show that a\nsimple change in the sampling procedure can greatly benefit performance. We\nfind that unscented sampling, which draws samples from any learned distribution\nin a deterministic manner, can naturally be better suited to trajectory\nprediction than potentially dangerous random sampling. We go further and offer\nadditional improvements including a more structured Gaussian mixture latent\nspace, as well as a novel, potentially more expressive way to do inference with\nCVAEs. We show wide applicability of our models by evaluating them on the\nINTERACTION prediction dataset, outperforming the state of the art, as well as\nat the task of image modeling on the CelebA dataset, outperforming the baseline\nvanilla CVAE. Code is available at\nhttps://github.com/boschresearch/cuae-prediction.\n","authors":["Faris Janjoš","Marcel Hallgarten","Anthony Knittel","Maxim Dolgov","Andreas Zell","J. Marius Zöllner"],"pdf_url":"https://arxiv.org/pdf/2310.19944v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17292v1","updated":"2024-02-27T08:10:31Z","published":"2024-02-27T08:10:31Z","title":"DivAvatar: Diverse 3D Avatar Generation with a Single Prompt","summary":"  Text-to-Avatar generation has recently made significant strides due to\nadvancements in diffusion models. However, most existing work remains\nconstrained by limited diversity, producing avatars with subtle differences in\nappearance for a given text prompt. We design DivAvatar, a novel framework that\ngenerates diverse avatars, empowering 3D creatives with a multitude of distinct\nand richly varied 3D avatars from a single text prompt. Different from most\nexisting work that exploits scene-specific 3D representations such as NeRF,\nDivAvatar finetunes a 3D generative model (i.e., EVA3D), allowing diverse\navatar generation from simply noise sampling in inference time. DivAvatar has\ntwo key designs that help achieve generation diversity and visual quality. The\nfirst is a noise sampling technique during training phase which is critical in\ngenerating diverse appearances. The second is a semantic-aware zoom mechanism\nand a novel depth loss, the former producing appearances of high textual\nfidelity by separate fine-tuning of specific body parts and the latter\nimproving geometry quality greatly by smoothing the generated mesh in the\nfeatures space. Extensive experiments show that DivAvatar is highly versatile\nin generating avatars of diverse appearances.\n","authors":["Weijing Tao","Biwen Lei","Kunhao Liu","Shijian Lu","Miaomiao Cui","Xuansong Xie","Chunyan Miao"],"pdf_url":"https://arxiv.org/pdf/2402.17292v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05408v2","updated":"2024-02-27T08:04:11Z","published":"2024-02-08T04:52:36Z","title":"MIGC: Multi-Instance Generation Controller for Text-to-Image Synthesis","summary":"  We present a Multi-Instance Generation (MIG) task, simultaneously generating\nmultiple instances with diverse controls in one image. Given a set of\npredefined coordinates and their corresponding descriptions, the task is to\nensure that generated instances are accurately at the designated locations and\nthat all instances' attributes adhere to their corresponding description. This\nbroadens the scope of current research on Single-instance generation, elevating\nit to a more versatile and practical dimension. Inspired by the idea of divide\nand conquer, we introduce an innovative approach named Multi-Instance\nGeneration Controller (MIGC) to address the challenges of the MIG task.\nInitially, we break down the MIG task into several subtasks, each involving the\nshading of a single instance. To ensure precise shading for each instance, we\nintroduce an instance enhancement attention mechanism. Lastly, we aggregate all\nthe shaded instances to provide the necessary information for accurately\ngenerating multiple instances in stable diffusion (SD). To evaluate how well\ngeneration models perform on the MIG task, we provide a COCO-MIG benchmark\nalong with an evaluation pipeline. Extensive experiments were conducted on the\nproposed COCO-MIG benchmark, as well as on various commonly used benchmarks.\nThe evaluation results illustrate the exceptional control capabilities of our\nmodel in terms of quantity, position, attribute, and interaction. Code and\ndemos will be released at https://migcproject.github.io/.\n","authors":["Dewei Zhou","You Li","Fan Ma","Xiaoting Zhang","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2402.05408v2.pdf","comment":"Accepted for publication in CVPR 2024"},{"id":"http://arxiv.org/abs/2402.17287v1","updated":"2024-02-27T08:00:52Z","published":"2024-02-27T08:00:52Z","title":"An Interpretable Evaluation of Entropy-based Novelty of Generative\n  Models","summary":"  The massive developments of generative model frameworks and architectures\nrequire principled methods for the evaluation of a model's novelty compared to\na reference dataset or baseline generative models. While the recent literature\nhas extensively studied the evaluation of the quality, diversity, and\ngeneralizability of generative models, the assessment of a model's novelty\ncompared to a baseline model has not been adequately studied in the machine\nlearning community. In this work, we focus on the novelty assessment under\nmulti-modal generative models and attempt to answer the following question:\nGiven the samples of a generative model $\\mathcal{G}$ and a reference dataset\n$\\mathcal{S}$, how can we discover and count the modes expressed by\n$\\mathcal{G}$ more frequently than in $\\mathcal{S}$. We introduce a spectral\napproach to the described task and propose the Kernel-based Entropic Novelty\n(KEN) score to quantify the mode-based novelty of distribution $P_\\mathcal{G}$\nwith respect to distribution $P_\\mathcal{S}$. We analytically interpret the\nbehavior of the KEN score under mixture distributions with sub-Gaussian\ncomponents. Next, we develop a method based on Cholesky decomposition to\ncompute the KEN score from observed samples. We support the KEN-based\nquantification of novelty by presenting several numerical results on synthetic\nand real image distributions. Our numerical results indicate the success of the\nproposed approach in detecting the novel modes and the comparison of\nstate-of-the-art generative models.\n","authors":["Jingwei Zhang","Cheuk Ting Li","Farzan Farnia"],"pdf_url":"https://arxiv.org/pdf/2402.17287v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17285v1","updated":"2024-02-27T07:57:28Z","published":"2024-02-27T07:57:28Z","title":"Enhancing Hyperspectral Images via Diffusion Model and Group-Autoencoder\n  Super-resolution Network","summary":"  Existing hyperspectral image (HSI) super-resolution (SR) methods struggle to\neffectively capture the complex spectral-spatial relationships and low-level\ndetails, while diffusion models represent a promising generative model known\nfor their exceptional performance in modeling complex relations and learning\nhigh and low-level visual features. The direct application of diffusion models\nto HSI SR is hampered by challenges such as difficulties in model convergence\nand protracted inference time. In this work, we introduce a novel\nGroup-Autoencoder (GAE) framework that synergistically combines with the\ndiffusion model to construct a highly effective HSI SR model (DMGASR). Our\nproposed GAE framework encodes high-dimensional HSI data into low-dimensional\nlatent space where the diffusion model works, thereby alleviating the\ndifficulty of training the diffusion model while maintaining band correlation\nand considerably reducing inference time. Experimental results on both natural\nand remote sensing hyperspectral datasets demonstrate that the proposed method\nis superior to other state-of-the-art methods both visually and metrically.\n","authors":["Zhaoyang Wang","Dongyang Li","Mingyang Zhang","Hao Luo","Maoguo Gong"],"pdf_url":"https://arxiv.org/pdf/2402.17285v1.pdf","comment":"Accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2311.07634v3","updated":"2024-02-27T07:52:16Z","published":"2023-11-13T14:35:18Z","title":"ActiveDC: Distribution Calibration for Active Finetuning","summary":"  The pretraining-finetuning paradigm has gained popularity in various computer\nvision tasks. In this paradigm, the emergence of active finetuning arises due\nto the abundance of large-scale data and costly annotation requirements. Active\nfinetuning involves selecting a subset of data from an unlabeled pool for\nannotation, facilitating subsequent finetuning. However, the use of a limited\nnumber of training samples can lead to a biased distribution, potentially\nresulting in model overfitting. In this paper, we propose a new method called\nActiveDC for the active finetuning tasks. Firstly, we select samples for\nannotation by optimizing the distribution similarity between the subset to be\nselected and the entire unlabeled pool in continuous space. Secondly, we\ncalibrate the distribution of the selected samples by exploiting implicit\ncategory information in the unlabeled pool. The feature visualization provides\nan intuitive sense of the effectiveness of our approach to distribution\ncalibration. We conducted extensive experiments on three image classification\ndatasets with different sampling ratios. The results indicate that ActiveDC\nconsistently outperforms the baseline performance in all image classification\ntasks. The improvement is particularly significant when the sampling ratio is\nlow, with performance gains of up to 10%. Our code will be released.\n","authors":["Wenshuai Xu","Zhenghui Hu","Yu Lu","Jinzhou Meng","Qingjie Liu","Yunhong Wang"],"pdf_url":"https://arxiv.org/pdf/2311.07634v3.pdf","comment":"CVPR 2024 Accept"},{"id":"http://arxiv.org/abs/2402.17275v1","updated":"2024-02-27T07:42:55Z","published":"2024-02-27T07:42:55Z","title":"One-Shot Structure-Aware Stylized Image Synthesis","summary":"  While GAN-based models have been successful in image stylization tasks, they\noften struggle with structure preservation while stylizing a wide range of\ninput images. Recently, diffusion models have been adopted for image\nstylization but still lack the capability to maintain the original quality of\ninput images. Building on this, we propose OSASIS: a novel one-shot stylization\nmethod that is robust in structure preservation. We show that OSASIS is able to\neffectively disentangle the semantics from the structure of an image, allowing\nit to control the level of content and style implemented to a given input. We\napply OSASIS to various experimental settings, including stylization with\nout-of-domain reference images and stylization with text-driven manipulation.\nResults show that OSASIS outperforms other stylization methods, especially for\ninput images that were rarely encountered during training, providing a\npromising solution to stylization via diffusion models.\n","authors":["Hansam Cho","Jonghyun Lee","Seunggyu Chang","Yonghyun Jeong"],"pdf_url":"https://arxiv.org/pdf/2402.17275v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2402.17264v1","updated":"2024-02-27T07:19:50Z","published":"2024-02-27T07:19:50Z","title":"Explicit Interaction for Fusion-Based Place Recognition","summary":"  Fusion-based place recognition is an emerging technique jointly utilizing\nmulti-modal perception data, to recognize previously visited places in\nGPS-denied scenarios for robots and autonomous vehicles. Recent fusion-based\nplace recognition methods combine multi-modal features in implicit manners.\nWhile achieving remarkable results, they do not explicitly consider what the\nindividual modality affords in the fusion system. Therefore, the benefit of\nmulti-modal feature fusion may not be fully explored. In this paper, we propose\na novel fusion-based network, dubbed EINet, to achieve explicit interaction of\nthe two modalities. EINet uses LiDAR ranges to supervise more robust vision\nfeatures for long time spans, and simultaneously uses camera RGB data to\nimprove the discrimination of LiDAR point clouds. In addition, we develop a new\nbenchmark for the place recognition task based on the nuScenes dataset. To\nestablish this benchmark for future research with comprehensive comparisons, we\nintroduce both supervised and self-supervised training schemes alongside\nevaluation protocols. We conduct extensive experiments on the proposed\nbenchmark, and the experimental results show that our EINet exhibits better\nrecognition performance as well as solid generalization ability compared to the\nstate-of-the-art fusion-based place recognition approaches. Our open-source\ncode and benchmark are released at: https://github.com/BIT-XJY/EINet.\n","authors":["Jingyi Xu","Junyi Ma","Qi Wu","Zijie Zhou","Yue Wang","Xieyuanli Chen","Ling Pei"],"pdf_url":"https://arxiv.org/pdf/2402.17264v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17251v1","updated":"2024-02-27T06:50:31Z","published":"2024-02-27T06:50:31Z","title":"Context-based and Diversity-driven Specificity in Compositional\n  Zero-Shot Learning","summary":"  Compositional Zero-Shot Learning (CZSL) aims to recognize unseen\nattribute-object pairs based on a limited set of observed examples. Current\nCZSL methodologies, despite their advancements, tend to neglect the distinct\nspecificity levels present in attributes. For instance, given images of sliced\nstrawberries, they may fail to prioritize `Sliced-Strawberry' over a generic\n`Red-Strawberry', despite the former being more informative. They also suffer\nfrom ballooning search space when shifting from Close-World (CW) to Open-World\n(OW) CZSL. To address the issues, we introduce the Context-based and\nDiversity-driven Specificity learning framework for CZSL (CDS-CZSL). Our\nframework evaluates the specificity of attributes by considering the diversity\nof objects they apply to and their related context. This novel approach allows\nfor more accurate predictions by emphasizing specific attribute-object pairs\nand improves composition filtering in OW-CZSL. We conduct experiments in both\nCW and OW scenarios, and our model achieves state-of-the-art results across\nthree datasets.\n","authors":["Yun Li","Zhe Liu","Hang Chen","Lina Yao"],"pdf_url":"https://arxiv.org/pdf/2402.17251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17249v1","updated":"2024-02-27T06:47:52Z","published":"2024-02-27T06:47:52Z","title":"Deep Learning-Based Speech and Vision Synthesis to Improve Phishing\n  Attack Detection through a Multi-layer Adaptive Framework","summary":"  The ever-evolving ways attacker continues to im prove their phishing\ntechniques to bypass existing state-of-the-art phishing detection methods pose\na mountain of challenges to researchers in both industry and academia research\ndue to the inability of current approaches to detect complex phishing attack.\nThus, current anti-phishing methods remain vulnerable to complex phishing\nbecause of the increasingly sophistication tactics adopted by attacker coupled\nwith the rate at which new tactics are being developed to evade detection. In\nthis research, we proposed an adaptable framework that combines Deep learning\nand Randon Forest to read images, synthesize speech from deep-fake videos, and\nnatural language processing at various predictions layered to significantly\nincrease the performance of machine learning models for phishing attack\ndetection.\n","authors":["Tosin Ige","Christopher Kiekintveld","Aritran Piplai"],"pdf_url":"https://arxiv.org/pdf/2402.17249v1.pdf","comment":"8"},{"id":"http://arxiv.org/abs/2402.17246v1","updated":"2024-02-27T06:32:56Z","published":"2024-02-27T06:32:56Z","title":"SDR-Former: A Siamese Dual-Resolution Transformer for Liver Lesion\n  Classification Using 3D Multi-Phase Imaging","summary":"  Automated classification of liver lesions in multi-phase CT and MR scans is\nof clinical significance but challenging. This study proposes a novel Siamese\nDual-Resolution Transformer (SDR-Former) framework, specifically designed for\nliver lesion classification in 3D multi-phase CT and MR imaging with varying\nphase counts. The proposed SDR-Former utilizes a streamlined Siamese Neural\nNetwork (SNN) to process multi-phase imaging inputs, possessing robust feature\nrepresentations while maintaining computational efficiency. The weight-sharing\nfeature of the SNN is further enriched by a hybrid Dual-Resolution Transformer\n(DR-Former), comprising a 3D Convolutional Neural Network (CNN) and a tailored\n3D Transformer for processing high- and low-resolution images, respectively.\nThis hybrid sub-architecture excels in capturing detailed local features and\nunderstanding global contextual information, thereby, boosting the SNN's\nfeature extraction capabilities. Additionally, a novel Adaptive Phase Selection\nModule (APSM) is introduced, promoting phase-specific intercommunication and\ndynamically adjusting each phase's influence on the diagnostic outcome. The\nproposed SDR-Former framework has been validated through comprehensive\nexperiments on two clinical datasets: a three-phase CT dataset and an\neight-phase MR dataset. The experimental results affirm the efficacy of the\nproposed framework. To support the scientific community, we are releasing our\nextensive multi-phase MR dataset for liver lesion analysis to the public. This\npioneering dataset, being the first publicly available multi-phase MR dataset\nin this field, also underpins the MICCAI LLD-MMRI Challenge. The dataset is\naccessible at:https://bit.ly/3IyYlgN.\n","authors":["Meng Lou","Hanning Ying","Xiaoqing Liu","Hong-Yu Zhou","Yuqing Zhang","Yizhou Yu"],"pdf_url":"https://arxiv.org/pdf/2402.17246v1.pdf","comment":"13 pages, 7 figures"},{"id":"http://arxiv.org/abs/2402.17245v1","updated":"2024-02-27T06:31:52Z","published":"2024-02-27T06:31:52Z","title":"Playground v2.5: Three Insights towards Enhancing Aesthetic Quality in\n  Text-to-Image Generation","summary":"  In this work, we share three insights for achieving state-of-the-art\naesthetic quality in text-to-image generative models. We focus on three\ncritical aspects for model improvement: enhancing color and contrast, improving\ngeneration across multiple aspect ratios, and improving human-centric fine\ndetails. First, we delve into the significance of the noise schedule in\ntraining a diffusion model, demonstrating its profound impact on realism and\nvisual fidelity. Second, we address the challenge of accommodating various\naspect ratios in image generation, emphasizing the importance of preparing a\nbalanced bucketed dataset. Lastly, we investigate the crucial role of aligning\nmodel outputs with human preferences, ensuring that generated images resonate\nwith human perceptual expectations. Through extensive analysis and experiments,\nPlayground v2.5 demonstrates state-of-the-art performance in terms of aesthetic\nquality under various conditions and aspect ratios, outperforming both\nwidely-used open-source models like SDXL and Playground v2, and closed-source\ncommercial systems such as DALLE 3 and Midjourney v5.2. Our model is\nopen-source, and we hope the development of Playground v2.5 provides valuable\nguidelines for researchers aiming to elevate the aesthetic quality of\ndiffusion-based image generation models.\n","authors":["Daiqing Li","Aleks Kamko","Ehsan Akhgari","Ali Sabet","Linmiao Xu","Suhail Doshi"],"pdf_url":"https://arxiv.org/pdf/2402.17245v1.pdf","comment":"Model weights:\n  https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic"},{"id":"http://arxiv.org/abs/2303.09044v3","updated":"2024-02-27T06:24:31Z","published":"2023-03-16T02:29:53Z","title":"CoLo-CAM: Class Activation Mapping for Object Co-Localization in\n  Weakly-Labeled Unconstrained Videos","summary":"  Leveraging spatiotemporal information in videos is critical for weakly\nsupervised video object localization (WSVOL) tasks. However, state-of-the-art\nmethods only rely on visual and motion cues, while discarding discriminative\ninformation, making them susceptible to inaccurate localizations. Recently,\ndiscriminative models have been explored for WSVOL tasks using a temporal class\nactivation mapping (CAM) method. Although their results are promising, objects\nare assumed to have limited movement from frame to frame, leading to\ndegradation in performance for relatively long-term dependencies. This paper\nproposes a novel CAM method for WSVOL that exploits spatiotemporal information\nin activation maps during training without constraining an object's position.\nIts training relies on Co-Localization, hence, the name CoLo-CAM. Given a\nsequence of frames, localization is jointly learned based on color cues\nextracted across the corresponding maps, by assuming that an object has similar\ncolor in consecutive frames. CAM activations are constrained to respond\nsimilarly over pixels with similar colors, achieving co-localization. This\nimproves localization performance because the joint learning creates direct\ncommunication among pixels across all image locations and over all frames,\nallowing for transfer, aggregation, and correction of localizations.\nCo-localization is integrated into training by minimizing the color term of a\nconditional random field (CRF) loss over a sequence of frames/CAMs. Extensive\nexperiments on two challenging YouTube-Objects datasets of unconstrained videos\nshow the merits of our CoLo-CAM method, and its robustness to long-term\ndependencies, leading to new state-of-the-art performance for WSVOL task.\n","authors":["Soufiane Belharbi","Shakeeb Murtaza","Marco Pedersoli","Ismail Ben Ayed","Luke McCaffrey","Eric Granger"],"pdf_url":"https://arxiv.org/pdf/2303.09044v3.pdf","comment":"18 pages, 6 figures"},{"id":"http://arxiv.org/abs/2309.17264v5","updated":"2024-02-27T06:14:59Z","published":"2023-09-29T14:17:24Z","title":"A Foundation Model for General Moving Object Segmentation in Medical\n  Images","summary":"  Medical image segmentation aims to delineate the anatomical or pathological\nstructures of interest, playing a crucial role in clinical diagnosis. A\nsubstantial amount of high-quality annotated data is crucial for constructing\nhigh-precision deep segmentation models. However, medical annotation is highly\ncumbersome and time-consuming, especially for medical videos or 3D volumes, due\nto the huge labeling space and poor inter-frame consistency. Recently, a\nfundamental task named Moving Object Segmentation (MOS) has made significant\nadvancements in natural images. Its objective is to delineate moving objects\nfrom the background within image sequences, requiring only minimal annotations.\nIn this paper, we propose the first foundation model, named iMOS, for MOS in\nmedical images. Extensive experiments on a large multi-modal medical dataset\nvalidate the effectiveness of the proposed iMOS. Specifically, with the\nannotation of only a small number of images in the sequence, iMOS can achieve\nsatisfactory tracking and segmentation performance of moving objects throughout\nthe entire sequence in bi-directions. We hope that the proposed iMOS can help\naccelerate the annotation speed of experts, and boost the development of\nmedical foundation models.\n","authors":["Zhongnuo Yan","Tong Han","Yuhao Huang","Lian Liu","Han Zhou","Jiongquan Chen","Wenlong Shi","Yan Cao","Xin Yang","Dong Ni"],"pdf_url":"https://arxiv.org/pdf/2309.17264v5.pdf","comment":"5 pages, 7 figures, 3 tables. This paper has been accepted by ISBI\n  2024"},{"id":"http://arxiv.org/abs/2402.17237v1","updated":"2024-02-27T06:11:54Z","published":"2024-02-27T06:11:54Z","title":"Image-Text Matching with Multi-View Attention","summary":"  Existing two-stream models for image-text matching show good performance\nwhile ensuring retrieval speed and have received extensive attention from\nindustry and academia. These methods use a single representation to encode\nimage and text separately and get a matching score with cosine similarity or\nthe inner product of vectors. However, the performance of the two-stream model\nis often sub-optimal. On the one hand, a single representation is challenging\nto cover complex content comprehensively. On the other hand, in this framework\nof lack of interaction, it is challenging to match multiple meanings which\nleads to information being ignored. To address the problems mentioned above and\nfacilitate the performance of the two-stream model, we propose a multi-view\nattention approach for two-stream image-text matching MVAM\n(\\textbf{M}ulti-\\textbf{V}iew \\textbf{A}ttention \\textbf{M}odel). It first\nlearns multiple image and text representations by diverse attention heads with\ndifferent view codes. And then concatenate these representations into one for\nmatching. A diversity objective is also used to promote diversity between\nattention heads. With this method, models are able to encode images and text\nfrom different views and attend to more key points. So we can get\nrepresentations that contain more information. When doing retrieval tasks, the\nmatching scores between images and texts can be calculated from different\naspects, leading to better matching performance. Experiment results on MSCOCO\nand Flickr30K show that our proposed model brings improvements over existing\nmodels. Further case studies show that different attention heads can focus on\ndifferent contents and finally obtain a more comprehensive representation.\n","authors":["Rui Cheng","Wanqing Cui"],"pdf_url":"https://arxiv.org/pdf/2402.17237v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17229v1","updated":"2024-02-27T05:47:33Z","published":"2024-02-27T05:47:33Z","title":"Preserving Fairness Generalization in Deepfake Detection","summary":"  Although effective deepfake detection models have been developed in recent\nyears, recent studies have revealed that these models can result in unfair\nperformance disparities among demographic groups, such as race and gender. This\ncan lead to particular groups facing unfair targeting or exclusion from\ndetection, potentially allowing misclassified deepfakes to manipulate public\nopinion and undermine trust in the model. The existing method for addressing\nthis problem is providing a fair loss function. It shows good fairness\nperformance for intra-domain evaluation but does not maintain fairness for\ncross-domain testing. This highlights the significance of fairness\ngeneralization in the fight against deepfakes. In this work, we propose the\nfirst method to address the fairness generalization problem in deepfake\ndetection by simultaneously considering features, loss, and optimization\naspects. Our method employs disentanglement learning to extract demographic and\ndomain-agnostic forgery features, fusing them to encourage fair learning across\na flattened loss landscape. Extensive experiments on prominent deepfake\ndatasets demonstrate our method's effectiveness, surpassing state-of-the-art\napproaches in preserving fairness during cross-domain deepfake detection. The\ncode is available at https://github.com/Purdue-M2/Fairness-Generalization\n","authors":["Li Lin","Xinan He","Yan Ju","Xin Wang","Feng Ding","Shu Hu"],"pdf_url":"https://arxiv.org/pdf/2402.17229v1.pdf","comment":"Accepted by The IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR 2024)"},{"id":"http://arxiv.org/abs/2305.07180v3","updated":"2024-02-27T05:44:55Z","published":"2023-05-12T00:13:17Z","title":"Robust Saliency-Aware Distillation for Few-shot Fine-grained Visual\n  Recognition","summary":"  Recognizing novel sub-categories with scarce samples is an essential and\nchallenging research topic in computer vision. Existing literature addresses\nthis challenge by employing local-based representation approaches, which may\nnot sufficiently facilitate meaningful object-specific semantic understanding,\nleading to a reliance on apparent background correlations. Moreover, they\nprimarily rely on high-dimensional local descriptors to construct complex\nembedding space, potentially limiting the generalization. To address the above\nchallenges, this article proposes a novel model, Robust Saliency-aware\nDistillation (RSaD), for few-shot fine-grained visual recognition. RSaD\nintroduces additional saliency-aware supervision via saliency detection to\nguide the model toward focusing on the intrinsic discriminative regions.\nSpecifically, RSaD utilizes the saliency detection model to emphasize the\ncritical regions of each sub-category, providing additional object-specific\ninformation for fine-grained prediction. RSaD transfers such information with\ntwo symmetric branches in a mutual learning paradigm. Furthermore, RSaD\nexploits inter-regional relationships to enhance the informativeness of the\nrepresentation and subsequently summarize the highlighted details into\ncontextual embeddings to facilitate the effective transfer, enabling quick\ngeneralization to novel sub-categories. The proposed approach is empirically\nevaluated on three widely used benchmarks, demonstrating its superior\nperformance.\n","authors":["Haiqi Liu","C. L. Philip Chen","Xinrong Gong","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.07180v3.pdf","comment":"Accepted by IEEE Transactions on Multimedia"},{"id":"http://arxiv.org/abs/2402.17228v1","updated":"2024-02-27T05:42:38Z","published":"2024-02-27T05:42:38Z","title":"Feature Re-Embedding: Towards Foundation Model-Level Performance in\n  Computational Pathology","summary":"  Multiple instance learning (MIL) is the most widely used framework in\ncomputational pathology, encompassing sub-typing, diagnosis, prognosis, and\nmore. However, the existing MIL paradigm typically requires an offline instance\nfeature extractor, such as a pre-trained ResNet or a foundation model. This\napproach lacks the capability for feature fine-tuning within the specific\ndownstream tasks, limiting its adaptability and performance. To address this\nissue, we propose a Re-embedded Regional Transformer (R$^2$T) for re-embedding\nthe instance features online, which captures fine-grained local features and\nestablishes connections across different regions. Unlike existing works that\nfocus on pre-training powerful feature extractor or designing sophisticated\ninstance aggregator, R$^2$T is tailored to re-embed instance features online.\nIt serves as a portable module that can seamlessly integrate into mainstream\nMIL models. Extensive experimental results on common computational pathology\ntasks validate that: 1) feature re-embedding improves the performance of MIL\nmodels based on ResNet-50 features to the level of foundation model features,\nand further enhances the performance of foundation model features; 2) the\nR$^2$T can introduce more significant performance improvements to various MIL\nmodels; 3) R$^2$T-MIL, as an R$^2$T-enhanced AB-MIL, outperforms other latest\nmethods by a large margin. The code is available\nat:~\\href{https://github.com/DearCaat/RRT-MIL}{https://github.com/DearCaat/RRT-MIL}.\n","authors":["Wenhao Tang","Fengtao Zhou","Sheng Huang","Xiang Zhu","Yi Zhang","Bo Liu"],"pdf_url":"https://arxiv.org/pdf/2402.17228v1.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2303.08064v2","updated":"2024-02-27T05:32:03Z","published":"2023-03-11T05:22:42Z","title":"Online Neural Path Guiding with Normalized Anisotropic Spherical\n  Gaussians","summary":"  The variance reduction speed of physically-based rendering is heavily\naffected by the adopted importance sampling technique. In this paper we propose\na novel online framework to learn the spatial-varying density model with a\nsingle small neural network using stochastic ray samples. To achieve this task,\nwe propose a novel closed-form density model called the normalized anisotropic\nspherical gaussian mixture, that can express complex irradiance fields with a\nsmall number of parameters. Our framework learns the distribution in a\nprogressive manner and does not need any warm-up phases. Due to the compact and\nexpressive representation of our density model, our framework can be\nimplemented entirely on the GPU, allowing it produce high quality images with\nlimited computational resources.\n","authors":["Jiawei Huang","Akito Iizuka","Hajime Tanaka","Taku Komura","Yoshifumi Kitamura"],"pdf_url":"https://arxiv.org/pdf/2303.08064v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17214v1","updated":"2024-02-27T05:10:59Z","published":"2024-02-27T05:10:59Z","title":"CharacterGen: Efficient 3D Character Generation from Single Images with\n  Multi-View Pose Canonicalization","summary":"  In the field of digital content creation, generating high-quality 3D\ncharacters from single images is challenging, especially given the complexities\nof various body poses and the issues of self-occlusion and pose ambiguity. In\nthis paper, we present CharacterGen, a framework developed to efficiently\ngenerate 3D characters. CharacterGen introduces a streamlined generation\npipeline along with an image-conditioned multi-view diffusion model. This model\neffectively calibrates input poses to a canonical form while retaining key\nattributes of the input image, thereby addressing the challenges posed by\ndiverse poses. A transformer-based, generalizable sparse-view reconstruction\nmodel is the other core component of our approach, facilitating the creation of\ndetailed 3D models from multi-view images. We also adopt a\ntexture-back-projection strategy to produce high-quality texture maps.\nAdditionally, we have curated a dataset of anime characters, rendered in\nmultiple poses and views, to train and evaluate our model. Our approach has\nbeen thoroughly evaluated through quantitative and qualitative experiments,\nshowing its proficiency in generating 3D characters with high-quality shapes\nand textures, ready for downstream applications such as rigging and animation.\n","authors":["Hao-Yang Peng","Jia-Peng Zhang","Meng-Hao Guo","Yan-Pei Cao","Shi-Min Hu"],"pdf_url":"https://arxiv.org/pdf/2402.17214v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17213v1","updated":"2024-02-27T05:10:44Z","published":"2024-02-27T05:10:44Z","title":"VCD: Knowledge Base Guided Visual Commonsense Discovery in Images","summary":"  Visual commonsense contains knowledge about object properties, relationships,\nand behaviors in visual data. Discovering visual commonsense can provide a more\ncomprehensive and richer understanding of images, and enhance the reasoning and\ndecision-making capabilities of computer vision systems. However, the visual\ncommonsense defined in existing visual commonsense discovery studies is\ncoarse-grained and incomplete. In this work, we draw inspiration from a\ncommonsense knowledge base ConceptNet in natural language processing, and\nsystematically define the types of visual commonsense. Based on this, we\nintroduce a new task, Visual Commonsense Discovery (VCD), aiming to extract\nfine-grained commonsense of different types contained within different objects\nin the image. We accordingly construct a dataset (VCDD) from Visual Genome and\nConceptNet for VCD, featuring over 100,000 images and 14 million\nobject-commonsense pairs. We furthermore propose a generative model (VCDM) that\nintegrates a vision-language model with instruction tuning to tackle VCD.\nAutomatic and human evaluations demonstrate VCDM's proficiency in VCD,\nparticularly outperforming GPT-4V in implicit commonsense discovery. The value\nof VCD is further demonstrated by its application to two downstream tasks,\nincluding visual commonsense evaluation and visual question answering. The data\nand code will be made available on GitHub.\n","authors":["Xiangqing Shen","Yurun Song","Siwei Wu","Rui Xia"],"pdf_url":"https://arxiv.org/pdf/2402.17213v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17210v1","updated":"2024-02-27T05:04:00Z","published":"2024-02-27T05:04:00Z","title":"Purified and Unified Steganographic Network","summary":"  Steganography is the art of hiding secret data into the cover media for\ncovert communication. In recent years, more and more deep neural network\n(DNN)-based steganographic schemes are proposed to train steganographic\nnetworks for secret embedding and recovery, which are shown to be promising.\nCompared with the handcrafted steganographic tools, steganographic networks\ntend to be large in size. It raises concerns on how to imperceptibly and\neffectively transmit these networks to the sender and receiver to facilitate\nthe covert communication. To address this issue, we propose in this paper a\nPurified and Unified Steganographic Network (PUSNet). It performs an ordinary\nmachine learning task in a purified network, which could be triggered into\nsteganographic networks for secret embedding or recovery using different keys.\nWe formulate the construction of the PUSNet into a sparse weight filling\nproblem to flexibly switch between the purified and steganographic networks. We\nfurther instantiate our PUSNet as an image denoising network with two\nsteganographic networks concealed for secret image embedding and recovery.\nComprehensive experiments demonstrate that our PUSNet achieves good performance\non secret image embedding, secret image recovery, and image denoising in a\nsingle architecture. It is also shown to be capable of imperceptibly carrying\nthe steganographic networks in a purified network. Code is available at\n\\url{https://github.com/albblgb/PUSNet}\n","authors":["Guobiao Li","Sheng Li","Zicong Luo","Zhenxing Qian","Xinpeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.17210v1.pdf","comment":"8 pages, 9 figures, Accepted at CVPR2024"},{"id":"http://arxiv.org/abs/2402.17207v1","updated":"2024-02-27T04:56:04Z","published":"2024-02-27T04:56:04Z","title":"Deployment Prior Injection for Run-time Calibratable Object Detection","summary":"  With a strong alignment between the training and test distributions, object\nrelation as a context prior facilitates object detection. Yet, it turns into a\nharmful but inevitable training set bias upon test distributions that shift\ndifferently across space and time. Nevertheless, the existing detectors cannot\nincorporate deployment context prior during the test phase without parameter\nupdate. Such kind of capability requires the model to explicitly learn\ndisentangled representations with respect to context prior. To achieve this, we\nintroduce an additional graph input to the detector, where the graph represents\nthe deployment context prior, and its edge values represent object relations.\nThen, the detector behavior is trained to bound to the graph with a modified\ntraining objective. As a result, during the test phase, any suitable deployment\ncontext prior can be injected into the detector via graph edits, hence\ncalibrating, or \"re-biasing\" the detector towards the given prior at run-time\nwithout parameter update. Even if the deployment prior is unknown, the detector\ncan self-calibrate using deployment prior approximated using its own\npredictions. Comprehensive experimental results on the COCO dataset, as well as\ncross-dataset testing on the Objects365 dataset, demonstrate the effectiveness\nof the run-time calibratable detector.\n","authors":["Mo Zhou","Yiding Yang","Haoxiang Li","Vishal M. Patel","Gang Hua"],"pdf_url":"https://arxiv.org/pdf/2402.17207v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17204v1","updated":"2024-02-27T04:53:53Z","published":"2024-02-27T04:53:53Z","title":"Advancing Generative Model Evaluation: A Novel Algorithm for Realistic\n  Image Synthesis and Comparison in OCR System","summary":"  This research addresses a critical challenge in the field of generative\nmodels, particularly in the generation and evaluation of synthetic images.\nGiven the inherent complexity of generative models and the absence of a\nstandardized procedure for their comparison, our study introduces a pioneering\nalgorithm to objectively assess the realism of synthetic images. This approach\nsignificantly enhances the evaluation methodology by refining the Fr\\'echet\nInception Distance (FID) score, allowing for a more precise and subjective\nassessment of image quality. Our algorithm is particularly tailored to address\nthe challenges in generating and evaluating realistic images of Arabic\nhandwritten digits, a task that has traditionally been near-impossible due to\nthe subjective nature of realism in image generation. By providing a systematic\nand objective framework, our method not only enables the comparison of\ndifferent generative models but also paves the way for improvements in their\ndesign and output. This breakthrough in evaluation and comparison is crucial\nfor advancing the field of OCR, especially for scripts that present unique\ncomplexities, and sets a new standard in the generation and assessment of\nhigh-quality synthetic images.\n","authors":["Majid Memari","Khaled R. Ahmed","Shahram Rahimi","Noorbakhsh Amiri Golilarz"],"pdf_url":"https://arxiv.org/pdf/2402.17204v1.pdf","comment":"My manuscript entitled \"Advancing Generative Model Evaluation: A\n  Novel Algorithm for Realistic Image Synthesis and Comparison in OCR Systems\"\n  has been submitted on 29-Jan-2024 to IEEE Access and is presently being given\n  full consideration for publication in IEEE Access"},{"id":"http://arxiv.org/abs/2402.16506v2","updated":"2024-02-27T04:46:35Z","published":"2024-02-26T11:41:28Z","title":"Stochastic Conditional Diffusion Models for Semantic Image Synthesis","summary":"  Semantic image synthesis (SIS) is a task to generate realistic images\ncorresponding to semantic maps (labels). It can be applied to diverse\nreal-world practices such as photo editing or content creation. However, in\nreal-world applications, SIS often encounters noisy user inputs. To address\nthis, we propose Stochastic Conditional Diffusion Model (SCDM), which is a\nrobust conditional diffusion model that features novel forward and generation\nprocesses tailored for SIS with noisy labels. It enhances robustness by\nstochastically perturbing the semantic label maps through Label Diffusion,\nwhich diffuses the labels with discrete diffusion. Through the diffusion of\nlabels, the noisy and clean semantic maps become similar as the timestep\nincreases, eventually becoming identical at $t=T$. This facilitates the\ngeneration of an image close to a clean image, enabling robust generation.\nFurthermore, we propose a class-wise noise schedule to differentially diffuse\nthe labels depending on the class. We demonstrate that the proposed method\ngenerates high-quality samples through extensive experiments and analyses on\nbenchmark datasets, including a novel experimental setup simulating human\nerrors during real-world applications.\n","authors":["Juyeon Ko","Inho Kong","Dogyun Park","Hyunwoo J. Kim"],"pdf_url":"https://arxiv.org/pdf/2402.16506v2.pdf","comment":"30 pages, 21 figures"},{"id":"http://arxiv.org/abs/2402.04249v2","updated":"2024-02-27T04:43:08Z","published":"2024-02-06T18:59:08Z","title":"HarmBench: A Standardized Evaluation Framework for Automated Red Teaming\n  and Robust Refusal","summary":"  Automated red teaming holds substantial promise for uncovering and mitigating\nthe risks associated with the malicious use of large language models (LLMs),\nyet the field lacks a standardized evaluation framework to rigorously assess\nnew methods. To address this issue, we introduce HarmBench, a standardized\nevaluation framework for automated red teaming. We identify several desirable\nproperties previously unaccounted for in red teaming evaluations and\nsystematically design HarmBench to meet these criteria. Using HarmBench, we\nconduct a large-scale comparison of 18 red teaming methods and 33 target LLMs\nand defenses, yielding novel insights. We also introduce a highly efficient\nadversarial training method that greatly enhances LLM robustness across a wide\nrange of attacks, demonstrating how HarmBench enables codevelopment of attacks\nand defenses. We open source HarmBench at\nhttps://github.com/centerforaisafety/HarmBench.\n","authors":["Mantas Mazeika","Long Phan","Xuwang Yin","Andy Zou","Zifan Wang","Norman Mu","Elham Sakhaee","Nathaniel Li","Steven Basart","Bo Li","David Forsyth","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2402.04249v2.pdf","comment":"Website: https://www.harmbench.org"},{"id":"http://arxiv.org/abs/2402.17200v1","updated":"2024-02-27T04:37:04Z","published":"2024-02-27T04:37:04Z","title":"Enhancing Quality of Compressed Images by Mitigating Enhancement Bias\n  Towards Compression Domain","summary":"  Existing quality enhancement methods for compressed images focus on aligning\nthe enhancement domain with the raw domain to yield realistic images. However,\nthese methods exhibit a pervasive enhancement bias towards the compression\ndomain, inadvertently regarding it as more realistic than the raw domain. This\nbias makes enhanced images closely resemble their compressed counterparts, thus\ndegrading their perceptual quality. In this paper, we propose a simple yet\neffective method to mitigate this bias and enhance the quality of compressed\nimages. Our method employs a conditional discriminator with the compressed\nimage as a key condition, and then incorporates a domain-divergence\nregularization to actively distance the enhancement domain from the compression\ndomain. Through this dual strategy, our method enables the discrimination\nagainst the compression domain, and brings the enhancement domain closer to the\nraw domain. Comprehensive quality evaluations confirm the superiority of our\nmethod over other state-of-the-art methods without incurring inference\noverheads.\n","authors":["Qunliang Xing","Mai Xu","Shengxi Li","Xin Deng","Meisong Zheng","Huaida Liu","Ying Chen"],"pdf_url":"https://arxiv.org/pdf/2402.17200v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17192v1","updated":"2024-02-27T04:18:15Z","published":"2024-02-27T04:18:15Z","title":"Differentiable Biomechanics Unlocks Opportunities for Markerless Motion\n  Capture","summary":"  Recent developments have created differentiable physics simulators designed\nfor machine learning pipelines that can be accelerated on a GPU. While these\ncan simulate biomechanical models, these opportunities have not been exploited\nfor biomechanics research or markerless motion capture. We show that these\nsimulators can be used to fit inverse kinematics to markerless motion capture\ndata, including scaling the model to fit the anthropomorphic measurements of an\nindividual. This is performed end-to-end with an implicit representation of the\nmovement trajectory, which is propagated through the forward kinematic model to\nminimize the error from the 3D markers reprojected into the images. The\ndifferential optimizer yields other opportunities, such as adding bundle\nadjustment during trajectory optimization to refine the extrinsic camera\nparameters or meta-optimization to improve the base model jointly over\ntrajectories from multiple participants. This approach improves the\nreprojection error from markerless motion capture over prior methods and\nproduces accurate spatial step parameters compared to an instrumented walkway\nfor control and clinical populations.\n","authors":["R. James Cotton"],"pdf_url":"https://arxiv.org/pdf/2402.17192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.11339v2","updated":"2024-02-27T04:16:20Z","published":"2023-06-20T07:17:38Z","title":"Masking Augmentation for Supervised Learning","summary":"  Pre-training using random masking has emerged as a novel trend in training\ntechniques. However, supervised learning faces a challenge in adopting masking\naugmentations, primarily due to unstable training. In this paper, we propose a\nnovel way to involve masking augmentations dubbed Masked Sub-model (MaskSub).\nMaskSub consists of the main-model and sub-model; while the former enjoys\nconventional training recipes, the latter leverages the benefit of strong\nmasking augmentations in training. MaskSub addresses the challenge by\nmitigating adverse effects through a relaxed loss function similar to a\nself-distillation loss. Our analysis shows that MaskSub improves performance,\nwith the training loss converging even faster than regular training, which\nsuggests our method facilitates training. We further validate MaskSub across\ndiverse training recipes and models, including DeiT-III, MAE fine-tuning, CLIP\nfine-tuning, ResNet, and Swin Transformer. Our results show that MaskSub\nconsistently provides significant performance gains across all the cases.\nMaskSub provides a practical and effective solution for introducing additional\nregularization under various training recipes. Code available at\nhttps://github.com/naver-ai/augsub\n","authors":["Byeongho Heo","Taekyung Kim","Sangdoo Yun","Dongyoon Han"],"pdf_url":"https://arxiv.org/pdf/2306.11339v2.pdf","comment":"17 pages, 3 figures"},{"id":"http://arxiv.org/abs/2402.17187v1","updated":"2024-02-27T03:53:27Z","published":"2024-02-27T03:53:27Z","title":"PE-MVCNet: Multi-view and Cross-modal Fusion Network for Pulmonary\n  Embolism Prediction","summary":"  The early detection of a pulmonary embolism (PE) is critical for enhancing\npatient survival rates. Both image-based and non-image-based features are of\nutmost importance in medical classification tasks. In a clinical setting,\nphysicians tend to rely on the contextual information provided by Electronic\nMedical Records (EMR) to interpret medical imaging. However, very few models\neffectively integrate clinical information with imaging data. To address this\nshortcoming, we suggest a multimodal fusion methodology, termed PE-MVCNet,\nwhich capitalizes on Computed Tomography Pulmonary Angiography imaging and EMR\ndata. This method comprises the Image-only module with an integrated multi-view\nblock, the EMR-only module, and the Cross-modal Attention Fusion (CMAF) module.\nThese modules cooperate to extract comprehensive features that subsequently\ngenerate predictions for PE. We conducted experiments using the publicly\naccessible Stanford University Medical Center dataset, achieving an AUROC of\n94.1%, an accuracy rate of 90.2%, and an F1 score of 90.6%. Our proposed model\noutperforms existing methodologies, corroborating that our multimodal fusion\nmodel excels compared to models that use a single data modality.\n","authors":["Zhaoxin Guo","Zhipeng Wang","Ruiquan Ge","Jianxun Yu","Feiwei Qin","Yuan Tian","Yuqing Peng","Yonghong Li","Changmiao Wang"],"pdf_url":"https://arxiv.org/pdf/2402.17187v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17177v1","updated":"2024-02-27T03:30:58Z","published":"2024-02-27T03:30:58Z","title":"Sora: A Review on Background, Technology, Limitations, and Opportunities\n  of Large Vision Models","summary":"  Sora is a text-to-video generative AI model, released by OpenAI in February\n2024. The model is trained to generate videos of realistic or imaginative\nscenes from text instructions and show potential in simulating the physical\nworld. Based on public technical reports and reverse engineering, this paper\npresents a comprehensive review of the model's background, related\ntechnologies, applications, remaining challenges, and future directions of\ntext-to-video AI models. We first trace Sora's development and investigate the\nunderlying technologies used to build this \"world simulator\". Then, we describe\nin detail the applications and potential impact of Sora in multiple industries\nranging from film-making and education to marketing. We discuss the main\nchallenges and limitations that need to be addressed to widely deploy Sora,\nsuch as ensuring safe and unbiased video generation. Lastly, we discuss the\nfuture development of Sora and video generation models in general, and how\nadvancements in the field could enable new ways of human-AI interaction,\nboosting productivity and creativity of video generation.\n","authors":["Yixin Liu","Kai Zhang","Yuan Li","Zhiling Yan","Chujie Gao","Ruoxi Chen","Zhengqing Yuan","Yue Huang","Hanchi Sun","Jianfeng Gao","Lifang He","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2402.17177v1.pdf","comment":"37 pages, 18 figures; Our GitHub Homepage:\n  https://github.com/lichao-sun/SoraReview"},{"id":"http://arxiv.org/abs/2402.17172v1","updated":"2024-02-27T03:13:48Z","published":"2024-02-27T03:13:48Z","title":"Lane2Seq: Towards Unified Lane Detection via Sequence Generation","summary":"  In this paper, we present a novel sequence generation-based framework for\nlane detection, called Lane2Seq. It unifies various lane detection formats by\ncasting lane detection as a sequence generation task. This is different from\nprevious lane detection methods, which depend on well-designed task-specific\nhead networks and corresponding loss functions. Lane2Seq only adopts a plain\ntransformer-based encoder-decoder architecture with a simple cross-entropy\nloss. Additionally, we propose a new multi-format model tuning based on\nreinforcement learning to incorporate the task-specific knowledge into\nLane2Seq. Experimental results demonstrate that such a simple sequence\ngeneration paradigm not only unifies lane detection but also achieves\ncompetitive performance on benchmarks. For example, Lane2Seq gets 97.95\\% and\n97.42\\% F1 score on Tusimple and LLAMAS datasets, establishing a new\nstate-of-the-art result for two benchmarks.\n","authors":["Kunyang Zhou"],"pdf_url":"https://arxiv.org/pdf/2402.17172v1.pdf","comment":"CVPR2024 acceptance"},{"id":"http://arxiv.org/abs/2402.17171v1","updated":"2024-02-27T03:08:44Z","published":"2024-02-27T03:08:44Z","title":"LiveHPS: LiDAR-based Scene-level Human Pose and Shape Estimation in Free\n  Environment","summary":"  For human-centric large-scale scenes, fine-grained modeling for 3D human\nglobal pose and shape is significant for scene understanding and can benefit\nmany real-world applications. In this paper, we present LiveHPS, a novel\nsingle-LiDAR-based approach for scene-level human pose and shape estimation\nwithout any limitation of light conditions and wearable devices. In particular,\nwe design a distillation mechanism to mitigate the distribution-varying effect\nof LiDAR point clouds and exploit the temporal-spatial geometric and dynamic\ninformation existing in consecutive frames to solve the occlusion and noise\ndisturbance. LiveHPS, with its efficient configuration and high-quality output,\nis well-suited for real-world applications. Moreover, we propose a huge human\nmotion dataset, named FreeMotion, which is collected in various scenarios with\ndiverse human poses, shapes and translations. It consists of multi-modal and\nmulti-view acquisition data from calibrated and synchronized LiDARs, cameras,\nand IMUs. Extensive experiments on our new dataset and other public datasets\ndemonstrate the SOTA performance and robustness of our approach. We will\nrelease our code and dataset soon.\n","authors":["Yiming Ren","Xiao Han","Chengfeng Zhao","Jingya Wang","Lan Xu","Jingyi Yu","Yuexin Ma"],"pdf_url":"https://arxiv.org/pdf/2402.17171v1.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2402.17169v1","updated":"2024-02-27T03:05:05Z","published":"2024-02-27T03:05:05Z","title":"Deep Umbra: A Generative Approach for Sunlight Access Computation in\n  Urban Spaces","summary":"  Sunlight and shadow play critical roles in how urban spaces are utilized,\nthrive, and grow. While access to sunlight is essential to the success of urban\nenvironments, shadows can provide shaded places to stay during the hot seasons,\nmitigate heat island effect, and increase pedestrian comfort levels. Properly\nquantifying sunlight access and shadows in large urban environments is key in\ntackling some of the important challenges facing cities today. In this paper,\nwe propose Deep Umbra, a novel computational framework that enables the\nquantification of sunlight access and shadows at a global scale. Our framework\nis based on a conditional generative adversarial network that considers the\nphysical form of cities to compute high-resolution spatial information of\naccumulated sunlight access for the different seasons of the year. We use data\nfrom seven different cities to train our model, and show, through an extensive\nset of experiments, its low overall RMSE (below 0.1) as well as its\nextensibility to cities that were not part of the training set. Additionally,\nwe contribute a set of case studies and a comprehensive dataset with sunlight\naccess information for more than 100 cities across six continents of the world.\nDeep Umbra is available at https://urbantk.org/shadows.\n","authors":["Kazi Shahrukh Omar","Gustavo Moreira","Daniel Hodczak","Maryam Hosseini","Nicola Colaninno","Marcos Lage","Fabio Miranda"],"pdf_url":"https://arxiv.org/pdf/2402.17169v1.pdf","comment":"Accepted at IEEE Transactions on Big Data. Deep Umbra is available at\n  https://urbantk.org/shadows"},{"id":"http://arxiv.org/abs/2402.17165v1","updated":"2024-02-27T02:54:22Z","published":"2024-02-27T02:54:22Z","title":"Few-shot adaptation for morphology-independent cell instance\n  segmentation","summary":"  Microscopy data collections are becoming larger and more frequent. Accurate\nand precise quantitative analysis tools like cell instance segmentation are\nnecessary to benefit from them. This is challenging due to the variability in\nthe data, which requires retraining the segmentation model to maintain high\naccuracy on new collections. This is needed especially for segmenting cells\nwith elongated and non-convex morphology like bacteria. We propose to reduce\nthe amount of annotation and computing power needed for retraining the model by\nintroducing a few-shot domain adaptation approach that requires annotating only\none to five cells of the new data to process and that quickly adapts the model\nto maintain high accuracy. Our results show a significant boost in accuracy\nafter adaptation to very challenging bacteria datasets.\n","authors":["Ram J. Zaveri","Voke Brume","Gianfranco Doretto"],"pdf_url":"https://arxiv.org/pdf/2402.17165v1.pdf","comment":"ISBI 2024"},{"id":"http://arxiv.org/abs/2312.02209v3","updated":"2024-02-27T02:47:55Z","published":"2023-12-03T03:20:10Z","title":"AttriHuman-3D: Editable 3D Human Avatar Generation with Attribute\n  Decomposition and Indexing","summary":"  Editable 3D-aware generation, which supports user-interacted editing, has\nwitnessed rapid development recently. However, existing editable 3D GANs either\nfail to achieve high-accuracy local editing or suffer from huge computational\ncosts. We propose AttriHuman-3D, an editable 3D human generation model, which\naddress the aforementioned problems with attribute decomposition and indexing.\nThe core idea of the proposed model is to generate all attributes (e.g. human\nbody, hair, clothes and so on) in an overall attribute space with six feature\nplanes, which are then decomposed and manipulated with different attribute\nindexes. To precisely extract features of different attributes from the\ngenerated feature planes, we propose a novel attribute indexing method as well\nas an orthogonal projection regularization to enhance the disentanglement. We\nalso introduce a hyper-latent training strategy and an attribute-specific\nsampling strategy to avoid style entanglement and misleading punishment from\nthe discriminator. Our method allows users to interactively edit selected\nattributes in the generated 3D human avatars while keeping others fixed. Both\nqualitative and quantitative experiments demonstrate that our model provides a\nstrong disentanglement between different attributes, allows fine-grained image\nediting and generates high-quality 3D human avatars.\n","authors":["Fan Yang","Tianyi Chen","Xiaosheng He","Zhongang Cai","Lei Yang","Si Wu","Guosheng Lin"],"pdf_url":"https://arxiv.org/pdf/2312.02209v3.pdf","comment":"accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2402.17159v1","updated":"2024-02-27T02:47:09Z","published":"2024-02-27T02:47:09Z","title":"NocPlace: Nocturnal Visual Place Recognition Using Generative and\n  Inherited Knowledge Transfer","summary":"  Visual Place Recognition (VPR) is crucial in computer vision, aiming to\nretrieve database images similar to a query image from an extensive collection\nof known images. However, like many vision-related tasks, learning-based VPR\noften experiences a decline in performance during nighttime due to the scarcity\nof nighttime images. Specifically, VPR needs to address the cross-domain\nproblem of night-to-day rather than just the issue of a single nighttime\ndomain. In response to these issues, we present NocPlace, which leverages a\ngenerated large-scale, multi-view, nighttime VPR dataset to embed resilience\nagainst dazzling lights and extreme darkness in the learned global descriptor.\nFirstly, we establish a day-night urban scene dataset called NightCities,\ncapturing diverse nighttime scenarios and lighting variations across 60 cities\nglobally. Following this, an unpaired image-to-image translation network is\ntrained on this dataset. Using this trained translation network, we process an\nexisting VPR dataset, thereby obtaining its nighttime version. The NocPlace is\nthen fine-tuned using night-style images, the original labels, and descriptors\ninherited from the Daytime VPR model. Comprehensive experiments on various\nnighttime VPR test sets reveal that NocPlace considerably surpasses previous\nstate-of-the-art methods.\n","authors":["Bingxi Liu","Yiqun Wang","Huaqi Tao","Tingjun Huang","Fulin Tang","Yihong Wu","Jinqiang Cui","Hong Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.17159v1.pdf","comment":"15 pages,9 figures"},{"id":"http://arxiv.org/abs/2305.03374v4","updated":"2024-02-27T02:45:34Z","published":"2023-05-05T09:08:25Z","title":"DisenBooth: Identity-Preserving Disentangled Tuning for Subject-Driven\n  Text-to-Image Generation","summary":"  Subject-driven text-to-image generation aims to generate customized images of\nthe given subject based on the text descriptions, which has drawn increasing\nattention. Existing methods mainly resort to finetuning a pretrained generative\nmodel, where the identity-relevant information (e.g., the boy) and the\nidentity-irrelevant information (e.g., the background or the pose of the boy)\nare entangled in the latent embedding space. However, the highly entangled\nlatent embedding may lead to the failure of subject-driven text-to-image\ngeneration as follows: (i) the identity-irrelevant information hidden in the\nentangled embedding may dominate the generation process, resulting in the\ngenerated images heavily dependent on the irrelevant information while ignoring\nthe given text descriptions; (ii) the identity-relevant information carried in\nthe entangled embedding can not be appropriately preserved, resulting in\nidentity change of the subject in the generated images. To tackle the problems,\nwe propose DisenBooth, an identity-preserving disentangled tuning framework for\nsubject-driven text-to-image generation. Specifically, DisenBooth finetunes the\npretrained diffusion model in the denoising process. Different from previous\nworks that utilize an entangled embedding to denoise each image, DisenBooth\ninstead utilizes disentangled embeddings to respectively preserve the subject\nidentity and capture the identity-irrelevant information. We further design the\nnovel weak denoising and contrastive embedding auxiliary tuning objectives to\nachieve the disentanglement. Extensive experiments show that our proposed\nDisenBooth framework outperforms baseline models for subject-driven\ntext-to-image generation with the identity-preserved embedding. Additionally,\nby combining the identity-preserved embedding and identity-irrelevant\nembedding, DisenBooth demonstrates more generation flexibility and\ncontrollability\n","authors":["Hong Chen","Yipeng Zhang","Simin Wu","Xin Wang","Xuguang Duan","Yuwei Zhou","Wenwu Zhu"],"pdf_url":"https://arxiv.org/pdf/2305.03374v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11488v2","updated":"2024-02-27T02:44:39Z","published":"2024-02-18T07:32:36Z","title":"IRFundusSet: An Integrated Retinal Fundus Dataset with a Harmonized\n  Healthy Label","summary":"  Ocular conditions are a global concern and computational tools utilizing\nretinal fundus color photographs can aid in routine screening and management.\nObtaining comprehensive and sufficiently sized datasets, however, is\nnon-trivial for the intricate retinal fundus, which exhibits heterogeneities\nwithin pathologies, in addition to variations from demographics and\nacquisition. Moreover, retinal fundus datasets in the public space suffer\nfragmentation in the organization of data and definition of a healthy\nobservation. We present Integrated Retinal Fundus Set (IRFundusSet), a dataset\nthat consolidates, harmonizes and curates several public datasets, facilitating\ntheir consumption as a unified whole and with a consistent is_normal label.\nIRFundusSet comprises a Python package that automates harmonization and avails\na dataset object in line with the PyTorch approach. Moreover, images are\nphysically reviewed and a new is_normal label is annotated for a consistent\ndefinition of a healthy observation. Ten public datasets are initially\nconsidered with a total of 46064 images, of which 25406 are curated for a new\nis_normal label and 3515 are deemed healthy across the sources.\n","authors":["P. Bilha Githinji","Keming Zhao","Jiantao Wang","Peiwu Qin"],"pdf_url":"https://arxiv.org/pdf/2402.11488v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12470v2","updated":"2024-02-27T02:40:48Z","published":"2023-12-19T08:14:14Z","title":"Rotated Multi-Scale Interaction Network for Referring Remote Sensing\n  Image Segmentation","summary":"  Referring Remote Sensing Image Segmentation (RRSIS) is a new challenge that\ncombines computer vision and natural language processing, delineating specific\nregions in aerial images as described by textual queries. Traditional Referring\nImage Segmentation (RIS) approaches have been impeded by the complex spatial\nscales and orientations found in aerial imagery, leading to suboptimal\nsegmentation results. To address these challenges, we introduce the Rotated\nMulti-Scale Interaction Network (RMSIN), an innovative approach designed for\nthe unique demands of RRSIS. RMSIN incorporates an Intra-scale Interaction\nModule (IIM) to effectively address the fine-grained detail required at\nmultiple scales and a Cross-scale Interaction Module (CIM) for integrating\nthese details coherently across the network. Furthermore, RMSIN employs an\nAdaptive Rotated Convolution (ARC) to account for the diverse orientations of\nobjects, a novel contribution that significantly enhances segmentation\naccuracy. To assess the efficacy of RMSIN, we have curated an expansive dataset\ncomprising 17,402 image-caption-mask triplets, which is unparalleled in terms\nof scale and variety. This dataset not only presents the model with a wide\nrange of spatial and rotational scenarios but also establishes a stringent\nbenchmark for the RRSIS task, ensuring a rigorous evaluation of performance.\nOur experimental evaluations demonstrate the exceptional performance of RMSIN,\nsurpassing existing state-of-the-art models by a significant margin. All\ndatasets and code are made available at https://github.com/Lsan2401/RMSIN.\n","authors":["Sihan Liu","Yiwei Ma","Xiaoqing Zhang","Haowei Wang","Jiayi Ji","Xiaoshuai Sun","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2312.12470v2.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2402.07452v2","updated":"2024-02-27T02:19:54Z","published":"2024-02-12T07:19:00Z","title":"TriAug: Out-of-Distribution Detection for Imbalanced Breast Lesion in\n  Ultrasound","summary":"  Different diseases, such as histological subtypes of breast lesions, have\nseverely varying incidence rates. Even trained with substantial amount of\nin-distribution (ID) data, models often encounter out-of-distribution (OOD)\nsamples belonging to unseen classes in clinical reality. To address this, we\npropose a novel framework built upon a long-tailed OOD detection task for\nbreast ultrasound images. It is equipped with a triplet state augmentation\n(TriAug) which improves ID classification accuracy while maintaining a\npromising OOD detection performance. Meanwhile, we designed a balanced sphere\nloss to handle the class imbalanced problem. Experimental results show that the\nmodel outperforms state-of-art OOD approaches both in ID classification\n(F1-score=42.12%) and OOD detection (AUROC=78.06%).\n","authors":["Yinyu Ye","Shijing Chen","Dong Ni","Ruobing Huang"],"pdf_url":"https://arxiv.org/pdf/2402.07452v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17456v2","updated":"2024-02-27T02:05:30Z","published":"2023-11-29T08:56:24Z","title":"DifFlow3D: Toward Robust Uncertainty-Aware Scene Flow Estimation with\n  Iterative Diffusion-Based Refinement","summary":"  Scene flow estimation, which aims to predict per-point 3D displacements of\ndynamic scenes, is a fundamental task in the computer vision field. However,\nprevious works commonly suffer from unreliable correlation caused by locally\nconstrained searching ranges, and struggle with accumulated inaccuracy arising\nfrom the coarse-to-fine structure. To alleviate these problems, we propose a\nnovel uncertainty-aware scene flow estimation network (DifFlow3D) with the\ndiffusion probabilistic model. Iterative diffusion-based refinement is designed\nto enhance the correlation robustness and resilience to challenging cases,\ne.g., dynamics, noisy inputs, repetitive patterns, etc. To restrain the\ngeneration diversity, three key flow-related features are leveraged as\nconditions in our diffusion model. Furthermore, we also develop an uncertainty\nestimation module within diffusion to evaluate the reliability of estimated\nscene flow. Our DifFlow3D achieves state-of-the-art performance, with 6.7\\% and\n19.1\\% EPE3D reduction respectively on FlyingThings3D and KITTI 2015 datasets.\nNotably, our method achieves an unprecedented millimeter-level accuracy\n(0.0089m in EPE3D) on the KITTI dataset. Additionally, our diffusion-based\nrefinement paradigm can be readily integrated as a plug-and-play module into\nexisting scene flow networks, significantly increasing their estimation\naccuracy. Codes will be released on https://github.com/IRMVLab/DifFlow3D.\n","authors":["Jiuming Liu","Guangming Wang","Weicai Ye","Chaokang Jiang","Jinru Han","Zhe Liu","Guofeng Zhang","Dalong Du","Hesheng Wang"],"pdf_url":"https://arxiv.org/pdf/2311.17456v2.pdf","comment":"Accepted by CVPR 2024. Codes will be released on\n  https://github.com/IRMVLab/DifFlow3D"},{"id":"http://arxiv.org/abs/2402.17139v1","updated":"2024-02-27T02:05:29Z","published":"2024-02-27T02:05:29Z","title":"Video as the New Language for Real-World Decision Making","summary":"  Both text and video data are abundant on the internet and support large-scale\nself-supervised learning through next token or frame prediction. However, they\nhave not been equally leveraged: language models have had significant\nreal-world impact, whereas video generation has remained largely limited to\nmedia entertainment. Yet video data captures important information about the\nphysical world that is difficult to express in language. To address this gap,\nwe discuss an under-appreciated opportunity to extend video generation to solve\ntasks in the real world. We observe how, akin to language, video can serve as a\nunified interface that can absorb internet knowledge and represent diverse\ntasks. Moreover, we demonstrate how, like language models, video generation can\nserve as planners, agents, compute engines, and environment simulators through\ntechniques such as in-context learning, planning and reinforcement learning. We\nidentify major impact opportunities in domains such as robotics, self-driving,\nand science, supported by recent work that demonstrates how such advanced\ncapabilities in video generation are plausibly within reach. Lastly, we\nidentify key challenges in video generation that mitigate progress. Addressing\nthese challenges will enable video generation models to demonstrate unique\nvalue alongside language models in a wider array of AI applications.\n","authors":["Sherry Yang","Jacob Walker","Jack Parker-Holder","Yilun Du","Jake Bruce","Andre Barreto","Pieter Abbeel","Dale Schuurmans"],"pdf_url":"https://arxiv.org/pdf/2402.17139v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11303v2","updated":"2024-02-27T02:04:04Z","published":"2024-02-17T15:03:25Z","title":"FViT: A Focal Vision Transformer with Gabor Filter","summary":"  Vision transformers have achieved encouraging progress in various computer\nvision tasks. A common belief is that this is attributed to the competence of\nself-attention in modeling the global dependencies among feature tokens.\nUnfortunately, self-attention still faces some challenges in dense prediction\ntasks, such as the high computational complexity and absence of desirable\ninductive bias. To address these issues, we revisit the potential benefits of\nintegrating vision transformer with Gabor filter, and propose a Learnable Gabor\nFilter (LGF) by using convolution. As an alternative to self-attention, we\nemploy LGF to simulate the response of simple cells in the biological visual\nsystem to input images, prompting models to focus on discriminative feature\nrepresentations of targets from various scales and orientations. Additionally,\nwe design a Bionic Focal Vision (BFV) block based on the LGF. This block draws\ninspiration from neuroscience and introduces a Multi-Path Feed Forward Network\n(MPFFN) to emulate the working way of biological visual cortex processing\ninformation in parallel. Furthermore, we develop a unified and efficient\npyramid backbone network family called Focal Vision Transformers (FViTs) by\nstacking BFV blocks. Experimental results show that FViTs exhibit highly\ncompetitive performance in various vision tasks. Especially in terms of\ncomputational efficiency and scalability, FViTs show significant advantages\ncompared with other counterparts. Code is available at\nhttps://github.com/nkusyl/FViT\n","authors":["Yulong Shi","Mingwei Sun","Yongshuai Wang","Rui Wang","Hui Sun","Zengqiang Chen"],"pdf_url":"https://arxiv.org/pdf/2402.11303v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2312.05849v2","updated":"2024-02-27T02:00:58Z","published":"2023-12-10T10:35:16Z","title":"InteractDiffusion: Interaction Control in Text-to-Image Diffusion Models","summary":"  Large-scale text-to-image (T2I) diffusion models have showcased incredible\ncapabilities in generating coherent images based on textual descriptions,\nenabling vast applications in content generation. While recent advancements\nhave introduced control over factors such as object localization, posture, and\nimage contours, a crucial gap remains in our ability to control the\ninteractions between objects in the generated content. Well-controlling\ninteractions in generated images could yield meaningful applications, such as\ncreating realistic scenes with interacting characters. In this work, we study\nthe problems of conditioning T2I diffusion models with Human-Object Interaction\n(HOI) information, consisting of a triplet label (person, action, object) and\ncorresponding bounding boxes. We propose a pluggable interaction control model,\ncalled InteractDiffusion that extends existing pre-trained T2I diffusion models\nto enable them being better conditioned on interactions. Specifically, we\ntokenize the HOI information and learn their relationships via interaction\nembeddings. A conditioning self-attention layer is trained to map HOI tokens to\nvisual tokens, thereby conditioning the visual tokens better in existing T2I\ndiffusion models. Our model attains the ability to control the interaction and\nlocation on existing T2I diffusion models, which outperforms existing baselines\nby a large margin in HOI detection score, as well as fidelity in FID and KID.\nProject page: https://jiuntian.github.io/interactdiffusion.\n","authors":["Jiun Tian Hoe","Xudong Jiang","Chee Seng Chan","Yap-Peng Tan","Weipeng Hu"],"pdf_url":"https://arxiv.org/pdf/2312.05849v2.pdf","comment":"Website: https://jiuntian.github.io/interactdiffusion. Accepted at\n  CVPR2024"},{"id":"http://arxiv.org/abs/2402.17134v1","updated":"2024-02-27T01:57:09Z","published":"2024-02-27T01:57:09Z","title":"Efficiently Leveraging Linguistic Priors for Scene Text Spotting","summary":"  Incorporating linguistic knowledge can improve scene text recognition, but it\nis questionable whether the same holds for scene text spotting, which typically\ninvolves text detection and recognition. This paper proposes a method that\nleverages linguistic knowledge from a large text corpus to replace the\ntraditional one-hot encoding used in auto-regressive scene text spotting and\nrecognition models. This allows the model to capture the relationship between\ncharacters in the same word. Additionally, we introduce a technique to generate\ntext distributions that align well with scene text datasets, removing the need\nfor in-domain fine-tuning. As a result, the newly created text distributions\nare more informative than pure one-hot encoding, leading to improved spotting\nand recognition performance. Our method is simple and efficient, and it can\neasily be integrated into existing auto-regressive-based approaches.\nExperimental results show that our method not only improves recognition\naccuracy but also enables more accurate localization of words. It significantly\nimproves both state-of-the-art scene text spotting and recognition pipelines,\nachieving state-of-the-art results on several benchmarks.\n","authors":["Nguyen Nguyen","Yapeng Tian","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2402.17134v1.pdf","comment":"10 pages"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2402.17535v1","updated":"2024-02-27T14:21:56Z","published":"2024-02-27T14:21:56Z","title":"Multimodal Learned Sparse Retrieval with Probabilistic Expansion Control","summary":"  Learned sparse retrieval (LSR) is a family of neural methods that encode\nqueries and documents into sparse lexical vectors that can be indexed and\nretrieved efficiently with an inverted index. We explore the application of LSR\nto the multi-modal domain, with a focus on text-image retrieval. While LSR has\nseen success in text retrieval, its application in multimodal retrieval remains\nunderexplored. Current approaches like LexLIP and STAIR require complex\nmulti-step training on massive datasets. Our proposed approach efficiently\ntransforms dense vectors from a frozen dense model into sparse lexical vectors.\nWe address issues of high dimension co-activation and semantic deviation\nthrough a new training algorithm, using Bernoulli random variables to control\nquery expansion. Experiments with two dense models (BLIP, ALBEF) and two\ndatasets (MSCOCO, Flickr30k) show that our proposed algorithm effectively\nreduces co-activation and semantic deviation. Our best-performing sparsified\nmodel outperforms state-of-the-art text-image LSR models with a shorter\ntraining time and lower GPU memory requirements. Our approach offers an\neffective solution for training LSR retrieval models in multimodal settings.\nOur code and model checkpoints are available at\ngithub.com/thongnt99/lsr-multimodal\n","authors":["Thong Nguyen","Mariya Hendriksen","Andrew Yates","Maarten de Rijke"],"pdf_url":"https://arxiv.org/pdf/2402.17535v1.pdf","comment":"17 pages, accepted as a full paper at ECIR 2024"},{"id":"http://arxiv.org/abs/2402.17505v1","updated":"2024-02-27T13:44:09Z","published":"2024-02-27T13:44:09Z","title":"BASES: Large-scale Web Search User Simulation with Large Language Model\n  based Agents","summary":"  Due to the excellent capacities of large language models (LLMs), it becomes\nfeasible to develop LLM-based agents for reliable user simulation. Considering\nthe scarcity and limit (e.g., privacy issues) of real user data, in this paper,\nwe conduct large-scale user simulation for web search, to improve the analysis\nand modeling of user search behavior. Specially, we propose BASES, a novel user\nsimulation framework with LLM-based agents, designed to facilitate\ncomprehensive simulations of web search user behaviors. Our simulation\nframework can generate unique user profiles at scale, which subsequently leads\nto diverse search behaviors. To demonstrate the effectiveness of BASES, we\nconduct evaluation experiments based on two human benchmarks in both Chinese\nand English, demonstrating that BASES can effectively simulate large-scale\nhuman-like search behaviors. To further accommodate the research on web search,\nwe develop WARRIORS, a new large-scale dataset encompassing web search user\nbehaviors, including both Chinese and English versions, which can greatly\nbolster research in the field of information retrieval. Our code and data will\nbe publicly released soon.\n","authors":["Ruiyang Ren","Peng Qiu","Yingqi Qu","Jing Liu","Wayne Xin Zhao","Hua Wu","Ji-Rong Wen","Haifeng Wang"],"pdf_url":"https://arxiv.org/pdf/2402.17505v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17497v1","updated":"2024-02-27T13:22:51Z","published":"2024-02-27T13:22:51Z","title":"REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain\n  Question Answering","summary":"  Considering the limited internal parametric knowledge, retrieval-augmented\ngeneration (RAG) has been widely used to extend the knowledge scope of large\nlanguage models (LLMs). Despite the extensive efforts on RAG research, in\nexisting methods, LLMs cannot precisely assess the relevance of retrieved\ndocuments, thus likely leading to misleading or even incorrect utilization of\nexternal knowledge (i.e., retrieved documents). To address this issue, in this\npaper, we propose REAR, a RElevance-Aware Retrieval-augmented approach for\nopen-domain question answering (QA). As the key motivation, we aim to enhance\nthe self-awareness of source relevance for LLMs, so as to adaptively utilize\nexternal knowledge in RAG systems. Specially, we develop a new architecture for\nLLM based RAG system, by incorporating a specially designed rank head that\nprecisely assesses the relevance of retrieved documents. Furthermore, we\npropose an improved training method based on bi-granularity relevance fusion\nand noise-resistant training. By combining the improvements in both\narchitecture and training, our proposed REAR can better utilize external\nknowledge by effectively perceiving the relevance of retrieved documents.\nExperiments on four open-domain QA tasks show that REAR significantly\noutperforms previous a number of competitive RAG approaches. Our code and data\ncan be accessed at https://github.com/RUCAIBox/REAR.\n","authors":["Yuhao Wang","Ruiyang Ren","Junyi Li","Wayne Xin Zhao","Jing Liu","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2402.17497v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17467v1","updated":"2024-02-27T12:48:01Z","published":"2024-02-27T12:48:01Z","title":"Natural Language Processing Methods for Symbolic Music Generation and\n  Information Retrieval: a Survey","summary":"  Several adaptations of Transformers models have been developed in various\ndomains since its breakthrough in Natural Language Processing (NLP). This trend\nhas spread into the field of Music Information Retrieval (MIR), including\nstudies processing music data. However, the practice of leveraging NLP tools\nfor symbolic music data is not novel in MIR. Music has been frequently compared\nto language, as they share several similarities, including sequential\nrepresentations of text and music. These analogies are also reflected through\nsimilar tasks in MIR and NLP. This survey reviews NLP methods applied to\nsymbolic music generation and information retrieval studies following two axes.\nWe first propose an overview of representations of symbolic music adapted from\nnatural language sequential representations. Such representations are designed\nby considering the specificities of symbolic music. These representations are\nthen processed by models. Such models, possibly originally developed for text\nand adapted for symbolic music, are trained on various tasks. We describe these\nmodels, in particular deep learning models, through different prisms,\nhighlighting music-specialized mechanisms. We finally present a discussion\nsurrounding the effective use of NLP tools for symbolic music data. This\nincludes technical issues regarding NLP methods and fundamental differences\nbetween text and music, which may open several doors for further research into\nmore effectively adapting NLP tools to symbolic MIR.\n","authors":["Dinh-Viet-Toan Le","Louis Bigo","Mikaela Keller","Dorien Herremans"],"pdf_url":"https://arxiv.org/pdf/2402.17467v1.pdf","comment":"36 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2402.17447v1","updated":"2024-02-27T12:03:56Z","published":"2024-02-27T12:03:56Z","title":"Deep Learning Based Named Entity Recognition Models for Recipes","summary":"  Food touches our lives through various endeavors, including flavor,\nnourishment, health, and sustainability. Recipes are cultural capsules\ntransmitted across generations via unstructured text. Automated protocols for\nrecognizing named entities, the building blocks of recipe text, are of immense\nvalue for various applications ranging from information extraction to novel\nrecipe generation. Named entity recognition is a technique for extracting\ninformation from unstructured or semi-structured data with known labels.\nStarting with manually-annotated data of 6,611 ingredient phrases, we created\nan augmented dataset of 26,445 phrases cumulatively. Simultaneously, we\nsystematically cleaned and analyzed ingredient phrases from RecipeDB, the\ngold-standard recipe data repository, and annotated them using the Stanford\nNER. Based on the analysis, we sampled a subset of 88,526 phrases using a\nclustering-based approach while preserving the diversity to create the\nmachine-annotated dataset. A thorough investigation of NER approaches on these\nthree datasets involving statistical, fine-tuning of deep learning-based\nlanguage models and few-shot prompting on large language models (LLMs) provides\ndeep insights. We conclude that few-shot prompting on LLMs has abysmal\nperformance, whereas the fine-tuned spaCy-transformer emerges as the best model\nwith macro-F1 scores of 95.9%, 96.04%, and 95.71% for the manually-annotated,\naugmented, and machine-annotated datasets, respectively.\n","authors":["Mansi Goel","Ayush Agarwal","Shubham Agrawal","Janak Kapuriya","Akhil Vamshi Konam","Rishabh Gupta","Shrey Rastogi"," Niharika","Ganesh Bagler"],"pdf_url":"https://arxiv.org/pdf/2402.17447v1.pdf","comment":"13 pages, 6 main figures and 2 in appendices, and 3 main tables;\n  Accepted for publication in LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2402.17334v1","updated":"2024-02-27T09:10:41Z","published":"2024-02-27T09:10:41Z","title":"BiVRec: Bidirectional View-based Multimodal Sequential Recommendation","summary":"  The integration of multimodal information into sequential recommender systems\nhas attracted significant attention in recent research. In the initial stages\nof multimodal sequential recommendation models, the mainstream paradigm was\nID-dominant recommendations, wherein multimodal information was fused as side\ninformation. However, due to their limitations in terms of transferability and\ninformation intrusion, another paradigm emerged, wherein multimodal features\nwere employed directly for recommendation, enabling recommendation across\ndatasets. Nonetheless, it overlooked user ID information, resulting in low\ninformation utilization and high training costs. To this end, we propose an\ninnovative framework, BivRec, that jointly trains the recommendation tasks in\nboth ID and multimodal views, leveraging their synergistic relationship to\nenhance recommendation performance bidirectionally. To tackle the information\nheterogeneity issue, we first construct structured user interest\nrepresentations and then learn the synergistic relationship between them.\nSpecifically, BivRec comprises three modules: Multi-scale Interest Embedding,\ncomprehensively modeling user interests by expanding user interaction sequences\nwith multi-scale patching; Intra-View Interest Decomposition, constructing\nhighly structured interest representations using carefully designed Gaussian\nattention and Cluster attention; and Cross-View Interest Learning, learning the\nsynergistic relationship between the two recommendation views through\ncoarse-grained overall semantic similarity and fine-grained interest allocation\nsimilarity BiVRec achieves state-of-the-art performance on five datasets and\nshowcases various practical advantages.\n","authors":["Jiaxi Hu","Jingtong Gao","Xiangyu Zhao","Yuehong Hu","Yuxuan Liang","Yiqi Wang","Ming He","Zitao Liu","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2402.17334v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17279v1","updated":"2024-02-27T07:51:06Z","published":"2024-02-27T07:51:06Z","title":"DiFashion: Towards Personalized Outfit Generation","summary":"  The evolution of Outfit Recommendation (OR) in the realm of fashion has\nprogressed through two distinct phases: Pre-defined Outfit Recommendation and\nPersonalized Outfit Composition. Despite these advancements, both phases face\nlimitations imposed by existing fashion products, hindering their effectiveness\nin meeting users' diverse fashion needs. The emergence of AI-generated content\nhas paved the way for OR to overcome these constraints, demonstrating the\npotential for personalized outfit generation.\n  In pursuit of this, we introduce an innovative task named Generative Outfit\nRecommendation (GOR), with the goal of synthesizing a set of fashion images and\nassembling them to form visually harmonious outfits customized to individual\nusers. The primary objectives of GOR revolve around achieving high fidelity,\ncompatibility, and personalization of the generated outfits. To accomplish\nthese, we propose DiFashion, a generative outfit recommender model that\nharnesses exceptional diffusion models for the simultaneous generation of\nmultiple fashion images. To ensure the fulfillment of these objectives, three\ntypes of conditions are designed to guide the parallel generation process and\nClassifier-Free-Guidance are employed to enhance the alignment between\ngenerated images and conditions. DiFashion is applied to both personalized\nFill-In-The-Blank and GOR tasks, and extensive experiments are conducted on the\niFashion and Polyvore-U datasets. The results of quantitative and\nhuman-involved qualitative evaluations highlight the superiority of DiFashion\nover competitive baselines.\n","authors":["Yiyan Xu","Wenjie Wang","Fuli Feng","Yunshan Ma","Jizhi Zhang","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2402.17279v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17188v1","updated":"2024-02-27T03:58:39Z","published":"2024-02-27T03:58:39Z","title":"PromptMM: Multi-Modal Knowledge Distillation for Recommendation with\n  Prompt-Tuning","summary":"  Multimedia online platforms (e.g., Amazon, TikTok) have greatly benefited\nfrom the incorporation of multimedia (e.g., visual, textual, and acoustic)\ncontent into their personal recommender systems. These modalities provide\nintuitive semantics that facilitate modality-aware user preference modeling.\nHowever, two key challenges in multi-modal recommenders remain unresolved: i)\nThe introduction of multi-modal encoders with a large number of additional\nparameters causes overfitting, given high-dimensional multi-modal features\nprovided by extractors (e.g., ViT, BERT). ii) Side information inevitably\nintroduces inaccuracies and redundancies, which skew the modality-interaction\ndependency from reflecting true user preference. To tackle these problems, we\npropose to simplify and empower recommenders through Multi-modal Knowledge\nDistillation (PromptMM) with the prompt-tuning that enables adaptive quality\ndistillation. Specifically, PromptMM conducts model compression through\ndistilling u-i edge relationship and multi-modal node content from cumbersome\nteachers to relieve students from the additional feature reduction parameters.\nTo bridge the semantic gap between multi-modal context and collaborative\nsignals for empowering the overfitting teacher, soft prompt-tuning is\nintroduced to perform student task-adaptive. Additionally, to adjust the impact\nof inaccuracies in multimedia data, a disentangled multi-modal list-wise\ndistillation is developed with modality-aware re-weighting mechanism.\nExperiments on real-world data demonstrate PromptMM's superiority over existing\ntechniques. Ablation tests confirm the effectiveness of key components.\nAdditional tests show the efficiency and effectiveness.\n","authors":["Wei Wei","Jiabin Tang","Yangqin Jiang","Lianghao Xia","Chao Huang"],"pdf_url":"https://arxiv.org/pdf/2402.17188v1.pdf","comment":"WWW 2024"},{"id":"http://arxiv.org/abs/2402.17152v1","updated":"2024-02-27T02:37:37Z","published":"2024-02-27T02:37:37Z","title":"Actions Speak Louder than Words: Trillion-Parameter Sequential\n  Transducers for Generative Recommendations","summary":"  Large-scale recommendation systems are characterized by their reliance on\nhigh cardinality, heterogeneous features and the need to handle tens of\nbillions of user actions on a daily basis. Despite being trained on huge volume\nof data with thousands of features, most Deep Learning Recommendation Models\n(DLRMs) in industry fail to scale with compute.\n  Inspired by success achieved by Transformers in language and vision domains,\nwe revisit fundamental design choices in recommendation systems. We reformulate\nrecommendation problems as sequential transduction tasks within a generative\nmodeling framework (``Generative Recommenders''), and propose a new\narchitecture, HSTU, designed for high cardinality, non-stationary streaming\nrecommendation data.\n  HSTU outperforms baselines over synthetic and public datasets by up to 65.8\\%\nin NDCG, and is 5.3x to 15.2x faster than FlashAttention2-based Transformers on\n8192 length sequences. HSTU-based Generative Recommenders, with 1.5 trillion\nparameters, improve metrics in online A/B tests by 12.4\\% and have been\ndeployed on multiple surfaces of a large internet platform with billions of\nusers. More importantly, the model quality of Generative Recommenders\nempirically scales as a power-law of training compute across three orders of\nmagnitude, up to GPT-3/LLaMa-2 scale, which reduces carbon footprint needed for\nfuture model developments, and further paves the way for the first foundational\nmodels in recommendations.\n","authors":["Jiaqi Zhai","Lucy Liao","Xing Liu","Yueming Wang","Rui Li","Xuan Cao","Leon Gao","Zhaojie Gong","Fangda Gu","Michael He","Yinghai Lu","Yu Shi"],"pdf_url":"https://arxiv.org/pdf/2402.17152v1.pdf","comment":"Full technical report to follow"},{"id":"http://arxiv.org/abs/2402.17129v1","updated":"2024-02-27T01:48:28Z","published":"2024-02-27T01:48:28Z","title":"Side Information-Driven Session-based Recommendation: A Survey","summary":"  The session-based recommendation (SBR) garners increasing attention due to\nits ability to predict anonymous user intents within limited interactions.\nEmerging efforts incorporate various kinds of side information into their\nmethods for enhancing task performance. In this survey, we thoroughly review\nthe side information-driven session-based recommendation from a data-centric\nperspective. Our survey commences with an illustration of the motivation and\nnecessity behind this research topic. This is followed by a detailed\nexploration of various benchmarks rich in side information, pivotal for\nadvancing research in this field. Moreover, we delve into how these diverse\ntypes of side information enhance SBR, underscoring their characteristics and\nutility. A systematic review of research progress is then presented, offering\nan analysis of the most recent and representative developments within this\ntopic. Finally, we present the future prospects of this vibrant topic.\n","authors":["Xiaokun Zhang","Bo Xu","Chenliang Li","Yao Zhou","Liangyue Li","Hongfei Lin"],"pdf_url":"https://arxiv.org/pdf/2402.17129v1.pdf","comment":"This is a survey on side information-driven session-based\n  recommendation"},{"id":"http://arxiv.org/abs/2109.02473v4","updated":"2024-02-27T00:18:22Z","published":"2021-08-30T00:48:48Z","title":"A Robust Cybersecurity Topic Classification Tool","summary":"  In this research, we use user defined labels from three internet text sources\n(Reddit, Stackexchange, Arxiv) to train 21 different machine learning models\nfor the topic classification task of detecting cybersecurity discussions in\nnatural text. We analyze the false positive and false negative rates of each of\nthe 21 model's in a cross validation experiment. Then we present a\nCybersecurity Topic Classification (CTC) tool, which takes the majority vote of\nthe 21 trained machine learning models as the decision mechanism for detecting\ncybersecurity related text. We also show that the majority vote mechanism of\nthe CTC tool provides lower false negative and false positive rates on average\nthan any of the 21 individual models. We show that the CTC tool is scalable to\nthe hundreds of thousands of documents with a wall clock time on the order of\nhours.\n","authors":["Elijah Pelofske","Lorie M. Liebrock","Vincent Urias"],"pdf_url":"https://arxiv.org/pdf/2109.02473v4.pdf","comment":"Improved formatting"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2402.17768v1","updated":"2024-02-27T18:59:18Z","published":"2024-02-27T18:59:18Z","title":"Diffusion Meets DAgger: Supercharging Eye-in-hand Imitation Learning","summary":"  A common failure mode for policies trained with imitation is compounding\nexecution errors at test time. When the learned policy encounters states that\nwere not present in the expert demonstrations, the policy fails, leading to\ndegenerate behavior. The Dataset Aggregation, or DAgger approach to this\nproblem simply collects more data to cover these failure states. However, in\npractice, this is often prohibitively expensive. In this work, we propose\nDiffusion Meets DAgger (DMD), a method to reap the benefits of DAgger without\nthe cost for eye-in-hand imitation learning problems. Instead of collecting new\nsamples to cover out-of-distribution states, DMD uses recent advances in\ndiffusion models to create these samples with diffusion models. This leads to\nrobust performance from few demonstrations. In experiments conducted for\nnon-prehensile pushing on a Franka Research 3, we show that DMD can achieve a\nsuccess rate of 80% with as few as 8 expert demonstrations, where naive\nbehavior cloning reaches only 20%. DMD also outperform competing NeRF-based\naugmentation schemes by 50%.\n","authors":["Xiaoyu Zhang","Matthew Chang","Pranav Kumar","Saurabh Gupta"],"pdf_url":"https://arxiv.org/pdf/2402.17768v1.pdf","comment":"for project website with video, see\n  https://sites.google.com/view/diffusion-meets-dagger"},{"id":"http://arxiv.org/abs/2308.00566v2","updated":"2024-02-27T18:59:14Z","published":"2023-07-31T17:59:08Z","title":"Stochastic positional embeddings improve masked image modeling","summary":"  Masked Image Modeling (MIM) is a promising self-supervised learning approach\nthat enables learning from unlabeled images. Despite its recent success,\nlearning good representations through MIM remains challenging because it\nrequires predicting the right semantic content in accurate locations. For\nexample, given an incomplete picture of a dog, we can guess that there is a\ntail, but we cannot determine its exact location. In this work, we propose to\nincorporate location uncertainty into MIM by using stochastic positional\nembeddings (StoP). Specifically, we condition the model on stochastic masked\ntoken positions drawn from a Gaussian distribution. StoP reduces overfitting to\nlocation features and guides the model toward learning features that are more\nrobust to location uncertainties. Quantitatively, StoP improves downstream MIM\nperformance on a variety of downstream tasks, including $+1.7\\%$ on ImageNet\nlinear probing using ViT-B, and $+2.5\\%$ for ViT-H using $1\\%$ of the data.\n","authors":["Amir Bar","Florian Bordes","Assaf Shocher","Mahmoud Assran","Pascal Vincent","Nicolas Ballas","Trevor Darrell","Amir Globerson","Yann LeCun"],"pdf_url":"https://arxiv.org/pdf/2308.00566v2.pdf","comment":"Code and models available in https://github.com/amirbar/StoP"},{"id":"http://arxiv.org/abs/2402.17767v1","updated":"2024-02-27T18:58:54Z","published":"2024-02-27T18:58:54Z","title":"Opening Cabinets and Drawers in the Real World using a Commodity Mobile\n  Manipulator","summary":"  Pulling open cabinets and drawers presents many difficult technical\nchallenges in perception (inferring articulation parameters for objects from\nonboard sensors), planning (producing motion plans that conform to tight task\nconstraints), and control (making and maintaining contact while applying forces\non the environment). In this work, we build an end-to-end system that enables a\ncommodity mobile manipulator (Stretch RE2) to pull open cabinets and drawers in\ndiverse previously unseen real world environments. We conduct 4 days of real\nworld testing of this system spanning 31 different objects from across 13\ndifferent real world environments. Our system achieves a success rate of 61% on\nopening novel cabinets and drawers in unseen environments zero-shot. An\nanalysis of the failure modes suggests that errors in perception are the most\nsignificant challenge for our system. We will open source code and models for\nothers to replicate and build upon our system.\n","authors":["Arjun Gupta","Michelle Zhang","Rishik Sathua","Saurabh Gupta"],"pdf_url":"https://arxiv.org/pdf/2402.17767v1.pdf","comment":"Project webpage:\n  https://arjung128.github.io/opening-cabinets-and-drawers"},{"id":"http://arxiv.org/abs/2402.17764v1","updated":"2024-02-27T18:56:19Z","published":"2024-02-27T18:56:19Z","title":"The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits","summary":"  Recent research, such as BitNet, is paving the way for a new era of 1-bit\nLarge Language Models (LLMs). In this work, we introduce a 1-bit LLM variant,\nnamely BitNet b1.58, in which every single parameter (or weight) of the LLM is\nternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16)\nTransformer LLM with the same model size and training tokens in terms of both\nperplexity and end-task performance, while being significantly more\ncost-effective in terms of latency, memory, throughput, and energy consumption.\nMore profoundly, the 1.58-bit LLM defines a new scaling law and recipe for\ntraining new generations of LLMs that are both high-performance and\ncost-effective. Furthermore, it enables a new computation paradigm and opens\nthe door for designing specific hardware optimized for 1-bit LLMs.\n","authors":["Shuming Ma","Hongyu Wang","Lingxiao Ma","Lei Wang","Wenhui Wang","Shaohan Huang","Li Dong","Ruiping Wang","Jilong Xue","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2402.17764v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2307.08816v2","updated":"2024-02-27T18:55:22Z","published":"2023-07-17T20:11:56Z","title":"Accelerating Cutting-Plane Algorithms via Reinforcement Learning\n  Surrogates","summary":"  Discrete optimization belongs to the set of $\\mathcal{NP}$-hard problems,\nspanning fields such as mixed-integer programming and combinatorial\noptimization. A current standard approach to solving convex discrete\noptimization problems is the use of cutting-plane algorithms, which reach\noptimal solutions by iteratively adding inequalities known as \\textit{cuts} to\nrefine a feasible set. Despite the existence of a number of general-purpose\ncut-generating algorithms, large-scale discrete optimization problems continue\nto suffer from intractability. In this work, we propose a method for\naccelerating cutting-plane algorithms via reinforcement learning. Our approach\nuses learned policies as surrogates for $\\mathcal{NP}$-hard elements of the cut\ngenerating procedure in a way that (i) accelerates convergence, and (ii)\nretains guarantees of optimality. We apply our method on two types of problems\nwhere cutting-plane algorithms are commonly used: stochastic optimization, and\nmixed-integer quadratic programming. We observe the benefits of our method when\napplied to Benders decomposition (stochastic optimization) and iterative loss\napproximation (quadratic programming), achieving up to $45\\%$ faster average\nconvergence when compared to modern alternative algorithms.\n","authors":["Kyle Mana","Fernando Acero","Stephen Mak","Parisa Zehtabi","Michael Cashmore","Daniele Magazzeni","Manuela Veloso"],"pdf_url":"https://arxiv.org/pdf/2307.08816v2.pdf","comment":"Extended version (includes Supplementary Material). Accepted at AAAI\n  24 Main Track with Oral Presentation"},{"id":"http://arxiv.org/abs/2402.17762v1","updated":"2024-02-27T18:55:17Z","published":"2024-02-27T18:55:17Z","title":"Massive Activations in Large Language Models","summary":"  We observe an empirical phenomenon in Large Language Models (LLMs) -- very\nfew activations exhibit significantly larger values than others (e.g., 100,000\ntimes larger). We call them massive activations. First, we demonstrate the\nwidespread existence of massive activations across various LLMs and\ncharacterize their locations. Second, we find their values largely stay\nconstant regardless of the input, and they function as indispensable bias terms\nin LLMs. Third, these massive activations lead to the concentration of\nattention probabilities to their corresponding tokens, and further, implicit\nbias terms in the self-attention output. Last, we also study massive\nactivations in Vision Transformers.\n","authors":["Mingjie Sun","Xinlei Chen","J. Zico Kolter","Zhuang Liu"],"pdf_url":"https://arxiv.org/pdf/2402.17762v1.pdf","comment":"Website at\n  https://eric-mingjie.github.io/massive-activations/index.html"},{"id":"http://arxiv.org/abs/2402.16359v2","updated":"2024-02-27T18:54:40Z","published":"2024-02-26T07:24:32Z","title":"Feedback Efficient Online Fine-Tuning of Diffusion Models","summary":"  Diffusion models excel at modeling complex data distributions, including\nthose of images, proteins, and small molecules. However, in many cases, our\ngoal is to model parts of the distribution that maximize certain properties:\nfor example, we may want to generate images with high aesthetic quality, or\nmolecules with high bioactivity. It is natural to frame this as a reinforcement\nlearning (RL) problem, in which the objective is to fine-tune a diffusion model\nto maximize a reward function that corresponds to some property. Even with\naccess to online queries of the ground-truth reward function, efficiently\ndiscovering high-reward samples can be challenging: they might have a low\nprobability in the initial distribution, and there might be many infeasible\nsamples that do not even have a well-defined reward (e.g., unnatural images or\nphysically impossible molecules). In this work, we propose a novel\nreinforcement learning procedure that efficiently explores on the manifold of\nfeasible samples. We present a theoretical analysis providing a regret\nguarantee, as well as empirical validation across three domains: images,\nbiological sequences, and molecules.\n","authors":["Masatoshi Uehara","Yulai Zhao","Kevin Black","Ehsan Hajiramezanali","Gabriele Scalia","Nathaniel Lee Diamant","Alex M Tseng","Sergey Levine","Tommaso Biancalani"],"pdf_url":"https://arxiv.org/pdf/2402.16359v2.pdf","comment":"Under review (codes will be released soon)"},{"id":"http://arxiv.org/abs/2402.17760v1","updated":"2024-02-27T18:53:18Z","published":"2024-02-27T18:53:18Z","title":"Learning to Program Variational Quantum Circuits with Fast Weights","summary":"  Quantum Machine Learning (QML) has surfaced as a pioneering framework\naddressing sequential control tasks and time-series modeling. It has\ndemonstrated empirical quantum advantages notably within domains such as\nReinforcement Learning (RL) and time-series prediction. A significant\nadvancement lies in Quantum Recurrent Neural Networks (QRNNs), specifically\ntailored for memory-intensive tasks encompassing partially observable\nenvironments and non-linear time-series prediction. Nevertheless, QRNN-based\nmodels encounter challenges, notably prolonged training duration stemming from\nthe necessity to compute quantum gradients using backpropagation-through-time\n(BPTT). This predicament exacerbates when executing the complete model on\nquantum devices, primarily due to the substantial demand for circuit evaluation\narising from the parameter-shift rule. This paper introduces the Quantum Fast\nWeight Programmers (QFWP) as a solution to the temporal or sequential learning\nchallenge. The QFWP leverages a classical neural network (referred to as the\n'slow programmer') functioning as a quantum programmer to swiftly modify the\nparameters of a variational quantum circuit (termed the 'fast programmer').\nInstead of completely overwriting the fast programmer at each time-step, the\nslow programmer generates parameter changes or updates for the quantum circuit\nparameters. This approach enables the fast programmer to incorporate past\nobservations or information. Notably, the proposed QFWP model achieves learning\nof temporal dependencies without necessitating the use of quantum recurrent\nneural networks. Numerical simulations conducted in this study showcase the\nefficacy of the proposed QFWP model in both time-series prediction and RL\ntasks. The model exhibits performance levels either comparable to or surpassing\nthose achieved by QLSTM-based models.\n","authors":["Samuel Yen-Chi Chen"],"pdf_url":"https://arxiv.org/pdf/2402.17760v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17756v1","updated":"2024-02-27T18:48:07Z","published":"2024-02-27T18:48:07Z","title":"Robustly Learning Single-Index Models via Alignment Sharpness","summary":"  We study the problem of learning Single-Index Models under the $L_2^2$ loss\nin the agnostic model. We give an efficient learning algorithm, achieving a\nconstant factor approximation to the optimal loss, that succeeds under a range\nof distributions (including log-concave distributions) and a broad class of\nmonotone and Lipschitz link functions. This is the first efficient constant\nfactor approximate agnostic learner, even for Gaussian data and for any\nnontrivial class of link functions. Prior work for the case of unknown link\nfunction either works in the realizable setting or does not attain constant\nfactor approximation. The main technical ingredient enabling our algorithm and\nanalysis is a novel notion of a local error bound in optimization that we term\nalignment sharpness and that may be of broader interest.\n","authors":["Nikos Zarifis","Puqian Wang","Ilias Diakonikolas","Jelena Diakonikolas"],"pdf_url":"https://arxiv.org/pdf/2402.17756v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.14591v4","updated":"2024-02-27T18:46:10Z","published":"2024-01-26T01:36:48Z","title":"Ricci flow-guided autoencoders in learning time-dependent dynamics","summary":"  We present a manifold-based autoencoder method for learning nonlinear\ndynamics in time, notably partial differential equations (PDEs), in which the\nmanifold latent space evolves according to Ricci flow. This can be accomplished\nby simulating Ricci flow in a physics-informed setting, and manifold quantities\ncan be matched so that Ricci flow is empirically achieved. With our\nmethodology, the manifold is learned as part of the training procedure, so\nideal geometries may be discerned, while the evolution simultaneously induces a\nmore accommodating latent representation over static methods. We present our\nmethod on a range of numerical experiments consisting of PDEs that encompass\ndesirable characteristics such as periodicity and randomness, remarking error\non in-distribution and extrapolation scenarios.\n","authors":["Andrew Gracyk"],"pdf_url":"https://arxiv.org/pdf/2401.14591v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14035v2","updated":"2024-02-27T18:44:36Z","published":"2024-02-21T04:33:26Z","title":"Wisdom of Committee: Distilling from Foundation Model to Specialized\n  Application Model","summary":"  Recent advancements in foundation models have yielded impressive performance\nacross a wide range of tasks. Meanwhile, for specific applications,\npractitioners have been developing specialized application models. To enjoy the\nbenefits of both kinds of models, one natural path is to transfer the knowledge\nin foundation models into specialized application models, which are generally\nmore efficient for serving. Techniques from knowledge distillation may be\napplied here, where the application model learns to mimic the foundation model.\nHowever, specialized application models and foundation models have substantial\ngaps in capacity, employing distinct architectures, using different input\nfeatures from different modalities, and being optimized on different\ndistributions. These differences in model characteristics lead to significant\nchallenges for distillation methods. In this work, we propose creating a\nteaching committee comprising both foundation model teachers and complementary\nteachers. Complementary teachers possess model characteristics akin to the\nstudent's, aiming to bridge the gap between the foundation model and\nspecialized application models for a smoother knowledge transfer. Further, to\naccommodate the dissimilarity among the teachers in the committee, we introduce\nDiverseDistill, which allows the student to understand the expertise of each\nteacher and extract task knowledge. Our evaluations demonstrate that adding\ncomplementary teachers enhances student performance. Finally, DiverseDistill\nconsistently outperforms baseline distillation methods, regardless of the\nteacher choices, resulting in significantly improved student performance.\n","authors":["Zichang Liu","Qingyun Liu","Yuening Li","Liang Liu","Anshumali Shrivastava","Shuchao Bi","Lichan Hong","Ed H. Chi","Zhe Zhao"],"pdf_url":"https://arxiv.org/pdf/2402.14035v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17753v1","updated":"2024-02-27T18:42:31Z","published":"2024-02-27T18:42:31Z","title":"Evaluating Very Long-Term Conversational Memory of LLM Agents","summary":"  Existing works on long-term open-domain dialogues focus on evaluating model\nresponses within contexts spanning no more than five chat sessions. Despite\nadvancements in long-context large language models (LLMs) and retrieval\naugmented generation (RAG) techniques, their efficacy in very long-term\ndialogues remains unexplored. To address this research gap, we introduce a\nmachine-human pipeline to generate high-quality, very long-term dialogues by\nleveraging LLM-based agent architectures and grounding their dialogues on\npersonas and temporal event graphs. Moreover, we equip each agent with the\ncapability of sharing and reacting to images. The generated conversations are\nverified and edited by human annotators for long-range consistency and\ngrounding to the event graphs. Using this pipeline, we collect LoCoMo, a\ndataset of very long-term conversations, each encompassing 300 turns and 9K\ntokens on avg., over up to 35 sessions. Based on LoCoMo, we present a\ncomprehensive evaluation benchmark to measure long-term memory in models,\nencompassing question answering, event summarization, and multi-modal dialogue\ngeneration tasks. Our experimental results indicate that LLMs exhibit\nchallenges in understanding lengthy conversations and comprehending long-range\ntemporal and causal dynamics within dialogues. Employing strategies like\nlong-context LLMs or RAG can offer improvements but these models still\nsubstantially lag behind human performance.\n","authors":["Adyasha Maharana","Dong-Ho Lee","Sergey Tulyakov","Mohit Bansal","Francesco Barbieri","Yuwei Fang"],"pdf_url":"https://arxiv.org/pdf/2402.17753v1.pdf","comment":"19 pages; Project page: https://snap-research.github.io/locomo/"},{"id":"http://arxiv.org/abs/2402.17750v1","updated":"2024-02-27T18:37:22Z","published":"2024-02-27T18:37:22Z","title":"Scaling on-chip photonic neural processors using arbitrarily\n  programmable wave propagation","summary":"  On-chip photonic processors for neural networks have potential benefits in\nboth speed and energy efficiency but have not yet reached the scale at which\nthey can outperform electronic processors. The dominant paradigm for designing\non-chip photonics is to make networks of relatively bulky discrete components\nconnected by one-dimensional waveguides. A far more compact alternative is to\navoid explicitly defining any components and instead sculpt the continuous\nsubstrate of the photonic processor to directly perform the computation using\nwaves freely propagating in two dimensions. We propose and demonstrate a device\nwhose refractive index as a function of space, $n(x,z)$, can be rapidly\nreprogrammed, allowing arbitrary control over the wave propagation in the\ndevice. Our device, a 2D-programmable waveguide, combines photoconductive gain\nwith the electro-optic effect to achieve massively parallel modulation of the\nrefractive index of a slab waveguide, with an index modulation depth of\n$10^{-3}$ and approximately $10^4$ programmable degrees of freedom. We used a\nprototype device with a functional area of $12\\,\\text{mm}^2$ to perform\nneural-network inference with up to 49-dimensional input vectors in a single\npass, achieving 96% accuracy on vowel classification and 86% accuracy on $7\n\\times 7$-pixel MNIST handwritten-digit classification. This is a scale beyond\nthat of previous photonic chips relying on discrete components, illustrating\nthe benefit of the continuous-waves paradigm. In principle, with large enough\nchip area, the reprogrammability of the device's refractive index distribution\nenables the reconfigurable realization of any passive, linear photonic circuit\nor device. This promises the development of more compact and versatile photonic\nsystems for a wide range of applications, including optical processing, smart\nsensing, spectroscopy, and optical communications.\n","authors":["Tatsuhiro Onodera","Martin M. Stein","Benjamin A. Ash","Mandar M. Sohoni","Melissa Bosch","Ryotatsu Yanagimoto","Marc Jankowski","Timothy P. McKenna","Tianyu Wang","Gennady Shvets","Maxim R. Shcherbakov","Logan G. Wright","Peter L. McMahon"],"pdf_url":"https://arxiv.org/pdf/2402.17750v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17747v1","updated":"2024-02-27T18:32:11Z","published":"2024-02-27T18:32:11Z","title":"When Your AI Deceives You: Challenges with Partial Observability of\n  Human Evaluators in Reward Learning","summary":"  Past analyses of reinforcement learning from human feedback (RLHF) assume\nthat the human fully observes the environment. What happens when human feedback\nis based only on partial observations? We formally define two failure cases:\ndeception and overjustification. Modeling the human as Boltzmann-rational\nw.r.t. a belief over trajectories, we prove conditions under which RLHF is\nguaranteed to result in policies that deceptively inflate their performance,\noverjustify their behavior to make an impression, or both. To help address\nthese issues, we mathematically characterize how partial observability of the\nenvironment translates into (lack of) ambiguity in the learned return function.\nIn some cases, accounting for partial observability makes it theoretically\npossible to recover the return function and thus the optimal policy, while in\nother cases, there is irreducible ambiguity. We caution against blindly\napplying RLHF in partially observable settings and propose research directions\nto help tackle these challenges.\n","authors":["Leon Lang","Davis Foote","Stuart Russell","Anca Dragan","Erik Jenner","Scott Emmons"],"pdf_url":"https://arxiv.org/pdf/2402.17747v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.06682v4","updated":"2024-02-27T18:29:18Z","published":"2021-06-12T03:55:15Z","title":"Solving PDEs on Unknown Manifolds with Machine Learning","summary":"  This paper proposes a mesh-free computational framework and machine learning\ntheory for solving elliptic PDEs on unknown manifolds, identified with point\nclouds, based on diffusion maps (DM) and deep learning. The PDE solver is\nformulated as a supervised learning task to solve a least-squares regression\nproblem that imposes an algebraic equation approximating a PDE (and boundary\nconditions if applicable). This algebraic equation involves a graph-Laplacian\ntype matrix obtained via DM asymptotic expansion, which is a consistent\nestimator of second-order elliptic differential operators. The resulting\nnumerical method is to solve a highly non-convex empirical risk minimization\nproblem subjected to a solution from a hypothesis space of neural networks\n(NNs). In a well-posed elliptic PDE setting, when the hypothesis space consists\nof neural networks with either infinite width or depth, we show that the global\nminimizer of the empirical loss function is a consistent solution in the limit\nof large training data. When the hypothesis space is a two-layer neural\nnetwork, we show that for a sufficiently large width, gradient descent can\nidentify a global minimizer of the empirical loss function. Supporting\nnumerical examples demonstrate the convergence of the solutions, ranging from\nsimple manifolds with low and high co-dimensions, to rough surfaces with and\nwithout boundaries. We also show that the proposed NN solver can robustly\ngeneralize the PDE solution on new data points with generalization errors that\nare almost identical to the training errors, superseding a Nystrom-based\ninterpolation method.\n","authors":["Senwei Liang","Shixiao W. Jiang","John Harlim","Haizhao Yang"],"pdf_url":"https://arxiv.org/pdf/2106.06682v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17739v1","updated":"2024-02-27T18:18:23Z","published":"2024-02-27T18:18:23Z","title":"reBandit: Random Effects based Online RL algorithm for Reducing Cannabis\n  Use","summary":"  The escalating prevalence of cannabis use, and associated cannabis-use\ndisorder (CUD), poses a significant public health challenge globally. With a\nnotably wide treatment gap, especially among emerging adults (EAs; ages 18-25),\naddressing cannabis use and CUD remains a pivotal objective within the 2030\nUnited Nations Agenda for Sustainable Development Goals (SDG). In this work, we\ndevelop an online reinforcement learning (RL) algorithm called reBandit which\nwill be utilized in a mobile health study to deliver personalized mobile health\ninterventions aimed at reducing cannabis use among EAs. reBandit utilizes\nrandom effects and informative Bayesian priors to learn quickly and efficiently\nin noisy mobile health environments. Moreover, reBandit employs Empirical Bayes\nand optimization techniques to autonomously update its hyper-parameters online.\nTo evaluate the performance of our algorithm, we construct a simulation testbed\nusing data from a prior study, and compare against commonly used algorithms in\nmobile health studies. We show that reBandit performs equally well or better\nthan all the baseline algorithms, and the performance gap widens as population\nheterogeneity increases in the simulation environment, proving its adeptness to\nadapt to diverse population of study participants.\n","authors":["Susobhan Ghosh","Yongyi Guo","Pei-Yao Hung","Lara Coughlin","Erin Bonar","Inbal Nahum-Shani","Maureen Walton","Susan Murphy"],"pdf_url":"https://arxiv.org/pdf/2402.17739v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17736v1","updated":"2024-02-27T18:12:58Z","published":"2024-02-27T18:12:58Z","title":"Learning-Based Algorithms for Graph Searching Problems","summary":"  We consider the problem of graph searching with prediction recently\nintroduced by Banerjee et al. (2022). In this problem, an agent, starting at\nsome vertex $r$ has to traverse a (potentially unknown) graph $G$ to find a\nhidden goal node $g$ while minimizing the total distance travelled. We study a\nsetting in which at any node $v$, the agent receives a noisy estimate of the\ndistance from $v$ to $g$. We design algorithms for this search task on unknown\ngraphs. We establish the first formal guarantees on unknown weighted graphs and\nprovide lower bounds showing that the algorithms we propose have optimal or\nnearly-optimal dependence on the prediction error. Further, we perform\nnumerical experiments demonstrating that in addition to being robust to\nadversarial error, our algorithms perform well in typical instances in which\nthe error is stochastic. Finally, we provide alternative simpler performance\nbounds on the algorithms of Banerjee et al. (2022) for the case of searching on\na known graph, and establish new lower bounds for this setting.\n","authors":["Adela Frances DePavia","Erasmo Tani","Ali Vakilian"],"pdf_url":"https://arxiv.org/pdf/2402.17736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04311v2","updated":"2024-02-27T18:10:02Z","published":"2023-10-06T15:17:45Z","title":"Distributed Deep Joint Source-Channel Coding with Decoder-Only Side\n  Information","summary":"  We consider low-latency image transmission over a noisy wireless channel when\ncorrelated side information is present only at the receiver side (the Wyner-Ziv\nscenario). In particular, we are interested in developing practical schemes\nusing a data-driven joint source-channel coding (JSCC) approach, which has been\npreviously shown to outperform conventional separation-based approaches in the\npractical finite blocklength regimes, and to provide graceful degradation with\nchannel quality. We propose a novel neural network architecture that\nincorporates the decoder-only side information at multiple stages at the\nreceiver side. Our results demonstrate that the proposed method succeeds in\nintegrating the side information, yielding improved performance at all channel\nconditions in terms of the various quality measures considered here, especially\nat low channel signal-to-noise ratios (SNRs) and small bandwidth ratios (BRs).\nWe have made the source code of the proposed method public to enable further\nresearch, and the reproducibility of the results.\n","authors":["Selim F. Yilmaz","Ezgi Ozyilkan","Deniz Gunduz","Elza Erkip"],"pdf_url":"https://arxiv.org/pdf/2310.04311v2.pdf","comment":"To appear in IEEE International Conference on Machine Learning for\n  Communication and Networking (ICMLCN) 2024"},{"id":"http://arxiv.org/abs/2402.16842v2","updated":"2024-02-27T18:06:29Z","published":"2024-02-26T18:59:12Z","title":"Asymmetry in Low-Rank Adapters of Foundation Models","summary":"  Parameter-efficient fine-tuning optimizes large, pre-trained foundation\nmodels by updating a subset of parameters; in this class, Low-Rank Adaptation\n(LoRA) is particularly effective. Inspired by an effort to investigate the\ndifferent roles of LoRA matrices during fine-tuning, this paper characterizes\nand leverages unexpected asymmetry in the importance of low-rank adapter\nmatrices. Specifically, when updating the parameter matrices of a neural\nnetwork by adding a product $BA$, we observe that the $B$ and $A$ matrices have\ndistinct functions: $A$ extracts features from the input, while $B$ uses these\nfeatures to create the desired output. Based on this observation, we\ndemonstrate that fine-tuning $B$ is inherently more effective than fine-tuning\n$A$, and that a random untrained $A$ should perform nearly as well as a\nfine-tuned one. Using an information-theoretic lens, we also bound the\ngeneralization of low-rank adapters, showing that the parameter savings of\nexclusively training $B$ improves the bound. We support our conclusions with\nexperiments on RoBERTa, BART-Large, LLaMA-2, and ViTs.\n","authors":["Jiacheng Zhu","Kristjan Greenewald","Kimia Nadjahi","Haitz Sáez de Ocáriz Borde","Rickard Brüel Gabrielsson","Leshem Choshen","Marzyeh Ghassemi","Mikhail Yurochkin","Justin Solomon"],"pdf_url":"https://arxiv.org/pdf/2402.16842v2.pdf","comment":"17 pages, 2 figures, 9 tables"},{"id":"http://arxiv.org/abs/2402.17732v1","updated":"2024-02-27T18:06:20Z","published":"2024-02-27T18:06:20Z","title":"Batched Nonparametric Contextual Bandits","summary":"  We study nonparametric contextual bandits under batch constraints, where the\nexpected reward for each action is modeled as a smooth function of covariates,\nand the policy updates are made at the end of each batch of observations. We\nestablish a minimax regret lower bound for this setting and propose Batched\nSuccessive Elimination with Dynamic Binning (BaSEDB) that achieves optimal\nregret (up to logarithmic factors). In essence, BaSEDB dynamically splits the\ncovariate space into smaller bins, carefully aligning their widths with the\nbatch size. We also show the suboptimality of static binning under batch\nconstraints, highlighting the necessity of dynamic binning. Additionally, our\nresults suggest that a nearly constant number of policy updates can attain\noptimal regret in the fully online setting.\n","authors":["Rong Jiang","Cong Ma"],"pdf_url":"https://arxiv.org/pdf/2402.17732v1.pdf","comment":"26 pages, 4 figures"},{"id":"http://arxiv.org/abs/2402.17730v1","updated":"2024-02-27T18:04:59Z","published":"2024-02-27T18:04:59Z","title":"Markovletics: Methods and A Novel Application for Learning\n  Continuous-Time Markov Chain Mixtures","summary":"  Sequential data naturally arises from user engagement on digital platforms\nlike social media, music streaming services, and web navigation, encapsulating\nevolving user preferences and behaviors through continuous information streams.\nA notable unresolved query in stochastic processes is learning mixtures of\ncontinuous-time Markov chains (CTMCs). While there is progress in learning\nmixtures of discrete-time Markov chains with recovery guarantees\n[GKV16,ST23,KTT2023], the continuous scenario uncovers unique unexplored\nchallenges. The intrigue in CTMC mixtures stems from their potential to model\nintricate continuous-time stochastic processes prevalent in various fields\nincluding social media, finance, and biology.\n  In this study, we introduce a novel framework for exploring CTMCs,\nemphasizing the influence of observed trails' length and mixture parameters on\nproblem regimes, which demands specific algorithms. Through thorough\nexperimentation, we examine the impact of discretizing continuous-time trails\non the learnability of the continuous-time mixture, given that these processes\nare often observed via discrete, resource-demanding observations. Our\ncomparative analysis with leading methods explores sample complexity and the\ntrade-off between the number of trails and their lengths, offering crucial\ninsights for method selection in different problem instances. We apply our\nalgorithms on an extensive collection of Lastfm's user-generated trails\nspanning three years, demonstrating the capability of our algorithms to\ndifferentiate diverse user preferences. We pioneer the use of CTMC mixtures on\na basketball passing dataset to unveil intricate offensive tactics of NBA\nteams. This underscores the pragmatic utility and versatility of our proposed\nframework. All results presented in this study are replicable, and we provide\nthe implementations to facilitate reproducibility.\n","authors":["Fabian Spaeh","Charalampos E. Tsourakakis"],"pdf_url":"https://arxiv.org/pdf/2402.17730v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17722v1","updated":"2024-02-27T17:56:49Z","published":"2024-02-27T17:56:49Z","title":"Taming Nonconvex Stochastic Mirror Descent with General Bregman\n  Divergence","summary":"  This paper revisits the convergence of Stochastic Mirror Descent (SMD) in the\ncontemporary nonconvex optimization setting. Existing results for batch-free\nnonconvex SMD restrict the choice of the distance generating function (DGF) to\nbe differentiable with Lipschitz continuous gradients, thereby excluding\nimportant setups such as Shannon entropy. In this work, we present a new\nconvergence analysis of nonconvex SMD supporting general DGF, that overcomes\nthe above limitations and relies solely on the standard assumptions. Moreover,\nour convergence is established with respect to the Bregman Forward-Backward\nenvelope, which is a stronger measure than the commonly used squared norm of\ngradient mapping. We further extend our results to guarantee high probability\nconvergence under sub-Gaussian noise and global convergence under the\ngeneralized Bregman Proximal Polyak-{\\L}ojasiewicz condition. Additionally, we\nillustrate the advantages of our improved SMD theory in various nonconvex\nmachine learning tasks by harnessing nonsmooth DGFs. Notably, in the context of\nnonconvex differentially private (DP) learning, our theory yields a simple\nalgorithm with a (nearly) dimension-independent utility bound. For the problem\nof training linear neural networks, we develop provably convergent stochastic\nalgorithms.\n","authors":["Ilyas Fatkhullin","Niao He"],"pdf_url":"https://arxiv.org/pdf/2402.17722v1.pdf","comment":"Accepted for publication at AISTATS 2024"},{"id":"http://arxiv.org/abs/2402.15968v2","updated":"2024-02-27T17:55:44Z","published":"2024-02-25T03:07:32Z","title":"CoDream: Exchanging dreams instead of models for federated aggregation\n  with heterogeneous models","summary":"  Federated Learning (FL) enables collaborative optimization of machine\nlearning models across decentralized data by aggregating model parameters. Our\napproach extends this concept by aggregating \"knowledge\" derived from models,\ninstead of model parameters. We present a novel framework called CoDream, where\nclients collaboratively optimize randomly initialized data using federated\noptimization in the input data space, similar to how randomly initialized model\nparameters are optimized in FL. Our key insight is that jointly optimizing this\ndata can effectively capture the properties of the global data distribution.\nSharing knowledge in data space offers numerous benefits: (1) model-agnostic\ncollaborative learning, i.e., different clients can have different model\narchitectures; (2) communication that is independent of the model size,\neliminating scalability concerns with model parameters; (3) compatibility with\nsecure aggregation, thus preserving the privacy benefits of federated learning;\n(4) allowing of adaptive optimization of knowledge shared for personalized\nlearning. We empirically validate CoDream on standard FL tasks, demonstrating\ncompetitive performance despite not sharing model parameters. Our code:\nhttps://mitmedialab.github.io/codream.github.io/\n","authors":["Abhishek Singh","Gauri Gupta","Ritvik Kapila","Yichuan Shi","Alex Dang","Sheshank Shankar","Mohammed Ehab","Ramesh Raskar"],"pdf_url":"https://arxiv.org/pdf/2402.15968v2.pdf","comment":"16 pages, 12 figures, 5 tables"},{"id":"http://arxiv.org/abs/2402.17720v1","updated":"2024-02-27T17:55:33Z","published":"2024-02-27T17:55:33Z","title":"The SMART approach to instance-optimal online learning","summary":"  We devise an online learning algorithm -- titled Switching via Monotone\nAdapted Regret Traces (SMART) -- that adapts to the data and achieves regret\nthat is instance optimal, i.e., simultaneously competitive on every input\nsequence compared to the performance of the follow-the-leader (FTL) policy and\nthe worst case guarantee of any other input policy. We show that the regret of\nthe SMART policy on any input sequence is within a multiplicative factor\n$e/(e-1) \\approx 1.58$ of the smaller of: 1) the regret obtained by FTL on the\nsequence, and 2) the upper bound on regret guaranteed by the given worst-case\npolicy. This implies a strictly stronger guarantee than typical\n`best-of-both-worlds' bounds as the guarantee holds for every input sequence\nregardless of how it is generated. SMART is simple to implement as it begins by\nplaying FTL and switches at most once during the time horizon to the worst-case\nalgorithm. Our approach and results follow from an operational reduction of\ninstance optimal online learning to competitive analysis for the ski-rental\nproblem. We complement our competitive ratio upper bounds with a fundamental\nlower bound showing that over all input sequences, no algorithm can get better\nthan a $1.43$-fraction of the minimum regret achieved by FTL and the\nminimax-optimal policy. We also present a modification of SMART that combines\nFTL with a ``small-loss\" algorithm to achieve instance optimality between the\nregret of FTL and the small loss regret bound.\n","authors":["Siddhartha Banerjee","Alankrita Bhatt","Christina Lee Yu"],"pdf_url":"https://arxiv.org/pdf/2402.17720v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17718v1","updated":"2024-02-27T17:53:13Z","published":"2024-02-27T17:53:13Z","title":"Towards a Digital Twin Framework in Additive Manufacturing: Machine\n  Learning and Bayesian Optimization for Time Series Process Optimization","summary":"  Laser-directed-energy deposition (DED) offers advantages in additive\nmanufacturing (AM) for creating intricate geometries and material grading. Yet,\nchallenges like material inconsistency and part variability remain, mainly due\nto its layer-wise fabrication. A key issue is heat accumulation during DED,\nwhich affects the material microstructure and properties. While closed-loop\ncontrol methods for heat management are common in DED research, few integrate\nreal-time monitoring, physics-based modeling, and control in a unified\nframework. Our work presents a digital twin (DT) framework for real-time\npredictive control of DED process parameters to meet specific design\nobjectives. We develop a surrogate model using Long Short-Term Memory\n(LSTM)-based machine learning with Bayesian Inference to predict temperatures\nin DED parts. This model predicts future temperature states in real time. We\nalso introduce Bayesian Optimization (BO) for Time Series Process Optimization\n(BOTSPO), based on traditional BO but featuring a unique time series process\nprofile generator with reduced dimensions. BOTSPO dynamically optimizes\nprocesses, identifying optimal laser power profiles to attain desired\nmechanical properties. The established process trajectory guides online\noptimizations, aiming to enhance performance. This paper outlines the digital\ntwin framework's components, promoting its integration into a comprehensive\nsystem for AM.\n","authors":["Vispi Karkaria","Anthony Goeckner","Rujing Zha","Jie Chen","Jianjing Zhang","Qi Zhu","Jian Cao","Robert X. Gao","Wei Chen"],"pdf_url":"https://arxiv.org/pdf/2402.17718v1.pdf","comment":"12 Pages, 10 Figures, 1 Table, NAMRC Conference"},{"id":"http://arxiv.org/abs/2402.17710v1","updated":"2024-02-27T17:43:51Z","published":"2024-02-27T17:43:51Z","title":"Understanding Neural Network Binarization with Forward and Backward\n  Proximal Quantizers","summary":"  In neural network binarization, BinaryConnect (BC) and its variants are\nconsidered the standard. These methods apply the sign function in their forward\npass and their respective gradients are backpropagated to update the weights.\nHowever, the derivative of the sign function is zero whenever defined, which\nconsequently freezes training. Therefore, implementations of BC (e.g., BNN)\nusually replace the derivative of sign in the backward computation with\nidentity or other approximate gradient alternatives. Although such practice\nworks well empirically, it is largely a heuristic or ''training trick.'' We aim\nat shedding some light on these training tricks from the optimization\nperspective. Building from existing theory on ProxConnect (PC, a generalization\nof BC), we (1) equip PC with different forward-backward quantizers and obtain\nProxConnect++ (PC++) that includes existing binarization techniques as special\ncases; (2) derive a principled way to synthesize forward-backward quantizers\nwith automatic theoretical guarantees; (3) illustrate our theory by proposing\nan enhanced binarization algorithm BNN++; (4) conduct image classification\nexperiments on CNNs and vision transformers, and empirically verify that BNN++\ngenerally achieves competitive results on binarizing these models.\n","authors":["Yiwei Lu","Yaoliang Yu","Xinlin Li","Vahid Partovi Nia"],"pdf_url":"https://arxiv.org/pdf/2402.17710v1.pdf","comment":"Accepted to NeurIPS 2023"},{"id":"http://arxiv.org/abs/2306.08649v2","updated":"2024-02-27T17:34:43Z","published":"2023-06-14T17:28:46Z","title":"OCAtari: Object-Centric Atari 2600 Reinforcement Learning Environments","summary":"  Cognitive science and psychology suggest that object-centric representations\nof complex scenes are a promising step towards enabling efficient abstract\nreasoning from low-level perceptual features. Yet, most deep reinforcement\nlearning approaches only rely on pixel-based representations that do not\ncapture the compositional properties of natural scenes. For this, we need\nenvironments and datasets that allow us to work and evaluate object-centric\napproaches. In our work, we extend the Atari Learning Environments, the\nmost-used evaluation framework for deep RL approaches, by introducing OCAtari,\nthat performs resource-efficient extractions of the object-centric states for\nthese games. Our framework allows for object discovery, object representation\nlearning, as well as object-centric RL. We evaluate OCAtari's detection\ncapabilities and resource efficiency. Our source code is available at\ngithub.com/k4ntz/OC_Atari.\n","authors":["Quentin Delfosse","Jannis Blüml","Bjarne Gregori","Sebastian Sztwiertnia","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2306.08649v2.pdf","comment":"26 pages, 8 main paper pages, 36 appendix pages. In main paper: 4\n  figures, 3 tables"},{"id":"http://arxiv.org/abs/2402.17705v1","updated":"2024-02-27T17:33:23Z","published":"2024-02-27T17:33:23Z","title":"Federated Learning for Estimating Heterogeneous Treatment Effects","summary":"  Machine learning methods for estimating heterogeneous treatment effects (HTE)\nfacilitate large-scale personalized decision-making across various domains such\nas healthcare, policy making, education, and more. Current machine learning\napproaches for HTE require access to substantial amounts of data per treatment,\nand the high costs associated with interventions makes centrally collecting so\nmuch data for each intervention a formidable challenge. To overcome this\nobstacle, in this work, we propose a novel framework for collaborative learning\nof HTE estimators across institutions via Federated Learning. We show that even\nunder a diversity of interventions and subject populations across clients, one\ncan jointly learn a common feature representation, while concurrently and\nprivately learning the specific predictive functions for outcomes under\ndistinct interventions across institutions. Our framework and the associated\nalgorithm are based on this insight, and leverage tabular transformers to map\nmultiple input data to feature representations which are then used for outcome\nprediction via multi-task learning. We also propose a novel way of federated\ntraining of personalised transformers that can work with heterogeneous input\nfeature spaces. Experimental results on real-world clinical trial data\ndemonstrate the effectiveness of our method.\n","authors":["Disha Makhija","Joydeep Ghosh","Yejin Kim"],"pdf_url":"https://arxiv.org/pdf/2402.17705v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17704v1","updated":"2024-02-27T17:30:33Z","published":"2024-02-27T17:30:33Z","title":"Transfer Learning Bayesian Optimization to Design Competitor DNA\n  Molecules for Use in Diagnostic Assays","summary":"  With the rise in engineered biomolecular devices, there is an increased need\nfor tailor-made biological sequences. Often, many similar biological sequences\nneed to be made for a specific application meaning numerous, sometimes\nprohibitively expensive, lab experiments are necessary for their optimization.\nThis paper presents a transfer learning design of experiments workflow to make\nthis development feasible. By combining a transfer learning surrogate model\nwith Bayesian optimization, we show how the total number of experiments can be\nreduced by sharing information between optimization tasks. We demonstrate the\nreduction in the number of experiments using data from the development of DNA\ncompetitors for use in an amplification-based diagnostic assay. We use\ncross-validation to compare the predictive accuracy of different transfer\nlearning models, and then compare the performance of the models for both single\nobjective and penalized optimization tasks.\n","authors":["Ruby Sedgwick","John P. Goertz","Molly M. Stevens","Ruth Misener","Mark van der Wilk"],"pdf_url":"https://arxiv.org/pdf/2402.17704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17701v1","updated":"2024-02-27T17:26:33Z","published":"2024-02-27T17:26:33Z","title":"Real-time Low-latency Music Source Separation using Hybrid\n  Spectrogram-TasNet","summary":"  There have been significant advances in deep learning for music demixing in\nrecent years. However, there has been little attention given to how these\nneural networks can be adapted for real-time low-latency applications, which\ncould be helpful for hearing aids, remixing audio streams and live shows. In\nthis paper, we investigate the various challenges involved in adapting current\ndemixing models in the literature for this use case. Subsequently, inspired by\nthe Hybrid Demucs architecture, we propose the Hybrid Spectrogram Time-domain\nAudio Separation Network HS-TasNet, which utilises the advantages of spectral\nand waveform domains. For a latency of 23 ms, the HS-TasNet obtains an overall\nsignal-to-distortion ratio (SDR) of 4.65 on the MusDB test set, and increases\nto 5.55 with additional training data. These results demonstrate the potential\nof efficient demixing for real-time low-latency music applications.\n","authors":["Satvik Venkatesh","Arthur Benilov","Philip Coleman","Frederic Roskam"],"pdf_url":"https://arxiv.org/pdf/2402.17701v1.pdf","comment":"Accepted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2402.17700v1","updated":"2024-02-27T17:25:37Z","published":"2024-02-27T17:25:37Z","title":"RAVEL: Evaluating Interpretability Methods on Disentangling Language\n  Model Representations","summary":"  Individual neurons participate in the representation of multiple high-level\nconcepts. To what extent can different interpretability methods successfully\ndisentangle these roles? To help address this question, we introduce RAVEL\n(Resolving Attribute-Value Entanglements in Language Models), a dataset that\nenables tightly controlled, quantitative comparisons between a variety of\nexisting interpretability methods. We use the resulting conceptual framework to\ndefine the new method of Multi-task Distributed Alignment Search (MDAS), which\nallows us to find distributed representations satisfying multiple causal\ncriteria. With Llama2-7B as the target language model, MDAS achieves\nstate-of-the-art results on RAVEL, demonstrating the importance of going beyond\nneuron-level analyses to identify features distributed across activations. We\nrelease our benchmark at https://github.com/explanare/ravel.\n","authors":["Jing Huang","Zhengxuan Wu","Christopher Potts","Mor Geva","Atticus Geiger"],"pdf_url":"https://arxiv.org/pdf/2402.17700v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17699v1","updated":"2024-02-27T17:23:40Z","published":"2024-02-27T17:23:40Z","title":"Gradient-based Discrete Sampling with Automatic Cyclical Scheduling","summary":"  Discrete distributions, particularly in high-dimensional deep models, are\noften highly multimodal due to inherent discontinuities. While gradient-based\ndiscrete sampling has proven effective, it is susceptible to becoming trapped\nin local modes due to the gradient information. To tackle this challenge, we\npropose an automatic cyclical scheduling, designed for efficient and accurate\nsampling in multimodal discrete distributions. Our method contains three key\ncomponents: (1) a cyclical step size schedule where large steps discover new\nmodes and small steps exploit each mode; (2) a cyclical balancing schedule,\nensuring ``balanced\" proposals for given step sizes and high efficiency of the\nMarkov chain; and (3) an automatic tuning scheme for adjusting the\nhyperparameters in the cyclical schedules, allowing adaptability across diverse\ndatasets with minimal tuning. We prove the non-asymptotic convergence and\ninference guarantee for our method in general discrete distributions. Extensive\nexperiments demonstrate the superiority of our method in sampling complex\nmultimodal discrete distributions.\n","authors":["Patrick Pynadath","Riddhiman Bhattacharya","Arun Hariharan","Ruqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.17699v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17698v1","updated":"2024-02-27T17:21:10Z","published":"2024-02-27T17:21:10Z","title":"Learning reduced-order Quadratic-Linear models in Process Engineering\n  using Operator Inference","summary":"  In this work, we address the challenge of efficiently modeling dynamical\nsystems in process engineering. We use reduced-order model learning,\nspecifically operator inference. This is a non-intrusive, data-driven method\nfor learning dynamical systems from time-domain data. The application in our\nstudy is carbon dioxide methanation, an important reaction within the\nPower-to-X framework, to demonstrate its potential. The numerical results show\nthe ability of the reduced-order models constructed with operator inference to\nprovide a reduced yet accurate surrogate solution. This represents an important\nmilestone towards the implementation of fast and reliable digital twin\narchitectures.\n","authors":["Ion Victor Gosea","Luisa Peterson","Pawan Goyal","Jens Bremer","Kai Sundmacher","Peter Benner"],"pdf_url":"https://arxiv.org/pdf/2402.17698v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2310.09650v2","updated":"2024-02-27T17:16:59Z","published":"2023-10-14T19:43:06Z","title":"Multimodal Federated Learning in Healthcare: a Review","summary":"  Recent advancements in multimodal machine learning have empowered the\ndevelopment of accurate and robust AI systems in the medical domain, especially\nwithin centralized database systems. Simultaneously, Federated Learning (FL)\nhas progressed, providing a decentralized mechanism where data need not be\nconsolidated, thereby enhancing the privacy and security of sensitive\nhealthcare data. The integration of these two concepts supports the ongoing\nprogress of multimodal learning in healthcare while ensuring the security and\nprivacy of patient records within local data-holding agencies. This paper\noffers a concise overview of the significance of FL in healthcare and outlines\nthe current state-of-the-art approaches to Multimodal Federated Learning (MMFL)\nwithin the healthcare domain. It comprehensively examines the existing\nchallenges in the field, shedding light on the limitations of present models.\nFinally, the paper outlines potential directions for future advancements in the\nfield, aiming to bridge the gap between cutting-edge AI technology and the\nimperative need for patient data privacy in healthcare applications.\n","authors":["Jacob Thrasher","Alina Devkota","Prasiddha Siwakotai","Rohit Chivukula","Pranav Poudel","Chaunbo Hu","Binod Bhattarai","Prashnna Gyawali"],"pdf_url":"https://arxiv.org/pdf/2310.09650v2.pdf","comment":"28 pages, 5 figures"},{"id":"http://arxiv.org/abs/2402.11525v3","updated":"2024-02-27T17:12:38Z","published":"2024-02-18T09:51:49Z","title":"Advancing Translation Preference Modeling with RLHF: A Step Towards\n  Cost-Effective Solution","summary":"  Faithfulness, expressiveness, and elegance is the constant pursuit in machine\ntranslation. However, traditional metrics like \\textit{BLEU} do not strictly\nalign with human preference of translation quality. In this paper, we explore\nleveraging reinforcement learning with human feedback (\\textit{RLHF}) to\nimprove translation quality. It is non-trivial to collect a large high-quality\ndataset of human comparisons between translations, especially for low-resource\nlanguages. To address this issue, we propose a cost-effective preference\nlearning strategy, optimizing reward models by distinguishing between human and\nmachine translations. In this manner, the reward model learns the deficiencies\nof machine translation compared to human and guides subsequent improvements in\nmachine translation. Experimental results demonstrate that \\textit{RLHF} can\neffectively enhance translation quality and this improvement benefits other\ntranslation directions not trained with \\textit{RLHF}. Further analysis\nindicates that the model's language capabilities play a crucial role in\npreference learning. A reward model with strong language capabilities can more\nsensitively learn the subtle differences in translation quality and align\nbetter with real human translation preferences.\n","authors":["Nuo Xu","Jun Zhao","Can Zu","Sixian Li","Lu Chen","Zhihao Zhang","Rui Zheng","Shihan Dou","Wenjuan Qin","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2402.11525v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.00003v2","updated":"2024-02-27T17:12:18Z","published":"2023-05-25T18:22:12Z","title":"Detecting Heart Disease from Multi-View Ultrasound Images via Supervised\n  Attention Multiple Instance Learning","summary":"  Aortic stenosis (AS) is a degenerative valve condition that causes\nsubstantial morbidity and mortality. This condition is under-diagnosed and\nunder-treated. In clinical practice, AS is diagnosed with expert review of\ntransthoracic echocardiography, which produces dozens of ultrasound images of\nthe heart. Only some of these views show the aortic valve. To automate\nscreening for AS, deep networks must learn to mimic a human expert's ability to\nidentify views of the aortic valve then aggregate across these relevant images\nto produce a study-level diagnosis. We find previous approaches to AS detection\nyield insufficient accuracy due to relying on inflexible averages across\nimages. We further find that off-the-shelf attention-based multiple instance\nlearning (MIL) performs poorly. We contribute a new end-to-end MIL approach\nwith two key methodological innovations. First, a supervised attention\ntechnique guides the learned attention mechanism to favor relevant views.\nSecond, a novel self-supervised pretraining strategy applies contrastive\nlearning on the representation of the whole study instead of individual images\nas commonly done in prior literature. Experiments on an open-access dataset and\nan external validation set show that our approach yields higher accuracy while\nreducing model size.\n","authors":["Zhe Huang","Benjamin S. Wessler","Michael C. Hughes"],"pdf_url":"https://arxiv.org/pdf/2306.00003v2.pdf","comment":"Echocardiogram; multiple-instance learning; self-supervised learning;\n  semi-supervised learning; medical imaging"},{"id":"http://arxiv.org/abs/2402.17695v1","updated":"2024-02-27T17:11:35Z","published":"2024-02-27T17:11:35Z","title":"Geometric Deep Learning for Computer-Aided Design: A Survey","summary":"  Geometric Deep Learning techniques have become a transformative force in the\nfield of Computer-Aided Design (CAD), and have the potential to revolutionize\nhow designers and engineers approach and enhance the design process. By\nharnessing the power of machine learning-based methods, CAD designers can\noptimize their workflows, save time and effort while making better informed\ndecisions, and create designs that are both innovative and practical. The\nability to process the CAD designs represented by geometric data and to analyze\ntheir encoded features enables the identification of similarities among diverse\nCAD models, the proposition of alternative designs and enhancements, and even\nthe generation of novel design alternatives. This survey offers a comprehensive\noverview of learning-based methods in computer-aided design across various\ncategories, including similarity analysis and retrieval, 2D and 3D CAD model\nsynthesis, and CAD generation from point clouds. Additionally, it provides a\ncomplete list of benchmark datasets and their characteristics, along with\nopen-source codes that have propelled research in this domain. The final\ndiscussion delves into the challenges prevalent in this field, followed by\npotential future research directions in this rapidly evolving field.\n","authors":["Negar Heidari","Alexandros Iosifidis"],"pdf_url":"https://arxiv.org/pdf/2402.17695v1.pdf","comment":"26 pages, 14 figures, journal article"},{"id":"http://arxiv.org/abs/2104.07324v2","updated":"2024-02-27T17:07:34Z","published":"2021-04-15T09:23:32Z","title":"OneLog: Towards End-to-End Training in Software Log Anomaly Detection","summary":"  With the growth of online services, IoT devices, and DevOps-oriented software\ndevelopment, software log anomaly detection is becoming increasingly important.\nPrior works mainly follow a traditional four-staged architecture (Preprocessor,\nParser, Vectorizer, and Classifier). This paper proposes OneLog, which utilizes\na single Deep Neural Network (DNN) instead of multiple separate components.\nOneLog harnesses Convolutional Neural Networks (CNN) at the character level to\ntake digits, numbers, and punctuations, which were removed in prior works, into\naccount alongside the main natural language text. We evaluate our approach in\nsix message- and sequence-based data sets: HDFS, Hadoop, BGL, Thunderbird,\nSpirit, and Liberty. We experiment with Onelog with single-, multi-, and\ncross-project setups. Onelog offers state-of-the-art performance in our\ndatasets. Onelog can utilize multi-project datasets simultaneously during\ntraining, which suggests our model can generalize between datasets.\nMulti-project training also improves Onelog performance making it ideal when\nlimited training data is available for an individual project. We also found\nthat cross-project anomaly detection is possible with a single project pair\n(Liberty and Spirit). Analysis of model internals shows that one log has\nmultiple modes of detecting anomalies and that the model learns manually\nvalidated parsing rules for the log messages. We conclude that character-based\nCNNs are a promising approach toward end-to-end learning in log anomaly\ndetection. They offer good performance and generalization over multiple\ndatasets. We will make our scripts publicly available upon the acceptance of\nthis paper.\n","authors":["Shayan Hashemi","Mika Mäntylä"],"pdf_url":"https://arxiv.org/pdf/2104.07324v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17690v1","updated":"2024-02-27T17:07:18Z","published":"2024-02-27T17:07:18Z","title":"Autonomous Vehicles: Evolution of Artificial Intelligence and Learning\n  Algorithms","summary":"  The advent of autonomous vehicles has heralded a transformative era in\ntransportation, reshaping the landscape of mobility through cutting-edge\ntechnologies. Central to this evolu- tion is the integration of Artificial\nIntelligence (AI) and learning algorithms, propelling vehicles into realms of\nunprecedented autonomy. This paper provides a comprehensive exploration of the\nevolutionary trajectory of AI within autonomous vehicles, tracing the journey\nfrom foundational principles to the most recent advancements. Commencing with a\ncurrent landscape overview, the paper delves into the fundamental role of AI in\nshaping the autonomous decision-making capabilities of vehicles. It elucidates\nthe steps involved in the AI-powered development life cycle in vehicles,\naddressing ethical considerations and bias in AI-driven software development\nfor autonomous vehicles. The study presents statis- tical insights into the\nusage and types of AI/learning algorithms over the years, showcasing the\nevolving research landscape within the automotive industry. Furthermore, the\npaper highlights the pivotal role of parameters in refining algorithms for both\ntrucks and cars, facilitating vehicles to adapt, learn, and improve performance\nover time. It concludes by outlining different levels of autonomy, elucidating\nthe nuanced usage of AI and learning algorithms, and automating key tasks at\neach level. Additionally, the document discusses the variation in software\npackage sizes across different autonomy levels\n","authors":["Sneha Sudhir Shetiya","Divya Garikapati"],"pdf_url":"https://arxiv.org/pdf/2402.17690v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2402.17689v1","updated":"2024-02-27T17:05:41Z","published":"2024-02-27T17:05:41Z","title":"QoS prediction in radio vehicular environments via prior user\n  information","summary":"  Reliable wireless communications play an important role in the automotive\nindustry as it helps to enhance current use cases and enable new ones such as\nconnected autonomous driving, platooning, cooperative maneuvering, teleoperated\ndriving, and smart navigation. These and other use cases often rely on specific\nquality of service (QoS) levels for communication. Recently, the area of\npredictive quality of service (QoS) has received a great deal of attention as a\nkey enabler to forecast communication quality well enough in advance. However,\npredicting QoS in a reliable manner is a notoriously difficult task. In this\npaper, we evaluate ML tree-ensemble methods to predict QoS in the range of\nminutes with data collected from a cellular test network. We discuss radio\nenvironment characteristics and we showcase how these can be used to improve ML\nperformance and further support the uptake of ML in commercial networks.\nSpecifically, we use the correlations of the measurements coming from the radio\nenvironment by including information of prior vehicles to enhance the\nprediction of the target vehicles. Moreover, we are extending prior art by\nshowing how longer prediction horizons can be supported.\n","authors":["Noor Ul Ain","Rodrigo Hernangómez","Alexandros Palaios","Martin Kasparick","Sławomir Stańczak"],"pdf_url":"https://arxiv.org/pdf/2402.17689v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17686v1","updated":"2024-02-27T17:01:21Z","published":"2024-02-27T17:01:21Z","title":"Outlier-Detection for Reactive Machine Learned Potential Energy Surfaces","summary":"  Uncertainty quantification (UQ) to detect samples with large expected errors\n(outliers) is applied to reactive molecular potential energy surfaces (PESs).\nThree methods - Ensembles, Deep Evidential Regression (DER), and Gaussian\nMixture Models (GMM) - were applied to the H-transfer reaction between ${\\it\nsyn-}$Criegee and vinyl hydroxyperoxide. The results indicate that ensemble\nmodels provide the best results for detecting outliers, followed by GMM. For\nexample, from a pool of 1000 structures with the largest uncertainty, the\ndetection quality for outliers is $\\sim 90$ \\% and $\\sim 50$ \\%, respectively,\nif 25 or 1000 structures with large errors are sought. On the contrary, the\nlimitations of the statistical assumptions of DER greatly impacted its\nprediction capabilities. Finally, a structure-based indicator was found to be\ncorrelated with large average error, which may help to rapidly classify new\nstructures into those that provide an advantage for refining the neural\nnetwork.\n","authors":["Luis Itza Vazquez-Salazar","Silvan Käser","Markus Meuwly"],"pdf_url":"https://arxiv.org/pdf/2402.17686v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.01663v2","updated":"2024-02-27T16:58:26Z","published":"2024-01-14T12:09:40Z","title":"Killer Apps: Low-Speed, Large-Scale AI Weapons","summary":"  The accelerating advancements in Artificial Intelligence (AI) and Machine\nLearning (ML), highlighted by the development of cutting-edge Generative\nPre-trained Transformer (GPT) models by organizations such as OpenAI, Meta, and\nAnthropic, present new challenges and opportunities in warfare and security.\nMuch of the current focus is on AI's integration within weapons systems and its\nrole in rapid decision-making in kinetic conflict. However, an equally\nimportant but often overlooked aspect is the potential of AI-based\npsychological manipulation at internet scales within the information domain.\nThese capabilities could pose significant threats to individuals,\norganizations, and societies globally. This paper explores the concept of AI\nweapons, their deployment, detection, and potential countermeasures.\n","authors":["Philip Feldman","Aaron Dant","James R. Foulds"],"pdf_url":"https://arxiv.org/pdf/2402.01663v2.pdf","comment":"10 pages with 10 pages of appendices. 3 Figures, 2 code listings"},{"id":"http://arxiv.org/abs/2402.17671v1","updated":"2024-02-27T16:44:09Z","published":"2024-02-27T16:44:09Z","title":"Securing Reliability: A Brief Overview on Enhancing In-Context Learning\n  for Foundation Models","summary":"  As foundation models (FMs) continue to shape the landscape of AI, the\nin-context learning (ICL) paradigm thrives but also encounters issues such as\ntoxicity, hallucination, disparity, adversarial vulnerability, and\ninconsistency. Ensuring the reliability and responsibility of FMs is crucial\nfor the sustainable development of the AI ecosystem. In this concise overview,\nwe investigate recent advancements in enhancing the reliability and\ntrustworthiness of FMs within ICL frameworks, focusing on four key\nmethodologies, each with its corresponding subgoals. We sincerely hope this\npaper can provide valuable insights for researchers and practitioners\nendeavoring to build safe and dependable FMs and foster a stable and consistent\nICL environment, thereby unlocking their vast potential.\n","authors":["Yunpeng Huang","Yaonan Gu","Jingwei Xu","Zhihong Zhu","Zhaorun Chen","Xiaoxing Ma"],"pdf_url":"https://arxiv.org/pdf/2402.17671v1.pdf","comment":"18 pages, 15 figures"},{"id":"http://arxiv.org/abs/2311.18460v2","updated":"2024-02-27T16:39:49Z","published":"2023-11-30T11:11:26Z","title":"Causal Fairness under Unobserved Confounding: A Neural Sensitivity\n  Framework","summary":"  Fairness for machine learning predictions is widely required in practice for\nlegal, ethical, and societal reasons. Existing work typically focuses on\nsettings without unobserved confounding, even though unobserved confounding can\nlead to severe violations of causal fairness and, thus, unfair predictions. In\nthis work, we analyze the sensitivity of causal fairness to unobserved\nconfounding. Our contributions are three-fold. First, we derive bounds for\ncausal fairness metrics under different sources of unobserved confounding. This\nenables practitioners to examine the sensitivity of their machine learning\nmodels to unobserved confounding in fairness-critical applications. Second, we\npropose a novel neural framework for learning fair predictions, which allows us\nto offer worst-case guarantees of the extent to which causal fairness can be\nviolated due to unobserved confounding. Third, we demonstrate the effectiveness\nof our framework in a series of experiments, including a real-world case study\nabout predicting prison sentences. To the best of our knowledge, ours is the\nfirst work to study causal fairness under unobserved confounding. To this end,\nour work is of direct practical value as a refutation strategy to ensure the\nfairness of predictions in high-stakes applications.\n","authors":["Maresa Schröder","Dennis Frauen","Stefan Feuerriegel"],"pdf_url":"https://arxiv.org/pdf/2311.18460v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17666v1","updated":"2024-02-27T16:36:53Z","published":"2024-02-27T16:36:53Z","title":"Multi-Agent Deep Reinforcement Learning for Distributed Satellite\n  Routing","summary":"  This paper introduces a Multi-Agent Deep Reinforcement Learning (MA-DRL)\napproach for routing in Low Earth Orbit Satellite Constellations (LSatCs). Each\nsatellite is an independent decision-making agent with a partial knowledge of\nthe environment, and supported by feedback received from the nearby agents.\nBuilding on our previous work that introduced a Q-routing solution, the\ncontribution of this paper is to extend it to a deep learning framework able to\nquickly adapt to the network and traffic changes, and based on two phases: (1)\nAn offline exploration learning phase that relies on a global Deep Neural\nNetwork (DNN) to learn the optimal paths at each possible position and\ncongestion level; (2) An online exploitation phase with local, on-board,\npre-trained DNNs. Results show that MA-DRL efficiently learns optimal routes\noffline that are then loaded for an efficient distributed routing online.\n","authors":["Federico Lozano-Cuadra","Beatriz Soret"],"pdf_url":"https://arxiv.org/pdf/2402.17666v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03797v2","updated":"2024-02-27T16:35:56Z","published":"2024-01-08T10:27:25Z","title":"Anatomy of Neural Language Models","summary":"  The fields of generative AI and transfer learning have experienced remarkable\nadvancements in recent years especially in the domain of Natural Language\nProcessing (NLP). Transformers have been at the heart of these advancements\nwhere the cutting-edge transformer-based Language Models (LMs) have led to new\nstate-of-the-art results in a wide spectrum of applications. While the number\nof research works involving neural LMs is exponentially increasing, their vast\nmajority are high-level and far from self-contained. Consequently, a deep\nunderstanding of the literature in this area is a tough task especially in the\nabsence of a unified mathematical framework explaining the main types of neural\nLMs. We address the aforementioned problem in this tutorial where the objective\nis to explain neural LMs in a detailed, simplified and unambiguous mathematical\nframework accompanied by clear graphical illustrations. Concrete examples on\nwidely used models like BERT and GPT2 are explored. Finally, since transformers\npretrained on language-modeling-like tasks have been widely adopted in computer\nvision and time series applications, we briefly explore some examples of such\nsolutions in order to enable readers to understand how transformers work in the\naforementioned domains and compare this use with the original one in NLP.\n","authors":["Majd Saleh","Stéphane Paquelet"],"pdf_url":"https://arxiv.org/pdf/2401.03797v2.pdf","comment":"36 Pages; 25 Figures; some typos and notation errors are corrected in\n  this version"},{"id":"http://arxiv.org/abs/2402.17660v1","updated":"2024-02-27T16:27:06Z","published":"2024-02-27T16:27:06Z","title":"TorchMD-Net 2.0: Fast Neural Network Potentials for Molecular\n  Simulations","summary":"  Achieving a balance between computational speed, prediction accuracy, and\nuniversal applicability in molecular simulations has been a persistent\nchallenge. This paper presents substantial advancements in the TorchMD-Net\nsoftware, a pivotal step forward in the shift from conventional force fields to\nneural network-based potentials. The evolution of TorchMD-Net into a more\ncomprehensive and versatile framework is highlighted, incorporating\ncutting-edge architectures such as TensorNet. This transformation is achieved\nthrough a modular design approach, encouraging customized applications within\nthe scientific community. The most notable enhancement is a significant\nimprovement in computational efficiency, achieving a very remarkable\nacceleration in the computation of energy and forces for TensorNet models, with\nperformance gains ranging from 2-fold to 10-fold over previous iterations.\nOther enhancements include highly optimized neighbor search algorithms that\nsupport periodic boundary conditions and the smooth integration with existing\nmolecular dynamics frameworks. Additionally, the updated version introduces the\ncapability to integrate physical priors, further enriching its application\nspectrum and utility in research. The software is available at\nhttps://github.com/torchmd/torchmd-net.\n","authors":["Raul P. Pelaez","Guillem Simeon","Raimondas Galvelis","Antonio Mirarchi","Peter Eastman","Stefan Doerr","Philipp Thölke","Thomas E. Markland","Gianni De Fabritiis"],"pdf_url":"https://arxiv.org/pdf/2402.17660v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17655v1","updated":"2024-02-27T16:24:28Z","published":"2024-02-27T16:24:28Z","title":"Confidence-Aware Multi-Field Model Calibration","summary":"  Accurately predicting the probabilities of user feedback, such as clicks and\nconversions, is critical for ad ranking and bidding. However, there often exist\nunwanted mismatches between predicted probabilities and true likelihoods due to\nthe shift of data distributions and intrinsic model biases. Calibration aims to\naddress this issue by post-processing model predictions, and field-aware\ncalibration can adjust model output on different feature field values to\nsatisfy fine-grained advertising demands. Unfortunately, the observed samples\ncorresponding to certain field values can be too limited to make confident\ncalibrations, which may yield bias amplification and online disturbance. In\nthis paper, we propose a confidence-aware multi-field calibration method, which\nadaptively adjusts the calibration intensity based on the confidence levels\nderived from sample statistics. It also utilizes multiple feature fields for\njoint model calibration with awareness of their importance to mitigate the data\nsparsity effect of a single field. Extensive offline and online experiments\nshow the superiority of our method in boosting advertising performance and\nreducing prediction deviations.\n","authors":["Yuang Zhao","Chuhan Wu","Qinglin Jia","Hong Zhu","Jia Yan","Libin Zong","Linxuan Zhang","Zhenhua Dong","Muyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.17655v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.02066v3","updated":"2024-02-27T16:18:27Z","published":"2023-06-03T09:43:59Z","title":"Variational Gaussian Process Diffusion Processes","summary":"  Diffusion processes are a class of stochastic differential equations (SDEs)\nproviding a rich family of expressive models that arise naturally in dynamic\nmodelling tasks. Probabilistic inference and learning under generative models\nwith latent processes endowed with a non-linear diffusion process prior are\nintractable problems. We build upon work within variational inference,\napproximating the posterior process as a linear diffusion process, and point\nout pathologies in the approach. We propose an alternative parameterization of\nthe Gaussian variational process using a site-based exponential family\ndescription. This allows us to trade a slow inference algorithm with\nfixed-point iterations for a fast algorithm for convex optimization akin to\nnatural gradient descent, which also provides a better objective for learning\nmodel parameters.\n","authors":["Prakhar Verma","Vincent Adam","Arno Solin"],"pdf_url":"https://arxiv.org/pdf/2306.02066v3.pdf","comment":"International Conference on Artificial Intelligence and Statistics\n  (AISTATS) 2024"},{"id":"http://arxiv.org/abs/2402.17641v1","updated":"2024-02-27T16:11:05Z","published":"2024-02-27T16:11:05Z","title":"Variational Learning is Effective for Large Deep Networks","summary":"  We give extensive empirical evidence against the common belief that\nvariational learning is ineffective for large neural networks. We show that an\noptimizer called Improved Variational Online Newton (IVON) consistently matches\nor outperforms Adam for training large networks such as GPT-2 and ResNets from\nscratch. IVON's computational costs are nearly identical to Adam but its\npredictive uncertainty is better. We show several new use cases of IVON where\nwe improve fine-tuning and model merging in Large Language Models, accurately\npredict generalization error, and faithfully estimate sensitivity to data. We\nfind overwhelming evidence in support of effectiveness of variational learning.\n","authors":["Yuesong Shen","Nico Daheim","Bai Cong","Peter Nickl","Gian Maria Marconi","Clement Bazan","Rio Yokota","Iryna Gurevych","Daniel Cremers","Mohammad Emtiyaz Khan","Thomas Möllenhoff"],"pdf_url":"https://arxiv.org/pdf/2402.17641v1.pdf","comment":"The first two authors contributed equally. Code is available here:\n  https://github.com/team-approx-bayes/ivon"},{"id":"http://arxiv.org/abs/2103.09603v5","updated":"2024-02-27T16:10:16Z","published":"2021-03-17T12:42:41Z","title":"DoubleML -- An Object-Oriented Implementation of Double Machine Learning\n  in R","summary":"  The R package DoubleML implements the double/debiased machine learning\nframework of Chernozhukov et al. (2018). It provides functionalities to\nestimate parameters in causal models based on machine learning methods. The\ndouble machine learning framework consist of three key ingredients: Neyman\northogonality, high-quality machine learning estimation and sample splitting.\nEstimation of nuisance components can be performed by various state-of-the-art\nmachine learning methods that are available in the mlr3 ecosystem. DoubleML\nmakes it possible to perform inference in a variety of causal models, including\npartially linear and interactive regression models and their extensions to\ninstrumental variable estimation. The object-oriented implementation of\nDoubleML enables a high flexibility for the model specification and makes it\neasily extendable. This paper serves as an introduction to the double machine\nlearning framework and the R package DoubleML. In reproducible code examples\nwith simulated and real data sets, we demonstrate how DoubleML users can\nperform valid inference based on machine learning methods.\n","authors":["Philipp Bach","Victor Chernozhukov","Malte S. Kurz","Martin Spindler","Sven Klaassen"],"pdf_url":"https://arxiv.org/pdf/2103.09603v5.pdf","comment":"56 pages, 8 Figures, 1 Table; Updated version for DoubleML 1.0.0"},{"id":"http://arxiv.org/abs/2402.17621v1","updated":"2024-02-27T15:49:26Z","published":"2024-02-27T15:49:26Z","title":"Supervised machine learning for microbiomics: bridging the gap between\n  current and best practices","summary":"  Machine learning (ML) is set to accelerate innovations in clinical\nmicrobiomics, such as in disease diagnostics and prognostics. This will require\nhigh-quality, reproducible, interpretable workflows whose predictive\ncapabilities meet or exceed the high thresholds set for clinical tools by\nregulatory agencies. Here, we capture a snapshot of current practices in the\napplication of supervised ML to microbiomics data, through an in-depth analysis\nof 100 peer-reviewed journal articles published in 2021-2022. We apply a\ndata-driven approach to steer discussion of the merits of varied approaches to\nexperimental design, including key considerations such as how to mitigate the\neffects of small dataset size while avoiding data leakage. We further provide\nguidance on how to avoid common experimental design pitfalls that can hurt\nmodel performance, trustworthiness, and reproducibility. Discussion is\naccompanied by an interactive online tutorial that demonstrates foundational\nprinciples of ML experimental design, tailored to the microbiomics community.\nFormalizing community best practices for supervised ML in microbiomics is an\nimportant step towards improving the success and efficiency of clinical\nresearch, to the benefit of patients and other stakeholders.\n","authors":["Natasha K. Dudek","Mariam Chakhvadze","Saba Kobakhidze","Omar Kantidze","Yuriy Gankin"],"pdf_url":"https://arxiv.org/pdf/2402.17621v1.pdf","comment":"25 pages, 5 figures"},{"id":"http://arxiv.org/abs/2210.15658v3","updated":"2024-02-27T15:43:00Z","published":"2022-10-27T17:58:43Z","title":"All the Feels: A dexterous hand with large-area tactile sensing","summary":"  High cost and lack of reliability has precluded the widespread adoption of\ndexterous hands in robotics. Furthermore, the lack of a viable tactile sensor\ncapable of sensing over the entire area of the hand impedes the rich, low-level\nfeedback that would improve learning of dexterous manipulation skills. This\npaper introduces an inexpensive, modular, robust, and scalable platform -- the\nDManus -- aimed at resolving these challenges while satisfying the large-scale\ndata collection capabilities demanded by deep robot learning paradigms. Studies\non human manipulation point to the criticality of low-level tactile feedback in\nperforming everyday dexterous tasks. The DManus comes with ReSkin sensing on\nthe entire surface of the palm as well as the fingertips. We demonstrate\neffectiveness of the fully integrated system in a tactile aware task -- bin\npicking and sorting. Code, documentation, design files, detailed assembly\ninstructions, trained models, task videos, and all supplementary materials\nrequired to recreate the setup can be found on\nhttps://sites.google.com/view/roboticsbenchmarks/platforms/dmanus.\n","authors":["Raunaq Bhirangi","Abigail DeFranco","Jacob Adkins","Carmel Majidi","Abhinav Gupta","Tess Hellebrekers","Vikash Kumar"],"pdf_url":"https://arxiv.org/pdf/2210.15658v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06823v3","updated":"2024-02-27T15:33:52Z","published":"2023-10-10T17:53:36Z","title":"NECO: NEural Collapse Based Out-of-distribution detection","summary":"  Detecting out-of-distribution (OOD) data is a critical challenge in machine\nlearning due to model overconfidence, often without awareness of their\nepistemological limits. We hypothesize that ``neural collapse'', a phenomenon\naffecting in-distribution data for models trained beyond loss convergence, also\ninfluences OOD data. To benefit from this interplay, we introduce NECO, a novel\npost-hoc method for OOD detection, which leverages the geometric properties of\n``neural collapse'' and of principal component spaces to identify OOD data. Our\nextensive experiments demonstrate that NECO achieves state-of-the-art results\non both small and large-scale OOD detection tasks while exhibiting strong\ngeneralization capabilities across different network architectures.\nFurthermore, we provide a theoretical explanation for the effectiveness of our\nmethod in OOD detection. Code is available at https://gitlab.com/drti/neco\n","authors":["Mouïn Ben Ammar","Nacim Belkhir","Sebastian Popescu","Antoine Manzanera","Gianni Franchi"],"pdf_url":"https://arxiv.org/pdf/2310.06823v3.pdf","comment":"Accepted to ICLR2024"},{"id":"http://arxiv.org/abs/2402.17606v1","updated":"2024-02-27T15:33:20Z","published":"2024-02-27T15:33:20Z","title":"Learning Topological Representations with Bidirectional Graph Attention\n  Network for Solving Job Shop Scheduling Problem","summary":"  Existing learning-based methods for solving job shop scheduling problem\n(JSSP) usually use off-the-shelf GNN models tailored to undirected graphs and\nneglect the rich and meaningful topological structures of disjunctive graphs\n(DGs). This paper proposes the topology-aware bidirectional graph attention\nnetwork (TBGAT), a novel GNN architecture based on the attention mechanism, to\nembed the DG for solving JSSP in a local search framework. Specifically, TBGAT\nembeds the DG from a forward and a backward view, respectively, where the\nmessages are propagated by following the different topologies of the views and\naggregated via graph attention. Then, we propose a novel operator based on the\nmessage-passing mechanism to calculate the forward and backward topological\nsorts of the DG, which are the features for characterizing the topological\nstructures and exploited by our model. In addition, we theoretically and\nexperimentally show that TBGAT has linear computational complexity to the\nnumber of jobs and machines, respectively, which strengthens the practical\nvalue of our method. Besides, extensive experiments on five synthetic datasets\nand seven classic benchmarks show that TBGAT achieves new SOTA results by\noutperforming a wide range of neural methods by a large margin.\n","authors":["Cong Zhang","Zhiguang Cao","Yaoxin Wu","Wen Song","Jing Sun"],"pdf_url":"https://arxiv.org/pdf/2402.17606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17601v1","updated":"2024-02-27T15:30:01Z","published":"2024-02-27T15:30:01Z","title":"Advancing sleep detection by modelling weak label sets: A novel weakly\n  supervised learning approach","summary":"  Understanding sleep and activity patterns plays a crucial role in physical\nand mental health. This study introduces a novel approach for sleep detection\nusing weakly supervised learning for scenarios where reliable ground truth\nlabels are unavailable. The proposed method relies on a set of weak labels,\nderived from the predictions generated by conventional sleep detection\nalgorithms. Introducing a novel approach, we suggest a novel generalised\nnon-linear statistical model in which the number of weak sleep labels is\nmodelled as outcome of a binomial distribution. The probability of sleep in the\nbinomial distribution is linked to the outcomes of neural networks trained to\ndetect sleep based on actigraphy. We show that maximizing the likelihood\nfunction of the model, is equivalent to minimizing the soft cross-entropy loss.\nAdditionally, we explored the use of the Brier score as a loss function for\nweak labels. The efficacy of the suggested modelling framework was demonstrated\nusing the Multi-Ethnic Study of Atherosclerosis dataset. A \\gls{lstm} trained\non the soft cross-entropy outperformed conventional sleep detection algorithms,\nother neural network architectures and loss functions in accuracy and model\ncalibration. This research not only advances sleep detection techniques in\nscenarios where ground truth data is scarce but also contributes to the broader\nfield of weakly supervised learning by introducing innovative approach in\nmodelling sets of weak labels.\n","authors":["Matthias Boeker","Vajira Thambawita","Michael Riegler","Pål Halvorsen","Hugo L. Hammer"],"pdf_url":"https://arxiv.org/pdf/2402.17601v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17595v1","updated":"2024-02-27T15:28:01Z","published":"2024-02-27T15:28:01Z","title":"Implicit Regularization via Spectral Neural Networks and Non-linear\n  Matrix Sensing","summary":"  The phenomenon of implicit regularization has attracted interest in recent\nyears as a fundamental aspect of the remarkable generalizing ability of neural\nnetworks. In a nutshell, it entails that gradient descent dynamics in many\nneural nets, even without any explicit regularizer in the loss function,\nconverges to the solution of a regularized learning problem. However, known\nresults attempting to theoretically explain this phenomenon focus\noverwhelmingly on the setting of linear neural nets, and the simplicity of the\nlinear structure is particularly crucial to existing arguments. In this paper,\nwe explore this problem in the context of more realistic neural networks with a\ngeneral class of non-linear activation functions, and rigorously demonstrate\nthe implicit regularization phenomenon for such networks in the setting of\nmatrix sensing problems, together with rigorous rate guarantees that ensure\nexponentially fast convergence of gradient descent.In this vein, we contribute\na network architecture called Spectral Neural Networks (abbrv. SNN) that is\nparticularly suitable for matrix learning problems. Conceptually, this entails\ncoordinatizing the space of matrices by their singular values and singular\nvectors, as opposed to by their entries, a potentially fruitful perspective for\nmatrix learning. We demonstrate that the SNN architecture is inherently much\nmore amenable to theoretical analysis than vanilla neural nets and confirm its\neffectiveness in the context of matrix sensing, via both mathematical\nguarantees and empirical investigations. We believe that the SNN architecture\nhas the potential to be of wide applicability in a broad class of matrix\nlearning scenarios.\n","authors":["Hong T. M. Chu","Subhro Ghosh","Chi Thanh Lam","Soumendu Sundar Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2402.17595v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16819v2","updated":"2024-02-27T15:22:57Z","published":"2024-02-26T18:43:45Z","title":"Nemotron-4 15B Technical Report","summary":"  We introduce Nemotron-4 15B, a 15-billion-parameter large multilingual\nlanguage model trained on 8 trillion text tokens. Nemotron-4 15B demonstrates\nstrong performance when assessed on English, multilingual, and coding tasks: it\noutperforms all existing similarly-sized open models on 4 out of 7 downstream\nevaluation areas and achieves competitive performance to the leading open\nmodels in the remaining ones. Specifically, Nemotron-4 15B exhibits the best\nmultilingual capabilities of all similarly-sized models, even outperforming\nmodels over four times larger and those explicitly specialized for multilingual\ntasks.\n","authors":["Jupinder Parmar","Shrimai Prabhumoye","Joseph Jennings","Mostofa Patwary","Sandeep Subramanian","Dan Su","Chen Zhu","Deepak Narayanan","Aastha Jhunjhunwala","Ayush Dattagupta","Vibhu Jawa","Jiwei Liu","Ameya Mahabaleshwarkar","Osvald Nitski","Annika Brundyn","James Maki","Miguel Martinez","Jiaxuan You","John Kamalu","Patrick LeGresley","Denys Fridman","Jared Casper","Ashwath Aithal","Oleksii Kuchaiev","Mohammad Shoeybi","Jonathan Cohen","Bryan Catanzaro"],"pdf_url":"https://arxiv.org/pdf/2402.16819v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2002.01444v6","updated":"2024-02-27T15:15:10Z","published":"2020-02-04T18:08:49Z","title":"Learning of Linear Dynamical Systems as a Non-Commutative Polynomial\n  Optimization Problem","summary":"  There has been much recent progress in forecasting the next observation of a\nlinear dynamical system (LDS), which is known as the improper learning, as well\nas in the estimation of its system matrices, which is known as the proper\nlearning of LDS. We present an approach to proper learning of LDS, which in\nspite of the non-convexity of the problem, guarantees global convergence of\nnumerical solutions to a least-squares estimator. We present promising\ncomputational results.\n","authors":["Quan Zhou","Jakub Marecek"],"pdf_url":"https://arxiv.org/pdf/2002.01444v6.pdf","comment":"14 pages, 4 figures; retitled to reflect the title of the the\n  published version"},{"id":"http://arxiv.org/abs/2402.17583v1","updated":"2024-02-27T15:14:19Z","published":"2024-02-27T15:14:19Z","title":"FaultProfIT: Hierarchical Fault Profiling of Incident Tickets in\n  Large-scale Cloud Systems","summary":"  Postmortem analysis is essential in the management of incidents within cloud\nsystems, which provides valuable insights to improve system's reliability and\nrobustness. At CloudA, fault pattern profiling is performed during the\npostmortem phase, which involves the classification of incidents' faults into\nunique categories, referred to as fault pattern. By aggregating and analyzing\nthese fault patterns, engineers can discern common faults, vulnerable\ncomponents and emerging fault trends. However, this process is currently\nconducted by manual labeling, which has inherent drawbacks. On the one hand,\nthe sheer volume of incidents means only the most severe ones are analyzed,\ncausing a skewed overview of fault patterns. On the other hand, the complexity\nof the task demands extensive domain knowledge, which leads to errors and\ninconsistencies. To address these limitations, we propose an automated\napproach, named FaultProfIT, for Fault pattern Profiling of Incident Tickets.\nIt leverages hierarchy-guided contrastive learning to train a hierarchy-aware\nincident encoder and predicts fault patterns with enhanced incident\nrepresentations. We evaluate FaultProfIT using the production incidents from\nCloudA. The results demonstrate that FaultProfIT outperforms state-of-the-art\nmethods. Our ablation study and analysis also verify the effectiveness of\nhierarchy-guided contrastive learning. Additionally, we have deployed\nFaultProfIT at CloudA for six months. To date, FaultProfIT has analyzed 10,000+\nincidents from 30+ cloud services, successfully revealing several fault trends\nthat have informed system improvements.\n","authors":["Junjie Huang","Jinyang Liu","Zhuangbin Chen","Zhihan Jiang","Yichen LI","Jiazhen Gu","Cong Feng","Zengyin Yang","Yongqiang Yang","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2402.17583v1.pdf","comment":"Accepted by Proceedings of the 46th International Conference on\n  Software Engineering: Software Engineering in Practice (ICSE SEIP 2024)"},{"id":"http://arxiv.org/abs/2402.17572v1","updated":"2024-02-27T15:09:20Z","published":"2024-02-27T15:09:20Z","title":"Hyperdimensional computing: a fast, robust and interpretable paradigm\n  for biological data","summary":"  Advances in bioinformatics are primarily due to new algorithms for processing\ndiverse biological data sources. While sophisticated alignment algorithms have\nbeen pivotal in analyzing biological sequences, deep learning has substantially\ntransformed bioinformatics, addressing sequence, structure, and functional\nanalyses. However, these methods are incredibly data-hungry, compute-intensive\nand hard to interpret. Hyperdimensional computing (HDC) has recently emerged as\nan intriguing alternative. The key idea is that random vectors of high\ndimensionality can represent concepts such as sequence identity or phylogeny.\nThese vectors can then be combined using simple operators for learning,\nreasoning or querying by exploiting the peculiar properties of high-dimensional\nspaces. Our work reviews and explores the potential of HDC for bioinformatics,\nemphasizing its efficiency, interpretability, and adeptness in handling\nmultimodal and structured data. HDC holds a lot of potential for various omics\ndata searching, biosignal analysis and health applications.\n","authors":["Michiel Stock","Dimitri Boeckaerts","Pieter Dewulf","Steff Taelman","Maxime Van Haeverbeke","Wim Van Criekinge","Bernard De Baets"],"pdf_url":"https://arxiv.org/pdf/2402.17572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17570v1","updated":"2024-02-27T15:08:57Z","published":"2024-02-27T15:08:57Z","title":"Sparse Variational Contaminated Noise Gaussian Process Regression for\n  Forecasting Geomagnetic Perturbations","summary":"  Gaussian Processes (GP) have become popular machine learning methods for\nkernel based learning on datasets with complicated covariance structures. In\nthis paper, we present a novel extension to the GP framework using a\ncontaminated normal likelihood function to better account for heteroscedastic\nvariance and outlier noise. We propose a scalable inference algorithm based on\nthe Sparse Variational Gaussian Process (SVGP) method for fitting sparse\nGaussian process regression models with contaminated normal noise on large\ndatasets. We examine an application to geomagnetic ground perturbations, where\nthe state-of-art prediction model is based on neural networks. We show that our\napproach yields shorter predictions intervals for similar coverage and accuracy\nwhen compared to an artificial dense neural network baseline.\n","authors":["Daniel Iong","Matthew McAnear","Yuezhou Qu","Shasha Zou","Gabor Toth Yang Chen"],"pdf_url":"https://arxiv.org/pdf/2402.17570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11996v2","updated":"2024-02-27T14:53:53Z","published":"2024-02-19T09:41:57Z","title":"ISCUTE: Instance Segmentation of Cables Using Text Embedding","summary":"  In the field of robotics and automation, conventional object recognition and\ninstance segmentation methods face a formidable challenge when it comes to\nperceiving Deformable Linear Objects (DLOs) like wires, cables, and flexible\ntubes. This challenge arises primarily from the lack of distinct attributes\nsuch as shape, color, and texture, which calls for tailored solutions to\nachieve precise identification. In this work, we propose a foundation\nmodel-based DLO instance segmentation technique that is text-promptable and\nuser-friendly. Specifically, our approach combines the text-conditioned\nsemantic segmentation capabilities of CLIPSeg model with the zero-shot\ngeneralization capabilities of Segment Anything Model (SAM). We show that our\nmethod exceeds SOTA performance on DLO instance segmentation, achieving a mIoU\nof $91.21\\%$. We also introduce a rich and diverse DLO-specific dataset for\ninstance segmentation.\n","authors":["Shir Kozlovsky","Omkar Joglekar","Dotan Di Castro"],"pdf_url":"https://arxiv.org/pdf/2402.11996v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17554v1","updated":"2024-02-27T14:48:07Z","published":"2024-02-27T14:48:07Z","title":"Evaluation of Predictive Reliability to Foster Trust in Artificial\n  Intelligence. A case study in Multiple Sclerosis","summary":"  Applying Artificial Intelligence (AI) and Machine Learning (ML) in critical\ncontexts, such as medicine, requires the implementation of safety measures to\nreduce risks of harm in case of prediction errors. Spotting ML failures is of\nparamount importance when ML predictions are used to drive clinical decisions.\nML predictive reliability measures the degree of trust of a ML prediction on a\nnew instance, thus allowing decision-makers to accept or reject it based on its\nreliability. To assess reliability, we propose a method that implements two\nprinciples. First, our approach evaluates whether an instance to be classified\nis coming from the same distribution of the training set. To do this, we\nleverage Autoencoders (AEs) ability to reconstruct the training set with low\nerror. An instance is considered Out-of-Distribution (OOD) if the AE\nreconstructs it with a high error. Second, it is evaluated whether the ML\nclassifier has good performances on samples similar to the newly classified\ninstance by using a proxy model. We show that this approach is able to assess\nreliability both in a simulated scenario and on a model trained to predict\ndisease progression of Multiple Sclerosis patients. We also developed a Python\npackage, named relAI, to embed reliability measures into ML pipelines. We\npropose a simple approach that can be used in the deployment phase of any ML\nmodel to suggest whether to trust predictions or not. Our method holds the\npromise to provide effective support to clinicians by spotting potential ML\nfailures during deployment.\n","authors":["Lorenzo Peracchio","Giovanna Nicora","Enea Parimbelli","Tommaso Mario Buonocore","Roberto Bergamaschi","Eleonora Tavazzi","Arianna Dagliati","Riccardo Bellazzi"],"pdf_url":"https://arxiv.org/pdf/2402.17554v1.pdf","comment":"20 pages, 7 figures"},{"id":"http://arxiv.org/abs/2402.17544v1","updated":"2024-02-27T14:34:14Z","published":"2024-02-27T14:34:14Z","title":"Adapting Learned Image Codecs to Screen Content via Adjustable\n  Transformations","summary":"  As learned image codecs (LICs) become more prevalent, their low coding\nefficiency for out-of-distribution data becomes a bottleneck for some\napplications. To improve the performance of LICs for screen content (SC) images\nwithout breaking backwards compatibility, we propose to introduce parameterized\nand invertible linear transformations into the coding pipeline without changing\nthe underlying baseline codec's operation flow. We design two neural networks\nto act as prefilters and postfilters in our setup to increase the coding\nefficiency and help with the recovery from coding artifacts. Our end-to-end\ntrained solution achieves up to 10% bitrate savings on SC compression compared\nto the baseline LICs while introducing only 1% extra parameters.\n","authors":["H. Burak Dogaroglu","A. Burakhan Koyuncu","Atanas Boev","Elena Alshina","Eckehard Steinbach"],"pdf_url":"https://arxiv.org/pdf/2402.17544v1.pdf","comment":"7 pages, 6 figures, 2 tables"},{"id":"http://arxiv.org/abs/2310.02174v3","updated":"2024-02-27T14:17:53Z","published":"2023-10-03T16:08:41Z","title":"Ask Again, Then Fail: Large Language Models' Vacillations in Judgement","summary":"  With the emergence of generative conversational large language models (LLMs)\nlike ChatGPT, serving as virtual assistants in various fields, the stability\nand reliability of their responses have become crucial. However, during usage,\nit has been observed that these models tend to waver in their judgements when\nconfronted with follow-up questions from users expressing skepticism or\ndisagreement. In this work, we draw inspiration from questioning strategies in\neducation and propose a \\textsc{Follow-up Questioning Mechanism} along with two\nevaluation metrics to assess the judgement consistency of LLMs before and after\nexposure to disturbances. We evaluate the judgement consistency of ChatGPT,\nPaLM2-Bison, and Vicuna-13B under this mechanism across eight reasoning\nbenchmarks. Empirical results show that even when the initial answers are\ncorrect, judgement consistency sharply decreases when LLMs face disturbances\nsuch as questioning, negation, or misleading. Additionally, we study these\nmodels' judgement consistency under various settings (sampling temperature and\nprompts) to validate this issue further, observing the impact of prompt tone\nand conducting an in-depth error analysis for deeper behavioral insights.\nFurthermore, we also explore several prompting methods to mitigate this issue\nand demonstrate their\neffectiveness\\footnote{\\url{https://github.com/NUSTM/LLMs-Waver-In-Judgements}}.\n","authors":["Qiming Xie","Zengzhi Wang","Yi Feng","Rui Xia"],"pdf_url":"https://arxiv.org/pdf/2310.02174v3.pdf","comment":"Update mitigation results of fine-tuning the model on synthesized\n  high-quality preference data with DPO algorithm"},{"id":"http://arxiv.org/abs/2310.14901v2","updated":"2024-02-27T14:13:44Z","published":"2023-10-23T13:11:30Z","title":"Series of Hessian-Vector Products for Tractable Saddle-Free Newton\n  Optimisation of Neural Networks","summary":"  Despite their popularity in the field of continuous optimisation,\nsecond-order quasi-Newton methods are challenging to apply in machine learning,\nas the Hessian matrix is intractably large. This computational burden is\nexacerbated by the need to address non-convexity, for instance by modifying the\nHessian's eigenvalues as in Saddle-Free Newton methods. We propose an\noptimisation algorithm which addresses both of these concerns - to our\nknowledge, the first efficiently-scalable optimisation algorithm to\nasymptotically use the exact inverse Hessian with absolute-value eigenvalues.\nOur method frames the problem as a series which principally square-roots and\ninverts the squared Hessian, then uses it to precondition a gradient vector,\nall without explicitly computing or eigendecomposing the Hessian. A truncation\nof this infinite series provides a new optimisation algorithm which is scalable\nand comparable to other first- and second-order optimisation methods in both\nruntime and optimisation performance. We demonstrate this in a variety of\nsettings, including a ResNet-18 trained on CIFAR-10.\n","authors":["Elre T. Oldewage","Ross M. Clarke","José Miguel Hernández-Lobato"],"pdf_url":"https://arxiv.org/pdf/2310.14901v2.pdf","comment":"37 pages, 10 figures, 5 tables. To appear in TMLR. First two authors'\n  order randomised"},{"id":"http://arxiv.org/abs/2306.14287v2","updated":"2024-02-27T14:01:23Z","published":"2023-06-25T16:29:51Z","title":"Efficient Contextformer: Spatio-Channel Window Attention for Fast\n  Context Modeling in Learned Image Compression","summary":"  Entropy estimation is essential for the performance of learned image\ncompression. It has been demonstrated that a transformer-based entropy model is\nof critical importance for achieving a high compression ratio, however, at the\nexpense of a significant computational effort. In this work, we introduce the\nEfficient Contextformer (eContextformer) - a computationally efficient\ntransformer-based autoregressive context model for learned image compression.\nThe eContextformer efficiently fuses the patch-wise, checkered, and\nchannel-wise grouping techniques for parallel context modeling, and introduces\na shifted window spatio-channel attention mechanism. We explore better training\nstrategies and architectural designs and introduce additional complexity\noptimizations. During decoding, the proposed optimization techniques\ndynamically scale the attention span and cache the previous attention\ncomputations, drastically reducing the model and runtime complexity. Compared\nto the non-parallel approach, our proposal has ~145x lower model complexity and\n~210x faster decoding speed, and achieves higher average bit savings on Kodak,\nCLIC2020, and Tecnick datasets. Additionally, the low complexity of our context\nmodel enables online rate-distortion algorithms, which further improve the\ncompression performance. We achieve up to 17% bitrate savings over the intra\ncoding of Versatile Video Coding (VVC) Test Model (VTM) 16.2 and surpass\nvarious learning-based compression models.\n","authors":["A. Burakhan Koyuncu","Panqi Jia","Atanas Boev","Elena Alshina","Eckehard Steinbach"],"pdf_url":"https://arxiv.org/pdf/2306.14287v2.pdf","comment":"Accepted for IEEE TCSVT (14 pages, 10 figures, 9 tables)"},{"id":"http://arxiv.org/abs/2402.17517v1","updated":"2024-02-27T14:00:34Z","published":"2024-02-27T14:00:34Z","title":"Label-Noise Robust Diffusion Models","summary":"  Conditional diffusion models have shown remarkable performance in various\ngenerative tasks, but training them requires large-scale datasets that often\ncontain noise in conditional inputs, a.k.a. noisy labels. This noise leads to\ncondition mismatch and quality degradation of generated data. This paper\nproposes Transition-aware weighted Denoising Score Matching (TDSM) for training\nconditional diffusion models with noisy labels, which is the first study in the\nline of diffusion models. The TDSM objective contains a weighted sum of score\nnetworks, incorporating instance-wise and time-dependent label transition\nprobabilities. We introduce a transition-aware weight estimator, which\nleverages a time-dependent noisy-label classifier distinctively customized to\nthe diffusion process. Through experiments across various datasets and noisy\nlabel settings, TDSM improves the quality of generated samples aligned with\ngiven conditions. Furthermore, our method improves generation performance even\non prevalent benchmark datasets, which implies the potential noisy labels and\ntheir risk of generative model learning. Finally, we show the improved\nperformance of TDSM on top of conventional noisy label corrections, which\nempirically proving its contribution as a part of label-noise robust generative\nmodels. Our code is available at: https://github.com/byeonghu-na/tdsm.\n","authors":["Byeonghu Na","Yeongmin Kim","HeeSun Bae","Jung Hyun Lee","Se Jung Kwon","Wanmo Kang","Il-Chul Moon"],"pdf_url":"https://arxiv.org/pdf/2402.17517v1.pdf","comment":"Accepted at ICLR 2024"},{"id":"http://arxiv.org/abs/2402.17516v1","updated":"2024-02-27T14:00:08Z","published":"2024-02-27T14:00:08Z","title":"QUCE: The Minimisation and Quantification of Path-Based Uncertainty for\n  Generative Counterfactual Explanations","summary":"  Deep Neural Networks (DNNs) stand out as one of the most prominent approaches\nwithin the Machine Learning (ML) domain. The efficacy of DNNs has surged\nalongside recent increases in computational capacity, allowing these approaches\nto scale to significant complexities for addressing predictive challenges in\nbig data. However, as the complexity of DNN models rises, interpretability\ndiminishes. In response to this challenge, explainable models such as\nAdversarial Gradient Integration (AGI) leverage path-based gradients provided\nby DNNs to elucidate their decisions. Yet the performance of path-based\nexplainers can be compromised when gradients exhibit irregularities during\nout-of-distribution path traversal. In this context, we introduce Quantified\nUncertainty Counterfactual Explanations (QUCE), a method designed to mitigate\nout-of-distribution traversal by minimizing path uncertainty. QUCE not only\nquantifies uncertainty when presenting explanations but also generates more\ncertain counterfactual examples. We showcase the performance of the QUCE method\nby comparing it with competing methods for both path-based explanations and\ngenerative counterfactual examples. The code repository for the QUCE method is\navailable at: https://github.com/jamie-duell/QUCE.\n","authors":["Jamie Duell","Hsuan Fu","Monika Seisenberger","Xiuyi Fan"],"pdf_url":"https://arxiv.org/pdf/2402.17516v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16569v2","updated":"2024-02-27T13:59:32Z","published":"2024-02-26T13:47:32Z","title":"Pretrained Visual Uncertainties","summary":"  Accurate uncertainty estimation is vital to trustworthy machine learning, yet\nuncertainties typically have to be learned for each task anew. This work\nintroduces the first pretrained uncertainty modules for vision models. Similar\nto standard pretraining this enables the zero-shot transfer of uncertainties\nlearned on a large pretraining dataset to specialized downstream datasets. We\nenable our large-scale pretraining on ImageNet-21k by solving a gradient\nconflict in previous uncertainty modules and accelerating the training by up to\n180x. We find that the pretrained uncertainties generalize to unseen datasets.\nIn scrutinizing the learned uncertainties, we find that they capture aleatoric\nuncertainty, disentangled from epistemic components. We demonstrate that this\nenables safe retrieval and uncertainty-aware dataset visualization. To\nencourage applications to further problems and domains, we release all\npretrained checkpoints and code under https://github.com/mkirchhof/url .\n","authors":["Michael Kirchhof","Mark Collier","Seong Joon Oh","Enkelejda Kasneci"],"pdf_url":"https://arxiv.org/pdf/2402.16569v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17506v1","updated":"2024-02-27T13:46:45Z","published":"2024-02-27T13:46:45Z","title":"Thermodynamics-informed super-resolution of scarce temporal dynamics\n  data","summary":"  We present a method to increase the resolution of measurements of a physical\nsystem and subsequently predict its time evolution using thermodynamics-aware\nneural networks. Our method uses adversarial autoencoders, which reduce the\ndimensionality of the full order model to a set of latent variables that are\nenforced to match a prior, for example a normal distribution. Adversarial\nautoencoders are seen as generative models, and they can be trained to generate\nhigh-resolution samples from low-resoution inputs, meaning they can address the\nso-called super-resolution problem. Then, a second neural network is trained to\nlearn the physical structure of the latent variables and predict their temporal\nevolution. This neural network is known as an structure-preserving neural\nnetwork. It learns the metriplectic-structure of the system and applies a\nphysical bias to ensure that the first and second principles of thermodynamics\nare fulfilled. The integrated trajectories are decoded to their original\ndimensionality, as well as to the higher dimensionality space produced by the\nadversarial autoencoder and they are compared to the ground truth solution. The\nmethod is tested with two examples of flow over a cylinder, where the fluid\nproperties are varied between both examples.\n","authors":["Carlos Bermejo-Barbanoj","Beatriz Moya","Alberto Badías","Francisco Chinesta","Elías Cueto"],"pdf_url":"https://arxiv.org/pdf/2402.17506v1.pdf","comment":"18 pages, 11 figures"},{"id":"http://arxiv.org/abs/2307.00676v2","updated":"2024-02-27T13:46:11Z","published":"2023-07-02T22:08:24Z","title":"Pay Attention to the Atlas: Atlas-Guided Test-Time Adaptation Method for\n  Robust 3D Medical Image Segmentation","summary":"  Convolutional neural networks (CNNs) often suffer from poor performance when\ntested on target data that differs from the training (source) data\ndistribution, particularly in medical imaging applications where variations in\nimaging protocols across different clinical sites and scanners lead to\ndifferent imaging appearances. However, re-accessing source training data for\nunsupervised domain adaptation or labeling additional test data for model\nfine-tuning can be difficult due to privacy issues and high labeling costs,\nrespectively. To solve this problem, we propose a novel atlas-guided test-time\nadaptation (TTA) method for robust 3D medical image segmentation, called\nAdaAtlas. AdaAtlas only takes one single unlabeled test sample as input and\nadapts the segmentation network by minimizing an atlas-based loss.\nSpecifically, the network is adapted so that its prediction after registration\nis aligned with the learned atlas in the atlas space, which helps to reduce\nanatomical segmentation errors at test time. In addition, different from most\nexisting TTA methods which restrict the adaptation to batch normalization\nblocks in the segmentation network only, we further exploit the use of channel\nand spatial attention blocks for improved adaptability at test time. Extensive\nexperiments on multiple datasets from different sites show that AdaAtlas with\nattention blocks adapted (AdaAtlas-Attention) achieves superior performance\nimprovements, greatly outperforming other competitive TTA methods.\n","authors":["Jingjie Guo","Weitong Zhang","Matthew Sinclair","Daniel Rueckert","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2307.00676v2.pdf","comment":"Accepted by MICCAI BTSD-1001AI workshop. (Oral\n  presentation).https://btsdmiccai.github.io/"},{"id":"http://arxiv.org/abs/2401.15330v2","updated":"2024-02-27T13:43:55Z","published":"2024-01-27T07:26:10Z","title":"Optimal Sparse Survival Trees","summary":"  Interpretability is crucial for doctors, hospitals, pharmaceutical companies\nand biotechnology corporations to analyze and make decisions for high stakes\nproblems that involve human health. Tree-based methods have been widely adopted\nfor survival analysis due to their appealing interpretablility and their\nability to capture complex relationships. However, most existing methods to\nproduce survival trees rely on heuristic (or greedy) algorithms, which risk\nproducing sub-optimal models. We present a dynamic-programming-with-bounds\napproach that finds provably-optimal sparse survival tree models, frequently in\nonly a few seconds.\n","authors":["Rui Zhang","Rui Xin","Margo Seltzer","Cynthia Rudin"],"pdf_url":"https://arxiv.org/pdf/2401.15330v2.pdf","comment":"AISTATS2024 camera ready version. arXiv admin note: text overlap with\n  arXiv:2211.14980"},{"id":"http://arxiv.org/abs/2306.05812v2","updated":"2024-02-27T13:40:40Z","published":"2023-06-09T11:05:09Z","title":"HRTF upsampling with a generative adversarial network using a gnomonic\n  equiangular projection","summary":"  An individualised head-related transfer function (HRTF) is very important for\ncreating realistic virtual reality (VR) and augmented reality (AR)\nenvironments. However, acoustically measuring high-quality HRTFs requires\nexpensive equipment and an acoustic lab setting. To overcome these limitations\nand to make this measurement more efficient HRTF upsampling has been exploited\nin the past where a high-resolution HRTF is created from a low-resolution one.\nThis paper demonstrates how generative adversarial networks (GANs) can be\napplied to HRTF upsampling. We propose a novel approach that transforms the\nHRTF data for direct use with a convolutional super-resolution generative\nadversarial network (SRGAN). This new approach is benchmarked against three\nbaselines: barycentric upsampling, spherical harmonic (SH) upsampling and an\nHRTF selection approach. Experimental results show that the proposed method\noutperforms all three baselines in terms of log-spectral distortion (LSD) and\nlocalisation performance using perceptual models when the input HRTF is sparse\n(less than 20 measured positions).\n","authors":["Aidan O. T. Hogg","Mads Jenkins","He Liu","Isaac Squires","Samuel J. Cooper","Lorenzo Picinali"],"pdf_url":"https://arxiv.org/pdf/2306.05812v2.pdf","comment":"15 pages, 9 figures, Preprint (Accepted to IEEE/ACM Transactions on\n  Audio, Speech, and Language Processing on the 15 Feb 2024)"},{"id":"http://arxiv.org/abs/2307.12499v3","updated":"2024-02-27T13:39:09Z","published":"2023-07-24T03:10:02Z","title":"AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion\n  Models","summary":"  Unrestricted adversarial attacks present a serious threat to deep learning\nmodels and adversarial defense techniques. They pose severe security problems\nfor deep learning applications because they can effectively bypass defense\nmechanisms. However, previous attack methods often utilize Generative\nAdversarial Networks (GANs), which are not theoretically provable and thus\ngenerate unrealistic examples by incorporating adversarial objectives,\nespecially for large-scale datasets like ImageNet. In this paper, we propose a\nnew method, called AdvDiff, to generate unrestricted adversarial examples with\ndiffusion models. We design two novel adversarial guidance techniques to\nconduct adversarial sampling in the reverse generation process of diffusion\nmodels. These two techniques are effective and stable to generate high-quality,\nrealistic adversarial examples by integrating gradients of the target\nclassifier interpretably. Experimental results on MNIST and ImageNet datasets\ndemonstrate that AdvDiff is effective to generate unrestricted adversarial\nexamples, which outperforms GAN-based methods in terms of attack performance\nand generation quality.\n","authors":["Xuelong Dai","Kaisheng Liang","Bin Xiao"],"pdf_url":"https://arxiv.org/pdf/2307.12499v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17501v1","updated":"2024-02-27T13:36:55Z","published":"2024-02-27T13:36:55Z","title":"Intensive Care as One Big Sequence Modeling Problem","summary":"  Reinforcement Learning in Healthcare is typically concerned with narrow\nself-contained tasks such as sepsis prediction or anesthesia control. However,\nprevious research has demonstrated the potential of generalist models (the\nprime example being Large Language Models) to outperform task-specific\napproaches due to their capability for implicit transfer learning. To enable\ntraining of foundation models for Healthcare as well as leverage the\ncapabilities of state of the art Transformer architectures, we propose the\nparadigm of Healthcare as Sequence Modeling, in which interaction between the\npatient and the healthcare provider is represented as an event stream and tasks\nlike diagnosis and treatment selection are modeled as prediction of future\nevents in the stream. To explore this paradigm experimentally we develop\nMIMIC-SEQ, a sequence modeling benchmark derived by translating heterogenous\nclinical records from MIMIC-IV dataset into a uniform event stream format,\ntrain a baseline model and explore its capabilities.\n","authors":["Vadim Liventsev","Tobias Fritz"],"pdf_url":"https://arxiv.org/pdf/2402.17501v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17500v1","updated":"2024-02-27T13:34:08Z","published":"2024-02-27T13:34:08Z","title":"Predicting Instability in Complex Oscillator Networks: Limitations and\n  Potentials of Network Measures and Machine Learning","summary":"  A central question of network science is how functional properties of systems\narise from their structure. For networked dynamical systems, structure is\ntypically quantified with network measures. A functional property that is of\ntheoretical and practical interest for oscillatory systems is the stability of\nsynchrony to localized perturbations. Recently, Graph Neural Networks (GNNs)\nhave been shown to predict this stability successfully; at the same time,\nnetwork measures have struggled to paint a clear picture. Here we collect 46\nrelevant network measures and find that no small subset can reliably predict\nstability. The performance of GNNs can only be matched by combining all network\nmeasures and nodewise machine learning. However, unlike GNNs, this approach\nfails to extrapolate from network ensembles to several real power grid\ntopologies. This suggests that correlations of network measures and function\nmay be misleading, and that GNNs capture the causal relationship between\nstructure and stability substantially better.\n","authors":["Christian Nauck","Michael Lindner","Nora Molkenthin","Jürgen Kurths","Eckehard Schöll","Jörg Raisch","Frank Hellmann"],"pdf_url":"https://arxiv.org/pdf/2402.17500v1.pdf","comment":"30 pages (16 pages main section), 15 figures, 4 tables"},{"id":"http://arxiv.org/abs/2402.11585v2","updated":"2024-02-27T13:33:05Z","published":"2024-02-18T13:24:48Z","title":"PolypNextLSTM: A lightweight and fast polyp video segmentation network\n  using ConvNext and ConvLSTM","summary":"  Commonly employed in polyp segmentation, single image UNet architectures lack\nthe temporal insight clinicians gain from video data in diagnosing polyps. To\nmirror clinical practices more faithfully, our proposed solution,\nPolypNextLSTM, leverages video-based deep learning, harnessing temporal\ninformation for superior segmentation performance with the least parameter\noverhead, making it possibly suitable for edge devices. PolypNextLSTM employs a\nUNet-like structure with ConvNext-Tiny as its backbone, strategically omitting\nthe last two layers to reduce parameter overhead. Our temporal fusion module, a\nConvolutional Long Short Term Memory (ConvLSTM), effectively exploits temporal\nfeatures. Our primary novelty lies in PolypNextLSTM, which stands out as the\nleanest in parameters and the fastest model, surpassing the performance of five\nstate-of-the-art image and video-based deep learning models. The evaluation of\nthe SUN-SEG dataset spans easy-to-detect and hard-to-detect polyp scenarios,\nalong with videos containing challenging artefacts like fast motion and\nocclusion. Comparison against 5 image-based and 5 video-based models\ndemonstrates PolypNextLSTM's superiority, achieving a Dice score of 0.7898 on\nthe hard-to-detect polyp test set, surpassing image-based PraNet (0.7519) and\nvideo-based PNSPlusNet (0.7486). Notably, our model excels in videos featuring\ncomplex artefacts such as ghosting and occlusion. PolypNextLSTM, integrating\npruned ConvNext-Tiny with ConvLSTM for temporal fusion, not only exhibits\nsuperior segmentation performance but also maintains the highest frames per\nspeed among evaluated models. Access code here\nhttps://github.com/mtec-tuhh/PolypNextLSTM\n","authors":["Debayan Bhattacharya","Konrad Reuter","Finn Behrendnt","Lennart Maack","Sarah Grube","Alexander Schlaefer"],"pdf_url":"https://arxiv.org/pdf/2402.11585v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.02920v3","updated":"2024-02-27T13:29:56Z","published":"2022-11-05T14:36:05Z","title":"GmGM: a Fast Multi-Axis Gaussian Graphical Model","summary":"  This paper introduces the Gaussian multi-Graphical Model, a model to\nconstruct sparse graph representations of matrix- and tensor-variate data. We\ngeneralize prior work in this area by simultaneously learning this\nrepresentation across several tensors that share axes, which is necessary to\nallow the analysis of multimodal datasets such as those encountered in\nmulti-omics. Our algorithm uses only a single eigendecomposition per axis,\nachieving an order of magnitude speedup over prior work in the ungeneralized\ncase. This allows the use of our methodology on large multi-modal datasets such\nas single-cell multi-omics data, which was challenging with previous\napproaches. We validate our model on synthetic data and five real-world\ndatasets.\n","authors":["Bailey Andrew","David Westhead","Luisa Cutillo"],"pdf_url":"https://arxiv.org/pdf/2211.02920v3.pdf","comment":"8 pages (33 additional in supplementary material), 19 figures,\n  accepted at AIStats 2024"},{"id":"http://arxiv.org/abs/2306.15924v2","updated":"2024-02-27T13:20:54Z","published":"2023-06-28T05:02:03Z","title":"The curse of dimensionality in operator learning","summary":"  Neural operator architectures employ neural networks to approximate operators\nmapping between Banach spaces of functions; they may be used to accelerate\nmodel evaluations via emulation, or to discover models from data. Consequently,\nthe methodology has received increasing attention over recent years, giving\nrise to the rapidly growing field of operator learning. The first contribution\nof this paper is to prove that for general classes of operators which are\ncharacterized only by their $C^r$- or Lipschitz-regularity, operator learning\nsuffers from a curse of dimensionality, defined precisely here in terms of\nrepresentations of the infinite-dimensional input and output function spaces.\nThe result is applicable to a wide variety of existing neural operators,\nincluding PCA-Net, DeepONet and the FNO. The second contribution of the paper\nis to prove that the general curse of dimensionality can be overcome for\nsolution operators defined by the Hamilton-Jacobi equation; this is achieved by\nleveraging additional structure in the underlying solution operator, going\nbeyond regularity. To this end, a novel neural operator architecture is\nintroduced, termed HJ-Net, which explicitly takes into account characteristic\ninformation of the underlying Hamiltonian system. Error and complexity\nestimates are derived for HJ-Net which show that this architecture can provably\nbeat the curse of dimensionality related to the infinite-dimensional input and\noutput function spaces.\n","authors":["Samuel Lanthaler","Andrew M. Stuart"],"pdf_url":"https://arxiv.org/pdf/2306.15924v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17492v1","updated":"2024-02-27T13:18:00Z","published":"2024-02-27T13:18:00Z","title":"syren-halofit: A fast, interpretable, high-precision formula for the\n  $Λ$CDM nonlinear matter power spectrum","summary":"  Rapid and accurate evaluation of the nonlinear matter power spectrum, $P(k)$,\nas a function of cosmological parameters and redshift is of fundamental\nimportance in cosmology. Analytic approximations provide an interpretable\nsolution, yet current approximations are neither fast nor accurate relative to\nblack-box numerical emulators. We use symbolic regression to obtain simple\nanalytic approximations to the nonlinear scale, $k_\\sigma$, the effective\nspectral index, $n_{\\rm eff}$, and the curvature, $C$, which are required for\nthe halofit model. We then re-optimise the coefficients of halofit to fit a\nwide range of cosmologies and redshifts. We then again exploit symbolic\nregression to explore the space of analytic expressions to fit the residuals\nbetween $P(k)$ and the optimised predictions of halofit. All methods are\nvalidated against $N$-body simulations. Our symbolic expressions for\n$k_\\sigma$, $n_{\\rm eff}$ and $C$ have root mean squared fractional errors of\n0.8%, 0.2% and 0.3%, respectively, for redshifts below 3 and a wide range of\ncosmologies. The re-optimised halofit parameters reduce the root mean squared\nfractional error from 3% to below 2% for wavenumbers $k=9\\times10^{-3}-9 \\,\nh{\\rm Mpc^{-1}}$. We introduce syren-halofit (symbolic-regression-enhanced\nhalofit), an extension to halofit containing a short symbolic correction which\nimproves this error to 1%. Our method is 2350 and 3170 times faster than\ncurrent halofit and hmcode implementations, respectively, and 2680 and 64 times\nfaster than EuclidEmulator2 (which requires running class) and the BACCO\nemulator. We obtain comparable accuracy to EuclidEmulator2 and the BACCO\nemulator when tested on $N$-body simulations. Our work greatly increases the\nspeed and accuracy of symbolic approximations to $P(k)$, making them\nsignificantly faster than their numerical counterparts without loss of\naccuracy.\n","authors":["Deaglan J. Bartlett","Benjamin D. Wandelt","Matteo Zennaro","Pedro G. Ferreira","Harry Desmond"],"pdf_url":"https://arxiv.org/pdf/2402.17492v1.pdf","comment":"11 pages, 8 figures. Submitted to A&A"},{"id":"http://arxiv.org/abs/2210.17264v2","updated":"2024-02-27T13:14:51Z","published":"2022-10-31T12:44:53Z","title":"Cross-lingual Text-To-Speech with Flow-based Voice Conversion for\n  Improved Pronunciation","summary":"  This paper presents a method for end-to-end cross-lingual text-to-speech\n(TTS) which aims to preserve the target language's pronunciation regardless of\nthe original speaker's language. The model used is based on a non-attentive\nTacotron architecture, where the decoder has been replaced with a normalizing\nflow network conditioned on the speaker identity, allowing both TTS and voice\nconversion (VC) to be performed by the same model due to the inherent\nlinguistic content and speaker identity disentanglement. When used in a\ncross-lingual setting, acoustic features are initially produced with a native\nspeaker of the target language and then voice conversion is applied by the same\nmodel in order to convert these features to the target speaker's voice. We\nverify through objective and subjective evaluations that our method can have\nbenefits compared to baseline cross-lingual synthesis. By including speakers\naveraging 7.5 minutes of speech, we also present positive results on\nlow-resource scenarios.\n","authors":["Nikolaos Ellinas","Georgios Vamvoukakis","Konstantinos Markopoulos","Georgia Maniati","Panos Kakoulidis","June Sig Sung","Inchul Hwang","Spyros Raptis","Aimilios Chalamandaris","Pirros Tsiakoulis"],"pdf_url":"https://arxiv.org/pdf/2210.17264v2.pdf","comment":"Fundamental changes to the model described and experimental procedure"},{"id":"http://arxiv.org/abs/2402.17487v1","updated":"2024-02-27T13:12:18Z","published":"2024-02-27T13:12:18Z","title":"Bit Rate Matching Algorithm Optimization in JPEG-AI Verification Model","summary":"  The research on neural network (NN) based image compression has shown\nsuperior performance compared to classical compression frameworks. Unlike the\nhand-engineered transforms in the classical frameworks, NN-based models learn\nthe non-linear transforms providing more compact bit representations, and\nachieve faster coding speed on parallel devices over their classical\ncounterparts. Those properties evoked the attention of both scientific and\nindustrial communities, resulting in the standardization activity JPEG-AI. The\nverification model for the standardization process of JPEG-AI is already in\ndevelopment and has surpassed the advanced VVC intra codec. To generate\nreconstructed images with the desired bits per pixel and assess the BD-rate\nperformance of both the JPEG-AI verification model and VVC intra, bit rate\nmatching is employed. However, the current state of the JPEG-AI verification\nmodel experiences significant slowdowns during bit rate matching, resulting in\nsuboptimal performance due to an unsuitable model. The proposed methodology\noffers a gradual algorithmic optimization for matching bit rates, resulting in\na fourfold acceleration and over 1% improvement in BD-rate at the base\noperation point. At the high operation point, the acceleration increases up to\nsixfold.\n","authors":["Panqi Jia","A. Burakhan Koyuncu","Jue Mao","Ze Cui","Yi Ma","Tiansheng Guo","Timofey Solovyev","Alexander Karabutov","Yin Zhao","Jing Wang","Elena Alshina","Andre Kaup"],"pdf_url":"https://arxiv.org/pdf/2402.17487v1.pdf","comment":"Accepted at (IEEE) PCS 2024; 6 pages"},{"id":"http://arxiv.org/abs/2310.14341v2","updated":"2024-02-27T13:10:07Z","published":"2023-10-22T16:17:24Z","title":"Pyramidal Hidden Markov Model For Multivariate Time Series Forecasting","summary":"  The Hidden Markov Model (HMM) can predict the future value of a time series\nbased on its current and previous values, making it a powerful algorithm for\nhandling various types of time series. Numerous studies have explored the\nimprovement of HMM using advanced techniques, leading to the development of\nseveral variations of HMM. Despite these studies indicating the increased\ncompetitiveness of HMM compared to other advanced algorithms, few have\nrecognized the significance and impact of incorporating multistep stochastic\nstates into its performance. In this work, we propose a Pyramidal Hidden Markov\nModel (PHMM) that can capture multiple multistep stochastic states. Initially,\na multistep HMM is designed for extracting short multistep stochastic states.\nNext, a novel time series forecasting structure is proposed based on PHMM,\nwhich utilizes pyramid-like stacking to adaptively identify long multistep\nstochastic states. By employing these two schemes, our model can effectively\nhandle non-stationary and noisy data, while also establishing long-term\ndependencies for more accurate and comprehensive forecasting. The experimental\nresults on diverse multivariate time series datasets convincingly demonstrate\nthe superior performance of our proposed PHMM compared to its competitive peers\nin time series forecasting.\n","authors":["YeXin Huang"],"pdf_url":"https://arxiv.org/pdf/2310.14341v2.pdf","comment":"8 pages,3 figures"},{"id":"http://arxiv.org/abs/2402.17472v1","updated":"2024-02-27T12:53:15Z","published":"2024-02-27T12:53:15Z","title":"Fraud Detection with Binding Global and Local Relational Interaction","summary":"  Graph Neural Network has been proved to be effective for fraud detection for\nits capability to encode node interaction and aggregate features in a holistic\nview. Recently, Transformer network with great sequence encoding ability, has\nalso outperformed other GNN-based methods in literatures. However, both\nGNN-based and Transformer-based networks only encode one perspective of the\nwhole graph, while GNN encodes global features and Transformer network encodes\nlocal ones. Furthermore, previous works ignored encoding global interaction\nfeatures of the heterogeneous graph with separate networks, thus leading to\nsuboptimal performance. In this work, we present a novel framework called\nRelation-Aware GNN with transFormer (RAGFormer) which simultaneously embeds\nlocal and global features into a target node. The simple yet effective network\napplies a modified GAGA module where each transformer layer is followed by a\ncross-relation aggregation layer, to encode local embeddings and node\ninteractions across different relations. Apart from the Transformer-based\nnetwork, we further introduce a Relation-Aware GNN module to learn global\nembeddings, which is later merged into the local embeddings by an attention\nfusion module and a skip connection. Extensive experiments on two popular\npublic datasets and an industrial dataset demonstrate that RAGFormer achieves\nthe state-of-the-art performance. Substantial analysis experiments validate the\neffectiveness of each submodule of RAGFormer and its high efficiency in\nutilizing small-scale data and low hyper-parameter sensitivity.\n","authors":["Haolin Li","Shuyang Jiang","Lifeng Zhang","Siyuan Du","Guangnan Ye","Hongfeng Chai"],"pdf_url":"https://arxiv.org/pdf/2402.17472v1.pdf","comment":"Under review for SIGKDD 2024"},{"id":"http://arxiv.org/abs/2402.17470v1","updated":"2024-02-27T12:52:44Z","published":"2024-02-27T12:52:44Z","title":"Bit Distribution Study and Implementation of Spatial Quality Map in the\n  JPEG-AI Standardization","summary":"  Currently, there is a high demand for neural network-based image compression\ncodecs. These codecs employ non-linear transforms to create compact bit\nrepresentations and facilitate faster coding speeds on devices compared to the\nhand-crafted transforms used in classical frameworks. The scientific and\nindustrial communities are highly interested in these properties, leading to\nthe standardization effort of JPEG-AI. The JPEG-AI verification model has been\nreleased and is currently under development for standardization. Utilizing\nneural networks, it can outperform the classic codec VVC intra by over 10%\nBD-rate operating at base operation point. Researchers attribute this success\nto the flexible bit distribution in the spatial domain, in contrast to VVC\nintra's anchor that is generated with a constant quality point. However, our\nstudy reveals that VVC intra displays a more adaptable bit distribution\nstructure through the implementation of various block sizes. As a result of our\nobservations, we have proposed a spatial bit allocation method to optimize the\nJPEG-AI verification model's bit distribution and enhance the visual quality.\nFurthermore, by applying the VVC bit distribution strategy, the objective\nperformance of JPEG-AI verification mode can be further improved, resulting in\na maximum gain of 0.45 dB in PSNR-Y.\n","authors":["Panqi Jia","Jue Mao","Esin Koyuncu","A. Burakhan Koyuncu","Timofey Solovyev","Alexander Karabutov","Yin Zhao","Elena Alshina","Andre Kaup"],"pdf_url":"https://arxiv.org/pdf/2402.17470v1.pdf","comment":"5 pages, 3 figures, 4 tables"},{"id":"http://arxiv.org/abs/2303.15963v2","updated":"2024-02-27T12:42:15Z","published":"2023-03-28T13:31:27Z","title":"Multimodal and multicontrast image fusion via deep generative models","summary":"  Recently, it has become progressively more evident that classic diagnostic\nlabels are unable to reliably describe the complexity and variability of\nseveral clinical phenotypes. This is particularly true for a broad range of\nneuropsychiatric illnesses (e.g., depression, anxiety disorders, behavioral\nphenotypes). Patient heterogeneity can be better described by grouping\nindividuals into novel categories based on empirically derived sections of\nintersecting continua that span across and beyond traditional categorical\nborders. In this context, neuroimaging data carry a wealth of spatiotemporally\nresolved information about each patient's brain. However, they are usually\nheavily collapsed a priori through procedures which are not learned as part of\nmodel training, and consequently not optimized for the downstream prediction\ntask. This is because every individual participant usually comes with multiple\nwhole-brain 3D imaging modalities often accompanied by a deep genotypic and\nphenotypic characterization, hence posing formidable computational challenges.\nIn this paper we design a deep learning architecture based on generative models\nrooted in a modular approach and separable convolutional blocks to a) fuse\nmultiple 3D neuroimaging modalities on a voxel-wise level, b) convert them into\ninformative latent embeddings through heavy dimensionality reduction, c)\nmaintain good generalizability and minimal information loss. As proof of\nconcept, we test our architecture on the well characterized Human Connectome\nProject database demonstrating that our latent embeddings can be clustered into\neasily separable subject strata which, in turn, map to different phenotypical\ninformation which was not included in the embedding creation process. This may\nbe of aid in predicting disease evolution as well as drug response, hence\nsupporting mechanistic disease understanding and empowering clinical trials.\n","authors":["Giovanna Maria Dimitri","Simeon Spasov","Andrea Duggento","Luca Passamonti","Pietro Li`o","Nicola Toschi"],"pdf_url":"https://arxiv.org/pdf/2303.15963v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07892v2","updated":"2024-02-27T12:37:06Z","published":"2022-12-15T15:21:28Z","title":"Integrating Multimodal Data for Joint Generative Modeling of Complex\n  Dynamics","summary":"  Many, if not most, systems of interest in science are naturally described as\nnonlinear dynamical systems. Empirically, we commonly access these systems\nthrough time series measurements. Often such time series may consist of\ndiscrete random variables rather than continuous measurements, or may be\ncomposed of measurements from multiple data modalities observed simultaneously.\nFor instance, in neuroscience we may have behavioral labels in addition to\nspike counts and continuous physiological recordings. While by now there is a\nburgeoning literature on deep learning for dynamical systems reconstruction\n(DSR), multimodal data integration has hardly been considered in this context.\nHere we provide such an efficient and flexible algorithmic framework that rests\non a multimodal variational autoencoder for generating a sparse teacher signal\nthat guides training of a reconstruction model, exploiting recent advances in\nDSR training techniques. It enables to combine various sources of information\nfor optimal reconstruction, even allows for reconstruction from symbolic data\n(class labels) alone, and connects different types of observations within a\ncommon latent dynamics space. In contrast to previous multimodal data\nintegration techniques for scientific applications, our framework is fully\n\\textit{generative}, producing, after training, trajectories with the same\ngeometrical and temporal structure as those of the ground truth system.\n","authors":["Manuel Brenner","Florian Hess","Georgia Koppe","Daniel Durstewitz"],"pdf_url":"https://arxiv.org/pdf/2212.07892v2.pdf","comment":"A previous version was published as a workshop paper for the AAAI\n  2023 Workshop MLmDS under the name \"Multimodal Teacher Forcing for\n  Reconstructing Nonlinear Dynamical Systems\""},{"id":"http://arxiv.org/abs/2402.17457v1","updated":"2024-02-27T12:28:01Z","published":"2024-02-27T12:28:01Z","title":"Why do Learning Rates Transfer? Reconciling Optimization and Scaling\n  Limits for Deep Learning","summary":"  Recently, there has been growing evidence that if the width and depth of a\nneural network are scaled toward the so-called rich feature learning limit\n($\\mu$P and its depth extension), then some hyperparameters - such as the\nlearning rate - exhibit transfer from small to very large models, thus reducing\nthe cost of hyperparameter tuning. From an optimization perspective, this\nphenomenon is puzzling, as it implies that the loss landscape is remarkably\nconsistent across very different model sizes. In this work, we find empirical\nevidence that learning rate transfer can be attributed to the fact that under\n$\\mu$P and its depth extension, the largest eigenvalue of the training loss\nHessian (i.e. the sharpness) is largely independent of the width and depth of\nthe network for a sustained period of training time. On the other hand, we show\nthat under the neural tangent kernel (NTK) regime, the sharpness exhibits very\ndifferent dynamics at different scales, thus preventing learning rate transfer.\nBut what causes these differences in the sharpness dynamics? Through a\nconnection between the spectra of the Hessian and the NTK matrix, we argue that\nthe cause lies in the presence (for $\\mu$P) or progressive absence (for the NTK\nregime) of feature learning, which results in a different evolution of the NTK,\nand thus of the sharpness. We corroborate our claims with a substantial suite\nof experiments, covering a wide range of datasets and architectures: from\nResNets and Vision Transformers trained on benchmark vision datasets to\nTransformers-based language models trained on WikiText\n","authors":["Lorenzo Noci","Alexandru Meterez","Thomas Hofmann","Antonio Orvieto"],"pdf_url":"https://arxiv.org/pdf/2402.17457v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17453v1","updated":"2024-02-27T12:26:07Z","published":"2024-02-27T12:26:07Z","title":"DS-Agent: Automated Data Science by Empowering Large Language Models\n  with Case-Based Reasoning","summary":"  In this work, we investigate the potential of large language models (LLMs)\nbased agents to automate data science tasks, with the goal of comprehending\ntask requirements, then building and training the best-fit machine learning\nmodels. Despite their widespread success, existing LLM agents are hindered by\ngenerating unreasonable experiment plans within this scenario. To this end, we\npresent DS-Agent, a novel automatic framework that harnesses LLM agent and\ncase-based reasoning (CBR). In the development stage, DS-Agent follows the CBR\nframework to structure an automatic iteration pipeline, which can flexibly\ncapitalize on the expert knowledge from Kaggle, and facilitate consistent\nperformance improvement through the feedback mechanism. Moreover, DS-Agent\nimplements a low-resource deployment stage with a simplified CBR paradigm to\nadapt past successful solutions from the development stage for direct code\ngeneration, significantly reducing the demand on foundational capabilities of\nLLMs. Empirically, DS-Agent with GPT-4 achieves an unprecedented 100% success\nrate in the development stage, while attaining 36% improvement on average one\npass rate across alternative LLMs in the deployment stage. In both stages,\nDS-Agent achieves the best rank in performance, costing \\$1.60 and \\$0.13 per\nrun with GPT-4, respectively.\n","authors":["Siyuan Guo","Cheng Deng","Ying Wen","Hechang Chen","Yi Chang","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2402.17453v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.01055v2","updated":"2024-02-27T12:24:13Z","published":"2024-02-01T23:03:53Z","title":"Multiclass Learning from Noisy Labels for Non-decomposable Performance\n  Measures","summary":"  There has been much interest in recent years in learning good classifiers\nfrom data with noisy labels. Most work on learning from noisy labels has\nfocused on standard loss-based performance measures. However, many machine\nlearning problems require using non-decomposable performance measures which\ncannot be expressed as the expectation or sum of a loss on individual examples;\nthese include for example the H-mean, Q-mean and G-mean in class imbalance\nsettings, and the Micro $F_1$ in information retrieval. In this paper, we\ndesign algorithms to learn from noisy labels for two broad classes of\nmulticlass non-decomposable performance measures, namely, monotonic convex and\nratio-of-linear, which encompass all the above examples. Our work builds on the\nFrank-Wolfe and Bisection based methods of Narasimhan et al. (2015). In both\ncases, we develop noise-corrected versions of the algorithms under the widely\nstudied family of class-conditional noise models. We provide regret (excess\nrisk) bounds for our algorithms, establishing that even though they are trained\non noisy data, they are Bayes consistent in the sense that their performance\nconverges to the optimal performance w.r.t. the clean (non-noisy) distribution.\nOur experiments demonstrate the effectiveness of our algorithms in handling\nlabel noise.\n","authors":["Mingyuan Zhang","Shivani Agarwal"],"pdf_url":"https://arxiv.org/pdf/2402.01055v2.pdf","comment":"Accepted to AISTATS 2024"},{"id":"http://arxiv.org/abs/2402.10232v3","updated":"2024-02-27T12:05:09Z","published":"2024-02-10T15:37:46Z","title":"Simple, unified analysis of Johnson-Lindenstrauss with applications","summary":"  We present a simple and unified analysis of the Johnson-Lindenstrauss (JL)\nlemma, a cornerstone in the field of dimensionality reduction critical for\nmanaging high-dimensional data. Our approach not only simplifies the\nunderstanding but also unifies various constructions under the JL framework,\nincluding spherical, binary-coin, sparse JL, Gaussian and sub-Gaussian models.\nThis simplification and unification make significant strides in preserving the\nintrinsic geometry of data, essential across diverse applications from\nstreaming algorithms to reinforcement learning. Notably, we deliver the first\nrigorous proof of the spherical construction's effectiveness and provide a\ngeneral class of sub-Gaussian constructions within this simplified framework.\nAt the heart of our contribution is an innovative extension of the\nHanson-Wright inequality to high dimensions, complete with explicit constants.\nBy employing simple yet powerful probabilistic tools and analytical techniques,\nsuch as an enhanced diagonalization process, our analysis not only solidifies\nthe JL lemma's theoretical foundation by removing an independence assumption\nbut also extends its practical reach, showcasing its adaptability and\nimportance in contemporary computational algorithms.\n","authors":["Yingru Li"],"pdf_url":"https://arxiv.org/pdf/2402.10232v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17440v1","updated":"2024-02-27T11:52:49Z","published":"2024-02-27T11:52:49Z","title":"Principled Architecture-aware Scaling of Hyperparameters","summary":"  Training a high-quality deep neural network requires choosing suitable\nhyperparameters, which is a non-trivial and expensive process. Current works\ntry to automatically optimize or design principles of hyperparameters, such\nthat they can generalize to diverse unseen scenarios. However, most designs or\noptimization methods are agnostic to the choice of network structures, and thus\nlargely ignore the impact of neural architectures on hyperparameters. In this\nwork, we precisely characterize the dependence of initializations and maximal\nlearning rates on the network architecture, which includes the network depth,\nwidth, convolutional kernel size, and connectivity patterns. By pursuing every\nparameter to be maximally updated with the same mean squared change in\npre-activations, we can generalize our initialization and learning rates across\nMLPs (multi-layer perception) and CNNs (convolutional neural network) with\nsophisticated graph topologies. We verify our principles with comprehensive\nexperiments. More importantly, our strategy further sheds light on advancing\ncurrent benchmarks for architecture design. A fair comparison of AutoML\nalgorithms requires accurate network rankings. However, we demonstrate that\nnetwork rankings can be easily changed by better training networks in\nbenchmarks with our architecture-aware learning rates and initialization.\n","authors":["Wuyang Chen","Junru Wu","Zhangyang Wang","Boris Hanin"],"pdf_url":"https://arxiv.org/pdf/2402.17440v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17431v1","updated":"2024-02-27T11:43:41Z","published":"2024-02-27T11:43:41Z","title":"The KANDY Benchmark: Incremental Neuro-Symbolic Learning and Reasoning\n  with Kandinsky Patterns","summary":"  Artificial intelligence is continuously seeking novel challenges and\nbenchmarks to effectively measure performance and to advance the\nstate-of-the-art. In this paper we introduce KANDY, a benchmarking framework\nthat can be used to generate a variety of learning and reasoning tasks inspired\nby Kandinsky patterns. By creating curricula of binary classification tasks\nwith increasing complexity and with sparse supervisions, KANDY can be used to\nimplement benchmarks for continual and semi-supervised learning, with a\nspecific focus on symbol compositionality. Classification rules are also\nprovided in the ground truth to enable analysis of interpretable solutions.\nTogether with the benchmark generation pipeline, we release two curricula, an\neasier and a harder one, that we propose as new challenges for the research\ncommunity. With a thorough experimental evaluation, we show how both\nstate-of-the-art neural models and purely symbolic approaches struggle with\nsolving most of the tasks, thus calling for the application of advanced\nneuro-symbolic methods trained over time.\n","authors":["Luca Salvatore Lorello","Marco Lippi","Stefano Melacci"],"pdf_url":"https://arxiv.org/pdf/2402.17431v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17423v1","updated":"2024-02-27T11:32:14Z","published":"2024-02-27T11:32:14Z","title":"Reinforced In-Context Black-Box Optimization","summary":"  Black-Box Optimization (BBO) has found successful applications in many fields\nof science and engineering. Recently, there has been a growing interest in\nmeta-learning particular components of BBO algorithms to speed up optimization\nand get rid of tedious hand-crafted heuristics. As an extension, learning the\nentire algorithm from data requires the least labor from experts and can\nprovide the most flexibility. In this paper, we propose RIBBO, a method to\nreinforce-learn a BBO algorithm from offline data in an end-to-end fashion.\nRIBBO employs expressive sequence models to learn the optimization histories\nproduced by multiple behavior algorithms and tasks, leveraging the in-context\nlearning ability of large models to extract task information and make decisions\naccordingly. Central to our method is to augment the optimization histories\nwith regret-to-go tokens, which are designed to represent the performance of an\nalgorithm based on cumulative regret of the histories. The integration of\nregret-to-go tokens enables RIBBO to automatically generate sequences of query\npoints that satisfy the user-desired regret, which is verified by its\nuniversally good empirical performance on diverse problems, including BBOB\nfunctions, hyper-parameter optimization and robot control problems.\n","authors":["Lei Song","Chenxiao Gao","Ke Xue","Chenyang Wu","Dong Li","Jianye Hao","Zongzhang Zhang","Chao Qian"],"pdf_url":"https://arxiv.org/pdf/2402.17423v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.10275v4","updated":"2024-02-27T11:32:09Z","published":"2021-11-19T15:25:06Z","title":"Composite Goodness-of-fit Tests with Kernels","summary":"  Model misspecification can create significant challenges for the\nimplementation of probabilistic models, and this has led to development of a\nrange of robust methods which directly account for this issue. However, whether\nthese more involved methods are required will depend on whether the model is\nreally misspecified, and there is a lack of generally applicable methods to\nanswer this question. In this paper, we propose one such method. More\nprecisely, we propose kernel-based hypothesis tests for the challenging\ncomposite testing problem, where we are interested in whether the data comes\nfrom any distribution in some parametric family. Our tests make use of minimum\ndistance estimators based on the maximum mean discrepancy and the kernel Stein\ndiscrepancy. They are widely applicable, including whenever the density of the\nparametric model is known up to normalisation constant, or if the model takes\nthe form of a simulator. As our main result, we show that we are able to\nestimate the parameter and conduct our test on the same data (without data\nsplitting), while maintaining a correct test level. Our approach is illustrated\non a range of problems, including testing for goodness-of-fit of an\nunnormalised non-parametric density model, and an intractable generative model\nof a biological cellular network.\n","authors":["Oscar Key","Arthur Gretton","François-Xavier Briol","Tamara Fernandez"],"pdf_url":"https://arxiv.org/pdf/2111.10275v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16823v2","updated":"2024-02-27T11:03:10Z","published":"2024-02-26T18:48:27Z","title":"Language Agents as Optimizable Graphs","summary":"  Various human-designed prompt engineering techniques have been proposed to\nimprove problem solvers based on Large Language Models (LLMs), yielding many\ndisparate code bases. We unify these approaches by describing LLM-based agents\nas computational graphs. The nodes implement functions to process multimodal\ndata or query LLMs, and the edges describe the information flow between\noperations. Graphs can be recursively combined into larger composite graphs\nrepresenting hierarchies of inter-agent collaboration (where edges connect\noperations of different agents). Our novel automatic graph optimizers (1)\nrefine node-level LLM prompts (node optimization) and (2) improve agent\norchestration by changing graph connectivity (edge optimization). Experiments\ndemonstrate that our framework can be used to efficiently develop, integrate,\nand automatically improve various LLM agents. The code can be found at\nhttps://github.com/metauto-ai/gptswarm.\n","authors":["Mingchen Zhuge","Wenyi Wang","Louis Kirsch","Francesco Faccio","Dmitrii Khizbullin","Jürgen Schmidhuber"],"pdf_url":"https://arxiv.org/pdf/2402.16823v2.pdf","comment":"Project Website: https://gptswarm.org ; Github Repo:\n  https://github.com/metauto-ai/gptswarm ; Replace to fix typos"},{"id":"http://arxiv.org/abs/2402.17410v1","updated":"2024-02-27T11:01:58Z","published":"2024-02-27T11:01:58Z","title":"A novel image space formalism of Fourier domain interpolation neural\n  networks for noise propagation analysis","summary":"  Purpose: To develop an image space formalism of multi-layer convolutional\nneural networks (CNNs) for Fourier domain interpolation in MRI reconstructions\nand analytically estimate noise propagation during CNN inference. Theory and\nMethods: Nonlinear activations in the Fourier domain (also known as k-space)\nusing complex-valued Rectifier Linear Units are expressed as elementwise\nmultiplication with activation masks. This operation is transformed into a\nconvolution in the image space. After network training in k-space, this\napproach provides an algebraic expression for the derivative of the\nreconstructed image with respect to the aliased coil images, which serve as the\ninput tensors to the network in the image space. This allows the variance in\nthe network inference to be estimated analytically and to be used to describe\nnoise characteristics. Monte-Carlo simulations and numerical approaches based\non auto-differentiation were used for validation. The framework was tested on\nretrospectively undersampled invivo brain images. Results: Inferences conducted\nin the image domain are quasi-identical to inferences in the k-space,\nunderlined by corresponding quantitative metrics. Noise variance maps obtained\nfrom the analytical expression correspond with those obtained via Monte-Carlo\nsimulations, as well as via an auto-differentiation approach. The noise\nresilience is well characterized, as in the case of classical Parallel Imaging.\nKomolgorov-Smirnov tests demonstrate Gaussian distributions of voxel magnitudes\nin variance maps obtained via Monte-Carlo simulations. Conclusion: The\nquasi-equivalent image space formalism for neural networks for k-space\ninterpolation enables fast and accurate description of the noise\ncharacteristics during CNN inference, analogous to geometry-factor maps in\ntraditional parallel imaging methods.\n","authors":["Peter Dawood","Felix Breuer","Istvan Homolya","Jannik Stebani","Maximilian Gram","Peter M. Jakob","Moritz Zaiss","Martin Blaimer"],"pdf_url":"https://arxiv.org/pdf/2402.17410v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13641v2","updated":"2024-02-27T11:00:35Z","published":"2024-01-24T18:10:39Z","title":"How Good is ChatGPT at Face Biometrics? A First Look into Recognition,\n  Soft Biometrics, and Explainability","summary":"  Large Language Models (LLMs) such as GPT developed by OpenAI, have already\nshown astonishing results, introducing quick changes in our society. This has\nbeen intensified by the release of ChatGPT which allows anyone to interact in a\nsimple conversational way with LLMs, without any experience in the field\nneeded. As a result, ChatGPT has been rapidly applied to many different tasks\nsuch as code- and song-writer, education, virtual assistants, etc., showing\nimpressive results for tasks for which it was not trained (zero-shot learning).\n  The present study aims to explore the ability of ChatGPT, based on the recent\nGPT-4 multimodal LLM, for the task of face biometrics. In particular, we\nanalyze the ability of ChatGPT to perform tasks such as face verification,\nsoft-biometrics estimation, and explainability of the results. ChatGPT could be\nvery valuable to further increase the explainability and transparency of\nautomatic decisions in human scenarios. Experiments are carried out in order to\nevaluate the performance and robustness of ChatGPT, using popular public\nbenchmarks and comparing the results with state-of-the-art methods in the\nfield. The results achieved in this study show the potential of LLMs such as\nChatGPT for face biometrics, especially to enhance explainability. For\nreproducibility reasons, we release all the code in GitHub.\n","authors":["Ivan DeAndres-Tame","Ruben Tolosana","Ruben Vera-Rodriguez","Aythami Morales","Julian Fierrez","Javier Ortega-Garcia"],"pdf_url":"https://arxiv.org/pdf/2401.13641v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.15862v2","updated":"2024-02-27T10:59:33Z","published":"2022-05-28T11:12:38Z","title":"Snapture -- A Novel Neural Architecture for Combined Static and Dynamic\n  Hand Gesture Recognition","summary":"  As robots are expected to get more involved in people's everyday lives,\nframeworks that enable intuitive user interfaces are in demand. Hand gesture\nrecognition systems provide a natural way of communication and, thus, are an\nintegral part of seamless Human-Robot Interaction (HRI). Recent years have\nwitnessed an immense evolution of computational models powered by deep\nlearning. However, state-of-the-art models fall short in expanding across\ndifferent gesture domains, such as emblems and co-speech. In this paper, we\npropose a novel hybrid hand gesture recognition system. Our architecture\nenables learning both static and dynamic gestures: by capturing a so-called\n\"snapshot\" of the gesture performance at its peak, we integrate the hand pose\nalong with the dynamic movement. Moreover, we present a method for analyzing\nthe motion profile of a gesture to uncover its dynamic characteristics and\nwhich allows regulating a static channel based on the amount of motion. Our\nevaluation demonstrates the superiority of our approach on two gesture\nbenchmarks compared to a CNNLSTM baseline. We also provide an analysis on a\ngesture class basis that unveils the potential of our Snapture architecture for\nperformance improvements. Thanks to its modular implementation, our framework\nallows the integration of other multimodal data like facial expressions and\nhead tracking, which are important cues in HRI scenarios, into one\narchitecture. Thus, our work contributes both to gesture recognition research\nand machine learning applications for non-verbal communication with robots.\n","authors":["Hassan Ali","Doreen Jirak","Stefan Wermter"],"pdf_url":"https://arxiv.org/pdf/2205.15862v2.pdf","comment":"In Cognitive Computation(Accepted:30/06/2023,\n  Published:17/07/2023),20 pages,20 figures,4 tables;Please find the published\n  version/info to cite:\n  https://doi.org/10.1007/s12559-023-10174-z;Repositories:\n  https://zenodo.org/doi/10.5281/zenodo.10679196,\n  https://zenodo.org/doi/10.5281/zenodo.10693816;This work was co-funded by\n  Horizon Europe project TERAIS under Grant agreement number 101079338"},{"id":"http://arxiv.org/abs/2402.17406v1","updated":"2024-02-27T10:55:07Z","published":"2024-02-27T10:55:07Z","title":"LSPT: Long-term Spatial Prompt Tuning for Visual Representation Learning","summary":"  Visual Prompt Tuning (VPT) techniques have gained prominence for their\ncapacity to adapt pre-trained Vision Transformers (ViTs) to downstream visual\ntasks using specialized learnable tokens termed as prompts. Contemporary VPT\nmethodologies, especially when employed with self-supervised vision\ntransformers, often default to the introduction of new learnable prompts or\ngated prompt tokens predominantly sourced from the model's previous block. A\npivotal oversight in such approaches is their failure to harness the potential\nof long-range previous blocks as sources of prompts within each self-supervised\nViT. To bridge this crucial gap, we introduce Long-term Spatial Prompt Tuning\n(LSPT) - a revolutionary approach to visual representation learning. Drawing\ninspiration from the intricacies of the human brain, LSPT ingeniously\nincorporates long-term gated prompts. This feature serves as temporal coding,\ncurbing the risk of forgetting parameters acquired from earlier blocks. Further\nenhancing its prowess, LSPT brings into play patch tokens, serving as spatial\ncoding. This is strategically designed to perpetually amass class-conscious\nfeatures, thereby fortifying the model's prowess in distinguishing and\nidentifying visual categories. To validate the efficacy of our proposed method,\nwe engaged in rigorous experimentation across 5 FGVC and 19 VTAB-1K benchmarks.\nOur empirical findings underscore the superiority of LSPT, showcasing its\nability to set new benchmarks in visual prompt tuning performance.\n","authors":["Shentong Mo","Yansen Wang","Xufang Luo","Dongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2402.17406v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06454v3","updated":"2024-02-27T10:54:36Z","published":"2023-12-11T15:41:05Z","title":"Point Transformer with Federated Learning for Predicting Breast Cancer\n  HER2 Status from Hematoxylin and Eosin-Stained Whole Slide Images","summary":"  Directly predicting human epidermal growth factor receptor 2 (HER2) status\nfrom widely available hematoxylin and eosin (HE)-stained whole slide images\n(WSIs) can reduce technical costs and expedite treatment selection. Accurately\npredicting HER2 requires large collections of multi-site WSIs. Federated\nlearning enables collaborative training of these WSIs without gigabyte-size\nWSIs transportation and data privacy concerns. However, federated learning\nencounters challenges in addressing label imbalance in multi-site WSIs from the\nreal world. Moreover, existing WSI classification methods cannot simultaneously\nexploit local context information and long-range dependencies in the site-end\nfeature representation of federated learning. To address these issues, we\npresent a point transformer with federated learning for multi-site HER2 status\nprediction from HE-stained WSIs. Our approach incorporates two novel designs.\nWe propose a dynamic label distribution strategy and an auxiliary classifier,\nwhich helps to establish a well-initialized model and mitigate label\ndistribution variations across sites. Additionally, we propose a farthest\ncosine sampling based on cosine distance. It can sample the most distinctive\nfeatures and capture the long-range dependencies. Extensive experiments and\nanalysis show that our method achieves state-of-the-art performance at four\nsites with a total of 2687 WSIs. Furthermore, we demonstrate that our model can\ngeneralize to two unseen sites with 229 WSIs.\n","authors":["Bao Li","Zhenyu Liu","Lizhi Shao","Bensheng Qiu","Hong Bu","Jie Tian"],"pdf_url":"https://arxiv.org/pdf/2312.06454v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17402v1","updated":"2024-02-27T10:48:56Z","published":"2024-02-27T10:48:56Z","title":"Beacon, a lightweight deep reinforcement learning benchmark library for\n  flow control","summary":"  Recently, the increasing use of deep reinforcement learning for flow control\nproblems has led to a new area of research, focused on the coupling and the\nadaptation of the existing algorithms to the control of numerical fluid\ndynamics environments. Although still in its infancy, the field has seen\nmultiple successes in a short time span, and its fast development pace can\ncertainly be partly imparted to the open-source effort that drives the\nexpansion of the community. Yet, this emerging domain still misses a common\nground to (i) ensure the reproducibility of the results, and (ii) offer a\nproper ad-hoc benchmarking basis. To this end, we propose Beacon, an\nopen-source benchmark library composed of seven lightweight 1D and 2D flow\ncontrol problems with various characteristics, action and observation space\ncharacteristics, and CPU requirements. In this contribution, the seven\nconsidered problems are described, and reference control solutions are\nprovided. The sources for the following work are available at\nhttps://github.com/jviquerat/beacon.\n","authors":["Jonathan Viquerat","Philippe Meliga","Pablo Jeken","Elie Hachem"],"pdf_url":"https://arxiv.org/pdf/2402.17402v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17398v1","updated":"2024-02-27T10:46:36Z","published":"2024-02-27T10:46:36Z","title":"A Quantum Approach to Synthetic Minority Oversampling Technique (SMOTE)","summary":"  The paper proposes the Quantum-SMOTE method, a novel solution that uses\nquantum computing techniques to solve the prevalent problem of class imbalance\nin machine learning datasets. Quantum-SMOTE, inspired by the Synthetic Minority\nOversampling Technique (SMOTE), generates synthetic data points using quantum\nprocesses such as swap tests and quantum rotation. The process varies from the\nconventional SMOTE algorithm's usage of K-Nearest Neighbors (KNN) and Euclidean\ndistances, enabling synthetic instances to be generated from minority class\ndata points without relying on neighbor proximity. The algorithm asserts\ngreater control over the synthetic data generation process by introducing\nhyperparameters such as rotation angle, minority percentage, and splitting\nfactor, which allow for customization to specific dataset requirements. The\napproach is tested on a public dataset of TelecomChurn and evaluated alongside\ntwo prominent classification algorithms, Random Forest and Logistic Regression,\nto determine its impact along with varying proportions of synthetic data.\n","authors":["Nishikanta Mohanty","Bikash K. Behera","Christopher Ferrie"],"pdf_url":"https://arxiv.org/pdf/2402.17398v1.pdf","comment":"18 pages, 22 Figures, 2 Tables"},{"id":"http://arxiv.org/abs/2308.00214v3","updated":"2024-02-27T10:41:58Z","published":"2023-08-01T01:12:29Z","title":"The Impact of Loss Functions and Scene Representations for 3D/2D\n  Registration on Single-view Fluoroscopic X-ray Pose Estimation","summary":"  Many tasks performed in image-guided procedures can be cast as pose\nestimation problems, where specific projections are chosen to reach a target in\n3D space. In this study, we first develop a differentiable projection\n(DiffProj) rendering framework for the efficient computation of Digitally\nReconstructed Radiographs (DRRs) with automatic differentiability from either\nCone-Beam Computerized Tomography (CBCT) or neural scene representations,\nincluding two newly proposed methods, Neural Tuned Tomography (NeTT) and masked\nNeural Radiance Fields (mNeRF). We then perform pose estimation by iterative\ngradient descent using various candidate loss functions, that quantify the\nimage discrepancy of the synthesized DRR with respect to the ground-truth\nfluoroscopic X-ray image. Compared to alternative loss functions, the Mutual\nInformation loss function can significantly improve pose estimation accuracy,\nas it can effectively prevent entrapment in local optima. Using the Mutual\nInformation loss, a comprehensive evaluation of pose estimation performed on a\ntomographic X-ray dataset of 50 patients$'$ skulls shows that utilizing either\ndiscretized (CBCT) or neural (NeTT/mNeRF) scene representations in DiffProj\nleads to comparable performance in DRR appearance and pose estimation (3D angle\nerrors: mean $\\leq$ 3.2{\\deg} and 90% quantile $\\leq$ 3.4{\\deg}), despite the\nlatter often incurring considerable training expenses and time. These findings\ncould be instrumental for selecting appropriate approaches to improve the\nefficiency and effectiveness of fluoroscopic X-ray pose estimation in\nwidespread image-guided interventions.\n","authors":["Chaochao Zhou","Syed Hasib Akhter Faruqui","Abhinav Patel","Ramez N. Abdalla","Michael C. Hurley","Ali Shaibani","Matthew B. Potts","Babak S. Jahromi","Sameer A. Ansari","Donald R. Cantrell"],"pdf_url":"https://arxiv.org/pdf/2308.00214v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17390v1","updated":"2024-02-27T10:37:13Z","published":"2024-02-27T10:37:13Z","title":"Robustness-Congruent Adversarial Training for Secure Machine Learning\n  Model Updates","summary":"  Machine-learning models demand for periodic updates to improve their average\naccuracy, exploiting novel architectures and additional data. However, a\nnewly-updated model may commit mistakes that the previous model did not make.\nSuch misclassifications are referred to as negative flips, and experienced by\nusers as a regression of performance. In this work, we show that this problem\nalso affects robustness to adversarial examples, thereby hindering the\ndevelopment of secure model update practices. In particular, when updating a\nmodel to improve its adversarial robustness, some previously-ineffective\nadversarial examples may become misclassified, causing a regression in the\nperceived security of the system. We propose a novel technique, named\nrobustness-congruent adversarial training, to address this issue. It amounts to\nfine-tuning a model with adversarial training, while constraining it to retain\nhigher robustness on the adversarial examples that were correctly classified\nbefore the update. We show that our algorithm and, more generally, learning\nwith non-regression constraints, provides a theoretically-grounded framework to\ntrain consistent estimators. Our experiments on robust models for computer\nvision confirm that (i) both accuracy and robustness, even if improved after\nmodel update, can be affected by negative flips, and (ii) our\nrobustness-congruent adversarial training can mitigate the problem,\noutperforming competing baseline methods.\n","authors":["Daniele Angioni","Luca Demetrio","Maura Pintor","Luca Oneto","Davide Anguita","Battista Biggio","Fabio Roli"],"pdf_url":"https://arxiv.org/pdf/2402.17390v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.01481v3","updated":"2024-02-27T10:25:17Z","published":"2024-02-02T15:07:09Z","title":"Multi-level protein pre-training with Vabs-Net","summary":"  In recent years, there has been a surge in the development of 3D\nstructure-based pre-trained protein models, representing a significant\nadvancement over pre-trained protein language models in various downstream\ntasks. However, most existing structure-based pre-trained models primarily\nfocus on the residue level, i.e., alpha carbon atoms, while ignoring other\natoms like side chain atoms. We argue that modeling proteins at both residue\nand atom levels is important since the side chain atoms can also be crucial for\nnumerous downstream tasks, for example, molecular docking. Nevertheless, we\nfind that naively combining residue and atom information during pre-training\ntypically fails. We identify a key reason is the information leakage caused by\nthe inclusion of atom structure in the input, which renders residue-level\npre-training tasks trivial and results in insufficiently expressive residue\nrepresentations. To address this issue, we introduce a span mask pre-training\nstrategy on 3D protein chains to learn meaningful representations of both\nresidues and atoms. This leads to a simple yet effective approach to learning\nprotein representation suitable for diverse downstream tasks. Extensive\nexperimental results on binding site prediction and function prediction tasks\ndemonstrate our proposed pre-training approach significantly outperforms other\nmethods. Our code will be made public.\n","authors":["Jiale Zhao","Wanru Zhuang","Jia Song","Yaqi Li","Shuqi Lu"],"pdf_url":"https://arxiv.org/pdf/2402.01481v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17376v1","updated":"2024-02-27T10:13:30Z","published":"2024-02-27T10:13:30Z","title":"Accelerating Diffusion Sampling with Optimized Time Steps","summary":"  Diffusion probabilistic models (DPMs) have shown remarkable performance in\nhigh-resolution image synthesis, but their sampling efficiency is still to be\ndesired due to the typically large number of sampling steps. Recent\nadvancements in high-order numerical ODE solvers for DPMs have enabled the\ngeneration of high-quality images with much fewer sampling steps. While this is\na significant development, most sampling methods still employ uniform time\nsteps, which is not optimal when using a small number of steps. To address this\nissue, we propose a general framework for designing an optimization problem\nthat seeks more appropriate time steps for a specific numerical ODE solver for\nDPMs. This optimization problem aims to minimize the distance between the\nground-truth solution to the ODE and an approximate solution corresponding to\nthe numerical solver. It can be efficiently solved using the constrained trust\nregion method, taking less than $15$ seconds. Our extensive experiments on both\nunconditional and conditional sampling using pixel- and latent-space DPMs\ndemonstrate that, when combined with the state-of-the-art sampling method\nUniPC, our optimized time steps significantly improve image generation\nperformance in terms of FID scores for datasets such as CIFAR-10 and ImageNet,\ncompared to using uniform time steps.\n","authors":["Shuchen Xue","Zhaoqiang Liu","Fei Chen","Shifeng Zhang","Tianyang Hu","Enze Xie","Zhenguo Li"],"pdf_url":"https://arxiv.org/pdf/2402.17376v1.pdf","comment":"Accepted to CVPR 2024. Under camera-ready revision"},{"id":"http://arxiv.org/abs/2402.17375v1","updated":"2024-02-27T10:12:47Z","published":"2024-02-27T10:12:47Z","title":"Impact of Computation in Integral Reinforcement Learning for\n  Continuous-Time Control","summary":"  Integral reinforcement learning (IntRL) demands the precise computation of\nthe utility function's integral at its policy evaluation (PEV) stage. This is\nachieved through quadrature rules, which are weighted sums of utility functions\nevaluated from state samples obtained in discrete time. Our research reveals a\ncritical yet underexplored phenomenon: the choice of the computational method\n-- in this case, the quadrature rule -- can significantly impact control\nperformance. This impact is traced back to the fact that computational errors\nintroduced in the PEV stage can affect the policy iteration's convergence\nbehavior, which in turn affects the learned controller. To elucidate how\ncomputation impacts control, we draw a parallel between IntRL's policy\niteration and Newton's method applied to the Hamilton-Jacobi-Bellman equation.\nIn this light, computational error in PEV manifests as an extra error term in\neach iteration of Newton's method, with its upper bound proportional to the\ncomputational error. Further, we demonstrate that when the utility function\nresides in a reproducing kernel Hilbert space (RKHS), the optimal quadrature is\nachievable by employing Bayesian quadrature with the RKHS-inducing kernel\nfunction. We prove that the local convergence rates for IntRL using the\ntrapezoidal rule and Bayesian quadrature with a Mat\\'ern kernel to be\n$O(N^{-2})$ and $O(N^{-b})$, where $N$ is the number of evenly-spaced samples\nand $b$ is the Mat\\'ern kernel's smoothness parameter. These theoretical\nfindings are finally validated by two canonical control tasks.\n","authors":["Wenhan Cao","Wei Pan"],"pdf_url":"https://arxiv.org/pdf/2402.17375v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.19891v4","updated":"2024-02-27T10:07:06Z","published":"2023-05-31T14:26:14Z","title":"Dynamic Neighborhood Construction for Structured Large Discrete Action\n  Spaces","summary":"  Large discrete action spaces (LDAS) remain a central challenge in\nreinforcement learning. Existing solution approaches can handle unstructured\nLDAS with up to a few million actions. However, many real-world applications in\nlogistics, production, and transportation systems have combinatorial action\nspaces, whose size grows well beyond millions of actions, even on small\ninstances. Fortunately, such action spaces exhibit structure, e.g., equally\nspaced discrete resource units. With this work, we focus on handling structured\nLDAS (SLDAS) with sizes that cannot be handled by current benchmarks: we\npropose Dynamic Neighborhood Construction (DNC), a novel exploitation paradigm\nfor SLDAS. We present a scalable neighborhood exploration heuristic that\nutilizes this paradigm and efficiently explores the discrete neighborhood\naround the continuous proxy action in structured action spaces with up to\n$10^{73}$ actions. We demonstrate the performance of our method by benchmarking\nit against three state-of-the-art approaches designed for large discrete action\nspaces across two distinct environments. Our results show that DNC matches or\noutperforms state-of-the-art approaches while being computationally more\nefficient. Furthermore, our method scales to action spaces that so far remained\ncomputationally intractable for existing methodologies.\n","authors":["Fabian Akkerman","Julius Luy","Wouter van Heeswijk","Maximilian Schiffer"],"pdf_url":"https://arxiv.org/pdf/2305.19891v4.pdf","comment":"ICLR 2024 Camera ready version.\n  https://openreview.net/forum?id=80wh3jjCZf"},{"id":"http://arxiv.org/abs/2402.17363v1","updated":"2024-02-27T09:55:34Z","published":"2024-02-27T09:55:34Z","title":"CGGM: A conditional graph generation model with adaptive sparsity for\n  node anomaly detection in IoT networks","summary":"  Dynamic graphs are extensively employed for detecting anomalous behavior in\nnodes within the Internet of Things (IoT). Generative models are often used to\naddress the issue of imbalanced node categories in dynamic graphs.\nNevertheless, the constraints it faces include the monotonicity of adjacency\nrelationships, the difficulty in constructing multi-dimensional features for\nnodes, and the lack of a method for end-to-end generation of multiple\ncategories of nodes. This paper presents a novel graph generation model, called\nCGGM, designed specifically to generate a larger number of nodes belonging to\nthe minority class. The mechanism for generating an adjacency matrix, through\nadaptive sparsity, enhances flexibility in its structure. The feature\ngeneration module, called multidimensional features generator (MFG) to generate\nnode features along with topological information. Labels are transformed into\nembedding vectors, serving as conditional constraints to control the generation\nof synthetic data across multiple categories. Using a multi-stage loss, the\ndistribution of synthetic data is adjusted to closely resemble that of real\ndata. In extensive experiments, we show that CGGM's synthetic data outperforms\nstate-of-the-art methods across various metrics. Our results demonstrate\nefficient generation of diverse data categories, robustly enhancing\nmulti-category classification model performance.\n","authors":["Xianshi Su","Munan Li","Tongbang Jiang","Hao Long"],"pdf_url":"https://arxiv.org/pdf/2402.17363v1.pdf","comment":"13 pages, 19 figures"},{"id":"http://arxiv.org/abs/2302.01622v4","updated":"2024-02-27T09:42:10Z","published":"2023-02-03T09:49:13Z","title":"Private, fair and accurate: Training large-scale, privacy-preserving AI\n  models in medical imaging","summary":"  Artificial intelligence (AI) models are increasingly used in the medical\ndomain. However, as medical data is highly sensitive, special precautions to\nensure its protection are required. The gold standard for privacy preservation\nis the introduction of differential privacy (DP) to model training. Prior work\nindicates that DP has negative implications on model accuracy and fairness,\nwhich are unacceptable in medicine and represent a main barrier to the\nwidespread use of privacy-preserving techniques. In this work, we evaluated the\neffect of privacy-preserving training of AI models regarding accuracy and\nfairness compared to non-private training. For this, we used two datasets: (1)\nA large dataset (N=193,311) of high quality clinical chest radiographs, and (2)\na dataset (N=1,625) of 3D abdominal computed tomography (CT) images, with the\ntask of classifying the presence of pancreatic ductal adenocarcinoma (PDAC).\nBoth were retrospectively collected and manually labeled by experienced\nradiologists. We then compared non-private deep convolutional neural networks\n(CNNs) and privacy-preserving (DP) models with respect to privacy-utility\ntrade-offs measured as area under the receiver-operator-characteristic curve\n(AUROC), and privacy-fairness trade-offs, measured as Pearson's r or\nStatistical Parity Difference. We found that, while the privacy-preserving\ntrainings yielded lower accuracy, they did largely not amplify discrimination\nagainst age, sex or co-morbidity. Our study shows that -- under the challenging\nrealistic circumstances of a real-life clinical dataset -- the\nprivacy-preserving training of diagnostic deep learning models is possible with\nexcellent diagnostic accuracy and fairness.\n","authors":["Soroosh Tayebi Arasteh","Alexander Ziller","Christiane Kuhl","Marcus Makowski","Sven Nebelung","Rickmer Braren","Daniel Rueckert","Daniel Truhn","Georgios Kaissis"],"pdf_url":"https://arxiv.org/pdf/2302.01622v4.pdf","comment":"Published in Communications Medicine. Nature Portfolio"},{"id":"http://arxiv.org/abs/2310.19608v3","updated":"2024-02-27T09:35:00Z","published":"2023-10-30T15:03:15Z","title":"On Feynman--Kac training of partial Bayesian neural networks","summary":"  Recently, partial Bayesian neural networks (pBNNs), which only consider a\nsubset of the parameters to be stochastic, were shown to perform competitively\nwith full Bayesian neural networks. However, pBNNs are often multi-modal in the\nlatent variable space and thus challenging to approximate with parametric\nmodels. To address this problem, we propose an efficient sampling-based\ntraining strategy, wherein the training of a pBNN is formulated as simulating a\nFeynman--Kac model. We then describe variations of sequential Monte Carlo\nsamplers that allow us to simultaneously estimate the parameters and the latent\nposterior distribution of this model at a tractable computational cost. Using\nvarious synthetic and real-world datasets we show that our proposed training\nscheme outperforms the state of the art in terms of predictive performance.\n","authors":["Zheng Zhao","Sebastian Mair","Thomas B. Schön","Jens Sjölund"],"pdf_url":"https://arxiv.org/pdf/2310.19608v3.pdf","comment":"In AISTATS 2024"},{"id":"http://arxiv.org/abs/2402.17346v1","updated":"2024-02-27T09:27:54Z","published":"2024-02-27T09:27:54Z","title":"Understanding the training of PINNs for unsteady flow past a plunging\n  foil through the lens of input subdomain level loss function gradients","summary":"  Recently immersed boundary method-inspired physics-informed neural networks\n(PINNs) including the moving boundary-enabled PINNs (MB-PINNs) have shown the\nability to accurately reconstruct velocity and recover pressure as a hidden\nvariable for unsteady flow past moving bodies. Considering flow past a plunging\nfoil, MB-PINNs were trained with global physics loss relaxation and also in\nconjunction with a physics-based undersampling method, obtaining good accuracy.\nThe purpose of this study was to investigate which input spatial subdomain\ncontributes to the training under the effect of physics loss relaxation and\nphysics-based undersampling. In the context of MB-PINNs training, three spatial\nzones: the moving body, wake, and outer zones were defined. To quantify which\nspatial zone drives the training, two novel metrics are computed from the zonal\nloss component gradient statistics and the proportion of sample points in each\nzone. Results confirm that the learning indeed depends on the combined effect\nof the zonal loss component gradients and the proportion of points in each\nzone. Moreover, the dominant input zones are also the ones that have the\nstrongest solution gradients in some sense.\n","authors":["Rahul Sundar","Didier Lucor","Sunetra Sarkar"],"pdf_url":"https://arxiv.org/pdf/2402.17346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17345v1","updated":"2024-02-27T09:23:54Z","published":"2024-02-27T09:23:54Z","title":"LocalGCL: Local-aware Contrastive Learning for Graphs","summary":"  Graph representation learning (GRL) makes considerable progress recently,\nwhich encodes graphs with topological structures into low-dimensional\nembeddings. Meanwhile, the time-consuming and costly process of annotating\ngraph labels manually prompts the growth of self-supervised learning (SSL)\ntechniques. As a dominant approach of SSL, Contrastive learning (CL) learns\ndiscriminative representations by differentiating between positive and negative\nsamples. However, when applied to graph data, it overemphasizes global patterns\nwhile neglecting local structures. To tackle the above issue, we propose\n\\underline{Local}-aware \\underline{G}raph \\underline{C}ontrastive\n\\underline{L}earning (\\textbf{\\methnametrim}), a self-supervised learning\nframework that supplementarily captures local graph information with\nmasking-based modeling compared with vanilla contrastive learning. Extensive\nexperiments validate the superiority of \\methname against state-of-the-art\nmethods, demonstrating its promise as a comprehensive graph representation\nlearner.\n","authors":["Haojun Jiang","Jiawei Sun","Jie Li","Chentao Wu"],"pdf_url":"https://arxiv.org/pdf/2402.17345v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17343v1","updated":"2024-02-27T09:23:13Z","published":"2024-02-27T09:23:13Z","title":"Enhanced Bayesian Optimization via Preferential Modeling of Abstract\n  Properties","summary":"  Experimental (design) optimization is a key driver in designing and\ndiscovering new products and processes. Bayesian Optimization (BO) is an\neffective tool for optimizing expensive and black-box experimental design\nprocesses. While Bayesian optimization is a principled data-driven approach to\nexperimental optimization, it learns everything from scratch and could greatly\nbenefit from the expertise of its human (domain) experts who often reason about\nsystems at different abstraction levels using physical properties that are not\nnecessarily directly measured (or measurable). In this paper, we propose a\nhuman-AI collaborative Bayesian framework to incorporate expert preferences\nabout unmeasured abstract properties into the surrogate modeling to further\nboost the performance of BO. We provide an efficient strategy that can also\nhandle any incorrect/misleading expert bias in preferential judgments. We\ndiscuss the convergence behavior of our proposed framework. Our experimental\nresults involving synthetic functions and real-world datasets show the\nsuperiority of our method against the baselines.\n","authors":["Arun Kumar A V","Alistair Shilton","Sunil Gupta","Santu Rana","Stewart Greenhill","Svetha Venkatesh"],"pdf_url":"https://arxiv.org/pdf/2402.17343v1.pdf","comment":"19 Pages, 6 Figures"},{"id":"http://arxiv.org/abs/2402.17336v1","updated":"2024-02-27T09:11:10Z","published":"2024-02-27T09:11:10Z","title":"Outdoor Environment Reconstruction with Deep Learning on Radio\n  Propagation Paths","summary":"  Conventional methods for outdoor environment reconstruction rely\npredominantly on vision-based techniques like photogrammetry and LiDAR, facing\nlimitations such as constrained coverage, susceptibility to environmental\nconditions, and high computational and energy demands. These challenges are\nparticularly pronounced in applications like augmented reality navigation,\nespecially when integrated with wearable devices featuring constrained\ncomputational resources and energy budgets. In response, this paper proposes a\nnovel approach harnessing ambient wireless signals for outdoor environment\nreconstruction. By analyzing radio frequency (RF) data, the paper aims to\ndeduce the environmental characteristics and digitally reconstruct the outdoor\nsurroundings. Investigating the efficacy of selected deep learning (DL)\ntechniques on the synthetic RF dataset WAIR-D, the study endeavors to address\nthe research gap in this domain. Two DL-driven approaches are evaluated\n(convolutional U-Net and CLIP+ based on vision transformers), with performance\nassessed using metrics like intersection-over-union (IoU), Hausdorff distance,\nand Chamfer distance. The results demonstrate promising performance of the\nRF-based reconstruction method, paving the way towards lightweight and scalable\nreconstruction solutions.\n","authors":["Hrant Khachatrian","Rafayel Mkrtchyan","Theofanis P. Raptis"],"pdf_url":"https://arxiv.org/pdf/2402.17336v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible. Work partly supported by the RA Science Committee grant\n  No. 22rl-052 (DISTAL) and the EU under Italian National Recovery and\n  Resilience Plan of NextGenerationEU on \"Telecommunications of the Future\"\n  (PE00000001 - program \"RESTART\")"},{"id":"http://arxiv.org/abs/2310.11714v3","updated":"2024-02-27T09:06:43Z","published":"2023-10-18T05:06:04Z","title":"On the Evaluation of Generative Models in Distributed Learning Tasks","summary":"  The evaluation of deep generative models including generative adversarial\nnetworks (GANs) and diffusion models has been extensively studied in the\nliterature. While the existing evaluation methods mainly target a centralized\nlearning problem with training data stored by a single client, many\napplications of generative models concern distributed learning settings, e.g.\nthe federated learning scenario, where training data are collected by and\ndistributed among several clients. In this paper, we study the evaluation of\ngenerative models in distributed learning tasks with heterogeneous data\ndistributions. First, we focus on the Fr\\'echet inception distance (FID) and\nconsider the following FID-based aggregate scores over the clients: 1) FID-avg\nas the mean of clients' individual FID scores, 2) FID-all as the FID distance\nof the trained model to the collective dataset containing all clients' data. We\nprove that the model rankings according to the FID-all and FID-avg scores could\nbe inconsistent, which can lead to different optimal generative models\naccording to the two aggregate scores. Next, we consider the kernel inception\ndistance (KID) and similarly define the KID-avg and KID-all aggregations.\nUnlike the FID case, we prove that KID-all and KID-avg result in the same\nrankings of generative models. We perform several numerical experiments on\nstandard image datasets and training schemes to support our theoretical\nfindings on the evaluation of generative models in distributed learning\nproblems.\n","authors":["Zixiao Wang","Farzan Farnia","Zhenghao Lin","Yunheng Shen","Bei Yu"],"pdf_url":"https://arxiv.org/pdf/2310.11714v3.pdf","comment":"20 pages, 20 figures"},{"id":"http://arxiv.org/abs/2402.17327v1","updated":"2024-02-27T09:03:43Z","published":"2024-02-27T09:03:43Z","title":"Data-Efficient Learning via Clustering-Based Sensitivity Sampling:\n  Foundation Models and Beyond","summary":"  We study the data selection problem, whose aim is to select a small\nrepresentative subset of data that can be used to efficiently train a machine\nlearning model. We present a new data selection approach based on $k$-means\nclustering and sensitivity sampling. Assuming access to an embedding\nrepresentation of the data with respect to which the model loss is H\\\"older\ncontinuous, our approach provably allows selecting a set of ``typical'' $k +\n1/\\varepsilon^2$ elements whose average loss corresponds to the average loss of\nthe whole dataset, up to a multiplicative $(1\\pm\\varepsilon)$ factor and an\nadditive $\\varepsilon \\lambda \\Phi_k$, where $\\Phi_k$ represents the $k$-means\ncost for the input embeddings and $\\lambda$ is the H\\\"older constant.\n  We furthermore demonstrate the performance and scalability of our approach on\nfine-tuning foundation models and show that it outperforms state-of-the-art\nmethods. We also show how it can be applied on linear regression, leading to a\nnew sampling strategy that surprisingly matches the performances of leverage\nscore sampling, while being conceptually simpler and more scalable.\n","authors":["Kyriakos Axiotis","Vincent Cohen-Addad","Monika Henzinger","Sammy Jerome","Vahab Mirrokni","David Saulpic","David Woodruff","Michael Wunder"],"pdf_url":"https://arxiv.org/pdf/2402.17327v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.01657v2","updated":"2024-02-27T08:53:49Z","published":"2023-09-04T15:16:55Z","title":"Locally Stationary Graph Processes","summary":"  Stationary graph process models are commonly used in the analysis and\ninference of data sets collected on irregular network topologies. While most of\nthe existing methods represent graph signals with a single stationary process\nmodel that is globally valid on the entire graph, in many practical problems,\nthe characteristics of the process may be subject to local variations in\ndifferent regions of the graph. In this work, we propose a locally stationary\ngraph process (LSGP) model that aims to extend the classical concept of local\nstationarity to irregular graph domains. We characterize local stationarity by\nexpressing the overall process as the combination of a set of component\nprocesses such that the extent to which the process adheres to each component\nvaries smoothly over the graph. We propose an algorithm for computing LSGP\nmodels from realizations of the process, and also study the approximation of\nLSGPs locally with WSS processes. Experiments on signal interpolation problems\nshow that the proposed process model provides accurate signal representations\ncompetitive with the state of the art.\n","authors":["Abdullah Canbolat","Elif Vural"],"pdf_url":"https://arxiv.org/pdf/2309.01657v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17318v1","updated":"2024-02-27T08:50:45Z","published":"2024-02-27T08:50:45Z","title":"Scaling Supervised Local Learning with Augmented Auxiliary Networks","summary":"  Deep neural networks are typically trained using global error signals that\nbackpropagate (BP) end-to-end, which is not only biologically implausible but\nalso suffers from the update locking problem and requires huge memory\nconsumption. Local learning, which updates each layer independently with a\ngradient-isolated auxiliary network, offers a promising alternative to address\nthe above problems. However, existing local learning methods are confronted\nwith a large accuracy gap with the BP counterpart, particularly for large-scale\nnetworks. This is due to the weak coupling between local layers and their\nsubsequent network layers, as there is no gradient communication across layers.\nTo tackle this issue, we put forward an augmented local learning method, dubbed\nAugLocal. AugLocal constructs each hidden layer's auxiliary network by\nuniformly selecting a small subset of layers from its subsequent network layers\nto enhance their synergy. We also propose to linearly reduce the depth of\nauxiliary networks as the hidden layer goes deeper, ensuring sufficient network\ncapacity while reducing the computational cost of auxiliary networks. Our\nextensive experiments on four image classification datasets (i.e., CIFAR-10,\nSVHN, STL-10, and ImageNet) demonstrate that AugLocal can effectively scale up\nto tens of local layers with a comparable accuracy to BP-trained networks while\nreducing GPU memory usage by around 40%. The proposed AugLocal method,\ntherefore, opens up a myriad of opportunities for training high-performance\ndeep neural networks on resource-constrained platforms.Code is available at\nhttps://github.com/ChenxiangMA/AugLocal.\n","authors":["Chenxiang Ma","Jibin Wu","Chenyang Si","Kay Chen Tan"],"pdf_url":"https://arxiv.org/pdf/2402.17318v1.pdf","comment":"Accepted by ICLR 2024"},{"id":"http://arxiv.org/abs/2402.17317v1","updated":"2024-02-27T08:49:30Z","published":"2024-02-27T08:49:30Z","title":"How we won BraTS 2023 Adult Glioma challenge? Just faking it! Enhanced\n  Synthetic Data Augmentation and Model Ensemble for brain tumour segmentation","summary":"  Deep Learning is the state-of-the-art technology for segmenting brain\ntumours. However, this requires a lot of high-quality data, which is difficult\nto obtain, especially in the medical field. Therefore, our solutions address\nthis problem by using unconventional mechanisms for data augmentation.\nGenerative adversarial networks and registration are used to massively increase\nthe amount of available samples for training three different deep learning\nmodels for brain tumour segmentation, the first task of the BraTS2023\nchallenge. The first model is the standard nnU-Net, the second is the Swin\nUNETR and the third is the winning solution of the BraTS 2021 Challenge. The\nentire pipeline is built on the nnU-Net implementation, except for the\ngeneration of the synthetic data. The use of convolutional algorithms and\ntransformers is able to fill each other's knowledge gaps. Using the new metric,\nour best solution achieves the dice results 0.9005, 0.8673, 0.8509 and HD95\n14.940, 14.467, 17.699 (whole tumour, tumour core and enhancing tumour) in the\nvalidation set.\n","authors":["André Ferreira","Naida Solak","Jianning Li","Philipp Dammann","Jens Kleesiek","Victor Alves","Jan Egger"],"pdf_url":"https://arxiv.org/pdf/2402.17317v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14228v2","updated":"2024-02-27T08:47:37Z","published":"2024-02-22T02:20:08Z","title":"COPR: Continual Human Preference Learning via Optimal Policy\n  Regularization","summary":"  Reinforcement Learning from Human Feedback (RLHF) is commonly utilized to\nimprove the alignment of Large Language Models (LLMs) with human preferences.\nGiven the evolving nature of human preferences, continual alignment becomes\nmore crucial and practical in comparison to traditional static alignment.\nNevertheless, making RLHF compatible with Continual Learning (CL) is\nchallenging due to its complex process. Meanwhile, directly learning new human\npreferences may lead to Catastrophic Forgetting (CF) of historical preferences,\nresulting in helpless or harmful outputs. To overcome these challenges, we\npropose the Continual Optimal Policy Regularization (COPR) method, which draws\ninspiration from the optimal policy theory. COPR utilizes a sampling\ndistribution as a demonstration and regularization constraints for CL. It\nadopts the Lagrangian Duality (LD) method to dynamically regularize the current\npolicy based on the historically optimal policy, which prevents CF and avoids\nover-emphasizing unbalanced objectives. We also provide formal proof for the\nlearnability of COPR. The experimental results show that COPR outperforms\nstrong CL baselines on our proposed benchmark, in terms of reward-based, GPT-4\nevaluations and human assessment. Furthermore, we validate the robustness of\nCOPR under various CL settings, including different backbones, replay memory\nsizes, and learning orders.\n","authors":["Han Zhang","Lin Gui","Yu Lei","Yuanzhao Zhai","Yehong Zhang","Yulan He","Hui Wang","Yue Yu","Kam-Fai Wong","Bin Liang","Ruifeng Xu"],"pdf_url":"https://arxiv.org/pdf/2402.14228v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06958v4","updated":"2024-02-27T08:34:43Z","published":"2023-10-10T19:21:41Z","title":"Comparing the Robustness of Modern No-Reference Image- and Video-Quality\n  Metrics to Adversarial Attacks","summary":"  Nowadays, neural-network-based image- and video-quality metrics perform\nbetter than traditional methods. However, they also became more vulnerable to\nadversarial attacks that increase metrics' scores without improving visual\nquality. The existing benchmarks of quality metrics compare their performance\nin terms of correlation with subjective quality and calculation time.\nNonetheless, the adversarial robustness of image-quality metrics is also an\narea worth researching. This paper analyses modern metrics' robustness to\ndifferent adversarial attacks. We adapted adversarial attacks from computer\nvision tasks and compared attacks' efficiency against 15 no-reference image-\nand video-quality metrics. Some metrics showed high resistance to adversarial\nattacks, which makes their usage in benchmarks safer than vulnerable metrics.\nThe benchmark accepts submissions of new metrics for researchers who want to\nmake their metrics more robust to attacks or to find such metrics for their\nneeds. The latest results can be found online:\nhttps://videoprocessing.ai/benchmarks/metrics-robustness.html.\n","authors":["Anastasia Antsiferova","Khaled Abud","Aleksandr Gushchin","Ekaterina Shumitskaya","Sergey Lavrushkin","Dmitriy Vatolin"],"pdf_url":"https://arxiv.org/pdf/2310.06958v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15183v2","updated":"2024-02-27T08:22:11Z","published":"2024-02-23T08:29:42Z","title":"GraphEdit: Large Language Models for Graph Structure Learning","summary":"  Graph Structure Learning (GSL) focuses on capturing intrinsic dependencies\nand interactions among nodes in graph-structured data by generating novel graph\nstructures. Graph Neural Networks (GNNs) have emerged as promising GSL\nsolutions, utilizing recursive message passing to encode node-wise\ninter-dependencies. However, many existing GSL methods heavily depend on\nexplicit graph structural information as supervision signals, leaving them\nsusceptible to challenges such as data noise and sparsity. In this work, we\npropose GraphEdit, an approach that leverages large language models (LLMs) to\nlearn complex node relationships in graph-structured data. By enhancing the\nreasoning capabilities of LLMs through instruction-tuning over graph\nstructures, we aim to overcome the limitations associated with explicit graph\nstructural information and enhance the reliability of graph structure learning.\nOur approach not only effectively denoises noisy connections but also\nidentifies node-wise dependencies from a global perspective, providing a\ncomprehensive understanding of the graph structure. We conduct extensive\nexperiments on multiple benchmark datasets to demonstrate the effectiveness and\nrobustness of GraphEdit across various settings.\n","authors":["Zirui Guo","Lianghao Xia","Yanhua Yu","Yuling Wang","Zixuan Yang","Wei Wei","Liang Pang","Tat-Seng Chua","Chao Huang"],"pdf_url":"https://arxiv.org/pdf/2402.15183v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.19944v2","updated":"2024-02-27T08:16:32Z","published":"2023-10-30T18:59:32Z","title":"Conditional Unscented Autoencoders for Trajectory Prediction","summary":"  The CVAE is one of the most widely-used models in trajectory prediction for\nAD. It captures the interplay between a driving context and its ground-truth\nfuture into a probabilistic latent space and uses it to produce predictions. In\nthis paper, we challenge key components of the CVAE. We leverage recent\nadvances in the space of the VAE, the foundation of the CVAE, which show that a\nsimple change in the sampling procedure can greatly benefit performance. We\nfind that unscented sampling, which draws samples from any learned distribution\nin a deterministic manner, can naturally be better suited to trajectory\nprediction than potentially dangerous random sampling. We go further and offer\nadditional improvements including a more structured Gaussian mixture latent\nspace, as well as a novel, potentially more expressive way to do inference with\nCVAEs. We show wide applicability of our models by evaluating them on the\nINTERACTION prediction dataset, outperforming the state of the art, as well as\nat the task of image modeling on the CelebA dataset, outperforming the baseline\nvanilla CVAE. Code is available at\nhttps://github.com/boschresearch/cuae-prediction.\n","authors":["Faris Janjoš","Marcel Hallgarten","Anthony Knittel","Maxim Dolgov","Andreas Zell","J. Marius Zöllner"],"pdf_url":"https://arxiv.org/pdf/2310.19944v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17295v1","updated":"2024-02-27T08:16:17Z","published":"2024-02-27T08:16:17Z","title":"Quantum Distance Approximation for Persistence Diagrams","summary":"  Topological Data Analysis methods can be useful for classification and\nclustering tasks in many different fields as they can provide two dimensional\npersistence diagrams that summarize important information about the shape of\npotentially complex and high dimensional data sets. The space of persistence\ndiagrams can be endowed with various metrics such as the Wasserstein distance\nwhich admit a statistical structure and allow to use these summaries for\nmachine learning algorithms. However, computing the distance between two\npersistence diagrams involves finding an optimal way to match the points of the\ntwo diagrams and may not always be an easy task for classical computers. In\nthis work we explore the potential of quantum computers to estimate the\ndistance between persistence diagrams, in particular we propose variational\nquantum algorithms for the Wasserstein distance as well as the $d^{c}_{p}$\ndistance. Our implementation is a weighted version of the Quantum Approximate\nOptimization Algorithm that relies on control clauses to encode the constraints\nof the optimization problem.\n","authors":["Bernardo Ameneyro","Rebekah Herrman","George Siopsis","Vasileios Maroulas"],"pdf_url":"https://arxiv.org/pdf/2402.17295v1.pdf","comment":"24 pages, 11 figures, submitted to SIAM Journal on Computing"},{"id":"http://arxiv.org/abs/2311.10801v4","updated":"2024-02-27T08:08:03Z","published":"2023-11-17T09:16:59Z","title":"Reinforcement Learning with Maskable Stock Representation for Portfolio\n  Management in Customizable Stock Pools","summary":"  Portfolio management (PM) is a fundamental financial trading task, which\nexplores the optimal periodical reallocation of capitals into different stocks\nto pursue long-term profits. Reinforcement learning (RL) has recently shown its\npotential to train profitable agents for PM through interacting with financial\nmarkets. However, existing work mostly focuses on fixed stock pools, which is\ninconsistent with investors' practical demand. Specifically, the target stock\npool of different investors varies dramatically due to their discrepancy on\nmarket states and individual investors may temporally adjust stocks they desire\nto trade (e.g., adding one popular stocks), which lead to customizable stock\npools (CSPs). Existing RL methods require to retrain RL agents even with a tiny\nchange of the stock pool, which leads to high computational cost and unstable\nperformance. To tackle this challenge, we propose EarnMore, a rEinforcement\nleARNing framework with Maskable stOck REpresentation to handle PM with CSPs\nthrough one-shot training in a global stock pool (GSP). Specifically, we first\nintroduce a mechanism to mask out the representation of the stocks outside the\ntarget pool. Second, we learn meaningful stock representations through a\nself-supervised masking and reconstruction process. Third, a re-weighting\nmechanism is designed to make the portfolio concentrate on favorable stocks and\nneglect the stocks outside the target pool. Through extensive experiments on 8\nsubset stock pools of the US stock market, we demonstrate that EarnMore\nsignificantly outperforms 14 state-of-the-art baselines in terms of 6 popular\nfinancial metrics with over 40% improvement on profit.\n","authors":["Wentao Zhang","Yilei Zhao","Shuo Sun","Jie Ying","Yonggang Xie","Zitao Song","Xinrun Wang","Bo An"],"pdf_url":"https://arxiv.org/pdf/2311.10801v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17287v1","updated":"2024-02-27T08:00:52Z","published":"2024-02-27T08:00:52Z","title":"An Interpretable Evaluation of Entropy-based Novelty of Generative\n  Models","summary":"  The massive developments of generative model frameworks and architectures\nrequire principled methods for the evaluation of a model's novelty compared to\na reference dataset or baseline generative models. While the recent literature\nhas extensively studied the evaluation of the quality, diversity, and\ngeneralizability of generative models, the assessment of a model's novelty\ncompared to a baseline model has not been adequately studied in the machine\nlearning community. In this work, we focus on the novelty assessment under\nmulti-modal generative models and attempt to answer the following question:\nGiven the samples of a generative model $\\mathcal{G}$ and a reference dataset\n$\\mathcal{S}$, how can we discover and count the modes expressed by\n$\\mathcal{G}$ more frequently than in $\\mathcal{S}$. We introduce a spectral\napproach to the described task and propose the Kernel-based Entropic Novelty\n(KEN) score to quantify the mode-based novelty of distribution $P_\\mathcal{G}$\nwith respect to distribution $P_\\mathcal{S}$. We analytically interpret the\nbehavior of the KEN score under mixture distributions with sub-Gaussian\ncomponents. Next, we develop a method based on Cholesky decomposition to\ncompute the KEN score from observed samples. We support the KEN-based\nquantification of novelty by presenting several numerical results on synthetic\nand real image distributions. Our numerical results indicate the success of the\nproposed approach in detecting the novel modes and the comparison of\nstate-of-the-art generative models.\n","authors":["Jingwei Zhang","Cheuk Ting Li","Farzan Farnia"],"pdf_url":"https://arxiv.org/pdf/2402.17287v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03563v2","updated":"2024-02-27T07:37:08Z","published":"2024-02-05T22:22:49Z","title":"Distinguishing the Knowable from the Unknowable with Language Models","summary":"  We study the feasibility of identifying epistemic uncertainty (reflecting a\nlack of knowledge), as opposed to aleatoric uncertainty (reflecting entropy in\nthe underlying distribution), in the outputs of large language models (LLMs)\nover free-form text. In the absence of ground-truth probabilities, we explore a\nsetting where, in order to (approximately) disentangle a given LLM's\nuncertainty, a significantly larger model stands in as a proxy for the ground\ntruth. We show that small linear probes trained on the embeddings of frozen,\npretrained models accurately predict when larger models will be more confident\nat the token level and that probes trained on one text domain generalize to\nothers. Going further, we propose a fully unsupervised method that achieves\nnon-trivial accuracy on the same task. Taken together, we interpret these\nresults as evidence that LLMs naturally contain internal representations of\ndifferent types of uncertainty that could potentially be leveraged to devise\nmore informative indicators of model confidence in diverse practical settings.\n","authors":["Gustaf Ahdritz","Tian Qin","Nikhil Vyas","Boaz Barak","Benjamin L. Edelman"],"pdf_url":"https://arxiv.org/pdf/2402.03563v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.06617v4","updated":"2024-02-27T07:36:52Z","published":"2022-11-12T09:41:02Z","title":"Empirical Risk Minimization with Relative Entropy Regularization","summary":"  The empirical risk minimization (ERM) problem with relative entropy\nregularization (ERM-RER) is investigated under the assumption that the\nreference measure is a $\\sigma$-finite measure, and not necessarily a\nprobability measure. Under this assumption, which leads to a generalization of\nthe ERM-RER problem allowing a larger degree of flexibility for incorporating\nprior knowledge, numerous relevant properties are stated. Among these\nproperties, the solution to this problem, if it exists, is shown to be a unique\nprobability measure, mutually absolutely continuous with the reference measure.\nSuch a solution exhibits a probably-approximately-correct guarantee for the ERM\nproblem independently of whether the latter possesses a solution. For a fixed\ndataset and under a specific condition, the empirical risk is shown to be a\nsub-Gaussian random variable when the models are sampled from the solution to\nthe ERM-RER problem. The generalization capabilities of the solution to the\nERM-RER problem (the Gibbs algorithm) are studied via the sensitivity of the\nexpected empirical risk to deviations from such a solution towards alternative\nprobability measures. Finally, an interesting connection between sensitivity,\ngeneralization error, and lautum information is established.\n","authors":["Samir M. Perlaza","Gaetan Bisson","Iñaki Esnaola","Alain Jean-Marie","Stefano Rini"],"pdf_url":"https://arxiv.org/pdf/2211.06617v4.pdf","comment":"To appear in the IEEE Transactions on Information Theory: Submitted\n  June 2023. Revised in October 2023. Accepted January 2024. Also available as:\n  Research Report, INRIA, No. RR-9454, Centre Inria d'Universit\\'e C\\^ote\n  d'Azur, Sophia Antipolis, France, Feb., 2022. Last version: Version 7"},{"id":"http://arxiv.org/abs/2402.17270v1","updated":"2024-02-27T07:31:30Z","published":"2024-02-27T07:31:30Z","title":"Multi-Agent, Human-Agent and Beyond: A Survey on Cooperation in Social\n  Dilemmas","summary":"  The study of cooperation within social dilemmas has long been a fundamental\ntopic across various disciplines, including computer science and social\nscience. Recent advancements in Artificial Intelligence (AI) have significantly\nreshaped this field, offering fresh insights into understanding and enhancing\ncooperation. This survey examines three key areas at the intersection of AI and\ncooperation in social dilemmas. First, focusing on multi-agent cooperation, we\nreview the intrinsic and external motivations that support cooperation among\nrational agents, and the methods employed to develop effective strategies\nagainst diverse opponents. Second, looking into human-agent cooperation, we\ndiscuss the current AI algorithms for cooperating with humans and the human\nbiases towards AI agents. Third, we review the emergent field of leveraging AI\nagents to enhance cooperation among humans. We conclude by discussing future\nresearch avenues, such as using large language models, establishing unified\ntheoretical frameworks, revisiting existing theories of human cooperation, and\nexploring multiple real-world applications.\n","authors":["Hao Guo","Chunjiang Mu","Yang Chen","Chen Shen","Shuyue Hu","Zhen Wang"],"pdf_url":"https://arxiv.org/pdf/2402.17270v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17269v1","updated":"2024-02-27T07:28:05Z","published":"2024-02-27T07:28:05Z","title":"Curriculum Learning Meets Directed Acyclic Graph for Multimodal Emotion\n  Recognition","summary":"  Emotion recognition in conversation (ERC) is a crucial task in natural\nlanguage processing and affective computing. This paper proposes MultiDAG+CL, a\nnovel approach for Multimodal Emotion Recognition in Conversation (ERC) that\nemploys Directed Acyclic Graph (DAG) to integrate textual, acoustic, and visual\nfeatures within a unified framework. The model is enhanced by Curriculum\nLearning (CL) to address challenges related to emotional shifts and data\nimbalance. Curriculum learning facilitates the learning process by gradually\npresenting training samples in a meaningful order, thereby improving the\nmodel's performance in handling emotional variations and data imbalance.\nExperimental results on the IEMOCAP and MELD datasets demonstrate that the\nMultiDAG+CL models outperform baseline models.\n","authors":["Cam-Van Thi Nguyen","Cao-Bach Nguyen","Quang-Thuy Ha","Duc-Trong Le"],"pdf_url":"https://arxiv.org/pdf/2402.17269v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2402.13516v2","updated":"2024-02-27T07:27:07Z","published":"2024-02-21T03:58:49Z","title":"ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity\n  within Large Language Models","summary":"  Activation sparsity refers to the existence of considerable\nweakly-contributed elements among activation outputs. As a prevalent property\nof the models using the ReLU activation function, it has been proven a\npromising paradigm to boost model inference efficiency. Nevertheless, most\nlarge language models (LLMs) adopt activation functions without intrinsic\nactivation sparsity (e.g., GELU and Swish). Some recent efforts have explored\nintroducing ReLU or its variants as the substitutive activation function to\nhelp LLMs achieve activation sparsity and inference acceleration, but few can\nsimultaneously obtain high sparsity and comparable model performance. This\npaper introduces an effective sparsification method named \"ProSparse\" to push\nLLMs for higher activation sparsity without decreasing model performance.\nSpecifically, after substituting the activation function of LLMs with ReLU,\nProSparse adopts progressive sparsity regularization with a factor smoothly\nincreasing along sine curves in multiple stages. This can enhance activation\nsparsity and alleviate performance degradation by avoiding radical shifts in\nactivation distribution. With ProSparse, we obtain high sparsity of 89.32% and\n88.80% for LLaMA2-7B and LLaMA2-13B, respectively, achieving comparable\nperformance to their original Swish-activated versions. Our inference\nacceleration experiments further demonstrate the practical acceleration brought\nby higher activation sparsity.\n","authors":["Chenyang Song","Xu Han","Zhengyan Zhang","Shengding Hu","Xiyu Shi","Kuai Li","Chen Chen","Zhiyuan Liu","Guangli Li","Tao Yang","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2402.13516v2.pdf","comment":"16 pages, 3 figures, 7 tables"},{"id":"http://arxiv.org/abs/2402.17257v1","updated":"2024-02-27T07:03:25Z","published":"2024-02-27T07:03:25Z","title":"RIME: Robust Preference-based Reinforcement Learning with Noisy\n  Preferences","summary":"  Preference-based Reinforcement Learning (PbRL) avoids the need for reward\nengineering by harnessing human preferences as the reward signal. However,\ncurrent PbRL algorithms over-reliance on high-quality feedback from domain\nexperts, which results in a lack of robustness. In this paper, we present RIME,\na robust PbRL algorithm for effective reward learning from noisy preferences.\nOur method incorporates a sample selection-based discriminator to dynamically\nfilter denoised preferences for robust training. To mitigate the accumulated\nerror caused by incorrect selection, we propose to warm start the reward model,\nwhich additionally bridges the performance gap during transition from\npre-training to online training in PbRL. Our experiments on robotic\nmanipulation and locomotion tasks demonstrate that RIME significantly enhances\nthe robustness of the current state-of-the-art PbRL method. Ablation studies\nfurther demonstrate that the warm start is crucial for both robustness and\nfeedback-efficiency in limited-feedback cases.\n","authors":["Jie Cheng","Gang Xiong","Xingyuan Dai","Qinghai Miao","Yisheng Lv","Fei-Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2402.17257v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17249v1","updated":"2024-02-27T06:47:52Z","published":"2024-02-27T06:47:52Z","title":"Deep Learning-Based Speech and Vision Synthesis to Improve Phishing\n  Attack Detection through a Multi-layer Adaptive Framework","summary":"  The ever-evolving ways attacker continues to im prove their phishing\ntechniques to bypass existing state-of-the-art phishing detection methods pose\na mountain of challenges to researchers in both industry and academia research\ndue to the inability of current approaches to detect complex phishing attack.\nThus, current anti-phishing methods remain vulnerable to complex phishing\nbecause of the increasingly sophistication tactics adopted by attacker coupled\nwith the rate at which new tactics are being developed to evade detection. In\nthis research, we proposed an adaptable framework that combines Deep learning\nand Randon Forest to read images, synthesize speech from deep-fake videos, and\nnatural language processing at various predictions layered to significantly\nincrease the performance of machine learning models for phishing attack\ndetection.\n","authors":["Tosin Ige","Christopher Kiekintveld","Aritran Piplai"],"pdf_url":"https://arxiv.org/pdf/2402.17249v1.pdf","comment":"8"},{"id":"http://arxiv.org/abs/2302.06701v2","updated":"2024-02-27T06:47:19Z","published":"2023-02-13T21:28:53Z","title":"Communication-Efficient Federated Bilevel Optimization with Local and\n  Global Lower Level Problems","summary":"  Bilevel Optimization has witnessed notable progress recently with new\nemerging efficient algorithms. However, its application in the Federated\nLearning setting remains relatively underexplored, and the impact of Federated\nLearning's inherent challenges on the convergence of bilevel algorithms remain\nobscure. In this work, we investigate Federated Bilevel Optimization problems\nand propose a communication-efficient algorithm, named FedBiOAcc. The algorithm\nleverages an efficient estimation of the hyper-gradient in the distributed\nsetting and utilizes the momentum-based variance-reduction acceleration.\nRemarkably, FedBiOAcc achieves a communication complexity $O(\\epsilon^{-1})$, a\nsample complexity $O(\\epsilon^{-1.5})$ and the linear speed up with respect to\nthe number of clients. We also analyze a special case of the Federated Bilevel\nOptimization problems, where lower level problems are locally managed by\nclients. We prove that FedBiOAcc-Local, a modified version of FedBiOAcc,\nconverges at the same rate for this type of problems. Finally, we validate the\nproposed algorithms through two real-world tasks: Federated Data-cleaning and\nFederated Hyper-representation Learning. Empirical results show superior\nperformance of our algorithms.\n","authors":["Junyi Li","Feihu Huang","Heng Huang"],"pdf_url":"https://arxiv.org/pdf/2302.06701v2.pdf","comment":"NeurIPS 2023 version (Algorithm 1 is updated to be more concise)"},{"id":"http://arxiv.org/abs/2402.17246v1","updated":"2024-02-27T06:32:56Z","published":"2024-02-27T06:32:56Z","title":"SDR-Former: A Siamese Dual-Resolution Transformer for Liver Lesion\n  Classification Using 3D Multi-Phase Imaging","summary":"  Automated classification of liver lesions in multi-phase CT and MR scans is\nof clinical significance but challenging. This study proposes a novel Siamese\nDual-Resolution Transformer (SDR-Former) framework, specifically designed for\nliver lesion classification in 3D multi-phase CT and MR imaging with varying\nphase counts. The proposed SDR-Former utilizes a streamlined Siamese Neural\nNetwork (SNN) to process multi-phase imaging inputs, possessing robust feature\nrepresentations while maintaining computational efficiency. The weight-sharing\nfeature of the SNN is further enriched by a hybrid Dual-Resolution Transformer\n(DR-Former), comprising a 3D Convolutional Neural Network (CNN) and a tailored\n3D Transformer for processing high- and low-resolution images, respectively.\nThis hybrid sub-architecture excels in capturing detailed local features and\nunderstanding global contextual information, thereby, boosting the SNN's\nfeature extraction capabilities. Additionally, a novel Adaptive Phase Selection\nModule (APSM) is introduced, promoting phase-specific intercommunication and\ndynamically adjusting each phase's influence on the diagnostic outcome. The\nproposed SDR-Former framework has been validated through comprehensive\nexperiments on two clinical datasets: a three-phase CT dataset and an\neight-phase MR dataset. The experimental results affirm the efficacy of the\nproposed framework. To support the scientific community, we are releasing our\nextensive multi-phase MR dataset for liver lesion analysis to the public. This\npioneering dataset, being the first publicly available multi-phase MR dataset\nin this field, also underpins the MICCAI LLD-MMRI Challenge. The dataset is\naccessible at:https://bit.ly/3IyYlgN.\n","authors":["Meng Lou","Hanning Ying","Xiaoqing Liu","Hong-Yu Zhou","Yuqing Zhang","Yizhou Yu"],"pdf_url":"https://arxiv.org/pdf/2402.17246v1.pdf","comment":"13 pages, 7 figures"},{"id":"http://arxiv.org/abs/2206.02902v5","updated":"2024-02-27T06:15:53Z","published":"2022-06-06T20:59:07Z","title":"Goal-Space Planning with Subgoal Models","summary":"  This paper investigates a new approach to model-based reinforcement learning\nusing background planning: mixing (approximate) dynamic programming updates and\nmodel-free updates, similar to the Dyna architecture. Background planning with\nlearned models is often worse than model-free alternatives, such as Double DQN,\neven though the former uses significantly more memory and computation. The\nfundamental problem is that learned models can be inaccurate and often generate\ninvalid states, especially when iterated many steps. In this paper, we avoid\nthis limitation by constraining background planning to a set of (abstract)\nsubgoals and learning only local, subgoal-conditioned models. This goal-space\nplanning (GSP) approach is more computationally efficient, naturally\nincorporates temporal abstraction for faster long-horizon planning and avoids\nlearning the transition dynamics entirely. We show that our GSP algorithm can\npropagate value from an abstract space in a manner that helps a variety of base\nlearners learn significantly faster in different domains.\n","authors":["Chunlok Lo","Kevin Roice","Parham Mohammad Panahi","Scott Jordan","Adam White","Gabor Mihucz","Farzane Aminmansour","Martha White"],"pdf_url":"https://arxiv.org/pdf/2206.02902v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.17264v5","updated":"2024-02-27T06:14:59Z","published":"2023-09-29T14:17:24Z","title":"A Foundation Model for General Moving Object Segmentation in Medical\n  Images","summary":"  Medical image segmentation aims to delineate the anatomical or pathological\nstructures of interest, playing a crucial role in clinical diagnosis. A\nsubstantial amount of high-quality annotated data is crucial for constructing\nhigh-precision deep segmentation models. However, medical annotation is highly\ncumbersome and time-consuming, especially for medical videos or 3D volumes, due\nto the huge labeling space and poor inter-frame consistency. Recently, a\nfundamental task named Moving Object Segmentation (MOS) has made significant\nadvancements in natural images. Its objective is to delineate moving objects\nfrom the background within image sequences, requiring only minimal annotations.\nIn this paper, we propose the first foundation model, named iMOS, for MOS in\nmedical images. Extensive experiments on a large multi-modal medical dataset\nvalidate the effectiveness of the proposed iMOS. Specifically, with the\nannotation of only a small number of images in the sequence, iMOS can achieve\nsatisfactory tracking and segmentation performance of moving objects throughout\nthe entire sequence in bi-directions. We hope that the proposed iMOS can help\naccelerate the annotation speed of experts, and boost the development of\nmedical foundation models.\n","authors":["Zhongnuo Yan","Tong Han","Yuhao Huang","Lian Liu","Han Zhou","Jiongquan Chen","Wenlong Shi","Yan Cao","Xin Yang","Dong Ni"],"pdf_url":"https://arxiv.org/pdf/2309.17264v5.pdf","comment":"5 pages, 7 figures, 3 tables. This paper has been accepted by ISBI\n  2024"},{"id":"http://arxiv.org/abs/2402.17238v1","updated":"2024-02-27T06:13:02Z","published":"2024-02-27T06:13:02Z","title":"Does Negative Sampling Matter? A Review with Insights into its Theory\n  and Applications","summary":"  Negative sampling has swiftly risen to prominence as a focal point of\nresearch, with wide-ranging applications spanning machine learning, computer\nvision, natural language processing, data mining, and recommender systems. This\ngrowing interest raises several critical questions: Does negative sampling\nreally matter? Is there a general framework that can incorporate all existing\nnegative sampling methods? In what fields is it applied? Addressing these\nquestions, we propose a general framework that leverages negative sampling.\nDelving into the history of negative sampling, we trace the development of\nnegative sampling through five evolutionary paths. We dissect and categorize\nthe strategies used to select negative sample candidates, detailing global,\nlocal, mini-batch, hop, and memory-based approaches. Our review categorizes\ncurrent negative sampling methods into five types: static, hard, GAN-based,\nAuxiliary-based, and In-batch methods, providing a clear structure for\nunderstanding negative sampling. Beyond detailed categorization, we highlight\nthe application of negative sampling in various areas, offering insights into\nits practical benefits. Finally, we briefly discuss open problems and future\ndirections for negative sampling.\n","authors":["Zhen Yang","Ming Ding","Tinglin Huang","Yukuo Cen","Junshuai Song","Bin Xu","Yuxiao Dong","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2402.17238v1.pdf","comment":"20 pages, 11 figures"},{"id":"http://arxiv.org/abs/2402.17236v1","updated":"2024-02-27T06:09:48Z","published":"2024-02-27T06:09:48Z","title":"A Review of Data Mining in Personalized Education: Current Trends and\n  Future Prospects","summary":"  Personalized education, tailored to individual student needs, leverages\neducational technology and artificial intelligence (AI) in the digital age to\nenhance learning effectiveness. The integration of AI in educational platforms\nprovides insights into academic performance, learning preferences, and\nbehaviors, optimizing the personal learning process. Driven by data mining\ntechniques, it not only benefits students but also provides educators and\ninstitutions with tools to craft customized learning experiences. To offer a\ncomprehensive review of recent advancements in personalized educational data\nmining, this paper focuses on four primary scenarios: educational\nrecommendation, cognitive diagnosis, knowledge tracing, and learning analysis.\nThis paper presents a structured taxonomy for each area, compiles commonly used\ndatasets, and identifies future research directions, emphasizing the role of\ndata mining in enhancing personalized education and paving the way for future\nexploration and innovation.\n","authors":["Zhang Xiong","Haoxuan Li","Zhuang Liu","Zhuofan Chen","Hao Zhou","Wenge Rong","Yuanxin Ouyang"],"pdf_url":"https://arxiv.org/pdf/2402.17236v1.pdf","comment":"25 pages, 5 figures"},{"id":"http://arxiv.org/abs/2402.17235v1","updated":"2024-02-27T06:05:01Z","published":"2024-02-27T06:05:01Z","title":"Stochastic Gradient Succeeds for Bandits","summary":"  We show that the \\emph{stochastic gradient} bandit algorithm converges to a\n\\emph{globally optimal} policy at an $O(1/t)$ rate, even with a \\emph{constant}\nstep size. Remarkably, global convergence of the stochastic gradient bandit\nalgorithm has not been previously established, even though it is an old\nalgorithm known to be applicable to bandits. The new result is achieved by\nestablishing two novel technical findings: first, the noise of the stochastic\nupdates in the gradient bandit algorithm satisfies a strong ``growth\ncondition'' property, where the variance diminishes whenever progress becomes\nsmall, implying that additional noise control via diminishing step sizes is\nunnecessary; second, a form of ``weak exploration'' is automatically achieved\nthrough the stochastic gradient updates, since they prevent the action\nprobabilities from decaying faster than $O(1/t)$, thus ensuring that every\naction is sampled infinitely often with probability $1$. These two findings can\nbe used to show that the stochastic gradient update is already ``sufficient''\nfor bandits in the sense that exploration versus exploitation is automatically\nbalanced in a manner that ensures almost sure convergence to a global optimum.\nThese novel theoretical findings are further verified by experimental results.\n","authors":["Jincheng Mei","Zixin Zhong","Bo Dai","Alekh Agarwal","Csaba Szepesvari","Dale Schuurmans"],"pdf_url":"https://arxiv.org/pdf/2402.17235v1.pdf","comment":"39 pages; Correction for a previous version published at ICML 2023\n  conference"},{"id":"http://arxiv.org/abs/2402.17233v1","updated":"2024-02-27T06:01:56Z","published":"2024-02-27T06:01:56Z","title":"Hybrid Square Neural ODE Causal Modeling","summary":"  Hybrid models combine mechanistic ODE-based dynamics with flexible and\nexpressive neural network components. Such models have grown rapidly in\npopularity, especially in scientific domains where such ODE-based modeling\noffers important interpretability and validated causal grounding (e.g., for\ncounterfactual reasoning). The incorporation of mechanistic models also\nprovides inductive bias in standard blackbox modeling approaches, critical when\nlearning from small datasets or partially observed, complex systems.\nUnfortunately, as hybrid models become more flexible, the causal grounding\nprovided by the mechanistic model can quickly be lost. We address this problem\nby leveraging another common source of domain knowledge: ranking of treatment\neffects for a set of interventions, even if the precise treatment effect is\nunknown. We encode this information in a causal loss that we combine with the\nstandard predictive loss to arrive at a hybrid loss that biases our learning\ntowards causally valid hybrid models. We demonstrate our ability to achieve a\nwin-win -- state-of-the-art predictive performance and causal validity -- in\nthe challenging task of modeling glucose dynamics during exercise.\n","authors":["Bob Junyi Zou","Matthew E. Levine","Dessi P. Zaharieva","Ramesh Johari","Emily B. Fox"],"pdf_url":"https://arxiv.org/pdf/2402.17233v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17232v1","updated":"2024-02-27T05:57:45Z","published":"2024-02-27T05:57:45Z","title":"Two-scale Neural Networks for Partial Differential Equations with Small\n  Parameters","summary":"  We propose a two-scale neural network method for solving partial differential\nequations (PDEs) with small parameters using physics-informed neural networks\n(PINNs). We directly incorporate the small parameters into the architecture of\nneural networks. The proposed method enables solving PDEs with small parameters\nin a simple fashion, without adding Fourier features or other computationally\ntaxing searches of truncation parameters. Various numerical examples\ndemonstrate reasonable accuracy in capturing features of large derivatives in\nthe solutions caused by small parameters.\n","authors":["Qiao Zhuang","Chris Ziyi Yao","Zhongqiang Zhang","George Em Karniadakis"],"pdf_url":"https://arxiv.org/pdf/2402.17232v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15958v2","updated":"2024-02-27T05:54:10Z","published":"2024-02-25T02:36:14Z","title":"On the dynamics of three-layer neural networks: initial condensation","summary":"  Empirical and theoretical works show that the input weights of two-layer\nneural networks, when initialized with small values, converge towards isolated\norientations. This phenomenon, referred to as condensation, indicates that the\ngradient descent methods tend to spontaneously reduce the complexity of neural\nnetworks during the training process. In this work, we elucidate the mechanisms\nbehind the condensation phenomena occurring in the training of three-layer\nneural networks and distinguish it from the training of two-layer neural\nnetworks. Through rigorous theoretical analysis, we establish the blow-up\nproperty of effective dynamics and present a sufficient condition for the\noccurrence of condensation, findings that are substantiated by experimental\nresults. Additionally, we explore the association between condensation and the\nlow-rank bias observed in deep matrix factorization.\n","authors":["Zheng-An Chen","Tao Luo"],"pdf_url":"https://arxiv.org/pdf/2402.15958v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17229v1","updated":"2024-02-27T05:47:33Z","published":"2024-02-27T05:47:33Z","title":"Preserving Fairness Generalization in Deepfake Detection","summary":"  Although effective deepfake detection models have been developed in recent\nyears, recent studies have revealed that these models can result in unfair\nperformance disparities among demographic groups, such as race and gender. This\ncan lead to particular groups facing unfair targeting or exclusion from\ndetection, potentially allowing misclassified deepfakes to manipulate public\nopinion and undermine trust in the model. The existing method for addressing\nthis problem is providing a fair loss function. It shows good fairness\nperformance for intra-domain evaluation but does not maintain fairness for\ncross-domain testing. This highlights the significance of fairness\ngeneralization in the fight against deepfakes. In this work, we propose the\nfirst method to address the fairness generalization problem in deepfake\ndetection by simultaneously considering features, loss, and optimization\naspects. Our method employs disentanglement learning to extract demographic and\ndomain-agnostic forgery features, fusing them to encourage fair learning across\na flattened loss landscape. Extensive experiments on prominent deepfake\ndatasets demonstrate our method's effectiveness, surpassing state-of-the-art\napproaches in preserving fairness during cross-domain deepfake detection. The\ncode is available at https://github.com/Purdue-M2/Fairness-Generalization\n","authors":["Li Lin","Xinan He","Yan Ju","Xin Wang","Feng Ding","Shu Hu"],"pdf_url":"https://arxiv.org/pdf/2402.17229v1.pdf","comment":"Accepted by The IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR 2024)"},{"id":"http://arxiv.org/abs/2312.11462v4","updated":"2024-02-27T05:42:31Z","published":"2023-12-18T18:59:46Z","title":"Cascade Speculative Drafting for Even Faster LLM Inference","summary":"  Introduced to enhance the efficiency of large language model (LLM) inference,\nspeculative decoding operates by having a smaller model generate a draft. A\nlarger target model then reviews this draft to align with its output, and any\nacceptance by the target model results in a reduction of the number of the\ntarget model runs, ultimately improving efficiency. However, the drafting\nprocess in speculative decoding includes slow autoregressive generation and\nallocates equal time to generating tokens, irrespective of their importance.\nThese inefficiencies collectively contribute to the suboptimal performance of\nspeculative decoding. To further improve LLM inference, we introduce Cascade\nSpeculative Drafting (CS Drafting), a speculative execution algorithm that\nincorporates two types of cascades. The Vertical Cascade eliminates\nautoregressive generation from neural models, while the Horizontal Cascade\noptimizes time allocation in drafting for improved efficiency. Combining both\ncascades, CS Drafting achieves up to an 81 percent additional speedup over\nspeculative decoding in our experiments, while maintaining the same output\ndistribution as the target model. Our code is publicly available at\nhttps://github.com/lfsszd/CS-Drafting.\n","authors":["Ziyi Chen","Xiaocong Yang","Jiacheng Lin","Chenkai Sun","Kevin Chen-Chuan Chang","Jie Huang"],"pdf_url":"https://arxiv.org/pdf/2312.11462v4.pdf","comment":"Preprint in progress"},{"id":"http://arxiv.org/abs/2402.17227v1","updated":"2024-02-27T05:40:36Z","published":"2024-02-27T05:40:36Z","title":"Efficient Backpropagation with Variance-Controlled Adaptive Sampling","summary":"  Sampling-based algorithms, which eliminate ''unimportant'' computations\nduring forward and/or back propagation (BP), offer potential solutions to\naccelerate neural network training. However, since sampling introduces\napproximations to training, such algorithms may not consistently maintain\naccuracy across various tasks. In this work, we introduce a variance-controlled\nadaptive sampling (VCAS) method designed to accelerate BP. VCAS computes an\nunbiased stochastic gradient with fine-grained layerwise importance sampling in\ndata dimension for activation gradient calculation and leverage score sampling\nin token dimension for weight gradient calculation. To preserve accuracy, we\ncontrol the additional variance by learning the sample ratio jointly with model\nparameters during training. We assessed VCAS on multiple fine-tuning and\npre-training tasks in both vision and natural language domains. On all the\ntasks, VCAS can preserve the original training loss trajectory and validation\naccuracy with an up to 73.87% FLOPs reduction of BP and 49.58% FLOPs reduction\nof the whole training process. The implementation is available at\nhttps://github.com/thu-ml/VCAS .\n","authors":["Ziteng Wang","Jianfei Chen","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2402.17227v1.pdf","comment":"ICLR 2024"}],"Multimedia":[{"id":"http://arxiv.org/abs/2301.09080v7","updated":"2024-02-27T14:08:22Z","published":"2023-01-22T08:35:51Z","title":"Dance2MIDI: Dance-driven multi-instruments music generation","summary":"  Dance-driven music generation aims to generate musical pieces conditioned on\ndance videos. Previous works focus on monophonic or raw audio generation, while\nthe multi-instruments scenario is under-explored. The challenges associated\nwith the dance-driven multi-instrument music (MIDI) generation are twofold: 1)\nno publicly available multi-instruments MIDI and video paired dataset and 2)\nthe weak correlation between music and video. To tackle these challenges, we\nbuild the first multi-instruments MIDI and dance paired dataset (D2MIDI). Based\non our proposed dataset, we introduce a multi-instruments MIDI generation\nframework (Dance2MIDI) conditioned on dance video. Specifically, 1) to capture\nthe relationship between dance and music, we employ the Graph Convolutional\nNetwork to encode the dance motion. This allows us to extract features related\nto dance movement and dance style, 2) to generate a harmonious rhythm, we\nutilize a Transformer model to decode the drum track sequence, leveraging a\ncross-attention mechanism, and 3) we model the task of generating the remaining\ntracks based on the drum track as a sequence understanding and completion task.\nA BERT-like model is employed to comprehend the context of the entire music\npiece through self-supervised learning. We evaluate the generated music of our\nframework trained on the D2MIDI dataset and demonstrate that our method\nachieves State-of-the-Art performance.\n","authors":["Bo Han","Yuheng Li","Yixuan Shen","Yi Ren","Feilin Han"],"pdf_url":"https://arxiv.org/pdf/2301.09080v7.pdf","comment":"has been accepted by Computational Visual Media Journal"},{"id":"http://arxiv.org/abs/2310.06958v4","updated":"2024-02-27T08:34:43Z","published":"2023-10-10T19:21:41Z","title":"Comparing the Robustness of Modern No-Reference Image- and Video-Quality\n  Metrics to Adversarial Attacks","summary":"  Nowadays, neural-network-based image- and video-quality metrics perform\nbetter than traditional methods. However, they also became more vulnerable to\nadversarial attacks that increase metrics' scores without improving visual\nquality. The existing benchmarks of quality metrics compare their performance\nin terms of correlation with subjective quality and calculation time.\nNonetheless, the adversarial robustness of image-quality metrics is also an\narea worth researching. This paper analyses modern metrics' robustness to\ndifferent adversarial attacks. We adapted adversarial attacks from computer\nvision tasks and compared attacks' efficiency against 15 no-reference image-\nand video-quality metrics. Some metrics showed high resistance to adversarial\nattacks, which makes their usage in benchmarks safer than vulnerable metrics.\nThe benchmark accepts submissions of new metrics for researchers who want to\nmake their metrics more robust to attacks or to find such metrics for their\nneeds. The latest results can be found online:\nhttps://videoprocessing.ai/benchmarks/metrics-robustness.html.\n","authors":["Anastasia Antsiferova","Khaled Abud","Aleksandr Gushchin","Ekaterina Shumitskaya","Sergey Lavrushkin","Dmitriy Vatolin"],"pdf_url":"https://arxiv.org/pdf/2310.06958v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07300v2","updated":"2024-02-27T03:36:01Z","published":"2024-02-11T20:42:01Z","title":"SPICA: Interactive Video Content Exploration through Augmented Audio\n  Descriptions for Blind or Low-Vision Viewers","summary":"  Blind or Low-Vision (BLV) users often rely on audio descriptions (AD) to\naccess video content. However, conventional static ADs can leave out detailed\ninformation in videos, impose a high mental load, neglect the diverse needs and\npreferences of BLV users, and lack immersion. To tackle these challenges, we\nintroduce SPICA, an AI-powered system that enables BLV users to interactively\nexplore video content. Informed by prior empirical studies on BLV video\nconsumption, SPICA offers novel interactive mechanisms for supporting temporal\nnavigation of frame captions and spatial exploration of objects within key\nframes. Leveraging an audio-visual machine learning pipeline, SPICA augments\nexisting ADs by adding interactivity, spatial sound effects, and individual\nobject descriptions without requiring additional human annotation. Through a\nuser study with 14 BLV participants, we evaluated the usability and usefulness\nof SPICA and explored user behaviors, preferences, and mental models when\ninteracting with augmented ADs.\n","authors":["Zheng Ning","Brianna L. Wimer","Kaiwen Jiang","Keyi Chen","Jerrick Ban","Yapeng Tian","Yuhang Zhao","Toby Jia-Jun Li"],"pdf_url":"https://arxiv.org/pdf/2402.07300v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05849v2","updated":"2024-02-27T02:00:58Z","published":"2023-12-10T10:35:16Z","title":"InteractDiffusion: Interaction Control in Text-to-Image Diffusion Models","summary":"  Large-scale text-to-image (T2I) diffusion models have showcased incredible\ncapabilities in generating coherent images based on textual descriptions,\nenabling vast applications in content generation. While recent advancements\nhave introduced control over factors such as object localization, posture, and\nimage contours, a crucial gap remains in our ability to control the\ninteractions between objects in the generated content. Well-controlling\ninteractions in generated images could yield meaningful applications, such as\ncreating realistic scenes with interacting characters. In this work, we study\nthe problems of conditioning T2I diffusion models with Human-Object Interaction\n(HOI) information, consisting of a triplet label (person, action, object) and\ncorresponding bounding boxes. We propose a pluggable interaction control model,\ncalled InteractDiffusion that extends existing pre-trained T2I diffusion models\nto enable them being better conditioned on interactions. Specifically, we\ntokenize the HOI information and learn their relationships via interaction\nembeddings. A conditioning self-attention layer is trained to map HOI tokens to\nvisual tokens, thereby conditioning the visual tokens better in existing T2I\ndiffusion models. Our model attains the ability to control the interaction and\nlocation on existing T2I diffusion models, which outperforms existing baselines\nby a large margin in HOI detection score, as well as fidelity in FID and KID.\nProject page: https://jiuntian.github.io/interactdiffusion.\n","authors":["Jiun Tian Hoe","Xudong Jiang","Chee Seng Chan","Yap-Peng Tan","Weipeng Hu"],"pdf_url":"https://arxiv.org/pdf/2312.05849v2.pdf","comment":"Website: https://jiuntian.github.io/interactdiffusion. Accepted at\n  CVPR2024"}]},"2024-02-26T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2401.12413v2","updated":"2024-02-26T23:23:31Z","published":"2024-01-22T23:55:00Z","title":"How Far Can 100 Samples Go? Unlocking Overall Zero-Shot Multilingual\n  Translation via Tiny Multi-Parallel Data","summary":"  Zero-shot translation aims to translate between language pairs not seen\nduring training in Multilingual Machine Translation (MMT) and is largely\nconsidered an open problem. A common, albeit resource-consuming, solution is to\nadd as many related translation directions as possible to the training corpus.\nIn this paper, we show that for an English-centric model, surprisingly large\nzero-shot improvements can be achieved by simply fine-tuning with a very small\namount of multi-parallel data. For example, on the EC30 dataset, we obtain up\nto +21.7 ChrF non-English overall improvements (870 directions) by using only\n100 multi-parallel samples while preserving English-centric translation\nquality. When investigating the size effect of fine-tuning data and its\ntransfer capabilities, we found that already a small, randomly sampled set of\nfine-tuning directions is sufficient to achieve comparable improvements. The\nresulting non-English performance is close to the complete translation upper\nbound. Even in a minimal setting -- fine-tuning with only one single sample --\nthe well-known off-target issue is almost completely resolved, explaining\nparts--but not all -- of the observed improvements in translation quality.\n","authors":["Di Wu","Shaomu Tan","Yan Meng","David Stap","Christof Monz"],"pdf_url":"https://arxiv.org/pdf/2401.12413v2.pdf","comment":"15 pages, 5 figures"},{"id":"http://arxiv.org/abs/2305.17588v3","updated":"2024-02-26T23:11:03Z","published":"2023-05-27T22:15:48Z","title":"Diagnosing Transformers: Illuminating Feature Spaces for Clinical\n  Decision-Making","summary":"  Pre-trained transformers are often fine-tuned to aid clinical decision-making\nusing limited clinical notes. Model interpretability is crucial, especially in\nhigh-stakes domains like medicine, to establish trust and ensure safety, which\nrequires human engagement. We introduce SUFO, a systematic framework that\nenhances interpretability of fine-tuned transformer feature spaces. SUFO\nutilizes a range of analytic and visualization techniques, including Supervised\nprobing, Unsupervised similarity analysis, Feature dynamics, and Outlier\nanalysis to address key questions about model trust and interpretability. We\nconduct a case study investigating the impact of pre-training data where we\nfocus on real-world pathology classification tasks, and validate our findings\non MedNLI. We evaluate five 110M-sized pre-trained transformer models,\ncategorized into general-domain (BERT, TNLR), mixed-domain (BioBERT, Clinical\nBioBERT), and domain-specific (PubMedBERT) groups. Our SUFO analyses reveal\nthat: (1) while PubMedBERT, the domain-specific model, contains valuable\ninformation for fine-tuning, it can overfit to minority classes when class\nimbalances exist. In contrast, mixed-domain models exhibit greater resistance\nto overfitting, suggesting potential improvements in domain-specific model\nrobustness; (2) in-domain pre-training accelerates feature disambiguation\nduring fine-tuning; and (3) feature spaces undergo significant sparsification\nduring this process, enabling clinicians to identify common outlier modes among\nfine-tuned models as demonstrated in this paper. These findings showcase the\nutility of SUFO in enhancing trust and safety when using transformers in\nmedicine, and we believe SUFO can aid practitioners in evaluating fine-tuned\nlanguage models for other applications in medicine and in more critical\ndomains.\n","authors":["Aliyah R. Hsu","Yeshwanth Cherapanamjeri","Briton Park","Tristan Naumann","Anobel Y. Odisho","Bin Yu"],"pdf_url":"https://arxiv.org/pdf/2305.17588v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.02547v4","updated":"2024-02-26T23:01:15Z","published":"2023-05-04T04:58:00Z","title":"PersonaLLM: Investigating the Ability of Large Language Models to\n  Express Personality Traits","summary":"  Despite the many use cases for large language models (LLMs) in creating\npersonalized chatbots, there has been limited research on evaluating the extent\nto which the behaviors of personalized LLMs accurately and consistently reflect\nspecific personality traits. We consider studying the behavior of LLM-based\nagents which we refer to as LLM personas and present a case study with GPT-3.5\nand GPT-4 to investigate whether LLMs can generate content that aligns with\ntheir assigned personality profiles. To this end, we simulate distinct LLM\npersonas based on the Big Five personality model, have them complete the\n44-item Big Five Inventory (BFI) personality test and a story writing task, and\nthen assess their essays with automatic and human evaluations. Results show\nthat LLM personas' self-reported BFI scores are consistent with their\ndesignated personality types, with large effect sizes observed across five\ntraits. Additionally, LLM personas' writings have emerging representative\nlinguistic patterns for personality traits when compared with a human writing\ncorpus. Furthermore, human evaluation shows that humans can perceive some\npersonality traits with an accuracy of up to 80\\%. Interestingly, the accuracy\ndrops significantly when the annotators were informed of the AI's authorship.\n","authors":["Hang Jiang","Xiajie Zhang","Xubo Cao","Cynthia Breazeal","Jad Kabbara","Deb Roy"],"pdf_url":"https://arxiv.org/pdf/2305.02547v4.pdf","comment":"First version uploaded at IC2S2 in May 2023. Full paper submitted in\n  Nov. 2023 and updated Feb. 2024"},{"id":"http://arxiv.org/abs/2305.12599v3","updated":"2024-02-26T22:44:36Z","published":"2023-05-21T23:16:26Z","title":"Abstract Meaning Representation-Based Logic-Driven Data Augmentation for\n  Logical Reasoning","summary":"  Combining large language models with logical reasoning enhances their\ncapacity to address problems in a robust and reliable manner. Nevertheless, the\nintricate nature of logical reasoning poses challenges to gathering reliable\ndata from the web for building comprehensive training datasets, subsequently\naffecting the performance on downstream tasks. To address this, we introduce a\nnovel logic-driven data augmentation approach, AMR-LDA. AMR-LDA converts the\noriginal text into an Abstract Meaning Representation (AMR) graph, a structured\nsemantic representation that encapsulates the logic structure of the sentence,\nupon which operations are performed to generate logically modified AMR graphs.\nThe modified AMR graphs are subsequently converted back into text to create\naugmented data. Notably, our methodology is architecture-agnostic and enhances\nboth generative large language models, such as GPT-3.5 and GPT-4, through\nprompt augmentation, and discriminative large language models through\ncontrastive learning with logic-driven data augmentation. Empirical evidence\nunderscores the efficacy of our proposed method with improvement in performance\nacross seven downstream tasks, such as reading comprehension requiring logical\nreasoning, textual entailment, and natural language inference. Furthermore, our\nmethod leads on the ReClor\nleaderboard\\footnote{\\url{https://eval.ai/web/challenges/challenge-page/503/leaderboard/1347}}.\nThe source code and data are publicly\navailable\\footnote{\\url{https://bit.ly/3OWKe8r}}.\n","authors":["Qiming Bao","Alex Yuxuan Peng","Zhenyun Deng","Wanjun Zhong","Gael Gendron","Timothy Pistotti","Neset Tan","Nathan Young","Yang Chen","Yonghua Zhu","Paul Denny","Michael Witbrock","Jiamou Liu"],"pdf_url":"https://arxiv.org/pdf/2305.12599v3.pdf","comment":"The short version (v2) was accepted for oral presentation at the\n  first LLM@IJCAI 2023 non-archival symposium; the full version is under review"},{"id":"http://arxiv.org/abs/2311.08640v3","updated":"2024-02-26T22:30:53Z","published":"2023-11-15T01:28:28Z","title":"Multistage Collaborative Knowledge Distillation from a Large Language\n  Model for Semi-Supervised Sequence Generation","summary":"  We study semi-supervised sequence generation tasks, where the few labeled\nexamples are too scarce to finetune a model, and meanwhile, few-shot prompted\nlarge language models (LLMs) exhibit room for improvement. In this paper, we\npresent the discovery that a student model distilled from a few-shot prompted\nLLM can commonly generalize better than its teacher to unseen examples on such\ntasks. We find that the student is able to learn a general pattern from the\nhigh-quality pseudolabels produced by the teacher during knowledge distillation\n(KD), and favorably not a general pattern from the low-quality pseudolables.\nLeveraging this discovery, we propose a new method, Multistage Collaborative\nKnowledge Distillation from an LLM (MCKD), for these tasks. MCKD first few-shot\nprompts an LLM to produce pseudolabels for unlabeled data. Then at each stage\nof an iterative KD process, a new pair of students is trained on disjoint\npartitions of the pseudolabeled data, and produces new and improved\npseudolabels for their unseen partitions. We conduct extensive experiments on\nfour syntactic and semantic parsing datasets and show the effectiveness of MCKD\nfor low-resource semi-supervised sequence generation. On CRAFT biomedical\nparsing, for example, 3-stage MCKD with 50 labeled examples outperforms an LLM\nteacher and vanilla KD by 7.5% and 3.7% parsing F1, respectively, and matches\nthe performance of supervised finetuning with 500 labeled examples.\n","authors":["Jiachen Zhao","Wenlong Zhao","Andrew Drozdov","Benjamin Rozonoyer","Md Arafat Sultan","Jay-Yoon Lee","Mohit Iyyer","Andrew McCallum"],"pdf_url":"https://arxiv.org/pdf/2311.08640v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.08541v2","updated":"2024-02-26T20:57:33Z","published":"2023-09-15T17:05:43Z","title":"When do Generative Query and Document Expansions Fail? A Comprehensive\n  Study Across Methods, Retrievers, and Datasets","summary":"  Using large language models (LMs) for query or document expansion can improve\ngeneralization in information retrieval. However, it is unknown whether these\ntechniques are universally beneficial or only effective in specific settings,\nsuch as for particular retrieval models, dataset domains, or query types. To\nanswer this, we conduct the first comprehensive analysis of LM-based expansion.\nWe find that there exists a strong negative correlation between retriever\nperformance and gains from expansion: expansion improves scores for weaker\nmodels, but generally harms stronger models. We show this trend holds across a\nset of eleven expansion techniques, twelve datasets with diverse distribution\nshifts, and twenty-four retrieval models. Through qualitative error analysis,\nwe hypothesize that although expansions provide extra information (potentially\nimproving recall), they add additional noise that makes it difficult to discern\nbetween the top relevant documents (thus introducing false positives). Our\nresults suggest the following recipe: use expansions for weaker models or when\nthe target dataset significantly differs from training corpus in format;\notherwise, avoid expansions to keep the relevance signal clear.\n","authors":["Orion Weller","Kyle Lo","David Wadden","Dawn Lawrie","Benjamin Van Durme","Arman Cohan","Luca Soldaini"],"pdf_url":"https://arxiv.org/pdf/2309.08541v2.pdf","comment":"EACL 2024 camera ready"},{"id":"http://arxiv.org/abs/2402.17019v1","updated":"2024-02-26T20:56:06Z","published":"2024-02-26T20:56:06Z","title":"Leveraging Large Language Models for Learning Complex Legal Concepts\n  through Storytelling","summary":"  Making legal knowledge accessible to non-experts is crucial for enhancing\ngeneral legal literacy and encouraging civic participation in democracy.\nHowever, legal documents are often challenging to understand for people without\nlegal backgrounds. In this paper, we present a novel application of large\nlanguage models (LLMs) in legal education to help non-experts learn intricate\nlegal concepts through storytelling, an effective pedagogical tool in conveying\ncomplex and abstract concepts. We also introduce a new dataset LegalStories,\nwhich consists of 295 complex legal doctrines, each accompanied by a story and\na set of multiple-choice questions generated by LLMs. To construct the dataset,\nwe experiment with various LLMs to generate legal stories explaining these\nconcepts. Furthermore, we use an expert-in-the-loop method to iteratively\ndesign multiple-choice questions. Then, we evaluate the effectiveness of\nstorytelling with LLMs through an RCT experiment with legal novices on 10\nsamples from the dataset. We find that LLM-generated stories enhance\ncomprehension of legal concepts and interest in law among non-native speakers\ncompared to only definitions. Moreover, stories consistently help participants\nrelate legal concepts to their lives. Finally, we find that learning with\nstories shows a higher retention rate for non-native speakers in the follow-up\nassessment. Our work has strong implications for using LLMs in promoting\nteaching and learning in the legal field and beyond.\n","authors":["Hang Jiang","Xiajie Zhang","Robert Mahari","Daniel Kessler","Eric Ma","Tal August","Irene Li","Alex 'Sandy' Pentland","Yoon Kim","Jad Kabbara","Deb Roy"],"pdf_url":"https://arxiv.org/pdf/2402.17019v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.07614v2","updated":"2024-02-26T20:55:25Z","published":"2023-05-12T17:05:54Z","title":"NevIR: Negation in Neural Information Retrieval","summary":"  Negation is a common everyday phenomena and has been a consistent area of\nweakness for language models (LMs). Although the Information Retrieval (IR)\ncommunity has adopted LMs as the backbone of modern IR architectures, there has\nbeen little to no research in understanding how negation impacts neural IR. We\ntherefore construct a straightforward benchmark on this theme: asking IR models\nto rank two documents that differ only by negation. We show that the results\nvary widely according to the type of IR architecture: cross-encoders perform\nbest, followed by late-interaction models, and in last place are bi-encoder and\nsparse neural architectures. We find that most information retrieval models\n(including SOTA ones) do not consider negation, performing the same or worse\nthan a random ranking. We show that although the obvious approach of continued\nfine-tuning on a dataset of contrastive documents containing negations\nincreases performance (as does model size), there is still a large gap between\nmachine and human performance.\n","authors":["Orion Weller","Dawn Lawrie","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2305.07614v2.pdf","comment":"Accepted to EACL 2024"},{"id":"http://arxiv.org/abs/2402.17016v1","updated":"2024-02-26T20:53:12Z","published":"2024-02-26T20:53:12Z","title":"Multi-Task Contrastive Learning for 8192-Token Bilingual Text Embeddings","summary":"  We introduce a novel suite of state-of-the-art bilingual text embedding\nmodels that are designed to support English and another target language. These\nmodels are capable of processing lengthy text inputs with up to 8192 tokens,\nmaking them highly versatile for a range of natural language processing tasks\nsuch as text retrieval, clustering, and semantic textual similarity (STS)\ncalculations.\n  By focusing on bilingual models and introducing a unique multi-task learning\nobjective, we have significantly improved the model performance on STS tasks,\nwhich outperforms the capabilities of existing multilingual models in both\ntarget language understanding and cross-lingual evaluation tasks. Moreover, our\nbilingual models are more efficient, requiring fewer parameters and less memory\ndue to their smaller vocabulary needs. Furthermore, we have expanded the\nMassive Text Embedding Benchmark (MTEB) to include benchmarks for German and\nSpanish embedding models. This integration aims to stimulate further research\nand advancement in text embedding technologies for these languages.\n","authors":["Isabelle Mohr","Markus Krimmel","Saba Sturua","Mohammad Kalim Akram","Andreas Koukounas","Michael Günther","Georgios Mastrapas","Vinit Ravishankar","Joan Fontanals Martínez","Feng Wang","Qi Liu","Ziniu Yu","Jie Fu","Saahil Ognawala","Susana Guzman","Bo Wang","Maximilian Werk","Nan Wang","Han Xiao"],"pdf_url":"https://arxiv.org/pdf/2402.17016v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.10002v3","updated":"2024-02-26T20:52:59Z","published":"2022-12-20T05:25:01Z","title":"Defending Against Disinformation Attacks in Open-Domain Question\n  Answering","summary":"  Recent work in open-domain question answering (ODQA) has shown that\nadversarial poisoning of the search collection can cause large drops in\naccuracy for production systems. However, little to no work has proposed\nmethods to defend against these attacks. To do so, we rely on the intuition\nthat redundant information often exists in large corpora. To find it, we\nintroduce a method that uses query augmentation to search for a diverse set of\npassages that could answer the original question but are less likely to have\nbeen poisoned. We integrate these new passages into the model through the\ndesign of a novel confidence method, comparing the predicted answer to its\nappearance in the retrieved contexts (what we call Confidence from Answer\nRedundancy, i.e. CAR). Together these methods allow for a simple but effective\nway to defend against poisoning attacks that provides gains of nearly 20% exact\nmatch across varying levels of data poisoning/knowledge conflicts.\n","authors":["Orion Weller","Aleem Khan","Nathaniel Weir","Dawn Lawrie","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2212.10002v3.pdf","comment":"Accepted to EACL 2024"},{"id":"http://arxiv.org/abs/2305.13252v2","updated":"2024-02-26T20:50:33Z","published":"2023-05-22T17:25:24Z","title":"\"According to ...\": Prompting Language Models Improves Quoting from\n  Pre-Training Data","summary":"  Large Language Models (LLMs) may hallucinate and generate fake information,\ndespite pre-training on factual data. Inspired by the journalistic device of\n\"according to sources\", we propose according-to prompting: directing LLMs to\nground responses against previously observed text. To quantify this grounding,\nwe propose a novel evaluation metric (QUIP-Score) that measures the extent to\nwhich model-produced answers are directly found in underlying text corpora. We\nillustrate with experiments on three corpora (Wikipedia, PubMed, and the U.S.\nlegal tax code) that these prompts improve grounding under our metrics, with\nthe additional benefit of often improving end-task performance. Furthermore,\nprompts that ask the model to decrease grounding (or to ground to other\ncorpora) indeed decrease QUIP-Score, indicating the ability of LLMs to increase\nor decrease grounded generations on request.\n","authors":["Orion Weller","Marc Marone","Nathaniel Weir","Dawn Lawrie","Daniel Khashabi","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2305.13252v2.pdf","comment":"Accepted to EACL 2024"},{"id":"http://arxiv.org/abs/2402.17014v1","updated":"2024-02-26T20:43:48Z","published":"2024-02-26T20:43:48Z","title":"Z-AGI Labs at ClimateActivism 2024: Stance and Hate Event Detection on\n  Social Media","summary":"  In the digital realm, rich data serves as a crucial source of insights into\nthe complexities of social, political, and economic landscapes. Addressing the\ngrowing need for high-quality information on events and the imperative to\ncombat hate speech, this research led to the establishment of the Shared Task\non Climate Activism Stance and Hate Event Detection at CASE 2024. Focused on\nclimate activists contending with hate speech on social media, our study\ncontributes to hate speech identification from tweets. Analyzing three\nsub-tasks - Hate Speech Detection (Sub-task A), Targets of Hate Speech\nIdentification (Sub-task B), and Stance Detection (Sub-task C) - Team Z-AGI\nLabs evaluated various models, including LSTM, Xgboost, and LGBM based on\nTf-Idf. Results unveiled intriguing variations, with Catboost excelling in\nSubtask-B (F1: 0.5604) and Subtask-C (F1: 0.7081), while LGBM emerged as the\ntop-performing model for Subtask-A (F1: 0.8684). This research provides\nvaluable insights into the suitability of classical machine learning models for\nclimate hate speech and stance detection, aiding informed model selection for\nrobust mechanisms.\n","authors":["Nikhil Narayan","Mrutyunjay Biswal"],"pdf_url":"https://arxiv.org/pdf/2402.17014v1.pdf","comment":"Accepted as Working Notes in CASE-EACL 2024"},{"id":"http://arxiv.org/abs/2402.17013v1","updated":"2024-02-26T20:42:40Z","published":"2024-02-26T20:42:40Z","title":"Towards Explainability and Fairness in Swiss Judgement Prediction:\n  Benchmarking on a Multilingual Dataset","summary":"  The assessment of explainability in Legal Judgement Prediction (LJP) systems\nis of paramount importance in building trustworthy and transparent systems,\nparticularly considering the reliance of these systems on factors that may lack\nlegal relevance or involve sensitive attributes. This study delves into the\nrealm of explainability and fairness in LJP models, utilizing Swiss Judgement\nPrediction (SJP), the only available multilingual LJP dataset. We curate a\ncomprehensive collection of rationales that `support' and `oppose' judgement\nfrom legal experts for 108 cases in German, French, and Italian. By employing\nan occlusion-based explainability approach, we evaluate the explainability\nperformance of state-of-the-art monolingual and multilingual BERT-based LJP\nmodels, as well as models developed with techniques such as data augmentation\nand cross-lingual transfer, which demonstrated prediction performance\nimprovement. Notably, our findings reveal that improved prediction performance\ndoes not necessarily correspond to enhanced explainability performance,\nunderscoring the significance of evaluating models from an explainability\nperspective. Additionally, we introduce a novel evaluation framework, Lower\nCourt Insertion (LCI), which allows us to quantify the influence of lower court\ninformation on model predictions, exposing current models' biases.\n","authors":["Santosh T. Y. S. S","Nina Baumgartner","Matthias Stürmer","Matthias Grabmair","Joel Niklaus"],"pdf_url":"https://arxiv.org/pdf/2402.17013v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2306.11698v5","updated":"2024-02-26T20:41:01Z","published":"2023-06-20T17:24:23Z","title":"DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT\n  Models","summary":"  Generative Pre-trained Transformer (GPT) models have exhibited exciting\nprogress in their capabilities, capturing the interest of practitioners and the\npublic alike. Yet, while the literature on the trustworthiness of GPT models\nremains limited, practitioners have proposed employing capable GPT models for\nsensitive applications such as healthcare and finance -- where mistakes can be\ncostly. To this end, this work proposes a comprehensive trustworthiness\nevaluation for large language models with a focus on GPT-4 and GPT-3.5,\nconsidering diverse perspectives -- including toxicity, stereotype bias,\nadversarial robustness, out-of-distribution robustness, robustness on\nadversarial demonstrations, privacy, machine ethics, and fairness. Based on our\nevaluations, we discover previously unpublished vulnerabilities to\ntrustworthiness threats. For instance, we find that GPT models can be easily\nmisled to generate toxic and biased outputs and leak private information in\nboth training data and conversation history. We also find that although GPT-4\nis usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is more\nvulnerable given jailbreaking system or user prompts, potentially because GPT-4\nfollows (misleading) instructions more precisely. Our work illustrates a\ncomprehensive trustworthiness evaluation of GPT models and sheds light on the\ntrustworthiness gaps. Our benchmark is publicly available at\nhttps://decodingtrust.github.io/ ; our dataset can be previewed at\nhttps://huggingface.co/datasets/AI-Secure/DecodingTrust ; a concise version of\nthis work is at https://openreview.net/pdf?id=kaHpo8OZw2 .\n","authors":["Boxin Wang","Weixin Chen","Hengzhi Pei","Chulin Xie","Mintong Kang","Chenhui Zhang","Chejian Xu","Zidi Xiong","Ritik Dutta","Rylan Schaeffer","Sang T. Truong","Simran Arora","Mantas Mazeika","Dan Hendrycks","Zinan Lin","Yu Cheng","Sanmi Koyejo","Dawn Song","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2306.11698v5.pdf","comment":"NeurIPS 2023 Outstanding Paper (Datasets and Benchmarks Track)"},{"id":"http://arxiv.org/abs/2402.17011v1","updated":"2024-02-26T20:35:34Z","published":"2024-02-26T20:35:34Z","title":"DiffuCOMET: Contextual Commonsense Knowledge Diffusion","summary":"  Inferring contextually-relevant and diverse commonsense to understand\nnarratives remains challenging for knowledge models. In this work, we develop a\nseries of knowledge models, DiffuCOMET, that leverage diffusion to learn to\nreconstruct the implicit semantic connections between narrative contexts and\nrelevant commonsense knowledge. Across multiple diffusion steps, our method\nprogressively refines a representation of commonsense facts that is anchored to\na narrative, producing contextually-relevant and diverse commonsense inferences\nfor an input context. To evaluate DiffuCOMET, we introduce new metrics for\ncommonsense inference that more closely measure knowledge diversity and\ncontextual relevance. Our results on two different benchmarks, ComFact and\nWebNLG+, show that knowledge generated by DiffuCOMET achieves a better\ntrade-off between commonsense diversity, contextual relevance and alignment to\nknown gold references, compared to baseline knowledge models.\n","authors":["Silin Gao","Mete Ismayilzada","Mengjie Zhao","Hiromi Wakaki","Yuki Mitsufuji","Antoine Bosselut"],"pdf_url":"https://arxiv.org/pdf/2402.17011v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17010v1","updated":"2024-02-26T20:35:32Z","published":"2024-02-26T20:35:32Z","title":"Can Large Language Models Recall Reference Location Like Humans?","summary":"  When completing knowledge-intensive tasks, humans sometimes need not just an\nanswer but also a corresponding reference passage for auxiliary reading.\nPrevious methods required obtaining pre-segmented article chunks through\nadditional retrieval models. This paper explores leveraging the parameterized\nknowledge stored during the pre-training phase of large language models (LLMs)\nto independently recall reference passage from any starting position. We\npropose a two-stage framework that simulates the scenario of humans recalling\neasily forgotten references. Initially, the LLM is prompted to recall document\ntitle identifiers to obtain a coarse-grained document set. Then, based on the\nacquired coarse-grained document set, it recalls fine-grained passage. In the\ntwo-stage recall process, we use constrained decoding to ensure that content\noutside of the stored documents is not generated. To increase speed, we only\nrecall a short prefix in the second stage, then locate its position to retrieve\na complete passage. Experiments on KILT knowledge-sensitive tasks have verified\nthat LLMs can independently recall reference passage location in various task\nforms, and the obtained reference significantly assist downstream tasks.\n","authors":["Ye Wang","Xinrun Xu","Rui Xie","Wenxin Hu","Wei Ye"],"pdf_url":"https://arxiv.org/pdf/2402.17010v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17008v1","updated":"2024-02-26T20:33:50Z","published":"2024-02-26T20:33:50Z","title":"Benchmarking LLMs on the Semantic Overlap Summarization Task","summary":"  Semantic Overlap Summarization (SOS) is a constrained multi-document\nsummarization task, where the constraint is to capture the common/overlapping\ninformation between two alternative narratives. While recent advancements in\nLarge Language Models (LLMs) have achieved superior performance in numerous\nsummarization tasks, a benchmarking study of the SOS task using LLMs is yet to\nbe performed. As LLMs' responses are sensitive to slight variations in prompt\ndesign, a major challenge in conducting such a benchmarking study is to\nsystematically explore a variety of prompts before drawing a reliable\nconclusion. Fortunately, very recently, the TELeR taxonomy has been proposed\nwhich can be used to design and explore various prompts for LLMs. Using this\nTELeR taxonomy and 15 popular LLMs, this paper comprehensively evaluates LLMs\non the SOS Task, assessing their ability to summarize overlapping information\nfrom multiple alternative narratives. For evaluation, we report\nwell-established metrics like ROUGE, BERTscore, and SEM-F1$ on two different\ndatasets of alternative narratives. We conclude the paper by analyzing the\nstrengths and limitations of various LLMs in terms of their capabilities in\ncapturing overlapping information The code and datasets used to conduct this\nstudy are available at https://anonymous.4open.science/r/llm_eval-E16D.\n","authors":["John Salvador","Naman Bansal","Mousumi Akter","Souvika Sarkar","Anupam Das","Shubhra Kanti Karmaker"],"pdf_url":"https://arxiv.org/pdf/2402.17008v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05797v3","updated":"2024-02-26T20:33:36Z","published":"2023-10-09T15:31:03Z","title":"Are Large Language Models Post Hoc Explainers?","summary":"  The increasing use of predictive models in high-stakes settings highlights\nthe need for ensuring that relevant stakeholders understand and trust the\ndecisions made by these models. To this end, several approaches have been\nproposed in recent literature to explain the behavior of complex predictive\nmodels in a post hoc fashion. However, despite the growing number of such post\nhoc explanation techniques, many require white-box access to the model and/or\nare computationally expensive, highlighting the need for next-generation post\nhoc explainers. Recently, Large Language Models (LLMs) have emerged as powerful\ntools that are effective at a wide variety of tasks. However, their potential\nto explain the behavior of other complex predictive models remains relatively\nunexplored. In this work, we carry out one of the initial explorations to\nanalyze the effectiveness of LLMs in explaining other complex predictive\nmodels. To this end, we propose three novel approaches that exploit the\nin-context learning (ICL) capabilities of LLMs to explain the predictions made\nby other complex models. We conduct extensive experimentation with these\napproaches on real-world datasets to demonstrate that LLMs perform on par with\nstate-of-the-art post hoc explainers, opening up promising avenues for future\nresearch into LLM-based post hoc explanations of complex predictive models.\n","authors":["Nicholas Kroeger","Dan Ley","Satyapriya Krishna","Chirag Agarwal","Himabindu Lakkaraju"],"pdf_url":"https://arxiv.org/pdf/2310.05797v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.17464v2","updated":"2024-02-26T20:26:40Z","published":"2024-01-30T21:53:30Z","title":"Efficient Tool Use with Chain-of-Abstraction Reasoning","summary":"  To achieve faithful reasoning that aligns with human expectations, large\nlanguage models (LLMs) need to ground their reasoning to real-world knowledge\n(e.g., web facts, math and physical rules). Tools help LLMs access this\nexternal knowledge, but there remains challenges for fine-tuning LLM agents\n(e.g., Toolformer) to invoke tools in multi-step reasoning problems, where\ninter-connected tool calls require holistic and efficient tool usage planning.\n  In this work, we propose a new method for LLMs to better leverage tools in\nmulti-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to\nfirst decode reasoning chains with abstract placeholders, and then call domain\ntools to reify each reasoning chain by filling in specific knowledge. This\nplanning with abstract chains enables LLMs to learn more general reasoning\nstrategies, which are robust to shifts of domain knowledge (e.g., math results)\nrelevant to different reasoning questions. It also allows LLMs to perform\ndecoding and calling of external tools in parallel, which avoids the inference\ndelay caused by waiting for tool responses. In mathematical reasoning and Wiki\nQA domains, we show that our method consistently outperforms previous\nchain-of-thought and tool-augmented baselines on both in-distribution and\nout-of-distribution test sets, with an average ~6% absolute QA accuracy\nimprovement. LLM agents trained with our method also show more efficient tool\nuse, with inference speed being on average ~1.4x faster than baseline\ntool-augmented LLMs.\n","authors":["Silin Gao","Jane Dwivedi-Yu","Ping Yu","Xiaoqing Ellen Tan","Ramakanth Pasunuru","Olga Golovneva","Koustuv Sinha","Asli Celikyilmaz","Antoine Bosselut","Tianlu Wang"],"pdf_url":"https://arxiv.org/pdf/2401.17464v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16998v1","updated":"2024-02-26T20:13:58Z","published":"2024-02-26T20:13:58Z","title":"What Do Language Models Hear? Probing for Auditory Representations in\n  Language Models","summary":"  This work explores whether language models encode meaningfully grounded\nrepresentations of sounds of objects. We learn a linear probe that retrieves\nthe correct text representation of an object given a snippet of audio related\nto that object, where the sound representation is given by a pretrained audio\nmodel. This probe is trained via a contrastive loss that pushes the language\nrepresentations and sound representations of an object to be close to one\nanother. After training, the probe is tested on its ability to generalize to\nobjects that were not seen during training. Across different language models\nand audio models, we find that the probe generalization is above chance in many\ncases, indicating that despite being trained only on raw text, language models\nencode grounded knowledge of sounds for some objects.\n","authors":["Jerry Ngo","Yoon Kim"],"pdf_url":"https://arxiv.org/pdf/2402.16998v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14537v2","updated":"2024-02-26T19:42:20Z","published":"2023-03-25T19:03:57Z","title":"Deep Augmentation: Self-Supervised Learning with Transformations in\n  Activation Space","summary":"  We introduce Deep Augmentation, an approach to implicit data augmentation\nusing dropout or PCA to transform a targeted layer within a neural network to\nimprove performance and generalization. We demonstrate Deep Augmentation\nthrough extensive experiments on contrastive learning tasks in NLP, computer\nvision, and graph learning. We observe substantial performance gains with\nTransformers, ResNets, and Graph Neural Networks as the underlying models in\ncontrastive learning, but observe inverse effects on the corresponding\nsupervised problems. Our analysis suggests that Deep Augmentation alleviates\nco-adaption between layers, a form of \"collapse.\" We use this observation to\nformulate a method for selecting which layer to target; in particular, our\nexperimentation reveals that targeting deeper layers with Deep Augmentation\noutperforms augmenting the input data. The simple network- and\nmodality-agnostic nature of this approach enables its integration into various\nmachine learning pipelines.\n","authors":["Rickard Brüel-Gabrielsson","Tongzhou Wang","Manel Baradad","Justin Solomon"],"pdf_url":"https://arxiv.org/pdf/2303.14537v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16986v1","updated":"2024-02-26T19:35:45Z","published":"2024-02-26T19:35:45Z","title":"Long Dialog Summarization: An Analysis","summary":"  Dialog summarization has become increasingly important in managing and\ncomprehending large-scale conversations across various domains. This task\npresents unique challenges in capturing the key points, context, and nuances of\nmulti-turn long conversations for summarization. It is worth noting that the\nsummarization techniques may vary based on specific requirements such as in a\nshopping-chatbot scenario, the dialog summary helps to learn user preferences,\nwhereas in the case of a customer call center, the summary may involve the\nproblem attributes that a user specified, and the final resolution provided.\nThis work emphasizes the significance of creating coherent and contextually\nrich summaries for effective communication in various applications. We explore\ncurrent state-of-the-art approaches for long dialog summarization in different\ndomains and benchmark metrics based evaluations show that one single model does\nnot perform well across various areas for distinct summarization tasks.\n","authors":["Ankan Mullick","Ayan Kumar Bhowmick","Raghav R","Ravi Kokku","Prasenjit Dey","Pawan Goyal","Niloy Ganguly"],"pdf_url":"https://arxiv.org/pdf/2402.16986v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16977v1","updated":"2024-02-26T19:19:47Z","published":"2024-02-26T19:19:47Z","title":"Dealing with Data for RE: Mitigating Challenges using NLP and Generative\n  AI","summary":"  Across the dynamic business landscape today, enterprises face an\never-increasing range of challenges. These include the constantly evolving\nregulatory environment, the growing demand for personalization within software\napplications, and the heightened emphasis on governance. In response to these\nmultifaceted demands, large enterprises have been adopting automation that\nspans from the optimization of core business processes to the enhancement of\ncustomer experiences. Indeed, Artificial Intelligence (AI) has emerged as a\npivotal element of modern software systems. In this context, data plays an\nindispensable role. AI-centric software systems based on supervised learning\nand operating at an industrial scale require large volumes of training data to\nperform effectively. Moreover, the incorporation of generative AI has led to a\ngrowing demand for adequate evaluation benchmarks. Our experience in this field\nhas revealed that the requirement for large datasets for training and\nevaluation introduces a host of intricate challenges. This book chapter\nexplores the evolving landscape of Software Engineering (SE) in general, and\nRequirements Engineering (RE) in particular, in this era marked by AI\nintegration. We discuss challenges that arise while integrating Natural\nLanguage Processing (NLP) and generative AI into enterprise-critical software\nsystems. The chapter provides practical insights, solutions, and examples to\nequip readers with the knowledge and tools necessary for effectively building\nsolutions with NLP at their cores. We also reflect on how these text\ndata-centric tasks sit together with the traditional RE process. We also\nhighlight new RE tasks that may be necessary for handling the increasingly\nimportant text data-centricity involved in developing software systems.\n","authors":["Smita Ghaisas","Anmol Singhal"],"pdf_url":"https://arxiv.org/pdf/2402.16977v1.pdf","comment":"24 pages, 2 figures, to be published in NLP for Requirements\n  Engineering Book"},{"id":"http://arxiv.org/abs/2402.16973v1","updated":"2024-02-26T19:16:04Z","published":"2024-02-26T19:16:04Z","title":"Successfully Guiding Humans with Imperfect Instructions by Highlighting\n  Potential Errors and Suggesting Corrections","summary":"  This paper addresses the challenge of leveraging imperfect language models to\nguide human decision-making in the context of a grounded navigation task. We\nshow that an imperfect instruction generation model can be complemented with an\neffective communication mechanism to become more successful at guiding humans.\nThe communication mechanism we build comprises models that can detect potential\nhallucinations in instructions and suggest practical alternatives, and an\nintuitive interface to present that information to users. We show that this\napproach reduces the human navigation error by up to 29% with no additional\ncognitive burden. This result underscores the potential of integrating diverse\ncommunication channels into AI systems to compensate for their imperfections\nand enhance their utility for humans.\n","authors":["Lingjun Zhao","Khanh Nguyen","Hal Daumé III"],"pdf_url":"https://arxiv.org/pdf/2402.16973v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16846v1","updated":"2024-02-26T18:59:33Z","published":"2024-02-26T18:59:33Z","title":"GROUNDHOG: Grounding Large Language Models to Holistic Segmentation","summary":"  Most multimodal large language models (MLLMs) learn language-to-object\ngrounding through causal language modeling where grounded objects are captured\nby bounding boxes as sequences of location tokens. This paradigm lacks\npixel-level representations that are important for fine-grained visual\nunderstanding and diagnosis. In this work, we introduce GROUNDHOG, an MLLM\ndeveloped by grounding Large Language Models to holistic segmentation.\nGROUNDHOG incorporates a masked feature extractor and converts extracted\nfeatures into visual entity tokens for the MLLM backbone, which then connects\ngroundable phrases to unified grounding masks by retrieving and merging the\nentity masks. To train GROUNDHOG, we carefully curated M3G2, a grounded visual\ninstruction tuning dataset with Multi-Modal Multi-Grained Grounding, by\nharvesting a collection of segmentation-grounded datasets with rich\nannotations. Our experimental results show that GROUNDHOG achieves superior\nperformance on various language grounding tasks without task-specific\nfine-tuning, and significantly reduces object hallucination. GROUNDHOG also\ndemonstrates better grounding towards complex forms of visual input and\nprovides easy-to-understand diagnosis in failure cases.\n","authors":["Yichi Zhang","Ziqiao Ma","Xiaofeng Gao","Suhaila Shakiah","Qiaozi Gao","Joyce Chai"],"pdf_url":"https://arxiv.org/pdf/2402.16846v1.pdf","comment":"Website: https://groundhog-mllm.github.io/"},{"id":"http://arxiv.org/abs/2402.16844v1","updated":"2024-02-26T18:59:28Z","published":"2024-02-26T18:59:28Z","title":"Think Big, Generate Quick: LLM-to-SLM for Fast Autoregressive Decoding","summary":"  Large language models (LLMs) have become ubiquitous in practice and are\nwidely used for generation tasks such as translation, summarization and\ninstruction following. However, their enormous size and reliance on\nautoregressive decoding increase deployment costs and complicate their use in\nlatency-critical applications. In this work, we propose a hybrid approach that\ncombines language models of different sizes to increase the efficiency of\nautoregressive decoding while maintaining high performance. Our method utilizes\na pretrained frozen LLM that encodes all prompt tokens once in parallel, and\nuses the resulting representations to condition and guide a small language\nmodel (SLM), which then generates the response more efficiently. We investigate\nthe combination of encoder-decoder LLMs with both encoder-decoder and\ndecoder-only SLMs from different model families and only require fine-tuning of\nthe SLM. Experiments with various benchmarks show substantial speedups of up to\n$4\\times$, with minor performance penalties of $1-2\\%$ for translation and\nsummarization tasks compared to the LLM.\n","authors":["Benjamin Bergner","Andrii Skliar","Amelie Royer","Tijmen Blankevoort","Yuki Asano","Babak Ehteshami Bejnordi"],"pdf_url":"https://arxiv.org/pdf/2402.16844v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16843v1","updated":"2024-02-26T18:59:18Z","published":"2024-02-26T18:59:18Z","title":"Multi-LoRA Composition for Image Generation","summary":"  Low-Rank Adaptation (LoRA) is extensively utilized in text-to-image models\nfor the accurate rendition of specific elements like distinct characters or\nunique styles in generated images. Nonetheless, existing methods face\nchallenges in effectively composing multiple LoRAs, especially as the number of\nLoRAs to be integrated grows, thus hindering the creation of complex imagery.\nIn this paper, we study multi-LoRA composition through a decoding-centric\nperspective. We present two training-free methods: LoRA Switch, which\nalternates between different LoRAs at each denoising step, and LoRA Composite,\nwhich simultaneously incorporates all LoRAs to guide more cohesive image\nsynthesis. To evaluate the proposed approaches, we establish ComposLoRA, a new\ncomprehensive testbed as part of this research. It features a diverse range of\nLoRA categories with 480 composition sets. Utilizing an evaluation framework\nbased on GPT-4V, our findings demonstrate a clear improvement in performance\nwith our methods over the prevalent baseline, particularly evident when\nincreasing the number of LoRAs in a composition.\n","authors":["Ming Zhong","Yelong Shen","Shuohang Wang","Yadong Lu","Yizhu Jiao","Siru Ouyang","Donghan Yu","Jiawei Han","Weizhu Chen"],"pdf_url":"https://arxiv.org/pdf/2402.16843v1.pdf","comment":"Project Website:\n  https://maszhongming.github.io/Multi-LoRA-Composition/"},{"id":"http://arxiv.org/abs/2402.16840v1","updated":"2024-02-26T18:59:03Z","published":"2024-02-26T18:59:03Z","title":"MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT","summary":"  \"Bigger the better\" has been the predominant trend in recent Large Language\nModels (LLMs) development. However, LLMs do not suit well for scenarios that\nrequire on-device processing, energy efficiency, low memory footprint, and\nresponse efficiency. These requisites are crucial for privacy, security, and\nsustainable deployment. This paper explores the \"less is more\" paradigm by\naddressing the challenge of designing accurate yet efficient Small Language\nModels (SLMs) for resource constrained devices. Our primary contribution is the\nintroduction of an accurate and fully transparent open-source 0.5 billion\n(0.5B) parameter SLM, named MobiLlama, catering to the specific needs of\nresource-constrained computing with an emphasis on enhanced performance with\nreduced resource demands. MobiLlama is a SLM design that initiates from a\nlarger model and applies a careful parameter sharing scheme to reduce both the\npre-training and the deployment cost. Our work strives to not only bridge the\ngap in open-source SLMs but also ensures full transparency, where complete\ntraining data pipeline, training code, model weights, and over 300 checkpoints\nalong with evaluation codes is available at :\nhttps://github.com/mbzuai-oryx/MobiLlama.\n","authors":["Omkar Thawakar","Ashmal Vayani","Salman Khan","Hisham Cholakal","Rao M. Anwer","Michael Felsberg","Tim Baldwin","Eric P. Xing","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2402.16840v1.pdf","comment":"Code available at : https://github.com/mbzuai-oryx/MobiLlama"},{"id":"http://arxiv.org/abs/2402.16837v1","updated":"2024-02-26T18:57:54Z","published":"2024-02-26T18:57:54Z","title":"Do Large Language Models Latently Perform Multi-Hop Reasoning?","summary":"  We study whether Large Language Models (LLMs) latently perform multi-hop\nreasoning with complex prompts such as \"The mother of the singer of\n'Superstition' is\". We look for evidence of a latent reasoning pathway where an\nLLM (1) latently identifies \"the singer of 'Superstition'\" as Stevie Wonder,\nthe bridge entity, and (2) uses its knowledge of Stevie Wonder's mother to\ncomplete the prompt. We analyze these two hops individually and consider their\nco-occurrence as indicative of latent multi-hop reasoning. For the first hop,\nwe test if changing the prompt to indirectly mention the bridge entity instead\nof any other entity increases the LLM's internal recall of the bridge entity.\nFor the second hop, we test if increasing this recall causes the LLM to better\nutilize what it knows about the bridge entity. We find strong evidence of\nlatent multi-hop reasoning for the prompts of certain relation types, with the\nreasoning pathway used in more than 80% of the prompts. However, the\nutilization is highly contextual, varying across different types of prompts.\nAlso, on average, the evidence for the second hop and the full multi-hop\ntraversal is rather moderate and only substantial for the first hop. Moreover,\nwe find a clear scaling trend with increasing model size for the first hop of\nreasoning but not for the second hop. Our experimental findings suggest\npotential challenges and opportunities for future development and applications\nof LLMs.\n","authors":["Sohee Yang","Elena Gribovskaya","Nora Kassner","Mor Geva","Sebastian Riedel"],"pdf_url":"https://arxiv.org/pdf/2402.16837v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16836v1","updated":"2024-02-26T18:57:52Z","published":"2024-02-26T18:57:52Z","title":"PhyGrasp: Generalizing Robotic Grasping with Physics-informed Large\n  Multimodal Models","summary":"  Robotic grasping is a fundamental aspect of robot functionality, defining how\nrobots interact with objects. Despite substantial progress, its\ngeneralizability to counter-intuitive or long-tailed scenarios, such as objects\nwith uncommon materials or shapes, remains a challenge. In contrast, humans can\neasily apply their intuitive physics to grasp skillfully and change grasps\nefficiently, even for objects they have never seen before.\n  This work delves into infusing such physical commonsense reasoning into\nrobotic manipulation. We introduce PhyGrasp, a multimodal large model that\nleverages inputs from two modalities: natural language and 3D point clouds,\nseamlessly integrated through a bridge module. The language modality exhibits\nrobust reasoning capabilities concerning the impacts of diverse physical\nproperties on grasping, while the 3D modality comprehends object shapes and\nparts. With these two capabilities, PhyGrasp is able to accurately assess the\nphysical properties of object parts and determine optimal grasping poses.\nAdditionally, the model's language comprehension enables human instruction\ninterpretation, generating grasping poses that align with human preferences. To\ntrain PhyGrasp, we construct a dataset PhyPartNet with 195K object instances\nwith varying physical properties and human preferences, alongside their\ncorresponding language descriptions. Extensive experiments conducted in the\nsimulation and on the real robots demonstrate that PhyGrasp achieves\nstate-of-the-art performance, particularly in long-tailed cases, e.g., about\n10% improvement in success rate over GraspNet. Project page:\nhttps://sites.google.com/view/phygrasp\n","authors":["Dingkun Guo","Yuqi Xiang","Shuqi Zhao","Xinghao Zhu","Masayoshi Tomizuka","Mingyu Ding","Wei Zhan"],"pdf_url":"https://arxiv.org/pdf/2402.16836v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16835v1","updated":"2024-02-26T18:57:37Z","published":"2024-02-26T18:57:37Z","title":"Eight Methods to Evaluate Robust Unlearning in LLMs","summary":"  Machine unlearning can be useful for removing harmful capabilities and\nmemorized text from large language models (LLMs), but there are not yet\nstandardized methods for rigorously evaluating it. In this paper, we first\nsurvey techniques and limitations of existing unlearning evaluations. Second,\nwe apply a comprehensive set of tests for the robustness and competitiveness of\nunlearning in the \"Who's Harry Potter\" (WHP) model from Eldan and Russinovich\n(2023). While WHP's unlearning generalizes well when evaluated with the\n\"Familiarity\" metric from Eldan and Russinovich, we find i)\nhigher-than-baseline amounts of knowledge can reliably be extracted, ii) WHP\nperforms on par with the original model on Harry Potter Q&A tasks, iii) it\nrepresents latent knowledge comparably to the original model, and iv) there is\ncollateral unlearning in related domains. Overall, our results highlight the\nimportance of comprehensive unlearning evaluation that avoids ad-hoc metrics.\n","authors":["Aengus Lynch","Phillip Guo","Aidan Ewart","Stephen Casper","Dylan Hadfield-Menell"],"pdf_url":"https://arxiv.org/pdf/2402.16835v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16832v1","updated":"2024-02-26T18:56:48Z","published":"2024-02-26T18:56:48Z","title":"Mysterious Projections: Multimodal LLMs Gain Domain-Specific Visual\n  Capabilities Without Richer Cross-Modal Projections","summary":"  Multimodal large language models (MLLMs) like LLaVA and GPT-4(V) enable\ngeneral-purpose conversations about images with the language modality. As\noff-the-shelf MLLMs may have limited capabilities on images from domains like\ndermatology and agriculture, they must be fine-tuned to unlock domain-specific\napplications. The prevalent architecture of current open-source MLLMs comprises\ntwo major modules: an image-language (cross-modal) projection network and a\nlarge language model. It is desirable to understand the roles of these two\nmodules in modeling domain-specific visual attributes to inform the design of\nfuture models and streamline the interpretability efforts on the current\nmodels. To this end, via experiments on 4 datasets and under 2 fine-tuning\nsettings, we find that as the MLLM is fine-tuned, it indeed gains\ndomain-specific visual capabilities, but the updates do not lead to the\nprojection extracting relevant domain-specific visual attributes. Our results\nindicate that the domain-specific visual attributes are modeled by the LLM,\neven when only the projection is fine-tuned. Through this study, we offer a\npotential reinterpretation of the role of cross-modal projections in MLLM\narchitectures. Projection webpage:\nhttps://claws-lab.github.io/projection-in-MLLMs/\n","authors":["Gaurav Verma","Minje Choi","Kartik Sharma","Jamelle Watson-Daniels","Sejoon Oh","Srijan Kumar"],"pdf_url":"https://arxiv.org/pdf/2402.16832v1.pdf","comment":"8 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2402.16830v1","updated":"2024-02-26T18:56:42Z","published":"2024-02-26T18:56:42Z","title":"SKILL: Similarity-aware Knowledge distILLation for Speech\n  Self-Supervised Learning","summary":"  Self-supervised learning (SSL) has achieved remarkable success across various\nspeech-processing tasks. To enhance its efficiency, previous works often\nleverage the use of compression techniques. A notable recent attempt is\nDPHuBERT, which applies joint knowledge distillation (KD) and structured\npruning to learn a significantly smaller SSL model. In this paper, we\ncontribute to this research domain by introducing SKILL, a novel method that\nconducts distillation across groups of layers instead of distilling individual\narbitrarily selected layers within the teacher network. The identification of\nthe layers to distill is achieved through a hierarchical clustering procedure\napplied to layer similarity measures. Extensive experiments demonstrate that\nour distilled version of WavLM Base+ not only outperforms DPHuBERT but also\nachieves state-of-the-art results in the 30M parameters model class across\nseveral SUPERB tasks.\n","authors":["Luca Zampierin","Ghouthi Boukli Hacene","Bac Nguyen","Mirco Ravanelli"],"pdf_url":"https://arxiv.org/pdf/2402.16830v1.pdf","comment":"Accepted at the Self-supervision in Audio, Speech and Beyond (SASB)\n  Workshop at ICASSP 2024"},{"id":"http://arxiv.org/abs/2310.14053v3","updated":"2024-02-26T18:56:08Z","published":"2023-10-21T16:14:56Z","title":"Beyond Accuracy: Evaluating Self-Consistency of Code Large Language\n  Models with IdentityChain","summary":"  Code Large Language Models (Code LLMs) are being increasingly employed in\nreal-life applications, so evaluating them is critical. While the conventional\naccuracy evaluates the performance of Code LLMs on a set of individual tasks,\ntheir self-consistency across different tasks is overlooked. Intuitively, a\ntrustworthy model should be self-consistent when generating natural language\nspecifications for its own code and generating code for its own specifications.\nFailure to preserve self-consistency reveals a lack of understanding of the\nshared semantics underlying natural language and programming language, and\ntherefore undermines the trustworthiness of a model. In this paper, we first\nformally define the self-consistency of Code LLMs and then design a framework,\nIdentityChain, which effectively and efficiently evaluates the self-consistency\nand conventional accuracy of a model at the same time. We study eleven Code\nLLMs and show that they fail to preserve self-consistency, which is indeed a\ndistinct aspect from conventional accuracy. Furthermore, we show that\nIdentityChain can be used as a model debugging tool to expose weaknesses of\nCode LLMs by demonstrating three major weaknesses that we identify in current\nmodels using IdentityChain. Our code is available at\nhttps://github.com/marcusm117/IdentityChain.\n","authors":["Marcus J. Min","Yangruibo Ding","Luca Buratti","Saurabh Pujar","Gail Kaiser","Suman Jana","Baishakhi Ray"],"pdf_url":"https://arxiv.org/pdf/2310.14053v3.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2402.16829v1","updated":"2024-02-26T18:55:15Z","published":"2024-02-26T18:55:15Z","title":"GISTEmbed: Guided In-sample Selection of Training Negatives for Text\n  Embedding Fine-tuning","summary":"  Embedding models are integral to AI applications like semantic search,\npersonalized recommendations, and retrieval augmented generation for LLMs,\nnecessitating high-quality training data. However, the limited scalability of\nmanual data curation prompts the need for automated methods to ensure data\nintegrity. Traditional unsupervised triplet mining automates training data\ngeneration, crucial for embedding model training, yet inadvertently injects\nbiases and noise, thereby degrading model performance. Addressing this, we\nintroduce GISTEmbed, a novel strategy that enhances in-batch negative selection\nduring contrastive training through a guide model. This approach departs from\nreliance on random sampling and equal utility assumption of batch negatives,\nsignificantly reducing noise from data quality issues and improving model\nfine-tuning. Benchmarked against the Massive Text Embedding Benchmark (MTEB),\nGISTEmbed showcases consistent performance improvements across various model\nsizes and achieves state-of-the-art results in select categories. This\nframework enables significant enhancements for smaller models by leveraging the\ncapabilities of powerful yet resource-intensive large models. GISTEmbed can\npotentially revolutionize the creation of highly efficient, smaller models,\ndemocratizing access to advanced AI technologies. Making these technologies\nmore accessible and cost-effective, especially for applications constrained by\nresources, significantly expands the impact and accessibility of\nstate-of-the-art AI solutions across diverse sectors.\n","authors":["Aivin V. Solatorio"],"pdf_url":"https://arxiv.org/pdf/2402.16829v1.pdf","comment":"GISTEmbed GitHub repository at\n  https://github.com/avsolatorio/GISTEmbed"},{"id":"http://arxiv.org/abs/2402.16827v1","updated":"2024-02-26T18:54:35Z","published":"2024-02-26T18:54:35Z","title":"A Survey on Data Selection for Language Models","summary":"  A major factor in the recent success of large language models is the use of\nenormous and ever-growing text datasets for unsupervised pre-training. However,\nnaively training a model on all available data may not be optimal (or\nfeasible), as the quality of available text data can vary. Filtering out data\ncan also decrease the carbon footprint and financial costs of training models\nby reducing the amount of training required.\n  Data selection methods aim to determine which candidate data points to\ninclude in the training dataset and how to appropriately sample from the\nselected data points. The promise of improved data selection methods has caused\nthe volume of research in the area to rapidly expand. However, because deep\nlearning is mostly driven by empirical evidence and experimentation on\nlarge-scale data is expensive, few organizations have the resources for\nextensive data selection research. Consequently, knowledge of effective data\nselection practices has become concentrated within a few organizations, many of\nwhich do not openly share their findings and methodologies.\n  To narrow this gap in knowledge, we present a comprehensive review of\nexisting literature on data selection methods and related research areas,\nproviding a taxonomy of existing approaches. By describing the current\nlandscape of research, this work aims to accelerate progress in data selection\nby establishing an entry point for new and established researchers.\nAdditionally, throughout this review we draw attention to noticeable holes in\nthe literature and conclude the paper by proposing promising avenues for future\nresearch.\n","authors":["Alon Albalak","Yanai Elazar","Sang Michael Xie","Shayne Longpre","Nathan Lambert","Xinyi Wang","Niklas Muennighoff","Bairu Hou","Liangming Pan","Haewon Jeong","Colin Raffel","Shiyu Chang","Tatsunori Hashimoto","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2402.16827v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16822v1","updated":"2024-02-26T18:47:27Z","published":"2024-02-26T18:47:27Z","title":"Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts","summary":"  As large language models (LLMs) become increasingly prevalent across many\nreal-world applications, understanding and enhancing their robustness to user\ninputs is of paramount importance. Existing methods for identifying adversarial\nprompts tend to focus on specific domains, lack diversity, or require extensive\nhuman annotations. To address these limitations, we present Rainbow Teaming, a\nnovel approach for producing a diverse collection of adversarial prompts.\nRainbow Teaming casts adversarial prompt generation as a quality-diversity\nproblem, and uses open-ended search to generate prompts that are both effective\nand diverse. It can uncover a model's vulnerabilities across a broad range of\ndomains including, in this paper, safety, question answering, and\ncybersecurity. We also demonstrate that fine-tuning on synthetic data generated\nby Rainbow Teaming improves the safety of state-of-the-art LLMs without hurting\ntheir general capabilities and helpfulness, paving the path to open-ended\nself-improvement.\n","authors":["Mikayel Samvelyan","Sharath Chandra Raparthy","Andrei Lupu","Eric Hambro","Aram H. Markosyan","Manish Bhatt","Yuning Mao","Minqi Jiang","Jack Parker-Holder","Jakob Foerster","Tim Rocktäschel","Roberta Raileanu"],"pdf_url":"https://arxiv.org/pdf/2402.16822v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16817v1","updated":"2024-02-26T18:42:26Z","published":"2024-02-26T18:42:26Z","title":"Investigating the Effectiveness of HyperTuning via Gisting","summary":"  Gisting (Mu et al., 2023) is a simple method for training models to compress\ninformation into fewer token representations using a modified attention mask,\nand can serve as an economical approach to training Transformer-based\nhypernetworks. We introduce HyperLlama, a set of Gisting-based hypernetworks\nbuilt on Llama-2 models that generates task-specific soft prefixes based on\nfew-shot inputs. In experiments across P3, Super-NaturalInstructions and Symbol\nTuning datasets, we show that HyperLlama models can effectively compress\ninformation from few-shot examples into soft prefixes. However, they still\nunderperform multi-task fine-tuned language models with full attention over\nfew-shot in-context examples. We also show that HyperLlama-generated soft\nprefixes can serve as better initializations for further prefix tuning.\nOverall, Gisting-based hypernetworks are economical and easy to implement, but\nhave mixed empirical performance.\n","authors":["Jason Phang"],"pdf_url":"https://arxiv.org/pdf/2402.16817v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16810v1","updated":"2024-02-26T18:33:13Z","published":"2024-02-26T18:33:13Z","title":"OncoGPT: A Medical Conversational Model Tailored with Oncology Domain\n  Expertise on a Large Language Model Meta-AI (LLaMA)","summary":"  In the past year, there has been a growing trend in applying Large Language\nModels (LLMs) to the field of medicine, particularly with the advent of\nadvanced language models such as ChatGPT developed by OpenAI. However, there is\nlimited research on LLMs specifically addressing oncology-related queries. The\nprimary aim of this research was to develop a specialized language model that\ndemonstrates improved accuracy in providing advice related to oncology. We\nperformed an extensive data collection of online question-answer interactions\ncentered around oncology, sourced from reputable doctor-patient platforms.\nFollowing data cleaning and anonymization, a dataset comprising over 180K+\noncology-related conversations was established. The conversations were\ncategorized and meticulously reviewed by field specialists and clinicians to\nensure precision. Employing the LLaMA model and other selected open-source\ndatasets, we conducted iterative fine-tuning to enhance the model's proficiency\nin basic medical conversation and specialized oncology knowledge. We observed a\nsubstantial enhancement in the model's understanding of genuine patient\ninquiries and its reliability in offering oncology-related advice through the\nutilization of real online question-answer interactions in the fine-tuning\nprocess. We release database and models to the research community\n(https://github.com/OncoGPT1).\n","authors":["Fujian Jia","Xin Liu","Lixi Deng","Jiwen Gu","Chunchao Pu","Tunan Bai","Mengjiang Huang","Yuanzhi Lu","Kang Liu"],"pdf_url":"https://arxiv.org/pdf/2402.16810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14403v5","updated":"2024-02-26T18:29:45Z","published":"2023-10-22T20:28:33Z","title":"O3D: Offline Data-driven Discovery and Distillation for Sequential\n  Decision-Making with Large Language Models","summary":"  Recent advancements in large language models (LLMs) have exhibited promising\nperformance in solving sequential decision-making problems. By imitating\nfew-shot examples provided in the prompts (i.e., in-context learning), an LLM\nagent can interact with an external environment and complete given tasks\nwithout additional training. However, such few-shot examples are often\ninsufficient to generate high-quality solutions for complex and long-horizon\ntasks, while the limited context length cannot consume larger-scale\ndemonstrations with long interaction horizons. To this end, we propose an\noffline learning framework that utilizes offline data at scale (e.g, logs of\nhuman interactions) to improve LLM-powered policies without finetuning. The\nproposed method O3D (Offline Data-driven Discovery and Distillation)\nautomatically discovers reusable skills and distills generalizable knowledge\nacross multiple tasks based on offline interaction data, advancing the\ncapability of solving downstream tasks. Empirical results under two interactive\ndecision-making benchmarks (ALFWorld and WebShop) verify that O3D can notably\nenhance the decision-making capabilities of LLMs through the offline discovery\nand distillation process, and consistently outperform baselines across various\nLLMs.\n","authors":["Yuchen Xiao","Yanchao Sun","Mengda Xu","Udari Madhushani","Jared Vann","Deepeka Garg","Sumitra Ganesh"],"pdf_url":"https://arxiv.org/pdf/2310.14403v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15506v2","updated":"2024-02-26T18:24:46Z","published":"2024-02-23T18:56:26Z","title":"AgentOhana: Design Unified Data and Training Pipeline for Effective\n  Agent Learning","summary":"  Autonomous agents powered by large language models (LLMs) have garnered\nsignificant research attention. However, fully harnessing the potential of LLMs\nfor agent-based tasks presents inherent challenges due to the heterogeneous\nnature of diverse data sources featuring multi-turn trajectories. In this\npaper, we introduce \\textbf{AgentOhana} as a comprehensive solution to address\nthese challenges. \\textit{AgentOhana} aggregates agent trajectories from\ndistinct environments, spanning a wide array of scenarios. It meticulously\nstandardizes and unifies these trajectories into a consistent format,\nstreamlining the creation of a generic data loader optimized for agent\ntraining. Leveraging the data unification, our training pipeline maintains\nequilibrium across different data sources and preserves independent randomness\nacross devices during dataset partitioning and model training. Additionally, we\npresent \\textbf{xLAM-v0.1}, a large action model tailored for AI agents, which\ndemonstrates exceptional performance across various benchmarks.\n","authors":["Jianguo Zhang","Tian Lan","Rithesh Murthy","Zhiwei Liu","Weiran Yao","Juntao Tan","Thai Hoang","Liangwei Yang","Yihao Feng","Zuxin Liu","Tulika Awalgaonkar","Juan Carlos Niebles","Silvio Savarese","Shelby Heinecke","Huan Wang","Caiming Xiong"],"pdf_url":"https://arxiv.org/pdf/2402.15506v2.pdf","comment":"Add testing results on ToolBench and correct typographical errors"},{"id":"http://arxiv.org/abs/2402.16797v1","updated":"2024-02-26T18:10:56Z","published":"2024-02-26T18:10:56Z","title":"Set the Clock: Temporal Alignment of Pretrained Language Models","summary":"  Language models (LMs) are trained on web text originating from many points in\ntime and, in general, without any explicit temporal grounding. This work\ninvestigates the temporal chaos of pretrained LMs and explores various methods\nto align their internal knowledge to a target time, which we call \"temporal\nalignment.\" To do this, we first automatically construct a dataset containing\n20K time-sensitive questions and their answers for each year from 2000 to 2023.\nBased on this dataset, we empirically show that pretrained LMs (e.g., LLaMa2),\ndespite having a recent pretraining cutoff (e.g., 2022), mostly answer\nquestions using earlier knowledge (e.g., in 2019). We then develop several\nmethods, from prompting to finetuning, to align LMs to use their most recent\nknowledge when answering questions, and investigate various factors in this\nalignment. Our experiments show that aligning LLaMa2 to the year 2022 can boost\nits performance by up to 62% relatively as measured by that year, even without\nmentioning time information explicitly, indicating the possibility of aligning\nmodels' internal sense of time after pretraining. Finally, we find that\nalignment to a historical time is also possible, with up to 2.8$\\times$ the\nperformance of the unaligned LM in 2010 if finetuning models to that year.\nThese findings hint at the sophistication of LMs' internal knowledge\norganization and the necessity of tuning them properly.\n","authors":["Bowen Zhao","Zander Brumbaugh","Yizhong Wang","Hannaneh Hajishirzi","Noah A. Smith"],"pdf_url":"https://arxiv.org/pdf/2402.16797v1.pdf","comment":"25 pages, 7 figures. Our code and data will be available at\n  https://github.com/yizhongw/llm-temporal-alignment"},{"id":"http://arxiv.org/abs/2402.16786v1","updated":"2024-02-26T18:00:49Z","published":"2024-02-26T18:00:49Z","title":"Political Compass or Spinning Arrow? Towards More Meaningful Evaluations\n  for Values and Opinions in Large Language Models","summary":"  Much recent work seeks to evaluate values and opinions in large language\nmodels (LLMs) using multiple-choice surveys and questionnaires. Most of this\nwork is motivated by concerns around real-world LLM applications. For example,\npolitically-biased LLMs may subtly influence society when they are used by\nmillions of people. Such real-world concerns, however, stand in stark contrast\nto the artificiality of current evaluations: real users do not typically ask\nLLMs survey questions. Motivated by this discrepancy, we challenge the\nprevailing constrained evaluation paradigm for values and opinions in LLMs and\nexplore more realistic unconstrained evaluations. As a case study, we focus on\nthe popular Political Compass Test (PCT). In a systematic review, we find that\nmost prior work using the PCT forces models to comply with the PCT's\nmultiple-choice format. We show that models give substantively different\nanswers when not forced; that answers change depending on how models are\nforced; and that answers lack paraphrase robustness. Then, we demonstrate that\nmodels give different answers yet again in a more realistic open-ended answer\nsetting. We distill these findings into recommendations and open challenges in\nevaluating values and opinions in LLMs.\n","authors":["Paul Röttger","Valentin Hofmann","Valentina Pyatkin","Musashi Hinck","Hannah Rose Kirk","Hinrich Schütze","Dirk Hovy"],"pdf_url":"https://arxiv.org/pdf/2402.16786v1.pdf","comment":"v1 prepared for conference submission"},{"id":"http://arxiv.org/abs/2402.16775v1","updated":"2024-02-26T17:45:36Z","published":"2024-02-26T17:45:36Z","title":"A Comprehensive Evaluation of Quantization Strategies for Large Language\n  Models","summary":"  Increasing the number of parameters in large language models (LLMs) usually\nimproves performance in downstream tasks but raises compute and memory costs,\nmaking deployment difficult in resource-limited settings. Quantization\ntechniques, which reduce the bits needed for model weights or activations with\nminimal performance loss, have become popular due to the rise of LLMs. However,\nmost quantization studies use pre-trained LLMs, and the impact of quantization\non instruction-tuned LLMs and the relationship between perplexity and benchmark\nperformance of quantized LLMs are not well understood. Evaluation of quantized\nLLMs is often limited to language modeling and a few classification tasks,\nleaving their performance on other benchmarks unclear. To address these gaps,\nwe propose a structured evaluation framework consisting of three critical\ndimensions: (1) knowledge \\& capacity, (2) alignment, and (3) efficiency, and\nconduct extensive experiments across ten diverse benchmarks. Our experimental\nresults indicate that LLMs with 4-bit quantization can retain performance\ncomparable to their non-quantized counterparts, and perplexity can serve as a\nproxy metric for quantized LLMs on most benchmarks. Furthermore, quantized LLMs\nwith larger parameter scales can outperform smaller LLMs. Despite the memory\nsavings achieved through quantization, it can also slow down the inference\nspeed of LLMs. Consequently, substantial engineering efforts and hardware\nsupport are imperative to achieve a balanced optimization of decoding speed and\nmemory consumption in the context of quantized LLMs.\n","authors":["Renren Jin","Jiangcun Du","Wuwei Huang","Wei Liu","Jian Luan","Bin Wang","Deyi Xiong"],"pdf_url":"https://arxiv.org/pdf/2402.16775v1.pdf","comment":"20 pages, 16 figures, 16 tables"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2402.17081v1","updated":"2024-02-26T23:37:59Z","published":"2024-02-26T23:37:59Z","title":"A Fine-tuning Enhanced RAG System with Quantized Influence Measure as AI\n  Judge","summary":"  This study presents an innovative enhancement to retrieval-augmented\ngeneration (RAG) systems by seamlessly integrating fine-tuned large language\nmodels (LLMs) with vector databases. This integration capitalizes on the\ncombined strengths of structured data retrieval and the nuanced comprehension\nprovided by advanced LLMs. Central to our approach are the LoRA and QLoRA\nmethodologies, which stand at the forefront of model refinement through\nparameter-efficient fine-tuning and memory optimization. A novel feature of our\nresearch is the incorporation of user feedback directly into the training\nprocess, ensuring the model's continuous adaptation to user expectations and\nthus, improving its performance and applicability. Additionally, we introduce a\nQuantized Influence Measure (QIM) as an innovative \"AI Judge\" mechanism to\nenhance the precision of result selection, further refining the system's\naccuracy. Accompanied by an executive diagram and a detailed algorithm for\nfine-tuning QLoRA, our work provides a comprehensive framework for implementing\nthese advancements within chatbot technologies. This research contributes\nsignificant insights into LLM optimization for specific uses and heralds new\ndirections for further development in retrieval-augmented models. Through\nextensive experimentation and analysis, our findings lay a robust foundation\nfor future advancements in chatbot technology and retrieval systems, marking a\nsignificant step forward in the creation of more sophisticated, precise, and\nuser-centric conversational AI systems.\n","authors":["Keshav Rangan","Yiqiao Yin"],"pdf_url":"https://arxiv.org/pdf/2402.17081v1.pdf","comment":"17 pages, 4 figures"},{"id":"http://arxiv.org/abs/2309.08541v2","updated":"2024-02-26T20:57:33Z","published":"2023-09-15T17:05:43Z","title":"When do Generative Query and Document Expansions Fail? A Comprehensive\n  Study Across Methods, Retrievers, and Datasets","summary":"  Using large language models (LMs) for query or document expansion can improve\ngeneralization in information retrieval. However, it is unknown whether these\ntechniques are universally beneficial or only effective in specific settings,\nsuch as for particular retrieval models, dataset domains, or query types. To\nanswer this, we conduct the first comprehensive analysis of LM-based expansion.\nWe find that there exists a strong negative correlation between retriever\nperformance and gains from expansion: expansion improves scores for weaker\nmodels, but generally harms stronger models. We show this trend holds across a\nset of eleven expansion techniques, twelve datasets with diverse distribution\nshifts, and twenty-four retrieval models. Through qualitative error analysis,\nwe hypothesize that although expansions provide extra information (potentially\nimproving recall), they add additional noise that makes it difficult to discern\nbetween the top relevant documents (thus introducing false positives). Our\nresults suggest the following recipe: use expansions for weaker models or when\nthe target dataset significantly differs from training corpus in format;\notherwise, avoid expansions to keep the relevance signal clear.\n","authors":["Orion Weller","Kyle Lo","David Wadden","Dawn Lawrie","Benjamin Van Durme","Arman Cohan","Luca Soldaini"],"pdf_url":"https://arxiv.org/pdf/2309.08541v2.pdf","comment":"EACL 2024 camera ready"},{"id":"http://arxiv.org/abs/2305.07614v2","updated":"2024-02-26T20:55:25Z","published":"2023-05-12T17:05:54Z","title":"NevIR: Negation in Neural Information Retrieval","summary":"  Negation is a common everyday phenomena and has been a consistent area of\nweakness for language models (LMs). Although the Information Retrieval (IR)\ncommunity has adopted LMs as the backbone of modern IR architectures, there has\nbeen little to no research in understanding how negation impacts neural IR. We\ntherefore construct a straightforward benchmark on this theme: asking IR models\nto rank two documents that differ only by negation. We show that the results\nvary widely according to the type of IR architecture: cross-encoders perform\nbest, followed by late-interaction models, and in last place are bi-encoder and\nsparse neural architectures. We find that most information retrieval models\n(including SOTA ones) do not consider negation, performing the same or worse\nthan a random ranking. We show that although the obvious approach of continued\nfine-tuning on a dataset of contrastive documents containing negations\nincreases performance (as does model size), there is still a large gap between\nmachine and human performance.\n","authors":["Orion Weller","Dawn Lawrie","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2305.07614v2.pdf","comment":"Accepted to EACL 2024"},{"id":"http://arxiv.org/abs/2402.17016v1","updated":"2024-02-26T20:53:12Z","published":"2024-02-26T20:53:12Z","title":"Multi-Task Contrastive Learning for 8192-Token Bilingual Text Embeddings","summary":"  We introduce a novel suite of state-of-the-art bilingual text embedding\nmodels that are designed to support English and another target language. These\nmodels are capable of processing lengthy text inputs with up to 8192 tokens,\nmaking them highly versatile for a range of natural language processing tasks\nsuch as text retrieval, clustering, and semantic textual similarity (STS)\ncalculations.\n  By focusing on bilingual models and introducing a unique multi-task learning\nobjective, we have significantly improved the model performance on STS tasks,\nwhich outperforms the capabilities of existing multilingual models in both\ntarget language understanding and cross-lingual evaluation tasks. Moreover, our\nbilingual models are more efficient, requiring fewer parameters and less memory\ndue to their smaller vocabulary needs. Furthermore, we have expanded the\nMassive Text Embedding Benchmark (MTEB) to include benchmarks for German and\nSpanish embedding models. This integration aims to stimulate further research\nand advancement in text embedding technologies for these languages.\n","authors":["Isabelle Mohr","Markus Krimmel","Saba Sturua","Mohammad Kalim Akram","Andreas Koukounas","Michael Günther","Georgios Mastrapas","Vinit Ravishankar","Joan Fontanals Martínez","Feng Wang","Qi Liu","Ziniu Yu","Jie Fu","Saahil Ognawala","Susana Guzman","Bo Wang","Maximilian Werk","Nan Wang","Han Xiao"],"pdf_url":"https://arxiv.org/pdf/2402.17016v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.10002v3","updated":"2024-02-26T20:52:59Z","published":"2022-12-20T05:25:01Z","title":"Defending Against Disinformation Attacks in Open-Domain Question\n  Answering","summary":"  Recent work in open-domain question answering (ODQA) has shown that\nadversarial poisoning of the search collection can cause large drops in\naccuracy for production systems. However, little to no work has proposed\nmethods to defend against these attacks. To do so, we rely on the intuition\nthat redundant information often exists in large corpora. To find it, we\nintroduce a method that uses query augmentation to search for a diverse set of\npassages that could answer the original question but are less likely to have\nbeen poisoned. We integrate these new passages into the model through the\ndesign of a novel confidence method, comparing the predicted answer to its\nappearance in the retrieved contexts (what we call Confidence from Answer\nRedundancy, i.e. CAR). Together these methods allow for a simple but effective\nway to defend against poisoning attacks that provides gains of nearly 20% exact\nmatch across varying levels of data poisoning/knowledge conflicts.\n","authors":["Orion Weller","Aleem Khan","Nathaniel Weir","Dawn Lawrie","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2212.10002v3.pdf","comment":"Accepted to EACL 2024"},{"id":"http://arxiv.org/abs/2402.16986v1","updated":"2024-02-26T19:35:45Z","published":"2024-02-26T19:35:45Z","title":"Long Dialog Summarization: An Analysis","summary":"  Dialog summarization has become increasingly important in managing and\ncomprehending large-scale conversations across various domains. This task\npresents unique challenges in capturing the key points, context, and nuances of\nmulti-turn long conversations for summarization. It is worth noting that the\nsummarization techniques may vary based on specific requirements such as in a\nshopping-chatbot scenario, the dialog summary helps to learn user preferences,\nwhereas in the case of a customer call center, the summary may involve the\nproblem attributes that a user specified, and the final resolution provided.\nThis work emphasizes the significance of creating coherent and contextually\nrich summaries for effective communication in various applications. We explore\ncurrent state-of-the-art approaches for long dialog summarization in different\ndomains and benchmark metrics based evaluations show that one single model does\nnot perform well across various areas for distinct summarization tasks.\n","authors":["Ankan Mullick","Ayan Kumar Bhowmick","Raghav R","Ravi Kokku","Prasenjit Dey","Pawan Goyal","Niloy Ganguly"],"pdf_url":"https://arxiv.org/pdf/2402.16986v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16767v1","updated":"2024-02-26T17:35:44Z","published":"2024-02-26T17:35:44Z","title":"CorpusBrain++: A Continual Generative Pre-Training Framework for\n  Knowledge-Intensive Language Tasks","summary":"  Knowledge-intensive language tasks (KILTs) typically require retrieving\nrelevant documents from trustworthy corpora, e.g., Wikipedia, to produce\nspecific answers. Very recently, a pre-trained generative retrieval model for\nKILTs, named CorpusBrain, was proposed and reached new state-of-the-art\nretrieval performance. However, most existing research on KILTs, including\nCorpusBrain, has predominantly focused on a static document collection,\noverlooking the dynamic nature of real-world scenarios, where new documents are\ncontinuously being incorporated into the source corpus. To address this gap, it\nis crucial to explore the capability of retrieval models to effectively handle\nthe dynamic retrieval scenario inherent in KILTs.\n  In this work, we first introduce the continual document learning (CDL) task\nfor KILTs and build a novel benchmark dataset named KILT++ based on the\noriginal KILT dataset for evaluation. Then, we conduct a comprehensive study\nover the use of pre-trained CorpusBrain on KILT++. Unlike the promising results\nin the stationary scenario, CorpusBrain is prone to catastrophic forgetting in\nthe dynamic scenario, hence hampering the retrieval performance. To alleviate\nthis issue, we propose CorpusBrain++, a continual generative pre-training\nframework. Empirical results demonstrate the significant effectiveness and\nremarkable efficiency of CorpusBrain++ in comparison to both traditional and\ngenerative IR methods.\n","authors":["Jiafeng Guo","Changjiang Zhou","Ruqing Zhang","Jiangui Chen","Maarten de Rijke","Yixing Fan","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2402.16767v1.pdf","comment":"Submitted to ACM Transactions on Information Systems"},{"id":"http://arxiv.org/abs/2402.16933v1","updated":"2024-02-26T17:20:16Z","published":"2024-02-26T17:20:16Z","title":"Avoiding Catastrophic Forgetting in Visual Classification Using Human\n  Concept Formation","summary":"  Deep neural networks have excelled in machine learning, particularly in\nvision tasks, however, they often suffer from catastrophic forgetting when\nlearning new tasks sequentially. In this work, we propose Cobweb4V, a novel\nvisual classification approach that builds on Cobweb, a human like learning\nsystem that is inspired by the way humans incrementally learn new concepts over\ntime. In this research, we conduct a comprehensive evaluation, showcasing the\nproficiency of Cobweb4V in learning visual concepts, requiring less data to\nachieve effective learning outcomes compared to traditional methods,\nmaintaining stable performance over time, and achieving commendable asymptotic\nbehavior, without catastrophic forgetting effects. These characteristics align\nwith learning strategies in human cognition, positioning Cobweb4V as a\npromising alternative to neural network approaches.\n","authors":["Nicki Barari","Xin Lian","Christopher J. MacLellan"],"pdf_url":"https://arxiv.org/pdf/2402.16933v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.02084v2","updated":"2024-02-26T16:28:13Z","published":"2023-10-22T15:39:44Z","title":"ITEm: Unsupervised Image-Text Embedding Learning for eCommerce","summary":"  Product embedding serves as a cornerstone for a wide range of applications in\neCommerce. The product embedding learned from multiple modalities shows\nsignificant improvement over that from a single modality, since different\nmodalities provide complementary information. However, some modalities are more\ninformatively dominant than others. How to teach a model to learn embedding\nfrom different modalities without neglecting information from the less dominant\nmodality is challenging. We present an image-text embedding model (ITEm), an\nunsupervised learning method that is designed to better attend to image and\ntext modalities. We extend BERT by (1) learning an embedding from text and\nimage without knowing the regions of interest; (2) training a global\nrepresentation to predict masked words and to construct masked image patches\nwithout their individual representations. We evaluate the pre-trained ITEm on\ntwo tasks: the search for extremely similar products and the prediction of\nproduct categories, showing substantial gains compared to strong baseline\nmodels.\n","authors":["Baohao Liao","Michael Kozielski","Sanjika Hewavitharana","Jiangbo Yuan","Shahram Khadivi","Tomer Lancewicki"],"pdf_url":"https://arxiv.org/pdf/2311.02084v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16664v1","updated":"2024-02-26T15:35:24Z","published":"2024-02-26T15:35:24Z","title":"LLM-Assisted Multi-Teacher Continual Learning for Visual Question\n  Answering in Robotic Surgery","summary":"  Visual question answering (VQA) can be fundamentally crucial for promoting\nrobotic-assisted surgical education. In practice, the needs of trainees are\nconstantly evolving, such as learning more surgical types, adapting to\ndifferent robots, and learning new surgical instruments and techniques for one\nsurgery. Therefore, continually updating the VQA system by a sequential data\nstream from multiple resources is demanded in robotic surgery to address new\ntasks. In surgical scenarios, the storage cost and patient data privacy often\nrestrict the availability of old data when updating the model, necessitating an\nexemplar-free continual learning (CL) setup. However, prior studies overlooked\ntwo vital problems of the surgical domain: i) large domain shifts from diverse\nsurgical operations collected from multiple departments or clinical centers,\nand ii) severe data imbalance arising from the uneven presence of surgical\ninstruments or activities during surgical procedures. This paper proposes to\naddress these two problems with a multimodal large language model (LLM) and an\nadaptive weight assignment methodology. We first develop a new multi-teacher CL\nframework that leverages a multimodal LLM as the additional teacher. The strong\ngeneralization ability of the LLM can bridge the knowledge gap when domain\nshifts and data imbalances occur. We then put forth a novel data processing\nmethod that transforms complex LLM embeddings into logits compatible with our\nCL framework. We further design an adaptive weight assignment approach that\nbalances the generalization ability of the LLM and the domain expertise of the\nold CL model. We construct a new dataset for surgical VQA tasks, providing\nvaluable data resources for future research. Extensive experimental results on\nthree datasets demonstrate the superiority of our method to other advanced CL\nmodels.\n","authors":["Kexin Chen","Yuyang Du","Tao You","Mobarakol Islam","Ziyu Guo","Yueming Jin","Guangyong Chen","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2402.16664v1.pdf","comment":"This paper has been accapted by 2024 IEEE International Conference on\n  Robotics and Automation (ICRA)"},{"id":"http://arxiv.org/abs/2402.16660v1","updated":"2024-02-26T15:34:06Z","published":"2024-02-26T15:34:06Z","title":"BOXREC: Recommending a Box of Preferred Outfits in Online Shopping","summary":"  Over the past few years, automation of outfit composition has gained much\nattention from the research community. Most of the existing outfit\nrecommendation systems focus on pairwise item compatibility prediction (using\nvisual and text features) to score an outfit combination having several items,\nfollowed by recommendation of top-n outfits or a capsule wardrobe having a\ncollection of outfits based on user's fashion taste. However, none of these\nconsider user's preference of price-range for individual clothing types or an\noverall shopping budget for a set of items. In this paper, we propose a box\nrecommendation framework - BOXREC - which at first, collects user preferences\nacross different item types (namely, top-wear, bottom-wear and foot-wear)\nincluding price-range of each type and a maximum shopping budget for a\nparticular shopping session. It then generates a set of preferred outfits by\nretrieving all types of preferred items from the database (according to user\nspecified preferences including price-ranges), creates all possible\ncombinations of three preferred items (belonging to distinct item types) and\nverifies each combination using an outfit scoring framework - BOXREC-OSF.\nFinally, it provides a box full of fashion items, such that different\ncombinations of the items maximize the number of outfits suitable for an\noccasion while satisfying maximum shopping budget. Empirical results show\nsuperior performance of BOXREC-OSF over the baseline methods.\n","authors":["Debopriyo Banerjee","Krothapalli Sreenivasa Rao","Shamik Sural","Niloy Ganguly"],"pdf_url":"https://arxiv.org/pdf/2402.16660v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16608v1","updated":"2024-02-26T14:40:34Z","published":"2024-02-26T14:40:34Z","title":"PAQA: Toward ProActive Open-Retrieval Question Answering","summary":"  Conversational systems have made significant progress in generating natural\nlanguage responses. However, their potential as conversational search systems\nis currently limited due to their passive role in the information-seeking\nprocess. One major limitation is the scarcity of datasets that provide labelled\nambiguous questions along with a supporting corpus of documents and relevant\nclarifying questions. This work aims to tackle the challenge of generating\nrelevant clarifying questions by taking into account the inherent ambiguities\npresent in both user queries and documents. To achieve this, we propose PAQA,\nan extension to the existing AmbiNQ dataset, incorporating clarifying\nquestions. We then evaluate various models and assess how passage retrieval\nimpacts ambiguity detection and the generation of clarifying questions. By\naddressing this gap in conversational search systems, we aim to provide\nadditional supervision to enhance their active participation in the\ninformation-seeking process and provide users with more accurate results.\n","authors":["Pierre Erbacher","Jian-Yun Nie","Philippe Preux","Laure Soulier"],"pdf_url":"https://arxiv.org/pdf/2402.16608v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16539v1","updated":"2024-02-26T12:55:51Z","published":"2024-02-26T12:55:51Z","title":"Integrating Large Language Models with Graphical Session-Based\n  Recommendation","summary":"  With the rapid development of Large Language Models (LLMs), various\nexplorations have arisen to utilize LLMs capability of context understanding on\nrecommender systems. While pioneering strategies have primarily transformed\ntraditional recommendation tasks into challenges of natural language\ngeneration, there has been a relative scarcity of exploration in the domain of\nsession-based recommendation (SBR) due to its specificity. SBR has been\nprimarily dominated by Graph Neural Networks, which have achieved many\nsuccessful outcomes due to their ability to capture both the implicit and\nexplicit relationships between adjacent behaviors. The structural nature of\ngraphs contrasts with the essence of natural language, posing a significant\nadaptation gap for LLMs. In this paper, we introduce large language models with\ngraphical Session-Based recommendation, named LLMGR, an effective framework\nthat bridges the aforementioned gap by harmoniously integrating LLMs with Graph\nNeural Networks (GNNs) for SBR tasks. This integration seeks to leverage the\ncomplementary strengths of LLMs in natural language understanding and GNNs in\nrelational data processing, leading to a more powerful session-based\nrecommender system that can understand and recommend items within a session.\nMoreover, to endow the LLM with the capability to empower SBR tasks, we design\na series of prompts for both auxiliary and major instruction tuning tasks.\nThese prompts are crafted to assist the LLM in understanding graph-structured\ndata and align textual information with nodes, effectively translating nuanced\nuser interactions into a format that can be understood and utilized by LLM\narchitectures. Extensive experiments on three real-world datasets demonstrate\nthat LLMGR outperforms several competitive baselines, indicating its\neffectiveness in enhancing SBR tasks and its potential as a research direction\nfor future exploration.\n","authors":["Naicheng Guo","Hongwei Cheng","Qianqiao Liang","Linxun Chen","Bing Han"],"pdf_url":"https://arxiv.org/pdf/2402.16539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.05200v2","updated":"2024-02-26T12:46:37Z","published":"2024-01-10T14:53:18Z","title":"Knowledge Sharing in Manufacturing using Large Language Models: User\n  Evaluation and Model Benchmarking","summary":"  Recent advances in natural language processing enable more intelligent ways\nto support knowledge sharing in factories. In manufacturing, operating\nproduction lines has become increasingly knowledge-intensive, putting strain on\na factory's capacity to train and support new operators. This paper introduces\na Large Language Model (LLM)-based system designed to retrieve information from\nthe extensive knowledge contained in factory documentation and knowledge shared\nby expert operators. The system aims to efficiently answer queries from\noperators and facilitate the sharing of new knowledge. We conducted a user\nstudy at a factory to assess its potential impact and adoption, eliciting\nseveral perceived benefits, namely, enabling quicker information retrieval and\nmore efficient resolution of issues. However, the study also highlighted a\npreference for learning from a human expert when such an option is available.\nFurthermore, we benchmarked several commercial and open-sourced LLMs for this\nsystem. The current state-of-the-art model, GPT-4, consistently outperformed\nits counterparts, with open-source models trailing closely, presenting an\nattractive option given their data privacy and customization benefits. In\nsummary, this work offers preliminary insights and a system design for\nfactories considering using LLM tools for knowledge management.\n","authors":["Samuel Kernan Freire","Chaofan Wang","Mina Foosherian","Stefan Wellsandt","Santiago Ruiz-Arenas","Evangelos Niforatos"],"pdf_url":"https://arxiv.org/pdf/2401.05200v2.pdf","comment":"11 pages, 3 figures, and 1 table. Under review"},{"id":"http://arxiv.org/abs/2402.16508v1","updated":"2024-02-26T11:42:29Z","published":"2024-02-26T11:42:29Z","title":"Pre-training Cross-lingual Open Domain Question Answering with\n  Large-scale Synthetic Supervision","summary":"  Cross-lingual question answering (CLQA) is a complex problem, comprising\ncross-lingual retrieval from a multilingual knowledge base, followed by answer\ngeneration either in English or the query language. Both steps are usually\ntackled by separate models, requiring substantial annotated datasets, and\ntypically auxiliary resources, like machine translation systems to bridge\nbetween languages. In this paper, we show that CLQA can be addressed using a\nsingle encoder-decoder model. To effectively train this model, we propose a\nself-supervised method based on exploiting the cross-lingual link structure\nwithin Wikipedia. We demonstrate how linked Wikipedia pages can be used to\nsynthesise supervisory signals for cross-lingual retrieval, through a form of\ncloze query, and generate more natural queries to supervise answer generation.\nTogether, we show our approach, \\texttt{CLASS}, outperforms comparable methods\non both supervised and zero-shot language adaptation settings, including those\nusing machine translation.\n","authors":["Fan Jiang","Tom Drummond","Trevor Cohn"],"pdf_url":"https://arxiv.org/pdf/2402.16508v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05391v4","updated":"2024-02-26T09:57:12Z","published":"2024-02-08T04:04:36Z","title":"Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey","summary":"  Knowledge Graphs (KGs) play a pivotal role in advancing various AI\napplications, with the semantic web community's exploration into multi-modal\ndimensions unlocking new avenues for innovation. In this survey, we carefully\nreview over 300 articles, focusing on KG-aware research in two principal\naspects: KG-driven Multi-Modal (KG4MM) learning, where KGs support multi-modal\ntasks, and Multi-Modal Knowledge Graph (MM4KG), which extends KG studies into\nthe MMKG realm. We begin by defining KGs and MMKGs, then explore their\nconstruction progress. Our review includes two primary task categories:\nKG-aware multi-modal learning tasks, such as Image Classification and Visual\nQuestion Answering, and intrinsic MMKG tasks like Multi-modal Knowledge Graph\nCompletion and Entity Alignment, highlighting specific research trajectories.\nFor most of these tasks, we provide definitions, evaluation benchmarks, and\nadditionally outline essential insights for conducting relevant research.\nFinally, we discuss current challenges and identify emerging trends, such as\nprogress in Large Language Modeling and Multi-modal Pre-training strategies.\nThis survey aims to serve as a comprehensive reference for researchers already\ninvolved in or considering delving into KG and multi-modal learning research,\noffering insights into the evolving landscape of MMKG research and supporting\nfuture work.\n","authors":["Zhuo Chen","Yichi Zhang","Yin Fang","Yuxia Geng","Lingbing Guo","Xiang Chen","Qian Li","Wen Zhang","Jiaoyan Chen","Yushan Zhu","Jiaqi Li","Xiaoze Liu","Jeff Z. Pan","Ningyu Zhang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2402.05391v4.pdf","comment":"Ongoing work; 41 pages (Main Text), 55 pages (Total), 11 Tables, 13\n  Figures, 619 citations; Paper list is available at\n  https://github.com/zjukg/KG-MM-Survey"},{"id":"http://arxiv.org/abs/2402.16440v1","updated":"2024-02-26T09:37:24Z","published":"2024-02-26T09:37:24Z","title":"Retrouver l'inventeur-auteur : la lev{é}e d'homonymies d'autorat entre\n  les brevets et les publications scientifiques","summary":"  Patents and scientific papers provide an essential source for measuring\nscience and technology output, to be used as a basis for the most varied\nscientometric analyzes. Authors' and inventors' names are the key identifiers\nto carry out these analyses, which however, run up against the issue of\ndisambiguation. By extension identifying inventors who are also academic\nauthors is a non-trivial challenge. We propose a method using the International\nPatent Classification (IPC) and the IPCCAT API to assess the degree of\nsimilarity of patents and papers abstracts of a given inventor, in order to\nmatch both types of documents. The method is developed and manually qualified\nbased on three corpora of patents extracted from the international EPO database\nEspacenet. Among a set of 4679 patents and 7720 inventors, we obtain 2501\nauthors. The proposed algorithm solves the general problem of disambiguation\nwith an error rate lower than 5%.\n","authors":["David Reymond","Heman Khouilla","Sandrine Wolff","Manuel Durand-Barthez"],"pdf_url":"https://arxiv.org/pdf/2402.16440v1.pdf","comment":"in French language"},{"id":"http://arxiv.org/abs/2402.16429v1","updated":"2024-02-26T09:28:16Z","published":"2024-02-26T09:28:16Z","title":"Effect of utterance duration and phonetic content on speaker\n  identification using second-order statistical methods","summary":"  Second-order statistical methods show very good results for automatic speaker\nidentification in controlled recording conditions. These approaches are\ngenerally used on the entire speech material available. In this paper, we study\nthe influence of the content of the test speech material on the performances of\nsuch methods, i.e. under a more analytical approach. The goal is to investigate\non the kind of information which is used by these methods, and where it is\nlocated in the speech signal. Liquids and glides together, vowels, and more\nparticularly nasal vowels and nasal consonants, are found to be particularly\nspeaker specific: test utterances of 1 second, composed in majority of acoustic\nmaterial from one of these classes provide better speaker identification\nresults than phonetically balanced test utterances, even though the training is\ndone, in both cases, with 15 seconds of phonetically balanced speech.\nNevertheless, results with other phoneme classes are never dramatically poor.\nThese results tend to show that the speaker-dependent information captured by\nlong-term second-order statistics is consistently common to all phonetic\nclasses, and that the homogeneity of the test material may improve the quality\nof the estimates.\n","authors":["Ivan Magrin-Chagnolleau","Jean François Bonastre","Frédéric Bimbot"],"pdf_url":"https://arxiv.org/pdf/2402.16429v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11157v2","updated":"2024-02-26T07:28:48Z","published":"2023-11-18T20:18:18Z","title":"Contextualizing Internet Memes Across Social Media Platforms","summary":"  Internet memes have emerged as a novel format for communication and\nexpressing ideas on the web. Their fluidity and creative nature are reflected\nin their widespread use, often across platforms and occasionally for unethical\nor harmful purposes. While computational work has already analyzed their\nhigh-level virality over time and developed specialized classifiers for hate\nspeech detection, there have been no efforts to date that aim to holistically\ntrack, identify, and map internet memes posted on social media. To bridge this\ngap, we investigate whether internet memes across social media platforms can be\ncontextualized by using a semantic repository of knowledge, namely, a knowledge\ngraph. We collect thousands of potential internet meme posts from two social\nmedia platforms, namely Reddit and Discord, and develop an\nextract-transform-load procedure to create a data lake with candidate meme\nposts. By using vision transformer-based similarity, we match these candidates\nagainst the memes cataloged in IMKG -- a recently released knowledge graph of\ninternet memes. We leverage this grounding to highlight the potential of our\nproposed framework to study the prevalence of memes on different platforms, map\nthem to IMKG, and provide context about memes on social media.\n","authors":["Saurav Joshi","Filip Ilievski","Luca Luceri"],"pdf_url":"https://arxiv.org/pdf/2311.11157v2.pdf","comment":"10 pages, 7 figures, 2 tables"},{"id":"http://arxiv.org/abs/2402.16358v1","updated":"2024-02-26T07:22:51Z","published":"2024-02-26T07:22:51Z","title":"An Integrated Data Processing Framework for Pretraining Foundation\n  Models","summary":"  The ability of the foundation models heavily relies on large-scale, diverse,\nand high-quality pretraining data. In order to improve data quality,\nresearchers and practitioners often have to manually curate datasets from\ndifference sources and develop dedicated data cleansing pipeline for each data\nrepository. Lacking a unified data processing framework, this process is\nrepetitive and cumbersome. To mitigate this issue, we propose a data processing\nframework that integrates a Processing Module which consists of a series of\noperators at different granularity levels, and an Analyzing Module which\nsupports probing and evaluation of the refined data. The proposed framework is\neasy to use and highly flexible. In this demo paper, we first introduce how to\nuse this framework with some example use cases and then demonstrate its\neffectiveness in improving the data quality with an automated evaluation with\nChatGPT and an end-to-end evaluation in pretraining the GPT-2 model. The code\nand demonstration videos are accessible on GitHub.\n","authors":["Yiding Sun","Feng Wang","Yutao Zhu","Wayne Xin Zhao","Jiaxin Mao"],"pdf_url":"https://arxiv.org/pdf/2402.16358v1.pdf","comment":"6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2402.14539v2","updated":"2024-02-26T07:21:42Z","published":"2024-02-22T13:30:52Z","title":"Transforming Norm-based To Graph-based Spatial Representation for\n  Spatio-Temporal Epidemiological Models","summary":"  Pandemics, with their profound societal and economic impacts, pose\nsignificant threats to global health, mortality rates, economic stability, and\npolitical landscapes. In response to these challenges, numerous studies have\nemployed spatio-temporal models to enhance our understanding and management of\nthese complex phenomena. These spatio-temporal models can be roughly divided\ninto two main spatial categories: norm-based and graph-based. Norm-based models\nare usually more accurate and easier to model but are more computationally\nintensive and require more data to fit. On the other hand, graph-based models\nare less accurate and harder to model but are less computationally intensive\nand require fewer data to fit. As such, ideally, one would like to use a\ngraph-based model while preserving the representation accuracy obtained by the\nnorm-based model. In this study, we explore the ability to transform from\nnorm-based to graph-based spatial representation for these models. We first\nshow no analytical mapping between the two exists, requiring one to use\napproximation numerical methods instead. We introduce a novel framework for\nthis task together with twelve possible implementations using a wide range of\nheuristic optimization approaches. Our findings show that by leveraging\nagent-based simulations and heuristic algorithms for the graph node's location\nand population's spatial walk dynamics approximation one can use graph-based\nspatial representation without losing much of the model's accuracy and\nexpressiveness. We investigate our framework for three real-world cases,\nachieving 94\\% accuracy preservation, on average. Moreover, an analysis of\nsynthetic cases shows the proposed framework is relatively robust for changes\nin both spatial and temporal properties.\n","authors":["Teddy Lazebnik"],"pdf_url":"https://arxiv.org/pdf/2402.14539v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16327v1","updated":"2024-02-26T06:21:01Z","published":"2024-02-26T06:21:01Z","title":"Deep Rating Elicitation for New Users in Collaborative Filtering","summary":"  Recent recommender systems started to use rating elicitation, which asks new\nusers to rate a small seed itemset for inferring their preferences, to improve\nthe quality of initial recommendations. The key challenge of the rating\nelicitation is to choose the seed items which can best infer the new users'\npreference. This paper proposes a novel end-to-end Deep learning framework for\nRating Elicitation (DRE), that chooses all the seed items at a time with\nconsideration of the non-linear interactions. To this end, it first defines\ncategorical distributions to sample seed items from the entire itemset, then it\ntrains both the categorical distributions and a neural reconstruction network\nto infer users' preferences on the remaining items from CF information of the\nsampled seed items. Through the end-to-end training, the categorical\ndistributions are learned to select the most representative seed items while\nreflecting the complex non-linear interactions. Experimental results show that\nDRE outperforms the state-of-the-art approaches in the recommendation quality\nby accurately inferring the new users' preferences and its seed itemset better\nrepresents the latent space than the seed itemset obtained by the other\nmethods.\n","authors":["Wonbin Kweon","SeongKu Kang","Junyoung Hwang","Hwanjo Yu"],"pdf_url":"https://arxiv.org/pdf/2402.16327v1.pdf","comment":"WWW 2020"},{"id":"http://arxiv.org/abs/2402.16325v1","updated":"2024-02-26T06:13:24Z","published":"2024-02-26T06:13:24Z","title":"Confidence Calibration for Recommender Systems and Its Applications","summary":"  Despite the importance of having a measure of confidence in recommendation\nresults, it has been surprisingly overlooked in the literature compared to the\naccuracy of the recommendation. In this dissertation, I propose a model\ncalibration framework for recommender systems for estimating accurate\nconfidence in recommendation results based on the learned ranking scores.\nMoreover, I subsequently introduce two real-world applications of confidence on\nrecommendations: (1) Training a small student model by treating the confidence\nof a big teacher model as additional learning guidance, (2) Adjusting the\nnumber of presented items based on the expected user utility estimated with\ncalibrated probability.\n","authors":["Wonbin Kweon"],"pdf_url":"https://arxiv.org/pdf/2402.16325v1.pdf","comment":"Doctoral Dissertation"},{"id":"http://arxiv.org/abs/2402.16304v1","updated":"2024-02-26T05:03:54Z","published":"2024-02-26T05:03:54Z","title":"Top-Personalized-K Recommendation","summary":"  The conventional top-K recommendation, which presents the top-K items with\nthe highest ranking scores, is a common practice for generating personalized\nranking lists. However, is this fixed-size top-K recommendation the optimal\napproach for every user's satisfaction? Not necessarily. We point out that\nproviding fixed-size recommendations without taking into account user utility\ncan be suboptimal, as it may unavoidably include irrelevant items or limit the\nexposure to relevant ones. To address this issue, we introduce\nTop-Personalized-K Recommendation, a new recommendation task aimed at\ngenerating a personalized-sized ranking list to maximize individual user\nsatisfaction. As a solution to the proposed task, we develop a model-agnostic\nframework named PerK. PerK estimates the expected user utility by leveraging\ncalibrated interaction probabilities, subsequently selecting the recommendation\nsize that maximizes this expected utility. Through extensive experiments on\nreal-world datasets, we demonstrate the superiority of PerK in\nTop-Personalized-K recommendation task. We expect that Top-Personalized-K\nrecommendation has the potential to offer enhanced solutions for various\nreal-world recommendation scenarios, based on its great compatibility with\nexisting models.\n","authors":["Wonbin Kweon","SeongKu Kang","Sanghwan Jang","Hwanjo Yu"],"pdf_url":"https://arxiv.org/pdf/2402.16304v1.pdf","comment":"WWW 2024"},{"id":"http://arxiv.org/abs/2402.16299v1","updated":"2024-02-26T04:43:44Z","published":"2024-02-26T04:43:44Z","title":"Against Filter Bubbles: Diversified Music Recommendation via Weighted\n  Hypergraph Embedding Learning","summary":"  Recommender systems serve a dual purpose for users: sifting out inappropriate\nor mismatched information while accurately identifying items that align with\ntheir preferences. Numerous recommendation algorithms are designed to provide\nusers with a personalized array of information tailored to their preferences.\nNevertheless, excessive personalization can confine users within a \"filter\nbubble\". Consequently, achieving the right balance between accuracy and\ndiversity in recommendations is a pressing concern. To address this challenge,\nexemplified by music recommendation, we introduce the Diversified Weighted\nHypergraph music Recommendation algorithm (DWHRec). In the DWHRec algorithm,\nthe initial connections between users and listened tracks are represented by a\nweighted hypergraph. Simultaneously, associations between artists, albums and\ntags with tracks are also appended to the hypergraph. To explore users' latent\npreferences, a hypergraph-based random walk embedding method is applied to the\nconstructed hypergraph. In our investigation, accuracy is gauged by the\nalignment between the user and the track, whereas the array of recommended\ntrack types measures diversity. We rigorously compared DWHRec against seven\nstate-of-the-art recommendation algorithms using two real-world music datasets.\nThe experimental results validate DWHRec as a solution that adeptly harmonizes\naccuracy and diversity, delivering a more enriched musical experience. Beyond\nmusic recommendation, DWHRec can be extended to cater to other scenarios with\nsimilar data structures.\n","authors":["Chaoguang Luo","Liuying Wen","Yong Qin","Liangwei Yang","Zhineng Hu","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2402.16299v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16288v1","updated":"2024-02-26T04:09:53Z","published":"2024-02-26T04:09:53Z","title":"PerLTQA: A Personal Long-Term Memory Dataset for Memory Classification,\n  Retrieval, and Synthesis in Question Answering","summary":"  Long-term memory plays a critical role in personal interaction, considering\nlong-term memory can better leverage world knowledge, historical information,\nand preferences in dialogues. Our research introduces PerLTQA, an innovative QA\ndataset that combines semantic and episodic memories, including world\nknowledge, profiles, social relationships, events, and dialogues. This dataset\nis collected to investigate the use of personalized memories, focusing on\nsocial interactions and events in the QA task. PerLTQA features two types of\nmemory and a comprehensive benchmark of 8,593 questions for 30 characters,\nfacilitating the exploration and application of personalized memories in Large\nLanguage Models (LLMs). Based on PerLTQA, we propose a novel framework for\nmemory integration and generation, consisting of three main components: Memory\nClassification, Memory Retrieval, and Memory Synthesis. We evaluate this\nframework using five LLMs and three retrievers. Experimental results\ndemonstrate that BERT-based classification models significantly outperform LLMs\nsuch as ChatGLM3 and ChatGPT in the memory classification task. Furthermore,\nour study highlights the importance of effective memory integration in the QA\ntask.\n","authors":["Yiming Du","Hongru Wang","Zhengyi Zhao","Bin Liang","Baojun Wang","Wanjun Zhong","Zezhong Wang","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2402.16288v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16261v1","updated":"2024-02-26T02:48:43Z","published":"2024-02-26T02:48:43Z","title":"UniRetriever: Multi-task Candidates Selection for Various\n  Context-Adaptive Conversational Retrieval","summary":"  Conversational retrieval refers to an information retrieval system that\noperates in an iterative and interactive manner, requiring the retrieval of\nvarious external resources, such as persona, knowledge, and even response, to\neffectively engage with the user and successfully complete the dialogue.\nHowever, most previous work trained independent retrievers for each specific\nresource, resulting in sub-optimal performance and low efficiency. Thus, we\npropose a multi-task framework function as a universal retriever for three\ndominant retrieval tasks during the conversation: persona selection, knowledge\nselection, and response selection. To this end, we design a dual-encoder\narchitecture consisting of a context-adaptive dialogue encoder and a candidate\nencoder, aiming to attention to the relevant context from the long dialogue and\nretrieve suitable candidates by simply a dot product. Furthermore, we introduce\ntwo loss constraints to capture the subtle relationship between dialogue\ncontext and different candidates by regarding historically selected candidates\nas hard negatives. Extensive experiments and analysis establish\nstate-of-the-art retrieval quality both within and outside its training domain,\nrevealing the promising potential and generalization capability of our model to\nserve as a universal retriever for different candidate selection tasks\nsimultaneously.\n","authors":["Hongru Wang","Boyang Xue","Baohang Zhou","Rui Wang","Fei Mi","Weichao Wang","Yasheng Wang","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2402.16261v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16240v1","updated":"2024-02-26T02:01:00Z","published":"2024-02-26T02:01:00Z","title":"High-Frequency-aware Hierarchical Contrastive Selective Coding for\n  Representation Learning on Text-attributed Graphs","summary":"  We investigate node representation learning on text-attributed graphs (TAGs),\nwhere nodes are associated with text information. Although recent studies on\ngraph neural networks (GNNs) and pretrained language models (PLMs) have\nexhibited their power in encoding network and text signals, respectively, less\nattention has been paid to delicately coupling these two types of models on\nTAGs. Specifically, existing GNNs rarely model text in each node in a\ncontextualized way; existing PLMs can hardly be applied to characterize graph\nstructures due to their sequence architecture. To address these challenges, we\npropose HASH-CODE, a High-frequency Aware Spectral Hierarchical Contrastive\nSelective Coding method that integrates GNNs and PLMs into a unified model.\nDifferent from previous \"cascaded architectures\" that directly add GNN layers\nupon a PLM, our HASH-CODE relies on five self-supervised optimization\nobjectives to facilitate thorough mutual enhancement between network and text\nsignals in diverse granularities. Moreover, we show that existing contrastive\nobjective learns the low-frequency component of the augmentation graph and\npropose a high-frequency component (HFC)-aware contrastive learning objective\nthat makes the learned embeddings more distinctive. Extensive experiments on\nsix real-world benchmarks substantiate the efficacy of our proposed approach.\nIn addition, theoretical analysis and item embedding visualization provide\ninsights into our model interoperability.\n","authors":["Peiyan Zhang","Chaozhuo Li","Liying Kang","Feiran Huang","Senzhang Wang","Xing Xie","Sunghun Kim"],"pdf_url":"https://arxiv.org/pdf/2402.16240v1.pdf","comment":"Accepted by WWW 2024. arXiv admin note: text overlap with\n  arXiv:2009.10273 by other authors"},{"id":"http://arxiv.org/abs/2305.07764v2","updated":"2024-02-26T00:52:06Z","published":"2023-05-12T21:12:55Z","title":"Long-Term Value of Exploration: Measurements, Findings and Algorithms","summary":"  Effective exploration is believed to positively influence the long-term user\nexperience on recommendation platforms. Determining its exact benefits,\nhowever, has been challenging. Regular A/B tests on exploration often measure\nneutral or even negative engagement metrics while failing to capture its\nlong-term benefits. We here introduce new experiment designs to formally\nquantify the long-term value of exploration by examining its effects on content\ncorpus, and connecting content corpus growth to the long-term user experience\nfrom real-world experiments. Once established the values of exploration, we\ninvestigate the Neural Linear Bandit algorithm as a general framework to\nintroduce exploration into any deep learning based ranking systems. We conduct\nlive experiments on one of the largest short-form video recommendation\nplatforms that serves billions of users to validate the new experiment designs,\nquantify the long-term values of exploration, and to verify the effectiveness\nof the adopted neural linear bandit algorithm for exploration.\n","authors":["Yi Su","Xiangyu Wang","Elaine Ya Le","Liang Liu","Yuening Li","Haokai Lu","Benjamin Lipshitz","Sriraj Badam","Lukasz Heldt","Shuchao Bi","Ed Chi","Cristos Goodrow","Su-Lin Wu","Lexi Baugher","Minmin Chen"],"pdf_url":"https://arxiv.org/pdf/2305.07764v2.pdf","comment":"11 pages, WSDM 2024"}],"Multimedia":[{"id":"http://arxiv.org/abs/2402.16665v1","updated":"2024-02-26T15:38:08Z","published":"2024-02-26T15:38:08Z","title":"The Interaction Fidelity Model: A Taxonomy to Distinguish the Aspects of\n  Fidelity in Virtual Reality","summary":"  Fidelity describes how closely a replication resembles the original. It can\nbe helpful to analyze how faithful interactions in virtual reality (VR) are to\na reference interaction. In prior research, fidelity has been restricted to the\nsimulation of reality - also called realism. Our definition includes other\nreference interactions, such as superpowers or fiction. Interaction fidelity is\na multilayered concept. Unfortunately, different aspects of fidelity have\neither not been distinguished in scientific discourse or referred to with\ninconsistent terminology. Therefore, we present the Interaction Fidelity Model\n(IntFi Model). Based on the human-computer interaction loop, it systematically\ncovers all stages of VR interactions. The conceptual model establishes a clear\nstructure and precise definitions of eight distinct components. It was reviewed\nthrough interviews with fourteen VR experts. We provide guidelines, diverse\nexamples, and educational material to universally apply the IntFi Model to any\nVR experience. We identify common patterns and propose foundational research\nopportunities.\n","authors":["Michael Bonfert","Thomas Muender","Ryan P. McMahan","Frank Steinicke","Doug Bowman","Rainer Malaka","Tanja Döring"],"pdf_url":"https://arxiv.org/pdf/2402.16665v1.pdf","comment":"34 pages incl. references and appendix"},{"id":"http://arxiv.org/abs/2402.16366v1","updated":"2024-02-26T07:40:45Z","published":"2024-02-26T07:40:45Z","title":"SPC-NeRF: Spatial Predictive Compression for Voxel Based Radiance Field","summary":"  Representing the Neural Radiance Field (NeRF) with the explicit voxel grid\n(EVG) is a promising direction for improving NeRFs. However, the EVG\nrepresentation is not efficient for storage and transmission because of the\nterrific memory cost. Current methods for compressing EVG mainly inherit the\nmethods designed for neural network compression, such as pruning and\nquantization, which do not take full advantage of the spatial correlation of\nvoxels. Inspired by prosperous digital image compression techniques, this paper\nproposes SPC-NeRF, a novel framework applying spatial predictive coding in EVG\ncompression. The proposed framework can remove spatial redundancy efficiently\nfor better compression performance.Moreover, we model the bitrate and design a\nnovel form of the loss function, where we can jointly optimize compression\nratio and distortion to achieve higher coding efficiency. Extensive experiments\ndemonstrate that our method can achieve 32% bit saving compared to the\nstate-of-the-art method VQRF on multiple representative test datasets, with\ncomparable training time.\n","authors":["Zetian Song","Wenhong Duan","Yuhuai Zhang","Shiqi Wang","Siwei Ma","Wen Gao"],"pdf_url":"https://arxiv.org/pdf/2402.16366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16364v1","updated":"2024-02-26T07:33:28Z","published":"2024-02-26T07:33:28Z","title":"Where Do We Go from Here? Multi-scale Allocentric Relational Inference\n  from Natural Spatial Descriptions","summary":"  When communicating routes in natural language, the concept of {\\em acquired\nspatial knowledge} is crucial for geographic information retrieval (GIR) and in\nspatial cognitive research. However, NLP navigation studies often overlook the\nimpact of such acquired knowledge on textual descriptions. Current navigation\nstudies concentrate on egocentric local descriptions (e.g., `it will be on your\nright') that require reasoning over the agent's local perception. These\ninstructions are typically given as a sequence of steps, with each action-step\nexplicitly mentioning and being followed by a landmark that the agent can use\nto verify they are on the right path (e.g., `turn right and then you will\nsee...'). In contrast, descriptions based on knowledge acquired through a map\nprovide a complete view of the environment and capture its overall structure.\nThese instructions (e.g., `it is south of Central Park and a block north of a\npolice station') are typically non-sequential, contain allocentric relations,\nwith multiple spatial relations and implicit actions, without any explicit\nverification. This paper introduces the Rendezvous (RVS) task and dataset,\nwhich includes 10,404 examples of English geospatial instructions for reaching\na target location using map-knowledge. Our analysis reveals that RVS exhibits a\nricher use of spatial allocentric relations, and requires resolving more\nspatial relations simultaneously compared to previous text-based navigation\nbenchmarks.\n","authors":["Tzuf Paz-Argaman","Sayali Kulkarni","John Palowitch","Jason Baldridge","Reut Tsarfaty"],"pdf_url":"https://arxiv.org/pdf/2402.16364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16318v1","updated":"2024-02-26T05:50:43Z","published":"2024-02-26T05:50:43Z","title":"Gradient-Guided Modality Decoupling for Missing-Modality Robustness","summary":"  Multimodal learning with incomplete input data (missing modality) is\npractical and challenging. In this work, we conduct an in-depth analysis of\nthis challenge and find that modality dominance has a significant negative\nimpact on the model training, greatly degrading the missing modality\nperformance. Motivated by Grad-CAM, we introduce a novel indicator, gradients,\nto monitor and reduce modality dominance which widely exists in the\nmissing-modality scenario. In aid of this indicator, we present a novel\nGradient-guided Modality Decoupling (GMD) method to decouple the dependency on\ndominating modalities. Specifically, GMD removes the conflicted gradient\ncomponents from different modalities to achieve this decoupling, significantly\nimproving the performance. In addition, to flexibly handle modal-incomplete\ndata, we design a parameter-efficient Dynamic Sharing (DS) framework which can\nadaptively switch on/off the network parameters based on whether one modality\nis available. We conduct extensive experiments on three popular multimodal\nbenchmarks, including BraTS 2018 for medical segmentation, CMU-MOSI, and\nCMU-MOSEI for sentiment analysis. The results show that our method can\nsignificantly outperform the competitors, showing the effectiveness of the\nproposed solutions. Our code is released here:\nhttps://github.com/HaoWang420/Gradient-guided-Modality-Decoupling.\n","authors":["Hao Wang","Shengda Luo","Guosheng Hu","Jianguo Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.16318v1.pdf","comment":"AAAI24"}]},"2024-02-25T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2310.13995v2","updated":"2024-02-25T22:34:50Z","published":"2023-10-21T12:43:27Z","title":"On Bilingual Lexicon Induction with Large Language Models","summary":"  Bilingual Lexicon Induction (BLI) is a core task in multilingual NLP that\nstill, to a large extent, relies on calculating cross-lingual word\nrepresentations. Inspired by the global paradigm shift in NLP towards Large\nLanguage Models (LLMs), we examine the potential of the latest generation of\nLLMs for the development of bilingual lexicons. We ask the following research\nquestion: Is it possible to prompt and fine-tune multilingual LLMs (mLLMs) for\nBLI, and how does this approach compare against and complement current BLI\napproaches? To this end, we systematically study 1) zero-shot prompting for\nunsupervised BLI and 2) few-shot in-context prompting with a set of seed\ntranslation pairs, both without any LLM fine-tuning, as well as 3) standard\nBLI-oriented fine-tuning of smaller LLMs. We experiment with 18 open-source\ntext-to-text mLLMs of different sizes (from 0.3B to 13B parameters) on two\nstandard BLI benchmarks covering a range of typologically diverse languages.\nOur work is the first to demonstrate strong BLI capabilities of text-to-text\nmLLMs. The results reveal that few-shot prompting with in-context examples from\nnearest neighbours achieves the best performance, establishing new\nstate-of-the-art BLI scores for many language pairs. We also conduct a series\nof in-depth analyses and ablation studies, providing more insights on BLI with\n(m)LLMs, also along with their limitations.\n","authors":["Yaoyiran Li","Anna Korhonen","Ivan Vulić"],"pdf_url":"https://arxiv.org/pdf/2310.13995v2.pdf","comment":"EMNLP 2023 Main Conference"},{"id":"http://arxiv.org/abs/2312.12430v3","updated":"2024-02-25T21:51:21Z","published":"2023-12-19T18:56:52Z","title":"Efficient Title Reranker for Fast and Improved Knowledge-Intense NLP","summary":"  In recent RAG approaches, rerankers play a pivotal role in refining retrieval\naccuracy with the ability of revealing logical relations for each pair of query\nand text. However, existing rerankers are required to repeatedly encode the\nquery and a large number of long retrieved text. This results in high\ncomputational costs and limits the number of retrieved text, hindering\naccuracy. As a remedy of the problem, we introduce the Efficient Title Reranker\nvia Broadcasting Query Encoder, a novel technique for title reranking that\nachieves a 20x-40x speedup over the vanilla passage reranker. Furthermore, we\nintroduce Sigmoid Trick, a novel loss function customized for title reranking.\nCombining both techniques, we empirically validated their effectiveness,\nachieving state-of-the-art results on all four datasets we experimented with\nfrom the KILT knowledge benchmark.\n","authors":["Ziyi Chen","Jize Jiang","Daqian Zuo","Heyi Tao","Jun Yang","Yuxiang Wei"],"pdf_url":"https://arxiv.org/pdf/2312.12430v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16200v1","updated":"2024-02-25T21:25:06Z","published":"2024-02-25T21:25:06Z","title":"IR2: Information Regularization for Information Retrieval","summary":"  Effective information retrieval (IR) in settings with limited training data,\nparticularly for complex queries, remains a challenging task. This paper\nintroduces IR2, Information Regularization for Information Retrieval, a\ntechnique for reducing overfitting during synthetic data generation. This\napproach, representing a novel application of regularization techniques in\nsynthetic data creation for IR, is tested on three recent IR tasks\ncharacterized by complex queries: DORIS-MAE, ArguAna, and WhatsThatBook.\nExperimental results indicate that our regularization techniques not only\noutperform previous synthetic query generation methods on the tasks considered\nbut also reduce cost by up to 50%. Furthermore, this paper categorizes and\nexplores three regularization methods at different stages of the query\nsynthesis pipeline-input, prompt, and output-each offering varying degrees of\nperformance improvement compared to models where no regularization is applied.\nThis provides a systematic approach for optimizing synthetic data generation in\ndata-limited, complex-query IR scenarios. All code, prompts and synthetic data\nare available at\nhttps://github.com/Info-Regularization/Information-Regularization.\n","authors":["Jianyou Wang","Kaicheng Wang","Xiaoyue Wang","Weili Cao","Ramamohan Paturi","Leon Bergen"],"pdf_url":"https://arxiv.org/pdf/2402.16200v1.pdf","comment":"Accepted by LREC-COLING 2024 - The 2024 Joint International\n  Conference on Computational Linguistics, Language Resources and Evaluation"},{"id":"http://arxiv.org/abs/2304.03516v2","updated":"2024-02-25T17:56:29Z","published":"2023-04-07T07:20:16Z","title":"Generative Recommendation: Towards Next-generation Recommender Paradigm","summary":"  Recommender systems typically retrieve items from an item corpus for\npersonalized recommendations. However, such a retrieval-based recommender\nparadigm faces two limitations: 1) the human-generated items in the corpus\nmight fail to satisfy the users' diverse information needs, and 2) users\nusually adjust the recommendations via inefficient passive feedback, e.g.,\nclicks. Nowadays, AI-Generated Content (AIGC) has revealed significant success,\noffering the potential to overcome these limitations: 1) generative AI can\nproduce personalized items to satisfy users' information needs, and 2) the\nnewly emerged large language models significantly reduce the efforts of users\nto precisely express information needs via natural language instructions. In\nthis light, the boom of AIGC points the way towards the next-generation\nrecommender paradigm with two new objectives: 1) generating personalized\ncontent through generative AI, and 2) integrating user instructions to guide\ncontent generation.\n  To this end, we propose a novel Generative Recommender paradigm named\nGeneRec, which adopts an AI generator to personalize content generation and\nleverages user instructions. Specifically, we pre-process users' instructions\nand traditional feedback via an instructor to output the generation guidance.\nGiven the guidance, we instantiate the AI generator through an AI editor and an\nAI creator to repurpose existing items and create new items. Eventually,\nGeneRec can perform content retrieval, repurposing, and creation to satisfy\nusers' information needs. Besides, to ensure the trustworthiness of the\ngenerated items, we emphasize various fidelity checks. Moreover, we provide a\nroadmap to envision future developments of GeneRec and several domain-specific\napplications of GeneRec with potential research tasks. Lastly, we study the\nfeasibility of implementing AI editor and AI creator on micro-video generation.\n","authors":["Wenjie Wang","Xinyu Lin","Fuli Feng","Xiangnan He","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2304.03516v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09101v2","updated":"2024-02-25T15:53:37Z","published":"2023-11-15T16:47:57Z","title":"Towards A Unified View of Answer Calibration for Multi-Step Reasoning","summary":"  Large Language Models (LLMs) employing Chain-of-Thought (CoT) prompting have\nbroadened the scope for improving multi-step reasoning capabilities. We\ngenerally divide multi-step reasoning into two phases: path generation to\ngenerate the reasoning path(s); and answer calibration post-processing the\nreasoning path(s) to obtain a final answer. However, the existing literature\nlacks systematic analysis on different answer calibration approaches. In this\npaper, we summarize the taxonomy of recent answer calibration techniques and\nbreak them down into step-level and path-level strategies. We then conduct a\nthorough evaluation on these strategies from a unified view, systematically\nscrutinizing step-level and path-level answer calibration across multiple\npaths. Experimental results reveal that integrating the dominance of both\nstrategies tends to derive optimal outcomes. Our study holds the potential to\nilluminate key insights for optimizing multi-step reasoning with answer\ncalibration.\n","authors":["Shumin Deng","Ningyu Zhang","Nay Oo","Bryan Hooi"],"pdf_url":"https://arxiv.org/pdf/2311.09101v2.pdf","comment":"Working in Progress"},{"id":"http://arxiv.org/abs/2402.16110v1","updated":"2024-02-25T15:20:01Z","published":"2024-02-25T15:20:01Z","title":"Disentangled Graph Variational Auto-Encoder for Multimodal\n  Recommendation with Interpretability","summary":"  Multimodal recommender systems amalgamate multimodal information (e.g.,\ntextual descriptions, images) into a collaborative filtering framework to\nprovide more accurate recommendations. While the incorporation of multimodal\ninformation could enhance the interpretability of these systems, current\nmultimodal models represent users and items utilizing entangled numerical\nvectors, rendering them arduous to interpret. To address this, we propose a\nDisentangled Graph Variational Auto-Encoder (DGVAE) that aims to enhance both\nmodel and recommendation interpretability. DGVAE initially projects multimodal\ninformation into textual contents, such as converting images to text, by\nharnessing state-of-the-art multimodal pre-training technologies. It then\nconstructs a frozen item-item graph and encodes the contents and interactions\ninto two sets of disentangled representations utilizing a simplified residual\ngraph convolutional network. DGVAE further regularizes these disentangled\nrepresentations through mutual information maximization, aligning the\nrepresentations derived from the interactions between users and items with\nthose learned from textual content. This alignment facilitates the\ninterpretation of user binary interactions via text. Our empirical analysis\nconducted on three real-world datasets demonstrates that DGVAE significantly\nsurpasses the performance of state-of-the-art baselines by a margin of 10.02%.\nWe also furnish a case study from a real-world dataset to illustrate the\ninterpretability of DGVAE. Code is available at:\n\\url{https://github.com/enoche/DGVAE}.\n","authors":["Xin Zhou","Chunyan Miao"],"pdf_url":"https://arxiv.org/pdf/2402.16110v1.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2308.07426v3","updated":"2024-02-25T13:40:56Z","published":"2023-08-14T19:36:57Z","title":"A Survey on Point-of-Interest Recommendations Leveraging Heterogeneous\n  Data","summary":"  Tourism is an important application domain for recommender systems. In this\ndomain, recommender systems are for example tasked with providing personalized\nrecommendations for transportation, accommodation, points-of-interest (POIs),\netc. Among these tasks, in particular the problem of recommending POIs that are\nof likely interest to individual tourists has gained growing attention in\nrecent years. Providing POI recommendations to tourists can however be\nespecially challenging due to the variability of the user's context. With the\nrapid development of the Web and today's multitude of online services, vast\namounts of data from various sources have become available, and these\nheterogeneous data represent a huge potential to better address the challenges\nof POI recommendation problems. In this work, we provide a survey of published\nresearch on the problem of POI recommendation between 2021 and 2023. The\nliterature was surveyed to identify the information types, techniques and\nevaluation methods employed. Based on the analysis, it was observed that the\ncurrent research tends to focus on a relatively narrow range of information\ntypes and there is a significant potential in improving POI recommendation by\nleveraging heterogeneous data. As the first information-centric survey on POI\nrecommendation research, this study serves as a reference for researchers\naiming to develop increasingly accurate, personalized and context-aware POI\nrecommender systems.\n","authors":["Zehui Wang","Wolfram Höpken","Dietmar Jannach"],"pdf_url":"https://arxiv.org/pdf/2308.07426v3.pdf","comment":"33 pages, 16 figures"},{"id":"http://arxiv.org/abs/2402.16073v1","updated":"2024-02-25T12:06:33Z","published":"2024-02-25T12:06:33Z","title":"Pfeed: Generating near real-time personalized feeds using precomputed\n  embedding similarities","summary":"  In personalized recommender systems, embeddings are often used to encode\ncustomer actions and items, and retrieval is then performed in the embedding\nspace using approximate nearest neighbor search. However, this approach can\nlead to two challenges: 1) user embeddings can restrict the diversity of\ninterests captured and 2) the need to keep them up-to-date requires an\nexpensive, real-time infrastructure. In this paper, we propose a method that\novercomes these challenges in a practical, industrial setting. The method\ndynamically updates customer profiles and composes a feed every two minutes,\nemploying precomputed embeddings and their respective similarities. We tested\nand deployed this method to personalise promotional items at Bol, one of the\nlargest e-commerce platforms of the Netherlands and Belgium. The method\nenhanced customer engagement and experience, leading to a significant 4.9%\nuplift in conversions.\n","authors":["Binyam Gebre","Karoliina Ranta","Stef van den Elzen","Ernst Kuiper","Thijs Baars","Tom Heskes"],"pdf_url":"https://arxiv.org/pdf/2402.16073v1.pdf","comment":"9 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.11916v3","updated":"2024-02-25T06:22:29Z","published":"2023-03-21T15:06:35Z","title":"CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion","summary":"  This paper proposes a novel diffusion-based model, CompoDiff, for solving\nzero-shot Composed Image Retrieval (ZS-CIR) with latent diffusion. This paper\nalso introduces a new synthetic dataset, named SynthTriplets18M, with 18.8\nmillion reference images, conditions, and corresponding target image triplets\nto train CIR models. CompoDiff and SynthTriplets18M tackle the shortages of the\nprevious CIR approaches, such as poor generalizability due to the small dataset\nscale and the limited types of conditions. CompoDiff not only achieves a new\nstate-of-the-art on four ZS-CIR benchmarks, including FashionIQ, CIRR, CIRCO,\nand GeneCIS, but also enables a more versatile and controllable CIR by\naccepting various conditions, such as negative text, and image mask conditions.\nCompoDiff also shows the controllability of the condition strength between text\nand image queries and the trade-off between inference speed and performance,\nwhich are unavailable with existing CIR methods. The code and dataset are\navailable at https://github.com/navervision/CompoDiff\n","authors":["Geonmo Gu","Sanghyuk Chun","Wonjae Kim","HeeJae Jun","Yoohoon Kang","Sangdoo Yun"],"pdf_url":"https://arxiv.org/pdf/2303.11916v3.pdf","comment":"First two authors contributed equally; 28 pages, 6.2MB"},{"id":"http://arxiv.org/abs/2305.02759v4","updated":"2024-02-25T05:53:34Z","published":"2023-05-04T11:53:38Z","title":"Disentangled Contrastive Collaborative Filtering","summary":"  Recent studies show that graph neural networks (GNNs) are prevalent to model\nhigh-order relationships for collaborative filtering (CF). Towards this\nresearch line, graph contrastive learning (GCL) has exhibited powerful\nperformance in addressing the supervision label shortage issue by learning\naugmented user and item representations. While many of them show their\neffectiveness, two key questions still remain unexplored: i) Most existing\nGCL-based CF models are still limited by ignoring the fact that user-item\ninteraction behaviors are often driven by diverse latent intent factors (e.g.,\nshopping for family party, preferred color or brand of products); ii) Their\nintroduced non-adaptive augmentation techniques are vulnerable to noisy\ninformation, which raises concerns about the model's robustness and the risk of\nincorporating misleading self-supervised signals. In light of these\nlimitations, we propose a Disentangled Contrastive Collaborative Filtering\nframework (DCCF) to realize intent disentanglement with self-supervised\naugmentation in an adaptive fashion. With the learned disentangled\nrepresentations with global context, our DCCF is able to not only distill\nfiner-grained latent factors from the entangled self-supervision signals but\nalso alleviate the augmentation-induced noise. Finally, the cross-view\ncontrastive learning task is introduced to enable adaptive augmentation with\nour parameterized interaction mask generator. Experiments on various public\ndatasets demonstrate the superiority of our method compared to existing\nsolutions. Our model implementation is released at the link\nhttps://github.com/HKUDS/DCCF.\n","authors":["Xubin Ren","Lianghao Xia","Jiashu Zhao","Dawei Yin","Chao Huang"],"pdf_url":"https://arxiv.org/pdf/2305.02759v4.pdf","comment":"Published as a SIGIR'23 full paper"},{"id":"http://arxiv.org/abs/2310.15950v4","updated":"2024-02-25T05:44:27Z","published":"2023-10-24T15:51:13Z","title":"Representation Learning with Large Language Models for Recommendation","summary":"  Recommender systems have seen significant advancements with the influence of\ndeep learning and graph neural networks, particularly in capturing complex\nuser-item relationships. However, these graph-based recommenders heavily depend\non ID-based data, potentially disregarding valuable textual information\nassociated with users and items, resulting in less informative learned\nrepresentations. Moreover, the utilization of implicit feedback data introduces\npotential noise and bias, posing challenges for the effectiveness of user\npreference learning. While the integration of large language models (LLMs) into\ntraditional ID-based recommenders has gained attention, challenges such as\nscalability issues, limitations in text-only reliance, and prompt input\nconstraints need to be addressed for effective implementation in practical\nrecommender systems. To address these challenges, we propose a model-agnostic\nframework RLMRec that aims to enhance existing recommenders with LLM-empowered\nrepresentation learning. It proposes a recommendation paradigm that integrates\nrepresentation learning with LLMs to capture intricate semantic aspects of user\nbehaviors and preferences. RLMRec incorporates auxiliary textual signals,\ndevelops a user/item profiling paradigm empowered by LLMs, and aligns the\nsemantic space of LLMs with the representation space of collaborative\nrelational signals through a cross-view alignment framework. This work further\nestablish a theoretical foundation demonstrating that incorporating textual\nsignals through mutual information maximization enhances the quality of\nrepresentations. In our evaluation, we integrate RLMRec with state-of-the-art\nrecommender models, while also analyzing its efficiency and robustness to noise\ndata. Our implementation codes are available at\nhttps://github.com/HKUDS/RLMRec.\n","authors":["Xubin Ren","Wei Wei","Lianghao Xia","Lixin Su","Suqi Cheng","Junfeng Wang","Dawei Yin","Chao Huang"],"pdf_url":"https://arxiv.org/pdf/2310.15950v4.pdf","comment":"Published as a WWW'24 full paper"}],"Multimedia":[{"id":"http://arxiv.org/abs/2402.16153v1","updated":"2024-02-25T17:19:41Z","published":"2024-02-25T17:19:41Z","title":"ChatMusician: Understanding and Generating Music Intrinsically with LLM","summary":"  While Large Language Models (LLMs) demonstrate impressive capabilities in\ntext generation, we find that their ability has yet to be generalized to music,\nhumanity's creative language. We introduce ChatMusician, an open-source LLM\nthat integrates intrinsic musical abilities. It is based on continual\npre-training and finetuning LLaMA2 on a text-compatible music representation,\nABC notation, and the music is treated as a second language. ChatMusician can\nunderstand and generate music with a pure text tokenizer without any external\nmulti-modal neural structures or tokenizers. Interestingly, endowing musical\nabilities does not harm language abilities, even achieving a slightly higher\nMMLU score. Our model is capable of composing well-structured, full-length\nmusic, conditioned on texts, chords, melodies, motifs, musical forms, etc,\nsurpassing GPT-4 baseline. On our meticulously curated college-level music\nunderstanding benchmark, MusicTheoryBench, ChatMusician surpasses LLaMA2 and\nGPT-3.5 on zero-shot setting by a noticeable margin. Our work reveals that LLMs\ncan be an excellent compressor for music, but there remains significant\nterritory to be conquered. We release our 4B token music-language corpora\nMusicPile, the collected MusicTheoryBench, code, model and demo in GitHub.\n","authors":["Ruibin Yuan","Hanfeng Lin","Yi Wang","Zeyue Tian","Shangda Wu","Tianhao Shen","Ge Zhang","Yuhang Wu","Cong Liu","Ziya Zhou","Ziyang Ma","Liumeng Xue","Ziyu Wang","Qin Liu","Tianyu Zheng","Yizhi Li","Yinghao Ma","Yiming Liang","Xiaowei Chi","Ruibo Liu","Zili Wang","Pengfei Li","Jingcheng Wu","Chenghua Lin","Qifeng Liu","Tao Jiang","Wenhao Huang","Wenhu Chen","Emmanouil Benetos","Jie Fu","Gus Xia","Roger Dannenberg","Wei Xue","Shiyin Kang","Yike Guo"],"pdf_url":"https://arxiv.org/pdf/2402.16153v1.pdf","comment":"GitHub: https://shanghaicannon.github.io/ChatMusician/"},{"id":"http://arxiv.org/abs/2402.16110v1","updated":"2024-02-25T15:20:01Z","published":"2024-02-25T15:20:01Z","title":"Disentangled Graph Variational Auto-Encoder for Multimodal\n  Recommendation with Interpretability","summary":"  Multimodal recommender systems amalgamate multimodal information (e.g.,\ntextual descriptions, images) into a collaborative filtering framework to\nprovide more accurate recommendations. While the incorporation of multimodal\ninformation could enhance the interpretability of these systems, current\nmultimodal models represent users and items utilizing entangled numerical\nvectors, rendering them arduous to interpret. To address this, we propose a\nDisentangled Graph Variational Auto-Encoder (DGVAE) that aims to enhance both\nmodel and recommendation interpretability. DGVAE initially projects multimodal\ninformation into textual contents, such as converting images to text, by\nharnessing state-of-the-art multimodal pre-training technologies. It then\nconstructs a frozen item-item graph and encodes the contents and interactions\ninto two sets of disentangled representations utilizing a simplified residual\ngraph convolutional network. DGVAE further regularizes these disentangled\nrepresentations through mutual information maximization, aligning the\nrepresentations derived from the interactions between users and items with\nthose learned from textual content. This alignment facilitates the\ninterpretation of user binary interactions via text. Our empirical analysis\nconducted on three real-world datasets demonstrates that DGVAE significantly\nsurpasses the performance of state-of-the-art baselines by a margin of 10.02%.\nWe also furnish a case study from a real-world dataset to illustrate the\ninterpretability of DGVAE. Code is available at:\n\\url{https://github.com/enoche/DGVAE}.\n","authors":["Xin Zhou","Chunyan Miao"],"pdf_url":"https://arxiv.org/pdf/2402.16110v1.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2311.06056v2","updated":"2024-02-25T14:00:33Z","published":"2023-11-10T13:39:47Z","title":"Learning Contrastive Self-Distillation for Ultra-Fine-Grained Visual\n  Categorization Targeting Limited Samples","summary":"  In the field of intelligent multimedia analysis, ultra-fine-grained visual\ncategorization (Ultra-FGVC) plays a vital role in distinguishing intricate\nsubcategories within broader categories. However, this task is inherently\nchallenging due to the complex granularity of category subdivisions and the\nlimited availability of data for each category. To address these challenges,\nthis work proposes CSDNet, a pioneering framework that effectively explores\ncontrastive learning and self-distillation to learn discriminative\nrepresentations specifically designed for Ultra-FGVC tasks. CSDNet comprises\nthree main modules: Subcategory-Specific Discrepancy Parsing (SSDP), Dynamic\nDiscrepancy Learning (DDL), and Subcategory-Specific Discrepancy Transfer\n(SSDT), which collectively enhance the generalization of deep models across\ninstance, feature, and logit prediction levels. To increase the diversity of\ntraining samples, the SSDP module introduces adaptive augmented samples to\nspotlight subcategory-specific discrepancies. Simultaneously, the proposed DDL\nmodule stores historical intermediate features by a dynamic memory queue, which\noptimizes the feature learning space through iterative contrastive learning.\nFurthermore, the SSDT module effectively distills subcategory-specific\ndiscrepancies knowledge from the inherent structure of limited training data\nusing a self-distillation paradigm at the logit prediction level. Experimental\nresults demonstrate that CSDNet outperforms current state-of-the-art Ultra-FGVC\nmethods, emphasizing its powerful efficacy and adaptability in addressing\nUltra-FGVC tasks.\n","authors":["Ziye Fang","Xin Jiang","Hao Tang","Zechao Li"],"pdf_url":"https://arxiv.org/pdf/2311.06056v2.pdf","comment":"Accepted for Publication in TCSVT"},{"id":"http://arxiv.org/abs/2402.10002v3","updated":"2024-02-25T07:58:07Z","published":"2024-02-15T15:10:17Z","title":"MM-Point: Multi-View Information-Enhanced Multi-Modal Self-Supervised 3D\n  Point Cloud Understanding","summary":"  In perception, multiple sensory information is integrated to map visual\ninformation from 2D views onto 3D objects, which is beneficial for\nunderstanding in 3D environments. But in terms of a single 2D view rendered\nfrom different angles, only limited partial information can be provided.The\nrichness and value of Multi-view 2D information can provide superior\nself-supervised signals for 3D objects. In this paper, we propose a novel\nself-supervised point cloud representation learning method, MM-Point, which is\ndriven by intra-modal and inter-modal similarity objectives. The core of\nMM-Point lies in the Multi-modal interaction and transmission between 3D\nobjects and multiple 2D views at the same time. In order to more effectively\nsimultaneously perform the consistent cross-modal objective of 2D multi-view\ninformation based on contrastive learning, we further propose Multi-MLP and\nMulti-level Augmentation strategies. Through carefully designed transformation\nstrategies, we further learn Multi-level invariance in 2D Multi-views. MM-Point\ndemonstrates state-of-the-art (SOTA) performance in various downstream tasks.\nFor instance, it achieves a peak accuracy of 92.4% on the synthetic dataset\nModelNet40, and a top accuracy of 87.8% on the real-world dataset ScanObjectNN,\ncomparable to fully supervised methods. Additionally, we demonstrate its\neffectiveness in tasks such as few-shot classification, 3D part segmentation\nand 3D semantic segmentation.\n","authors":["Hai-Tao Yu","Mofei Song"],"pdf_url":"https://arxiv.org/pdf/2402.10002v3.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2402.05608v2","updated":"2024-02-25T03:06:17Z","published":"2024-02-08T12:08:42Z","title":"Scalable Diffusion Models with State Space Backbone","summary":"  This paper presents a new exploration into a category of diffusion models\nbuilt upon state space architecture. We endeavor to train diffusion models for\nimage data, wherein the traditional U-Net backbone is supplanted by a state\nspace backbone, functioning on raw patches or latent space. Given its notable\nefficacy in accommodating long-range dependencies, Diffusion State Space Models\n(DiS) are distinguished by treating all inputs including time, condition, and\nnoisy image patches as tokens. Our assessment of DiS encompasses both\nunconditional and class-conditional image generation scenarios, revealing that\nDiS exhibits comparable, if not superior, performance to CNN-based or\nTransformer-based U-Net architectures of commensurate size. Furthermore, we\nanalyze the scalability of DiS, gauged by the forward pass complexity\nquantified in Gflops. DiS models with higher Gflops, achieved through\naugmentation of depth/width or augmentation of input tokens, consistently\ndemonstrate lower FID. In addition to demonstrating commendable scalability\ncharacteristics, DiS-H/2 models in latent space achieve performance levels akin\nto prior diffusion models on class-conditional ImageNet benchmarks at the\nresolution of 256$\\times$256 and 512$\\times$512, while significantly reducing\nthe computational burden. The code and models are available at:\nhttps://github.com/feizc/DiS.\n","authors":["Zhengcong Fei","Mingyuan Fan","Changqian Yu","Junshi Huang"],"pdf_url":"https://arxiv.org/pdf/2402.05608v2.pdf","comment":null}]},"2024-02-24T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2402.15925v1","updated":"2024-02-24T23:01:21Z","published":"2024-02-24T23:01:21Z","title":"MultiContrievers: Analysis of Dense Retrieval Representations","summary":"  Dense retrievers compress source documents into (possibly lossy) vector\nrepresentations, yet there is little analysis of what information is lost\nversus preserved, and how it affects downstream tasks. We conduct the first\nanalysis of the information captured by dense retrievers compared to the\nlanguage models they are based on (e.g., BERT versus Contriever). We use 25\nMultiBert checkpoints as randomized initialisations to train MultiContrievers,\na set of 25 contriever models. We test whether specific pieces of information\n-- such as gender and occupation -- can be extracted from contriever vectors of\nwikipedia-like documents. We measure this extractability via information\ntheoretic probing. We then examine the relationship of extractability to\nperformance and gender bias, as well as the sensitivity of these results to\nmany random initialisations and data shuffles. We find that (1) contriever\nmodels have significantly increased extractability, but extractability usually\ncorrelates poorly with benchmark performance 2) gender bias is present, but is\nnot caused by the contriever representations 3) there is high sensitivity to\nboth random initialisation and to data shuffle, suggesting that future\nretrieval research should test across a wider spread of both.\n","authors":["Seraphina Goldfarb-Tarrant","Pedro Rodriguez","Jane Dwivedi-Yu","Patrick Lewis"],"pdf_url":"https://arxiv.org/pdf/2402.15925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.11648v4","updated":"2024-02-24T16:08:22Z","published":"2024-01-22T01:58:32Z","title":"Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal\n  Contrastive EHR Modelling with Hierarchical Regularisation","summary":"  Predicting next visit diagnosis using Electronic Health Records (EHR) is an\nessential task in healthcare, critical for devising proactive future plans for\nboth healthcare providers and patients. Nonetheless, many preceding studies\nhave not sufficiently addressed the heterogeneous and hierarchical\ncharacteristics inherent in EHR data, inevitably leading to sub-optimal\nperformance. To this end, we propose NECHO, a novel medical code-centric\nmultimodal contrastive EHR learning framework with hierarchical regularisation.\nFirst, we integrate multifaceted information encompassing medical codes,\ndemographics, and clinical notes using a tailored network design and a pair of\nbimodal contrastive losses, all of which pivot around a medical codes\nrepresentation. We also regularise modality-specific encoders using a parental\nlevel information in medical ontology to learn hierarchical structure of EHR\ndata. A series of experiments on MIMIC-III data demonstrates effectiveness of\nour approach.\n","authors":["Heejoon Koo"],"pdf_url":"https://arxiv.org/pdf/2401.11648v4.pdf","comment":"Accepted to EACL 2024 (The 18th Conference of the European Chapter of\n  the Association for Computational Linguistics)"},{"id":"http://arxiv.org/abs/2402.15838v1","updated":"2024-02-24T15:31:59Z","published":"2024-02-24T15:31:59Z","title":"ListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot\n  Retrieval","summary":"  We propose ListT5, a novel reranking approach based on Fusion-in-Decoder\n(FiD) that handles multiple candidate passages at both train and inference\ntime. We also introduce an efficient inference framework for listwise ranking\nbased on m-ary tournament sort with output caching. We evaluate and compare our\nmodel on the BEIR benchmark for zero-shot retrieval task, demonstrating that\nListT5 (1) outperforms the state-of-the-art RankT5 baseline with a notable +1.3\ngain in the average NDCG@10 score, (2) has an efficiency comparable to\npointwise ranking models and surpasses the efficiency of previous listwise\nranking models, and (3) overcomes the lost-in-the-middle problem of previous\nlistwise rerankers. Our code, model checkpoints, and the evaluation framework\nare fully open-sourced at \\url{https://github.com/soyoung97/ListT5}.\n","authors":["Soyoung Yoon","Eunbi Lee","Jiyeon Kim","Yireun Kim","Hyeongu Yun","Seung-won Hwang"],"pdf_url":"https://arxiv.org/pdf/2402.15838v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15819v1","updated":"2024-02-24T14:10:04Z","published":"2024-02-24T14:10:04Z","title":"Debiased Model-based Interactive Recommendation","summary":"  Existing model-based interactive recommendation systems are trained by\nquerying a world model to capture the user preference, but learning the world\nmodel from historical logged data will easily suffer from bias issues such as\npopularity bias and sampling bias. This is why some debiased methods have been\nproposed recently. However, two essential drawbacks still remain: 1) ignoring\nthe dynamics of the time-varying popularity results in a false reweighting of\nitems. 2) taking the unknown samples as negative samples in negative sampling\nresults in the sampling bias. To overcome these two drawbacks, we develop a\nmodel called \\textbf{i}dentifiable \\textbf{D}ebiased \\textbf{M}odel-based\n\\textbf{I}nteractive \\textbf{R}ecommendation (\\textbf{iDMIR} in short). In\niDMIR, for the first drawback, we devise a debiased causal world model based on\nthe causal mechanism of the time-varying recommendation generation process with\nidentification guarantees; for the second drawback, we devise a debiased\ncontrastive policy, which coincides with the debiased contrastive learning and\navoids sampling bias. Moreover, we demonstrate that the proposed method not\nonly outperforms several latest interactive recommendation algorithms but also\nenjoys diverse recommendation performance.\n","authors":["Zijian Li","Ruichu Cai","Haiqin Huang","Sili Zhang","Yuguang Yan","Zhifeng Hao","Zhenghua Dong"],"pdf_url":"https://arxiv.org/pdf/2402.15819v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.12650v2","updated":"2024-02-24T12:44:41Z","published":"2023-05-22T02:51:27Z","title":"When Federated Recommendation Meets Cold-Start Problem: Separating Item\n  Attributes and User Interactions","summary":"  Federated recommendation system usually trains a global model on the server\nwithout direct access to users' private data on their own devices. However,\nthis separation of the recommendation model and users' private data poses a\nchallenge in providing quality service, particularly when it comes to new\nitems, namely cold-start recommendations in federated settings. This paper\nintroduces a novel method called Item-aligned Federated Aggregation (IFedRec)\nto address this challenge. It is the first research work in federated\nrecommendation to specifically study the cold-start scenario. The proposed\nmethod learns two sets of item representations by leveraging item attributes\nand interaction records simultaneously. Additionally, an item representation\nalignment mechanism is designed to align two item representations and learn the\nmeta attribute network at the server within a federated learning framework.\nExperiments on four benchmark datasets demonstrate IFedRec's superior\nperformance for cold-start scenarios. Furthermore, we also verify IFedRec owns\ngood robustness when the system faces limited client participation and noise\ninjection, which brings promising practical application potential in\nprivacy-protection enhanced federated recommendation systems. The\nimplementation code is available\n","authors":["Chunxu Zhang","Guodong Long","Tianyi Zhou","Zijian Zhang","Peng Yan","Bo Yang"],"pdf_url":"https://arxiv.org/pdf/2305.12650v2.pdf","comment":"Accepted as a regular paper of WWW'24"},{"id":"http://arxiv.org/abs/2402.15779v1","updated":"2024-02-24T10:02:21Z","published":"2024-02-24T10:02:21Z","title":"Cryptanalysis and improvement of multimodal data encryption by\n  machine-learning-based system","summary":"  With the rising popularity of the internet and the widespread use of networks\nand information systems via the cloud and data centers, the privacy and\nsecurity of individuals and organizations have become extremely crucial. In\nthis perspective, encryption consolidates effective technologies that can\neffectively fulfill these requirements by protecting public information\nexchanges. To achieve these aims, the researchers used a wide assortment of\nencryption algorithms to accommodate the varied requirements of this field, as\nwell as focusing on complex mathematical issues during their work to\nsubstantially complicate the encrypted communication mechanism. as much as\npossible to preserve personal information while significantly reducing the\npossibility of attacks. Depending on how complex and distinct the requirements\nestablished by these various applications are, the potential of trying to break\nthem continues to occur, and systems for evaluating and verifying the\ncryptographic algorithms implemented continue to be necessary. The best\napproach to analyzing an encryption algorithm is to identify a practical and\nefficient technique to break it or to learn ways to detect and repair weak\naspects in algorithms, which is known as cryptanalysis. Experts in\ncryptanalysis have discovered several methods for breaking the cipher, such as\ndiscovering a critical vulnerability in mathematical equations to derive the\nsecret key or determining the plaintext from the ciphertext. There are various\nattacks against secure cryptographic algorithms in the literature, and the\nstrategies and mathematical solutions widely employed empower cryptanalysts to\ndemonstrate their findings, identify weaknesses, and diagnose maintenance\nfailures in algorithms.\n","authors":["Zakaria Tolba"],"pdf_url":"https://arxiv.org/pdf/2402.15779v1.pdf","comment":"Doctoral thesis. Keywords: Cryptanalysis, Black-box, Deep learning,\n  Machine learning, Ciphertext, Plaintext, Genetic algorithm, Permutation box,\n  Substitution Box"},{"id":"http://arxiv.org/abs/2204.11602v5","updated":"2024-02-24T07:00:04Z","published":"2022-04-20T01:25:08Z","title":"Broad Recommender System: An Efficient Nonlinear Collaborative Filtering\n  Approach","summary":"  Recently, Deep Neural Networks (DNNs) have been widely introduced into\nCollaborative Filtering (CF) to produce more accurate recommendation results\ndue to their capability of capturing the complex nonlinear relationships\nbetween items and users.However, the DNNs-based models usually suffer from high\ncomputational complexity, i.e., consuming very long training time and storing\nhuge amount of trainable parameters. To address these problems, we propose a\nnew broad recommender system called Broad Collaborative Filtering (BroadCF),\nwhich is an efficient nonlinear collaborative filtering approach. Instead of\nDNNs, Broad Learning System (BLS) is used as a mapping function to learn the\ncomplex nonlinear relationships between users and items, which can avoid the\nabove issues while achieving very satisfactory recommendation performance.\nHowever, it is not feasible to directly feed the original rating data into BLS.\nTo this end, we propose a user-item rating collaborative vector preprocessing\nprocedure to generate low-dimensional user-item input data, which is able to\nharness quality judgments of the most similar users/items. Extensive\nexperiments conducted on seven benchmark datasets have confirmed the\neffectiveness of the proposed BroadCF algorithm\n","authors":["Ling Huang","Can-Rong Guan","Zhen-Wei Huang","Yuefang Gao","Yingjie Kuang","Chang-Dong Wang","C. L. Philip Chen"],"pdf_url":"https://arxiv.org/pdf/2204.11602v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14651v2","updated":"2024-02-24T06:48:20Z","published":"2023-05-24T02:39:20Z","title":"Revisit and Outstrip Entity Alignment: A Perspective of Generative\n  Models","summary":"  Recent embedding-based methods have achieved great successes in exploiting\nentity alignment from knowledge graph (KG) embeddings of multiple modalities.\nIn this paper, we study embedding-based entity alignment (EEA) from a\nperspective of generative models. We show that EEA shares similarities with\ntypical generative models and prove the effectiveness of the recently developed\ngenerative adversarial network (GAN)-based EEA methods theoretically. We then\nreveal that their incomplete objective limits the capacity on both entity\nalignment and entity synthesis (i.e., generating new entities). We mitigate\nthis problem by introducing a generative EEA (GEEA) framework with the proposed\nmutual variational autoencoder (M-VAE) as the generative model. M-VAE enables\nentity conversion between KGs and generation of new entities from random noise\nvectors. We demonstrate the power of GEEA with theoretical analysis and\nempirical experiments on both entity alignment and entity synthesis tasks.\n","authors":["Lingbing Guo","Zhuo Chen","Jiaoyan Chen","Yin Fang","Wen Zhang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2305.14651v2.pdf","comment":"Accepted at ICLR 2024"},{"id":"http://arxiv.org/abs/2402.15708v1","updated":"2024-02-24T04:08:51Z","published":"2024-02-24T04:08:51Z","title":"Query Augmentation by Decoding Semantics from Brain Signals","summary":"  Query augmentation is a crucial technique for refining semantically imprecise\nqueries. Traditionally, query augmentation relies on extracting information\nfrom initially retrieved, potentially relevant documents. If the quality of the\ninitially retrieved documents is low, then the effectiveness of query\naugmentation would be limited as well. We propose Brain-Aug, which enhances a\nquery by incorporating semantic information decoded from brain signals.\nBrainAug generates the continuation of the original query with a prompt\nconstructed with brain signal information and a ranking-oriented inference\napproach. Experimental results on fMRI (functional magnetic resonance imaging)\ndatasets show that Brain-Aug produces semantically more accurate queries,\nleading to improved document ranking performance. Such improvement brought by\nbrain signals is particularly notable for ambiguous queries.\n","authors":["Ziyi Ye","Jingtao Zhan","Qingyao Ai","Yiqun Liu","Maarten de Rijke","Christina Lioma","Tuukka Ruotsalo"],"pdf_url":"https://arxiv.org/pdf/2402.15708v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15666v1","updated":"2024-02-24T00:41:16Z","published":"2024-02-24T00:41:16Z","title":"Universal Model in Online Customer Service","summary":"  Building machine learning models can be a time-consuming process that often\ntakes several months to implement in typical business scenarios. To ensure\nconsistent model performance and account for variations in data distribution,\nregular retraining is necessary. This paper introduces a solution for improving\nonline customer service in e-commerce by presenting a universal model for\npredict-ing labels based on customer questions, without requiring training. Our\nnovel approach involves using machine learning techniques to tag customer\nquestions in transcripts and create a repository of questions and corresponding\nlabels. When a customer requests assistance, an information retrieval model\nsearches the repository for similar questions, and statistical analysis is used\nto predict the corresponding label. By eliminating the need for individual\nmodel training and maintenance, our approach reduces both the model development\ncycle and costs. The repository only requires periodic updating to maintain\naccuracy.\n","authors":["Shu-Ting Pi","Cheng-Ping Hsieh","Qun Liu","Yuying Zhu"],"pdf_url":"https://arxiv.org/pdf/2402.15666v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2402.15923v1","updated":"2024-02-24T22:36:23Z","published":"2024-02-24T22:36:23Z","title":"Predicting Outcomes in Video Games with Long Short Term Memory Networks","summary":"  Forecasting winners in E-sports with real-time analytics has the potential to\nfurther engage audiences watching major tournament events. However, making such\nreal-time predictions is challenging due to unpredictable variables within the\ngame involving diverse player strategies and decision-making. Our work attempts\nto enhance audience engagement within video game tournaments by introducing a\nreal-time method of predicting wins. Our Long Short Term Memory Network (LSTMs)\nbased approach enables efficient predictions of win-lose outcomes by only using\nthe health indicator of each player as a time series. As a proof of concept, we\nevaluate our model's performance within a classic, two-player arcade game,\nSuper Street Fighter II Turbo. We also benchmark our method against state of\nthe art methods for time series forecasting; i.e. Transformer models found in\nlarge language models (LLMs). Finally, we open-source our data set and code in\nhopes of furthering work in predictive analysis for arcade games.\n","authors":["Kittimate Chulajata","Sean Wu","Fabien Scalzo","Eun Sang Cha"],"pdf_url":"https://arxiv.org/pdf/2402.15923v1.pdf","comment":"7 pages, 2 Figures, 2 Tables. Kittimate Chulajata and Sean Wu are\n  considered co-first authors"},{"id":"http://arxiv.org/abs/2402.15746v1","updated":"2024-02-24T06:58:15Z","published":"2024-02-24T06:58:15Z","title":"Intelligent Director: An Automatic Framework for Dynamic Visual\n  Composition using ChatGPT","summary":"  With the rise of short video platforms represented by TikTok, the trend of\nusers expressing their creativity through photos and videos has increased\ndramatically. However, ordinary users lack the professional skills to produce\nhigh-quality videos using professional creation software. To meet the demand\nfor intelligent and user-friendly video creation tools, we propose the Dynamic\nVisual Composition (DVC) task, an interesting and challenging task that aims to\nautomatically integrate various media elements based on user requirements and\ncreate storytelling videos. We propose an Intelligent Director framework,\nutilizing LENS to generate descriptions for images and video frames and\ncombining ChatGPT to generate coherent captions while recommending appropriate\nmusic names. Then, the best-matched music is obtained through music retrieval.\nThen, materials such as captions, images, videos, and music are integrated to\nseamlessly synthesize the video. Finally, we apply AnimeGANv2 for style\ntransfer. We construct UCF101-DVC and Personal Album datasets and verified the\neffectiveness of our framework in solving DVC through qualitative and\nquantitative comparisons, along with user studies, demonstrating its\nsubstantial potential.\n","authors":["Sixiao Zheng","Jingyang Huo","Yu Wang","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2402.15746v1.pdf","comment":"Project Page: https://sixiaozheng.github.io/IntelligentDirector/"},{"id":"http://arxiv.org/abs/2308.03475v2","updated":"2024-02-24T04:29:51Z","published":"2023-08-07T11:05:59Z","title":"COPA: Efficient Vision-Language Pre-training Through Collaborative\n  Object- and Patch-Text Alignment","summary":"  Vision-Language Pre-training (VLP) methods based on object detection enjoy\nthe rich knowledge of fine-grained object-text alignment but at the cost of\ncomputationally expensive inference. Recent Visual-Transformer (ViT)-based\napproaches circumvent this issue while struggling with long visual sequences\nwithout detailed cross-modal alignment information. This paper introduces a\nViT-based VLP technique that efficiently incorporates object information\nthrough a novel patch-text alignment mechanism. Specifically, we convert\nobject-level signals into patch-level ones and devise a Patch-Text Alignment\npre-training task (PTA) to learn a text-aware patch detector. By using\noff-the-shelf delicate object annotations in 5\\% training images, we jointly\ntrain PTA with other conventional VLP objectives in an end-to-end manner,\nbypassing the high computational cost of object detection and yielding an\neffective patch detector that accurately detects text-relevant patches, thus\nconsiderably reducing patch sequences and accelerating computation within the\nViT backbone. Our experiments on a variety of widely-used benchmarks reveal\nthat our method achieves a speedup of nearly 88\\% compared to prior VLP models\nwhile maintaining competitive or superior performance on downstream tasks with\nsimilar model size and data scale.\n","authors":["Chaoya Jiang","Haiyang Xu","Wei Ye","Qinghao Ye","Chenliang Li","Ming Yan","Bin Bi","Shikun Zhang","Ji Zhang","Fei Huang"],"pdf_url":"https://arxiv.org/pdf/2308.03475v2.pdf","comment":"Accepted on ACM MM2023"},{"id":"http://arxiv.org/abs/2402.15695v1","updated":"2024-02-24T02:55:31Z","published":"2024-02-24T02:55:31Z","title":"Applied User Research in Virtual Reality: Tools, Methods, and Challenges","summary":"  This chapter explores the practice of conducting user research studies and\ndesign assessments in virtual reality (VR). An overview of key VR hardware and\nsoftware tools is provided, including game engines, such as Unity and Unreal\nEngine. Qualitative and quantitative research methods, along with their various\nsynergies with VR, are likewise discussed, and some of the challenges\nassociated with VR, such as limited sensory stimulation, are reflected upon. VR\nis proving particularly useful in the context of space systems development,\nwhere its utilisation offers a cost-effective and secure method for simulating\nextraterrestrial environments, allowing for rapid prototyping and evaluation of\ninnovative concepts under representative operational conditions. To illustrate\nthis, we present a case study detailing the application of VR to aid aerospace\nengineers testing their ideas with end-users and stakeholders during early\ndesign stages of the European Space Agency's (ESA) prospective Argonaut lunar\nlander. This case study demonstrates the effectiveness of VR simulations in\ngathering important feedback concerning the operability of the Argonaut lander\nin poor lighting conditions as well as surfacing relevant ergonomics\nconsiderations and constraints. The chapter concludes by discussing the\nstrengths and weaknesses associated with VR-based user studies and proposes\nfuture research directions, emphasising the necessity for novel VR interfaces\nto overcome existing technical limitations.\n","authors":["Leonie Bensch","Andrea Casini","Aidan Cowley","Florian Dufresne","Enrico Guerra","Paul de Medeiros","Tommy Nilsson","Flavie Rometsch","Andreas Treuer","Anna Vock"],"pdf_url":"https://arxiv.org/pdf/2402.15695v1.pdf","comment":null}]},"2024-02-23T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2402.15643v1","updated":"2024-02-23T22:57:33Z","published":"2024-02-23T22:57:33Z","title":"Artful Path to Healing: Using Machine Learning for Visual Art\n  Recommendation to Prevent and Reduce Post-Intensive Care","summary":"  Staying in the intensive care unit (ICU) is often traumatic, leading to\npost-intensive care syndrome (PICS), which encompasses physical, psychological,\nand cognitive impairments. Currently, there are limited interventions available\nfor PICS. Studies indicate that exposure to visual art may help address the\npsychological aspects of PICS and be more effective if it is personalized. We\ndevelop Machine Learning-based Visual Art Recommendation Systems (VA RecSys) to\nenable personalized therapeutic visual art experiences for post-ICU patients.\nWe investigate four state-of-the-art VA RecSys engines, evaluating the\nrelevance of their recommendations for therapeutic purposes compared to\nexpert-curated recommendations. We conduct an expert pilot test and a\nlarge-scale user study (n=150) to assess the appropriateness and effectiveness\nof these recommendations. Our results suggest all recommendations enhance\ntemporal affective states. Visual and multimodal VA RecSys engines compare\nfavourably with expert-curated recommendations, indicating their potential to\nsupport the delivery of personalized art therapy for PICS prevention and\ntreatment.\n","authors":["Bereket A. Yilma","Chan Mi Kim","Gerald C. Cupchik","Luis A. Leiva"],"pdf_url":"https://arxiv.org/pdf/2402.15643v1.pdf","comment":"Proceedings of the 2024 CHI Conference on Human Factors in Computing\n  Systems (CHI 24)"},{"id":"http://arxiv.org/abs/2402.15623v1","updated":"2024-02-23T21:58:50Z","published":"2024-02-23T21:58:50Z","title":"Language-Based User Profiles for Recommendation","summary":"  Most conventional recommendation methods (e.g., matrix factorization)\nrepresent user profiles as high-dimensional vectors. Unfortunately, these\nvectors lack interpretability and steerability, and often perform poorly in\ncold-start settings. To address these shortcomings, we explore the use of user\nprofiles that are represented as human-readable text. We propose the\nLanguage-based Factorization Model (LFM), which is essentially an\nencoder/decoder model where both the encoder and the decoder are large language\nmodels (LLMs). The encoder LLM generates a compact natural-language profile of\nthe user's interests from the user's rating history. The decoder LLM uses this\nsummary profile to complete predictive downstream tasks. We evaluate our LFM\napproach on the MovieLens dataset, comparing it against matrix factorization\nand an LLM model that directly predicts from the user's rating history. In\ncold-start settings, we find that our method can have higher accuracy than\nmatrix factorization. Furthermore, we find that generating a compact and\nhuman-readable summary often performs comparably with or better than direct LLM\nprediction, while enjoying better interpretability and shorter model input\nlength. Our results motivate a number of future research directions and\npotential improvements.\n","authors":["Joyce Zhou","Yijia Dai","Thorsten Joachims"],"pdf_url":"https://arxiv.org/pdf/2402.15623v1.pdf","comment":"8 pages (4 in appendix), 22 tables/figures (16 in appendix). Accepted\n  to LLM-IGS@WSDM2024 workshop, now sharing this slightly updated revision\n  version with workshop"},{"id":"http://arxiv.org/abs/2308.14296v2","updated":"2024-02-23T21:05:48Z","published":"2023-08-28T04:31:04Z","title":"RecMind: Large Language Model Powered Agent For Recommendation","summary":"  While the recommendation system (RS) has advanced significantly through deep\nlearning, current RS approaches usually train and fine-tune models on\ntask-specific datasets, limiting their generalizability to new recommendation\ntasks and their ability to leverage external knowledge due to model scale and\ndata size constraints. Thus, we designed an LLM-powered autonomous recommender\nagent, RecMind, which is capable of leveraging external knowledge, utilizing\ntools with careful planning to provide zero-shot personalized recommendations.\nWe propose a Self-Inspiring algorithm to improve the planning ability. At each\nintermediate step, the LLM self-inspires to consider all previously explored\nstates to plan for the next step. This mechanism greatly improves the model's\nability to comprehend and utilize historical information in planning for\nrecommendation. We evaluate RecMind's performance in various recommendation\nscenarios. Our experiment shows that RecMind outperforms existing zero/few-shot\nLLM-based recommendation baseline methods in various tasks and achieves\ncomparable performance to a fully trained recommendation model P5.\n","authors":["Yancheng Wang","Ziyan Jiang","Zheng Chen","Fan Yang","Yingxue Zhou","Eunah Cho","Xing Fan","Xiaojiang Huang","Yanbin Lu","Yingzhen Yang"],"pdf_url":"https://arxiv.org/pdf/2308.14296v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15591v1","updated":"2024-02-23T20:16:13Z","published":"2024-02-23T20:16:13Z","title":"RecWizard: A Toolkit for Conversational Recommendation with Modular,\n  Portable Models and Interactive User Interface","summary":"  We present a new Python toolkit called RecWizard for Conversational\nRecommender Systems (CRS). RecWizard offers support for development of models\nand interactive user interface, drawing from the best practices of the\nHuggingface ecosystems. CRS with RecWizard are modular, portable, interactive\nand Large Language Models (LLMs)-friendly, to streamline the learning process\nand reduce the additional effort for CRS research. For more comprehensive\ninformation about RecWizard, please check our GitHub\nhttps://github.com/McAuley-Lab/RecWizard.\n","authors":["Zeyuan Zhang","Tanmay Laud","Zihang He","Xiaojie Chen","Xinshuang Liu","Zhouhang Xie","Julian McAuley","Zhankui He"],"pdf_url":"https://arxiv.org/pdf/2402.15591v1.pdf","comment":"AAAI'24 Demo Track"},{"id":"http://arxiv.org/abs/2111.02168v4","updated":"2024-02-23T19:22:23Z","published":"2021-11-03T12:13:52Z","title":"The Klarna Product Page Dataset: Web Element Nomination with Graph\n  Neural Networks and Large Language Models","summary":"  Web automation holds the potential to revolutionize how users interact with\nthe digital world, offering unparalleled assistance and simplifying tasks via\nsophisticated computational methods. Central to this evolution is the web\nelement nomination task, which entails identifying unique elements on webpages.\nUnfortunately, the development of algorithmic designs for web automation is\nhampered by the scarcity of comprehensive and realistic datasets that reflect\nthe complexity faced by real-world applications on the Web. To address this, we\nintroduce the Klarna Product Page Dataset, a comprehensive and diverse\ncollection of webpages that surpasses existing datasets in richness and\nvariety. The dataset features 51,701 manually labeled product pages from 8,175\ne-commerce websites across eight geographic regions, accompanied by a dataset\nof rendered page screenshots. To initiate research on the Klarna Product Page\nDataset, we empirically benchmark a range of Graph Neural Networks (GNNs) on\nthe web element nomination task. We make three important contributions. First,\nwe found that a simple Convolutional GNN (GCN) outperforms complex\nstate-of-the-art nomination methods. Second, we introduce a training refinement\nprocedure that involves identifying a small number of relevant elements from\neach page using the aforementioned GCN. These elements are then passed to a\nlarge language model for the final nomination. This procedure significantly\nimproves the nomination accuracy by 16.8 percentage points on our challenging\ndataset, without any need for fine-tuning. Finally, in response to another\nprevalent challenge in this field - the abundance of training methodologies\nsuitable for element nomination - we introduce the Challenge Nomination\nTraining Procedure, a novel training approach that further boosts nomination\naccuracy.\n","authors":["Alexandra Hotti","Riccardo Sven Risuleo","Stefan Magureanu","Aref Moradi","Jens Lagergren"],"pdf_url":"https://arxiv.org/pdf/2111.02168v4.pdf","comment":"12 pages, 8 figures, 3 tables, under review"},{"id":"http://arxiv.org/abs/2402.09727v2","updated":"2024-02-23T18:21:28Z","published":"2024-02-15T05:40:21Z","title":"A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts","summary":"  Current Large Language Models (LLMs) are not only limited to some maximum\ncontext length, but also are not able to robustly consume long inputs. To\naddress these limitations, we propose ReadAgent, an LLM agent system that\nincreases effective context length up to 20x in our experiments. Inspired by\nhow humans interactively read long documents, we implement ReadAgent as a\nsimple prompting system that uses the advanced language capabilities of LLMs to\n(1) decide what content to store together in a memory episode, (2) compress\nthose memory episodes into short episodic memories called gist memories, and\n(3) take actions to look up passages in the original text if ReadAgent needs to\nremind itself of relevant details to complete a task. We evaluate ReadAgent\nagainst baselines using retrieval methods, using the original long contexts,\nand using the gist memories. These evaluations are performed on three\nlong-document reading comprehension tasks: QuALITY, NarrativeQA, and QMSum.\nReadAgent outperforms the baselines on all three tasks while extending the\neffective context window by 3-20x.\n","authors":["Kuang-Huei Lee","Xinyun Chen","Hiroki Furuta","John Canny","Ian Fischer"],"pdf_url":"https://arxiv.org/pdf/2402.09727v2.pdf","comment":"Website: https://read-agent.github.io"},{"id":"http://arxiv.org/abs/2402.15400v1","updated":"2024-02-23T16:03:17Z","published":"2024-02-23T16:03:17Z","title":"Faithful Temporal Question Answering over Heterogeneous Sources","summary":"  Temporal question answering (QA) involves time constraints, with phrases such\nas \"... in 2019\" or \"... before COVID\". In the former, time is an explicit\ncondition, in the latter it is implicit. State-of-the-art methods have\nlimitations along three dimensions. First, with neural inference, time\nconstraints are merely soft-matched, giving room to invalid or inexplicable\nanswers. Second, questions with implicit time are poorly supported. Third,\nanswers come from a single source: either a knowledge base (KB) or a text\ncorpus. We propose a temporal QA system that addresses these shortcomings.\nFirst, it enforces temporal constraints for faithful answering with tangible\nevidence. Second, it properly handles implicit questions. Third, it operates\nover heterogeneous sources, covering KB, text and web tables in a unified\nmanner. The method has three stages: (i) understanding the question and its\ntemporal conditions, (ii) retrieving evidence from all sources, and (iii)\nfaithfully answering the question. As implicit questions are sparse in prior\nbenchmarks, we introduce a principled method for generating diverse questions.\nExperiments show superior performance over a suite of baselines.\n","authors":["Zhen Jia","Philipp Christmann","Gerhard Weikum"],"pdf_url":"https://arxiv.org/pdf/2402.15400v1.pdf","comment":"Accepted at WWW 2024"},{"id":"http://arxiv.org/abs/2402.15276v1","updated":"2024-02-23T11:47:16Z","published":"2024-02-23T11:47:16Z","title":"Text2Pic Swift: Enhancing Long-Text to Image Retrieval for Large-Scale\n  Libraries","summary":"  Text-to-image retrieval plays a crucial role across various applications,\nincluding digital libraries, e-commerce platforms, and multimedia databases, by\nenabling the search for images using text queries. Despite the advancements in\nMultimodal Large Language Models (MLLMs), which offer leading-edge performance,\ntheir applicability in large-scale, varied, and ambiguous retrieval scenarios\nis constrained by significant computational demands and the generation of\ninjective embeddings. This paper introduces the Text2Pic Swift framework,\ntailored for efficient and robust retrieval of images corresponding to\nextensive textual descriptions in sizable datasets. The framework employs a\ntwo-tier approach: the initial Entity-based Ranking (ER) stage addresses the\nambiguity inherent in lengthy text queries through a\nmultiple-queries-to-multiple-targets strategy, effectively narrowing down\npotential candidates for subsequent analysis. Following this, the Summary-based\nRe-ranking (SR) stage further refines these selections based on concise query\nsummaries. Additionally, we present a novel Decoupling-BEiT-3 encoder,\nspecifically designed to tackle the challenges of ambiguous queries and to\nfacilitate both stages of the retrieval process, thereby significantly\nimproving computational efficiency via vector-based similarity assessments. Our\nevaluation, conducted on the AToMiC dataset, demonstrates that Text2Pic Swift\noutperforms current MLLMs by achieving up to an 11.06% increase in Recall@1000,\nalongside reductions in training and retrieval durations by 68.75% and 99.79%,\nrespectively.\n","authors":["Zijun Long","Xuri Ge","Richard Mccreadie","Joemon Jose"],"pdf_url":"https://arxiv.org/pdf/2402.15276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15263v1","updated":"2024-02-23T11:21:05Z","published":"2024-02-23T11:21:05Z","title":"Countries pushing the boundaries of knowledge: the US dominance, China\n  rise, and the EU stagnation","summary":"  Knowing which countries contribute the most to pushing the boundaries of\nknowledge in science and technology has social and political importance.\nHowever, common citation metrics do not adequately measure this contribution.\nThis measure requires more stringent metrics appropriate for the highly\ninfluential breakthrough papers that push the boundaries of knowledge, which\nare very highly cited but very rare. Here I used the recently described Rk\nindex, specifically designed to address this issue. I applied this index to 25\ncountries and the EU across 10 key research topics, five technological and five\nbiomedical, studying domestic and international collaborative papers\nindependently. In technological topics, the Rk indices of domestic papers show\nthat overall, the USA, China, and the EU are leaders; other countries are\nclearly behind. The USA is notably ahead of China, and the EU is far behind\nChina. The same approach to biomedical topics shows an overwhelming dominance\nof the USA and that the EU is ahead of China. The analysis of internationally\ncollaborative papers further demonstrates the US dominance. These results\nconflict with current country rankings based on less stringent indicators.\n","authors":["Alonso Rodriguez-Navarro"],"pdf_url":"https://arxiv.org/pdf/2402.15263v1.pdf","comment":"18 pages, 1 figure, 6 tables"},{"id":"http://arxiv.org/abs/2402.15235v1","updated":"2024-02-23T09:57:20Z","published":"2024-02-23T09:57:20Z","title":"Multi-Agent Collaboration Framework for Recommender Systems","summary":"  LLM-based agents have gained considerable attention for their decision-making\nskills and ability to handle complex tasks. Recognizing the current gap in\nleveraging agent capabilities for multi-agent collaboration in recommendation\nsystems, we introduce MACRec, a novel framework designed to enhance\nrecommendation systems through multi-agent collaboration. Unlike existing work\non using agents for user/item simulation, we aim to deploy multi-agents to\ntackle recommendation tasks directly. In our framework, recommendation tasks\nare addressed through the collaborative efforts of various specialized agents,\nincluding Manager, User/Item Analyst, Reflector, Searcher, and Task\nInterpreter, with different working flows. Furthermore, we provide application\nexamples of how developers can easily use MACRec on various recommendation\ntasks, including rating prediction, sequential recommendation, conversational\nrecommendation, and explanation generation of recommendation results. The\nframework and demonstration video are publicly available at\nhttps://github.com/wzf2000/MACRec.\n","authors":["Zhefan Wang","Yuanqing Yu","Wendi Zheng","Weizhi Ma","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.15235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15215v1","updated":"2024-02-23T09:24:04Z","published":"2024-02-23T09:24:04Z","title":"Item-side Fairness of Large Language Model-based Recommendation System","summary":"  Recommendation systems for Web content distribution intricately connect to\nthe information access and exposure opportunities for vulnerable populations.\nThe emergence of Large Language Models-based Recommendation System (LRS) may\nintroduce additional societal challenges to recommendation systems due to the\ninherent biases in Large Language Models (LLMs). From the perspective of\nitem-side fairness, there remains a lack of comprehensive investigation into\nthe item-side fairness of LRS given the unique characteristics of LRS compared\nto conventional recommendation systems. To bridge this gap, this study examines\nthe property of LRS with respect to item-side fairness and reveals the\ninfluencing factors of both historical users' interactions and inherent\nsemantic biases of LLMs, shedding light on the need to extend conventional\nitem-side fairness methods for LRS. Towards this goal, we develop a concise and\neffective framework called IFairLRS to enhance the item-side fairness of an\nLRS. IFairLRS covers the main stages of building an LRS with specifically\nadapted strategies to calibrate the recommendations of LRS. We utilize IFairLRS\nto fine-tune LLaMA, a representative LLM, on \\textit{MovieLens} and\n\\textit{Steam} datasets, and observe significant item-side fairness\nimprovements. The code can be found in\nhttps://github.com/JiangM-C/IFairLRS.git.\n","authors":["Meng Jiang","Keqin Bao","Jizhi Zhang","Wenjie Wang","Zhengyi Yang","Fuli Feng","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2402.15215v1.pdf","comment":"Accepted by the Proceedings of the ACM Web Conference 2024"},{"id":"http://arxiv.org/abs/2312.10743v3","updated":"2024-02-23T08:52:20Z","published":"2023-12-17T15:28:06Z","title":"A Unified Framework for Multi-Domain CTR Prediction via Large Language\n  Models","summary":"  Click-Through Rate (CTR) prediction is a crucial task in online\nrecommendation platforms as it involves estimating the probability of user\nengagement with advertisements or items by clicking on them. Given the\navailability of various services like online shopping, ride-sharing, food\ndelivery, and professional services on commercial platforms, recommendation\nsystems in these platforms are required to make CTR predictions across multiple\ndomains rather than just a single domain. However, multi-domain click-through\nrate (MDCTR) prediction remains a challenging task in online recommendation due\nto the complex mutual influence between domains. Traditional MDCTR models\ntypically encode domains as discrete identifiers, ignoring rich semantic\ninformation underlying. Consequently, they can hardly generalize to new\ndomains. Besides, existing models can be easily dominated by some specific\ndomains, which results in significant performance drops in the other domains\n(i.e. the \"seesaw phenomenon\"). In this paper, we propose a novel solution\nUni-CTR to address the above challenges. Uni-CTR leverages a backbone Large\nLanguage Model (LLM) to learn layer-wise semantic representations that capture\ncommonalities between domains. Uni-CTR also uses several domain-specific\nnetworks to capture the characteristics of each domain. Note that we design a\nmasked loss strategy so that these domain-specific networks are decoupled from\nbackbone LLM. This allows domain-specific networks to remain unchanged when\nincorporating new or removing domains, thereby enhancing the flexibility and\nscalability of the system significantly. Experimental results on three public\ndatasets show that Uni-CTR outperforms the state-of-the-art (SOTA) MDCTR models\nsignificantly. Furthermore, Uni-CTR demonstrates remarkable effectiveness in\nzero-shot prediction. We have applied Uni-CTR in industrial scenarios,\nconfirming its efficiency.\n","authors":["Zichuan Fu","Xiangyang Li","Chuhan Wu","Yichao Wang","Kuicai Dong","Xiangyu Zhao","Mengchen Zhao","Huifeng Guo","Ruiming Tang"],"pdf_url":"https://arxiv.org/pdf/2312.10743v3.pdf","comment":"submited to TOIS"},{"id":"http://arxiv.org/abs/2311.04590v4","updated":"2024-02-23T08:03:25Z","published":"2023-11-08T10:44:20Z","title":"Rethinking Cross-Domain Sequential Recommendation under Open-World\n  Assumptions","summary":"  Cross-Domain Sequential Recommendation (CDSR) methods aim to tackle the data\nsparsity and cold-start problems present in Single-Domain Sequential\nRecommendation (SDSR). Existing CDSR works design their elaborate structures\nrelying on overlapping users to propagate the cross-domain information.\nHowever, current CDSR methods make closed-world assumptions, assuming fully\noverlapping users across multiple domains and that the data distribution\nremains unchanged from the training environment to the test environment. As a\nresult, these methods typically result in lower performance on online\nreal-world platforms due to the data distribution shifts. To address these\nchallenges under open-world assumptions, we design an \\textbf{A}daptive\n\\textbf{M}ulti-\\textbf{I}nterest \\textbf{D}ebiasing framework for cross-domain\nsequential recommendation (\\textbf{AMID}), which consists of a multi-interest\ninformation module (\\textbf{MIM}) and a doubly robust estimator (\\textbf{DRE}).\nOur framework is adaptive for open-world environments and can improve the model\nof most off-the-shelf single-domain sequential backbone models for CDSR. Our\nMIM establishes interest groups that consider both overlapping and\nnon-overlapping users, allowing us to effectively explore user intent and\nexplicit interest. To alleviate biases across multiple domains, we developed\nthe DRE for the CDSR methods. We also provide a theoretical analysis that\ndemonstrates the superiority of our proposed estimator in terms of bias and\ntail bound, compared to the IPS estimator used in previous work.\n","authors":["Wujiang Xu","Qitian Wu","Runzhong Wang","Mingming Ha","Qiongxu Ma","Linxun Chen","Bing Han","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2311.04590v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15164v1","updated":"2024-02-23T07:54:26Z","published":"2024-02-23T07:54:26Z","title":"EasyRL4Rec: A User-Friendly Code Library for Reinforcement Learning\n  Based Recommender Systems","summary":"  Reinforcement Learning (RL)-Based Recommender Systems (RSs) are increasingly\nrecognized for their ability to improve long-term user engagement. Yet, the\nfield grapples with challenges such as the absence of accessible frameworks,\ninconsistent evaluation standards, and the complexity of replicating prior\nwork. Addressing these obstacles, we present EasyRL4Rec, a user-friendly and\nefficient library tailored for RL-based RSs. EasyRL4Rec features lightweight,\ndiverse RL environments built on five widely-used public datasets, and is\nequipped with comprehensive core modules that offer rich options to ease the\ndevelopment of models. It establishes consistent evaluation criteria with a\nfocus on long-term impacts and introduces customized solutions for state\nmodeling and action representation tailored to recommender systems.\nAdditionally, we share valuable insights gained from extensive experiments with\ncurrent methods. EasyRL4Rec aims to facilitate the model development and\nexperimental process in the domain of RL-based RSs. The library is openly\naccessible at https://github.com/chongminggao/EasyRL4Rec.\n","authors":["Yuanqing Yu","Chongming Gao","Jiawei Chen","Heng Tang","Yuefeng Sun","Qian Chen","Weizhi Ma","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.15164v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.16979v3","updated":"2024-02-23T07:11:44Z","published":"2024-01-30T13:04:20Z","title":"Re3val: Reinforced and Reranked Generative Retrieval","summary":"  Generative retrieval models encode pointers to information in a corpus as an\nindex within the model's parameters. These models serve as part of a larger\npipeline, where retrieved information conditions generation for\nknowledge-intensive NLP tasks. However, we identify two limitations: the\ngenerative retrieval does not account for contextual information. Secondly, the\nretrieval can't be tuned for the downstream readers as decoding the page title\nis a non-differentiable operation. This paper introduces Re3val, trained with\ngenerative reranking and reinforcement learning using limited data. Re3val\nleverages context acquired via Dense Passage Retrieval to rerank the retrieved\npage titles and utilizes REINFORCE to maximize rewards generated by constrained\ndecoding. Additionally, we generate questions from our pre-training dataset to\nmitigate epistemic uncertainty and bridge the domain gap between the\npre-training and fine-tuning datasets. Subsequently, we extract and rerank\ncontexts from the KILT database using the rerank page titles. Upon grounding\nthe top five reranked contexts, Re3val demonstrates the Top 1 KILT scores\ncompared to all other generative retrieval models across five KILT datasets.\n","authors":["EuiYul Song","Sangryul Kim","Haeju Lee","Joonkee Kim","James Thorne"],"pdf_url":"https://arxiv.org/pdf/2401.16979v3.pdf","comment":"17 pages, 4 figures, Findings of the Association for Computational\n  Linguistics: EACL 2023"},{"id":"http://arxiv.org/abs/2401.16509v2","updated":"2024-02-23T06:24:34Z","published":"2024-01-29T19:26:41Z","title":"Dissecting users' needs for search result explanations","summary":"  There is a growing demand for transparency in search engines to understand\nhow search results are curated and to enhance users' trust. Prior research has\nintroduced search result explanations with a focus on how to explain, assuming\nexplanations are beneficial. Our study takes a step back to examine if search\nexplanations are needed and when they are likely to provide benefits.\nAdditionally, we summarize key characteristics of helpful explanations and\nshare users' perspectives on explanation features provided by Google and Bing.\nInterviews with non-technical individuals reveal that users do not always seek\nor understand search explanations and mostly desire them for complex and\ncritical tasks. They find Google's search explanations too obvious but\nappreciate the ability to contest search results. Based on our findings, we\noffer design recommendations for search engines and explanations to help users\nbetter evaluate search results and enhance their search experience.\n","authors":["Prerna Juneja","Wenjuan Zhang","Alison Marie Smith-Renner","Hemank Lamba","Joel Tetreault","Alex Jaimes"],"pdf_url":"https://arxiv.org/pdf/2401.16509v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15102v1","updated":"2024-02-23T05:20:23Z","published":"2024-02-23T05:20:23Z","title":"Trajectory-wise Iterative Reinforcement Learning Framework for\n  Auto-bidding","summary":"  In online advertising, advertisers participate in ad auctions to acquire ad\nopportunities, often by utilizing auto-bidding tools provided by demand-side\nplatforms (DSPs). The current auto-bidding algorithms typically employ\nreinforcement learning (RL). However, due to safety concerns, most RL-based\nauto-bidding policies are trained in simulation, leading to a performance\ndegradation when deployed in online environments. To narrow this gap, we can\ndeploy multiple auto-bidding agents in parallel to collect a large interaction\ndataset. Offline RL algorithms can then be utilized to train a new policy. The\ntrained policy can subsequently be deployed for further data collection,\nresulting in an iterative training framework, which we refer to as iterative\noffline RL. In this work, we identify the performance bottleneck of this\niterative offline RL framework, which originates from the ineffective\nexploration and exploitation caused by the inherent conservatism of offline RL\nalgorithms. To overcome this bottleneck, we propose Trajectory-wise Exploration\nand Exploitation (TEE), which introduces a novel data collecting and data\nutilization method for iterative offline RL from a trajectory perspective.\nFurthermore, to ensure the safety of online exploration while preserving the\ndataset quality for TEE, we propose Safe Exploration by Adaptive Action\nSelection (SEAS). Both offline experiments and real-world experiments on\nAlibaba display advertising platform demonstrate the effectiveness of our\nproposed method.\n","authors":["Haoming Li","Yusen Huo","Shuai Dou","Zhenzhe Zheng","Zhilin Zhang","Chuan Yu","Jian Xu","Fan Wu"],"pdf_url":"https://arxiv.org/pdf/2402.15102v1.pdf","comment":"Accepted by The Web Conference 2024"},{"id":"http://arxiv.org/abs/2402.15059v1","updated":"2024-02-23T02:21:24Z","published":"2024-02-23T02:21:24Z","title":"ColBERT-XM: A Modular Multi-Vector Representation Model for Zero-Shot\n  Multilingual Information Retrieval","summary":"  State-of-the-art neural retrievers predominantly focus on high-resource\nlanguages like English, which impedes their adoption in retrieval scenarios\ninvolving other languages. Current approaches circumvent the lack of\nhigh-quality labeled data in non-English languages by leveraging multilingual\npretrained language models capable of cross-lingual transfer. However, these\nmodels require substantial task-specific fine-tuning across multiple languages,\noften perform poorly in languages with minimal representation in the\npretraining corpus, and struggle to incorporate new languages after the\npretraining phase. In this work, we present a novel modular dense retrieval\nmodel that learns from the rich data of a single high-resource language and\neffectively zero-shot transfers to a wide array of languages, thereby\neliminating the need for language-specific labeled data. Our model, ColBERT-XM,\ndemonstrates competitive performance against existing state-of-the-art\nmultilingual retrievers trained on more extensive datasets in various\nlanguages. Further analysis reveals that our modular approach is highly\ndata-efficient, effectively adapts to out-of-distribution data, and\nsignificantly reduces energy consumption and carbon emissions. By demonstrating\nits proficiency in zero-shot scenarios, ColBERT-XM marks a shift towards more\nsustainable and inclusive retrieval systems, enabling effective information\naccessibility in numerous languages. We publicly release our code and models\nfor the community.\n","authors":["Antoine Louis","Vageesh Saxena","Gijs van Dijck","Gerasimos Spanakis"],"pdf_url":"https://arxiv.org/pdf/2402.15059v1.pdf","comment":"Under review. Code is available at\n  https://github.com/ant-louis/xm-retrievers"},{"id":"http://arxiv.org/abs/2402.13787v2","updated":"2024-02-23T00:13:51Z","published":"2024-02-21T13:14:45Z","title":"Fairness Rising from the Ranks: HITS and PageRank on Homophilic Networks","summary":"  In this paper, we investigate the conditions under which link analysis\nalgorithms prevent minority groups from reaching high ranking slots. We find\nthat the most common link-based algorithms using centrality metrics, such as\nPageRank and HITS, can reproduce and even amplify bias against minority groups\nin networks. Yet, their behavior differs: one one hand, we empirically show\nthat PageRank mirrors the degree distribution for most of the ranking positions\nand it can equalize representation of minorities among the top ranked nodes; on\nthe other hand, we find that HITS amplifies pre-existing bias in homophilic\nnetworks through a novel theoretical analysis, supported by empirical results.\nWe find the root cause of bias amplification in HITS to be the level of\nhomophily present in the network, modeled through an evolving network model\nwith two communities. We illustrate our theoretical analysis on both synthetic\nand real datasets and we present directions for future work.\n","authors":["Ana-Andreea Stoica","Nelly Litvak","Augustin Chaintreau"],"pdf_url":"https://arxiv.org/pdf/2402.13787v2.pdf","comment":"Accepted for publication in Proceedings of The Web Conference, 2024"}],"Multimedia":[{"id":"http://arxiv.org/abs/2402.15300v1","updated":"2024-02-23T12:57:16Z","published":"2024-02-23T12:57:16Z","title":"Seeing is Believing: Mitigating Hallucination in Large Vision-Language\n  Models via CLIP-Guided Decoding","summary":"  Large Vision-Language Models (LVLMs) are susceptible to object\nhallucinations, an issue in which their generated text contains non-existent\nobjects, greatly limiting their reliability and practicality. Current\napproaches often rely on the model's token likelihoods or other internal\ninformation, instruction tuning on additional datasets, or incorporating\ncomplex external tools. We first perform empirical analysis on sentence-level\nLVLM hallucination, finding that CLIP similarity to the image acts as a\nstronger and more robust indicator of hallucination compared to token\nlikelihoods. Motivated by this, we introduce our CLIP-Guided Decoding (CGD)\napproach, a straightforward but effective training-free approach to reduce\nobject hallucination at decoding time. CGD uses CLIP to guide the model's\ndecoding process by enhancing visual grounding of generated text with the\nimage. Experiments demonstrate that CGD effectively mitigates object\nhallucination across multiple LVLM families while preserving the utility of\ntext generation.\n","authors":["Ailin Deng","Zhirui Chen","Bryan Hooi"],"pdf_url":"https://arxiv.org/pdf/2402.15300v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.03870v2","updated":"2024-02-23T06:20:19Z","published":"2023-01-30T22:20:24Z","title":"DanceAnyWay: Synthesizing Beat-Guided 3D Dances with Randomized Temporal\n  Contrastive Learning","summary":"  We present DanceAnyWay, a generative learning method to synthesize\nbeat-guided dances of 3D human characters synchronized with music. Our method\nlearns to disentangle the dance movements at the beat frames from the dance\nmovements at all the remaining frames by operating at two hierarchical levels.\nAt the coarser \"beat\" level, it encodes the rhythm, pitch, and melody\ninformation of the input music via dedicated feature representations only at\nthe beat frames. It leverages them to synthesize the beat poses of the target\ndances using a sequence-to-sequence learning framework. At the finer\n\"repletion\" level, our method encodes similar rhythm, pitch, and melody\ninformation from all the frames of the input music via dedicated feature\nrepresentations. It generates the full dance sequences by combining the\nsynthesized beat and repletion poses and enforcing plausibility through an\nadversarial learning framework. Our training paradigm also enforces\nfine-grained diversity in the synthesized dances through a randomized temporal\ncontrastive loss, which ensures different segments of the dance sequences have\ndifferent movements and avoids motion freezing or collapsing to repetitive\nmovements. We evaluate the performance of our approach through extensive\nexperiments on the benchmark AIST++ dataset and observe improvements of about\n7%-12% in motion quality metrics and 1.5%-4% in motion diversity metrics over\nthe current baselines, respectively. We also conducted a user study to evaluate\nthe visual quality of our synthesized dances. We note that, on average, the\nsamples generated by our method were about 9-48% more preferred by the\nparticipants and had a 4-27% better five-point Likert-scale score over the best\navailable current baseline in terms of motion quality and synchronization. Our\nsource code and project page are available at\nhttps://github.com/aneeshbhattacharya/DanceAnyWay.\n","authors":["Aneesh Bhattacharya","Manas Paranjape","Uttaran Bhattacharya","Aniket Bera"],"pdf_url":"https://arxiv.org/pdf/2303.03870v2.pdf","comment":"11 pages, 7 figures, 3 tables. To appear as part of the proceedings\n  of the 38th Annual AAAI Conference on Artificial Intelligence, 2024"},{"id":"http://arxiv.org/abs/2402.15096v1","updated":"2024-02-23T05:09:35Z","published":"2024-02-23T05:09:35Z","title":"Multimodal Transformer With a Low-Computational-Cost Guarantee","summary":"  Transformer-based models have significantly improved performance across a\nrange of multimodal understanding tasks, such as visual question answering and\naction recognition. However, multimodal Transformers significantly suffer from\na quadratic complexity of the multi-head attention with the input sequence\nlength, especially as the number of modalities increases. To address this, we\nintroduce Low-Cost Multimodal Transformer (LoCoMT), a novel multimodal\nattention mechanism that aims to reduce computational cost during training and\ninference with minimal performance loss. Specifically, by assigning different\nmultimodal attention patterns to each attention head, LoCoMT can flexibly\ncontrol multimodal signals and theoretically ensures a reduced computational\ncost compared to existing multimodal Transformer variants. Experimental results\non two multimodal datasets, namely Audioset and MedVidCL demonstrate that\nLoCoMT not only reduces GFLOPs but also matches or even outperforms established\nmodels.\n","authors":["Sungjin Park","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2402.15096v1.pdf","comment":"Accepted to ICASSP 2024 (5 pages)"}]},"2024-02-22T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2402.15013v1","updated":"2024-02-22T23:12:20Z","published":"2024-02-22T23:12:20Z","title":"Filter Bubble or Homogenization? Disentangling the Long-Term Effects of\n  Recommendations on User Consumption Patterns","summary":"  Recommendation algorithms play a pivotal role in shaping our media choices,\nwhich makes it crucial to comprehend their long-term impact on user behavior.\nThese algorithms are often linked to two critical outcomes: homogenization,\nwherein users consume similar content despite disparate underlying preferences,\nand the filter bubble effect, wherein individuals with differing preferences\nonly consume content aligned with their preferences (without much overlap with\nother users). Prior research assumes a trade-off between homogenization and\nfilter bubble effects and then shows that personalized recommendations mitigate\nfilter bubbles by fostering homogenization. However, because of this assumption\nof a tradeoff between these two effects, prior work cannot develop a more\nnuanced view of how recommendation systems may independently impact\nhomogenization and filter bubble effects. We develop a more refined definition\nof homogenization and the filter bubble effect by decomposing them into two key\nmetrics: how different the average consumption is between users (inter-user\ndiversity) and how varied an individual's consumption is (intra-user\ndiversity). We then use a novel agent-based simulation framework that enables a\nholistic view of the impact of recommendation systems on homogenization and\nfilter bubble effects. Our simulations show that traditional recommendation\nalgorithms (based on past behavior) mainly reduce filter bubbles by affecting\ninter-user diversity without significantly impacting intra-user diversity.\nBuilding on these findings, we introduce two new recommendation algorithms that\ntake a more nuanced approach by accounting for both types of diversity.\n","authors":["Md Sanzeed Anwar","Grant Schoenebeck","Paramveer S. Dhillon"],"pdf_url":"https://arxiv.org/pdf/2402.15013v1.pdf","comment":"This paper was accepted at the ACM Web Conference 2024 (WWW '24)"},{"id":"http://arxiv.org/abs/2402.14802v1","updated":"2024-02-22T18:56:31Z","published":"2024-02-22T18:56:31Z","title":"Link Prediction under Heterophily: A Physics-Inspired Graph Neural\n  Network Approach","summary":"  In the past years, Graph Neural Networks (GNNs) have become the `de facto'\nstandard in various deep learning domains, thanks to their flexibility in\nmodeling real-world phenomena represented as graphs. However, the\nmessage-passing mechanism of GNNs faces challenges in learnability and\nexpressivity, hindering high performance on heterophilic graphs, where adjacent\nnodes frequently have different labels. Most existing solutions addressing\nthese challenges are primarily confined to specific benchmarks focused on node\nclassification tasks. This narrow focus restricts the potential impact that\nlink prediction under heterophily could offer in several applications,\nincluding recommender systems. For example, in social networks, two users may\nbe connected for some latent reason, making it challenging to predict such\nconnections in advance. Physics-Inspired GNNs such as GRAFF provided a\nsignificant contribution to enhance node classification performance under\nheterophily, thanks to the adoption of physics biases in the message-passing.\nDrawing inspiration from these findings, we advocate that the methodology\nemployed by GRAFF can improve link prediction performance as well. To further\nexplore this hypothesis, we introduce GRAFF-LP, an extension of GRAFF to link\nprediction. We evaluate its efficacy within a recent collection of heterophilic\ngraphs, establishing a new benchmark for link prediction under heterophily. Our\napproach surpasses previous methods, in most of the datasets, showcasing a\nstrong flexibility in different contexts, and achieving relative AUROC\nimprovements of up to 26.7%.\n","authors":["Andrea Giuseppe Di Francesco","Francesco Caso","Maria Sofia Bucarelli","Fabrizio Silvestri"],"pdf_url":"https://arxiv.org/pdf/2402.14802v1.pdf","comment":"7 pages, 1 figure"},{"id":"http://arxiv.org/abs/2401.01330v2","updated":"2024-02-22T17:29:03Z","published":"2024-01-02T18:40:03Z","title":"TREC iKAT 2023: The Interactive Knowledge Assistance Track Overview","summary":"  Conversational Information Seeking has evolved rapidly in the last few years\nwith the development of Large Language Models providing the basis for\ninterpreting and responding in a naturalistic manner to user requests. iKAT\nemphasizes the creation and research of conversational search agents that adapt\nresponses based on the user's prior interactions and present context. This\nmeans that the same question might yield varied answers, contingent on the\nuser's profile and preferences. The challenge lies in enabling Conversational\nSearch Agents (CSA) to incorporate personalized context to effectively guide\nusers through the relevant information to them. iKAT's first year attracted\nseven teams and a total of 24 runs. Most of the runs leveraged Large Language\nModels (LLMs) in their pipelines, with a few focusing on a\ngenerate-then-retrieve approach.\n","authors":["Mohammad Aliannejadi","Zahra Abbasiantaeb","Shubham Chatterjee","Jeffery Dalton","Leif Azzopardi"],"pdf_url":"https://arxiv.org/pdf/2401.01330v2.pdf","comment":"TREC iKAT 2023 Overview Paper"},{"id":"http://arxiv.org/abs/2402.14710v1","updated":"2024-02-22T17:11:38Z","published":"2024-02-22T17:11:38Z","title":"IEPile: Unearthing Large-Scale Schema-Based Information Extraction\n  Corpus","summary":"  Large Language Models (LLMs) demonstrate remarkable potential across various\ndomains; however, they exhibit a significant performance gap in Information\nExtraction (IE). Note that high-quality instruction data is the vital key for\nenhancing the specific capabilities of LLMs, while current IE datasets tend to\nbe small in scale, fragmented, and lack standardized schema. To this end, we\nintroduce IEPile, a comprehensive bilingual (English and Chinese) IE\ninstruction corpus, which contains approximately 0.32B tokens. We construct\nIEPile by collecting and cleaning 33 existing IE datasets, and introduce\nschema-based instruction generation to unearth a large-scale corpus.\nExperimental results on LLaMA and Baichuan demonstrate that using IEPile can\nenhance the performance of LLMs for IE, especially the zero-shot\ngeneralization. We open-source the resource and pre-trained models, hoping to\nprovide valuable support to the NLP community.\n","authors":["Honghao Gui","Hongbin Ye","Lin Yuan","Ningyu Zhang","Mengshu Sun","Lei Liang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2402.14710v1.pdf","comment":"Ongoing work; 18 pages; Github: https://github.com/zjunlp/IEPile"},{"id":"http://arxiv.org/abs/2402.14622v1","updated":"2024-02-22T15:10:45Z","published":"2024-02-22T15:10:45Z","title":"From Keywords to Structured Summaries: Streamlining Scholarly Knowledge\n  Access","summary":"  This short paper highlights the growing importance of information retrieval\n(IR) engines in the scientific community, addressing the inefficiency of\ntraditional keyword-based search engines due to the rising volume of\npublications. The proposed solution involves structured records, underpinning\nadvanced information technology (IT) tools, including visualization dashboards,\nto revolutionize how researchers access and filter articles, replacing the\ntraditional text-heavy approach. This vision is exemplified through a proof of\nconcept centered on the ``reproductive number estimate of infectious diseases''\nresearch theme, using a fine-tuned large language model (LLM) to automate the\ncreation of structured records to populate a backend database that now goes\nbeyond keywords. The result is a next-generation IR method accessible at\nhttps://orkg.org/usecases/r0-estimates.\n","authors":["Mahsa Shamsabadi","Jennifer D'Souza"],"pdf_url":"https://arxiv.org/pdf/2402.14622v1.pdf","comment":"6 pages, 1 figure"},{"id":"http://arxiv.org/abs/2401.06311v2","updated":"2024-02-22T14:50:49Z","published":"2024-01-12T00:48:35Z","title":"MuGI: Enhancing Information Retrieval through Multi-Text Generation\n  Integration with Large Language Models","summary":"  Large Language Models (LLMs) have emerged as a pivotal force in language\ntechnology. Their robust reasoning capabilities and expansive knowledge\nrepositories have enabled exceptional zero-shot generalization abilities across\nvarious facets of the natural language processing field, including information\nretrieval (IR). In this paper, we conduct an in-depth investigation into the\nutility of documents generated by LLMs for IR. We introduce a simple yet\neffective framework, Multi-Text Generation Integration (MuGI), to augment\nexisting IR methodologies. Specifically, we prompt LLMs to generate multiple\npseudo references and integrate with query for retrieval. The training-free\nMuGI model eclipses existing query expansion strategies, setting a new standard\nin sparse retrieval. It outstrips supervised counterparts like ANCE and DPR,\nachieving a notable over 18% enhancement in BM25 on the TREC DL dataset and a\n7.5% increase on BEIR. Through MuGI, we have forged a rapid and high-fidelity\nre-ranking pipeline. This allows a relatively small 110M parameter retriever to\nsurpass the performance of larger 3B models in in-domain evaluations, while\nalso bridging the gap in out-of-distribution situations. We release our code\nand all generated references at https://github.com/lezhang7/Retrieval_MuGI.\n","authors":["Le Zhang","Qian Yang","Yihong Wu"],"pdf_url":"https://arxiv.org/pdf/2401.06311v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.08469v2","updated":"2024-02-22T13:26:31Z","published":"2023-09-15T15:19:53Z","title":"Silver Retriever: Advancing Neural Passage Retrieval for Polish Question\n  Answering","summary":"  Modern open-domain question answering systems often rely on accurate and\nefficient retrieval components to find passages containing the facts necessary\nto answer the question. Recently, neural retrievers have gained popularity over\nlexical alternatives due to their superior performance. However, most of the\nwork concerns popular languages such as English or Chinese. For others, such as\nPolish, few models are available. In this work, we present Silver Retriever, a\nneural retriever for Polish trained on a diverse collection of manually or\nweakly labeled datasets. Silver Retriever achieves much better results than\nother Polish models and is competitive with larger multilingual models.\nTogether with the model, we open-source five new passage retrieval datasets.\n","authors":["Piotr Rybak","Maciej Ogrodniczuk"],"pdf_url":"https://arxiv.org/pdf/2309.08469v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14473v1","updated":"2024-02-22T12:03:21Z","published":"2024-02-22T12:03:21Z","title":"Personalized Behavior-Aware Transformer for Multi-Behavior Sequential\n  Recommendation","summary":"  Sequential Recommendation (SR) captures users' dynamic preferences by\nmodeling how users transit among items. However, SR models that utilize only\nsingle type of behavior interaction data encounter performance degradation when\nthe sequences are short. To tackle this problem, we focus on Multi-Behavior\nSequential Recommendation (MBSR) in this paper, which aims to leverage\ntime-evolving heterogeneous behavioral dependencies for better exploring users'\npotential intents on the target behavior. Solving MBSR is challenging. On the\none hand, users exhibit diverse multi-behavior patterns due to personal\ncharacteristics. On the other hand, there exists comprehensive co-influence\nbetween behavior correlations and item collaborations, the intensity of which\nis deeply affected by temporal factors. To tackle these challenges, we propose\na Personalized Behavior-Aware Transformer framework (PBAT) for MBSR problem,\nwhich models personalized patterns and multifaceted sequential collaborations\nin a novel way to boost recommendation performance. First, PBAT develops a\npersonalized behavior pattern generator in the representation layer, which\nextracts dynamic and discriminative behavior patterns for sequential learning.\nSecond, PBAT reforms the self-attention layer with a behavior-aware\ncollaboration extractor, which introduces a fused behavior-aware attention\nmechanism for incorporating both behavioral and temporal impacts into\ncollaborative transitions. We conduct experiments on three benchmark datasets\nand the results demonstrate the effectiveness and interpretability of our\nframework. Our implementation code is released at\nhttps://github.com/TiliaceaeSU/PBAT.\n","authors":["Jiajie Su","Chaochao Chen","Zibin Lin","Xi Li","Weiming Liu","Xiaolin Zheng"],"pdf_url":"https://arxiv.org/pdf/2402.14473v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12997v2","updated":"2024-02-22T11:00:16Z","published":"2024-02-20T13:25:16Z","title":"Towards Trustworthy Reranking: A Simple yet Effective Abstention\n  Mechanism","summary":"  Neural Information Retrieval (NIR) has significantly improved upon\nheuristic-based IR systems. Yet, failures remain frequent, the models used\noften being unable to retrieve documents relevant to the user's query. We\naddress this challenge by proposing a lightweight abstention mechanism tailored\nfor real-world constraints, with particular emphasis placed on the reranking\nphase. We introduce a protocol for evaluating abstention strategies in a\nblack-box scenario, demonstrating their efficacy, and propose a simple yet\neffective data-driven mechanism. We provide open-source code for experiment\nreplication and abstention implementation, fostering wider adoption and\napplication in diverse contexts.\n","authors":["Hippolyte Gisserot-Boukhlef","Manuel Faysse","Emmanuel Malherbe","Céline Hudelot","Pierre Colombo"],"pdf_url":"https://arxiv.org/pdf/2402.12997v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14440v1","updated":"2024-02-22T10:39:03Z","published":"2024-02-22T10:39:03Z","title":"Recommender for Its Purpose: Repeat and Exploration in Food Delivery\n  Recommendations","summary":"  Recommender systems have been widely used for various scenarios, such as\ne-commerce, news, and music, providing online contents to help and enrich\nusers' daily life. Different scenarios hold distinct and unique\ncharacteristics, calling for domain-specific investigations and corresponding\ndesigned recommender systems. Therefore, in this paper, we focus on food\ndelivery recommendations to unveil unique features in this domain, where users\norder food online and enjoy their meals shortly after delivery. We first\nconduct an in-depth analysis on food delivery datasets. The analysis shows that\nrepeat orders are prevalent for both users and stores, and situations'\ndifferently influence repeat and exploration consumption in the food delivery\nrecommender systems. Moreover, we revisit the ability of existing\nsituation-aware methods for repeat and exploration recommendations\nrespectively, and find them unable to effectively solve both tasks\nsimultaneously. Based on the analysis and experiments, we have designed two\nseparate recommendation models -- ReRec for repeat orders and ExpRec for\nexploration orders; both are simple in their design and computation. We conduct\nexperiments on three real-world food delivery datasets, and our proposed models\noutperform various types of baselines on repeat, exploration, and combined\nrecommendation tasks. This paper emphasizes the importance of dedicated\nanalyses and methods for domain-specific characteristics for the recommender\nsystem studies.\n","authors":["Jiayu Li","Aixin Sun","Weizhi Ma","Peijie Sun","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.14440v1.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2305.13168v2","updated":"2024-02-22T10:15:25Z","published":"2023-05-22T15:56:44Z","title":"LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities\n  and Future Opportunities","summary":"  This paper presents an exhaustive quantitative and qualitative evaluation of\nLarge Language Models (LLMs) for Knowledge Graph (KG) construction and\nreasoning. We engage in experiments across eight diverse datasets, focusing on\nfour representative tasks encompassing entity and relation extraction, event\nextraction, link prediction, and question-answering, thereby thoroughly\nexploring LLMs' performance in the domain of construction and inference.\nEmpirically, our findings suggest that LLMs, represented by GPT-4, are more\nsuited as inference assistants rather than few-shot information extractors.\nSpecifically, while GPT-4 exhibits good performance in tasks related to KG\nconstruction, it excels further in reasoning tasks, surpassing fine-tuned\nmodels in certain cases. Moreover, our investigation extends to the potential\ngeneralization ability of LLMs for information extraction, leading to the\nproposition of a Virtual Knowledge Extraction task and the development of the\ncorresponding VINE dataset. Based on these empirical findings, we further\npropose AutoKG, a multi-agent-based approach employing LLMs and external\nsources for KG construction and reasoning. We anticipate that this research can\nprovide invaluable insights for future undertakings in the field of knowledge\ngraphs. The code and datasets are in https://github.com/zjunlp/AutoKG.\n","authors":["Yuqi Zhu","Xiaohan Wang","Jing Chen","Shuofei Qiao","Yixin Ou","Yunzhi Yao","Shumin Deng","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.13168v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2402.14409v1","updated":"2024-02-22T09:51:08Z","published":"2024-02-22T09:51:08Z","title":"Tug-of-War Between Knowledge: Exploring and Resolving Knowledge\n  Conflicts in Retrieval-Augmented Language Models","summary":"  Retrieval-augmented language models (RALMs) have demonstrated significant\npotential in refining and expanding their internal memory by retrieving\nevidence from external sources. However, RALMs will inevitably encounter\nknowledge conflicts when integrating their internal memory with external\nsources. Knowledge conflicts can ensnare RALMs in a tug-of-war between\nknowledge, limiting their practical applicability. In this paper, we focus on\nexploring and resolving knowledge conflicts in RALMs. First, we present an\nevaluation framework for assessing knowledge conflicts across various\ndimensions. Then, we investigate the behavior and preference of RALMs from the\nfollowing two perspectives: (1) Conflicts between internal memory and external\nsources: We find that stronger RALMs emerge with the Dunning-Kruger effect,\npersistently favoring their faulty internal memory even when correct evidence\nis provided. Besides, RALMs exhibit an availability bias towards common\nknowledge; (2) Conflicts between truthful, irrelevant and misleading evidence:\nWe reveal that RALMs follow the principle of majority rule, leaning towards\nplacing trust in evidence that appears more frequently. Moreover, we find that\nRALMs exhibit confirmation bias, and are more willing to choose evidence that\nis consistent with their internal memory. To solve the challenge of knowledge\nconflicts, we propose a method called Conflict-Disentangle Contrastive Decoding\n(CD2) to better calibrate the model's confidence. Experimental results\ndemonstrate that our CD2 can effectively resolve knowledge conflicts in RALMs.\n","authors":["Zhuoran Jin","Pengfei Cao","Yubo Chen","Kang Liu","Xiaojian Jiang","Jiexin Xu","Qiuxia Li","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2402.14409v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2402.14399v1","updated":"2024-02-22T09:32:34Z","published":"2024-02-22T09:32:34Z","title":"Ensure Timeliness and Accuracy: A Novel Sliding Window Data Stream\n  Paradigm for Live Streaming Recommendation","summary":"  Live streaming recommender system is specifically designed to recommend\nreal-time live streaming of interest to users. Due to the dynamic changes of\nlive content, improving the timeliness of the live streaming recommender system\nis a critical problem. Intuitively, the timeliness of the data determines the\nupper bound of the timeliness that models can learn. However, none of the\nprevious works addresses the timeliness problem of the live streaming\nrecommender system from the perspective of data stream design. Employing the\nconventional fixed window data stream paradigm introduces a trade-off dilemma\nbetween labeling accuracy and timeliness. In this paper, we propose a new data\nstream design paradigm, dubbed Sliver, that addresses the timeliness and\naccuracy problem of labels by reducing the window size and implementing a\nsliding window correspondingly. Meanwhile, we propose a time-sensitive re-reco\nstrategy reducing the latency between request and impression to improve the\ntimeliness of the recommendation service and features by periodically\nrequesting the recommendation service. To demonstrate the effectiveness of our\napproach, we conduct offline experiments on a multi-task live streaming dataset\nwith labeling timestamps collected from the Kuaishou live streaming platform.\nExperimental results demonstrate that Sliver outperforms two fixed-window data\nstreams with varying window sizes across all targets in four typical multi-task\nrecommendation models. Furthermore, we deployed Sliver on the Kuaishou live\nstreaming platform. Results of the online A/B test show a significant\nimprovement in click-through rate (CTR), and new follow number (NFN), further\nvalidating the effectiveness of Sliver.\n","authors":["Fengqi Liang","Baigong Zheng","Liqin Zhao","Guorui Zhou","Qian Wang","Yanan Niu"],"pdf_url":"https://arxiv.org/pdf/2402.14399v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14369v1","updated":"2024-02-22T08:16:43Z","published":"2024-02-22T08:16:43Z","title":"Scalable and Provably Fair Exposure Control for Large-Scale Recommender\n  Systems","summary":"  Typical recommendation and ranking methods aim to optimize the satisfaction\nof users, but they are often oblivious to their impact on the items (e.g.,\nproducts, jobs, news, video) and their providers. However, there has been a\ngrowing understanding that the latter is crucial to consider for a wide range\nof applications, since it determines the utility of those being recommended.\nPrior approaches to fairness-aware recommendation optimize a regularized\nobjective to balance user satisfaction and item fairness based on some notion\nsuch as exposure fairness. These existing methods have been shown to be\neffective in controlling fairness, however, most of them are computationally\ninefficient, limiting their applications to only unrealistically small-scale\nsituations. This indeed implies that the literature does not yet provide a\nsolution to enable a flexible control of exposure in the industry-scale\nrecommender systems where millions of users and items exist. To enable a\ncomputationally efficient exposure control even for such large-scale systems,\nthis work develops a scalable, fast, and fair method called\n\\emph{\\textbf{ex}posure-aware \\textbf{ADMM} (\\textbf{exADMM})}. exADMM is based\non implicit alternating least squares (iALS), a conventional scalable algorithm\nfor collaborative filtering, but optimizes a regularized objective to achieve a\nflexible control of accuracy-fairness tradeoff. A particular technical\nchallenge in developing exADMM is the fact that the fairness regularizer\ndestroys the separability of optimization subproblems for users and items,\nwhich is an essential property to ensure the scalability of iALS. Therefore, we\ndevelop a set of optimization tools to enable yet scalable fairness control\nwith provable convergence guarantees as a basis of our algorithm.\n","authors":["Riku Togashi","Kenshi Abe","Yuta Saito"],"pdf_url":"https://arxiv.org/pdf/2402.14369v1.pdf","comment":"accepted at WWW2024"},{"id":"http://arxiv.org/abs/2212.03533v2","updated":"2024-02-22T06:21:51Z","published":"2022-12-07T09:25:54Z","title":"Text Embeddings by Weakly-Supervised Contrastive Pre-training","summary":"  This paper presents E5, a family of state-of-the-art text embeddings that\ntransfer well to a wide range of tasks. The model is trained in a contrastive\nmanner with weak supervision signals from our curated large-scale text pair\ndataset (called CCPairs). E5 can be readily used as a general-purpose embedding\nmodel for any tasks requiring a single-vector representation of texts such as\nretrieval, clustering, and classification, achieving strong performance in both\nzero-shot and fine-tuned settings. We conduct extensive evaluations on 56\ndatasets from the BEIR and MTEB benchmarks. For zero-shot settings, E5 is the\nfirst model that outperforms the strong BM25 baseline on the BEIR retrieval\nbenchmark without using any labeled data. When fine-tuned, E5 obtains the best\nresults on the MTEB benchmark, beating existing embedding models with 40x more\nparameters.\n","authors":["Liang Wang","Nan Yang","Xiaolong Huang","Binxing Jiao","Linjun Yang","Daxin Jiang","Rangan Majumder","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2212.03533v2.pdf","comment":"17 pages, v2 fixes the SummEval numbers"},{"id":"http://arxiv.org/abs/2402.14305v1","updated":"2024-02-22T05:48:54Z","published":"2024-02-22T05:48:54Z","title":"Towards Efficient Pareto-optimal Utility-Fairness between Groups in\n  Repeated Rankings","summary":"  In this paper, we tackle the problem of computing a sequence of rankings with\nthe guarantee of the Pareto-optimal balance between (1) maximizing the utility\nof the consumers and (2) minimizing unfairness between producers of the items.\nSuch a multi-objective optimization problem is typically solved using a\ncombination of a scalarization method and linear programming on bi-stochastic\nmatrices, representing the distribution of possible rankings of items. However,\nthe above-mentioned approach relies on Birkhoff-von Neumann (BvN)\ndecomposition, of which the computational complexity is $\\mathcal{O}(n^5)$ with\n$n$ being the number of items, making it impractical for large-scale systems.\nTo address this drawback, we introduce a novel approach to the above problem by\nusing the Expohedron - a permutahedron whose points represent all achievable\nexposures of items. On the Expohedron, we profile the Pareto curve which\ncaptures the trade-off between group fairness and user utility by identifying a\nfinite number of Pareto optimal solutions. We further propose an efficient\nmethod by relaxing our optimization problem on the Expohedron's circumscribed\n$n$-sphere, which significantly improve the running time. Moreover, the\napproximate Pareto curve is asymptotically close to the real Pareto optimal\ncurve as the number of substantial solutions increases. Our methods are\napplicable with different ranking merits that are non-decreasing functions of\nitem relevance. The effectiveness of our methods are validated through\nexperiments on both synthetic and real-world datasets.\n","authors":["Phuong Dinh Mai","Duc-Trong Le","Tuan-Anh Hoang","Dung D. Le"],"pdf_url":"https://arxiv.org/pdf/2402.14305v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14301v1","updated":"2024-02-22T05:41:24Z","published":"2024-02-22T05:41:24Z","title":"GenSERP: Large Language Models for Whole Page Presentation","summary":"  The advent of large language models (LLMs) brings an opportunity to minimize\nthe effort in search engine result page (SERP) organization. In this paper, we\npropose GenSERP, a framework that leverages LLMs with vision in a few-shot\nsetting to dynamically organize intermediate search results, including\ngenerated chat answers, website snippets, multimedia data, knowledge panels\ninto a coherent SERP layout based on a user's query. Our approach has three\nmain stages: (1) An information gathering phase where the LLM continuously\norchestrates API tools to retrieve different types of items, and proposes\ncandidate layouts based on the retrieved items, until it's confident enough to\ngenerate the final result. (2) An answer generation phase where the LLM\npopulates the layouts with the retrieved content. In this phase, the LLM\nadaptively optimize the ranking of items and UX configurations of the SERP.\nConsequently, it assigns a location on the page to each item, along with the UX\ndisplay details. (3) A scoring phase where an LLM with vision scores all the\ngenerated SERPs based on how likely it can satisfy the user. It then send the\none with highest score to rendering. GenSERP features two generation paradigms.\nFirst, coarse-to-fine, which allow it to approach optimal layout in a more\nmanageable way, (2) beam search, which give it a better chance to hit the\noptimal solution compared to greedy decoding. Offline experimental results on\nreal-world data demonstrate how LLMs can contextually organize heterogeneous\nsearch results on-the-fly and provide a promising user experience.\n","authors":["Zhenning Zhang","Yunan Zhang","Suyu Ge","Guangwei Weng","Mridu Narang","Xia Song","Saurabh Tiwary"],"pdf_url":"https://arxiv.org/pdf/2402.14301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03032v2","updated":"2024-02-22T04:15:36Z","published":"2023-09-24T04:09:16Z","title":"Graph-enhanced Optimizers for Structure-aware Recommendation Embedding\n  Evolution","summary":"  Embedding plays a critical role in modern recommender systems because they\nare virtual representations of real-world entities and the foundation for\nsubsequent decision models. In this paper, we propose a novel embedding update\nmechanism, Structure-aware Embedding Evolution (SEvo for short), to encourage\nrelated nodes to evolve similarly at each step. Unlike GNN (Graph Neural\nNetwork) that typically serves as an intermediate part, SEvo is able to\ndirectly inject the graph structure information into embedding with negligible\ncomputational overhead in training. The convergence properties of SEvo as well\nas its possible variants are theoretically analyzed to justify the validity of\nthe designs. Moreover, SEvo can be seamlessly integrated into existing\noptimizers for state-of-the-art performance. In particular, SEvo-enhanced AdamW\nwith moment estimate correction demonstrates consistent improvements across a\nspectrum of models and datasets, suggesting a novel technical route to\neffectively utilize graph structure information beyond explicit GNN modules.\n","authors":["Cong Xu","Jun Wang","Jianyong Wang","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.03032v2.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2402.06216v2","updated":"2024-02-22T03:58:21Z","published":"2024-02-09T07:01:53Z","title":"Understanding the Role of Cross-Entropy Loss in Fairly Evaluating Large\n  Language Model-based Recommendation","summary":"  Large language models (LLMs) have gained much attention in the recommendation\ncommunity; some studies have observed that LLMs, fine-tuned by the\ncross-entropy loss with a full softmax, could achieve state-of-the-art\nperformance already. However, these claims are drawn from unobjective and\nunfair comparisons. In view of the substantial quantity of items in reality,\nconventional recommenders typically adopt a pointwise/pairwise loss function\ninstead for training. This substitute however causes severe performance\ndegradation, leading to under-estimation of conventional methods and\nover-confidence in the ranking capability of LLMs.\n  In this work, we theoretically justify the superiority of cross-entropy, and\nshowcase that it can be adequately replaced by some elementary approximations\nwith certain necessary modifications. The remarkable results across three\npublic datasets corroborate that even in a practical sense, existing LLM-based\nmethods are not as effective as claimed for next-item recommendation. We hope\nthat these theoretical understandings in conjunction with the empirical results\nwill facilitate an objective evaluation of LLM-based recommendation in the\nfuture.\n","authors":["Cong Xu","Zhangchi Zhu","Jun Wang","Jianyong Wang","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.06216v2.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2402.14230v1","updated":"2024-02-22T02:21:59Z","published":"2024-02-22T02:21:59Z","title":"MerRec: A Large-scale Multipurpose Mercari Dataset for\n  Consumer-to-Consumer Recommendation Systems","summary":"  In the evolving e-commerce field, recommendation systems crucially shape user\nexperience and engagement. The rise of Consumer-to-Consumer (C2C)\nrecommendation systems, noted for their flexibility and ease of access for\ncustomer vendors, marks a significant trend. However, the academic focus\nremains largely on Business-to-Consumer (B2C) models, leaving a gap filled by\nthe limited C2C recommendation datasets that lack in item attributes, user\ndiversity, and scale. The intricacy of C2C recommendation systems is further\naccentuated by the dual roles users assume as both sellers and buyers,\nintroducing a spectrum of less uniform and varied inputs. Addressing this, we\nintroduce MerRec, the first large-scale dataset specifically for C2C\nrecommendations, sourced from the Mercari e-commerce platform, covering\nmillions of users and products over 6 months in 2023. MerRec not only includes\nstandard features such as user_id, item_id, and session_id, but also unique\nelements like timestamped action types, product taxonomy, and textual product\nattributes, offering a comprehensive dataset for research. This dataset,\nextensively evaluated across six recommendation tasks, establishes a new\nbenchmark for the development of advanced recommendation algorithms in\nreal-world scenarios, bridging the gap between academia and industry and\npropelling the study of C2C recommendations.\n","authors":["Lichi Li","Zainul Abi Din","Zhen Tan","Sam London","Tianlong Chen","Ajay Daptardar"],"pdf_url":"https://arxiv.org/pdf/2402.14230v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.12515v2","updated":"2024-02-22T02:14:55Z","published":"2022-07-25T20:23:25Z","title":"A Survey on Trustworthy Recommender Systems","summary":"  Recommender systems (RS), serving at the forefront of Human-centered AI, are\nwidely deployed in almost every corner of the web and facilitate the human\ndecision-making process. However, despite their enormous capabilities and\npotential, RS may also lead to undesired effects on users, items, producers,\nplatforms, or even the society at large, such as compromised user trust due to\nnon-transparency, unfair treatment of different consumers, or producers,\nprivacy concerns due to extensive use of user's private data for\npersonalization, just to name a few. All of these create an urgent need for\nTrustworthy Recommender Systems (TRS) so as to mitigate or avoid such adverse\nimpacts and risks. In this survey, we will introduce techniques related to\ntrustworthy recommendation, including but not limited to explainable\nrecommendation, fairness in recommendation, privacy-aware recommendation,\nrobustness in recommendation, user-controllable recommendation, as well as the\nrelationship between these different perspectives in terms of trustworthy\nrecommendation. Through this survey, we hope to deliver readers with a\ncomprehensive view of the research area and raise attention to the community\nabout the importance, existing research achievements, and future research\ndirections on trustworthy recommendation.\n","authors":["Yingqiang Ge","Shuchang Liu","Zuohui Fu","Juntao Tan","Zelong Li","Shuyuan Xu","Yunqi Li","Yikun Xian","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2207.12515v2.pdf","comment":"Accepted by ACM Transactions on Recommender Systems (TORS)"}],"Multimedia":[{"id":"http://arxiv.org/abs/2402.14947v1","updated":"2024-02-22T20:04:10Z","published":"2024-02-22T20:04:10Z","title":"An Avalanche of Images on Telegram Preceded Russia's Full-Scale Invasion\n  of Ukraine","summary":"  Governments use propaganda, including through visual content -- or\nPolitically Salient Image Patterns (PSIP) -- on social media, to influence and\nmanipulate public opinion. In the present work, we collected Telegram\npost-history of from 989 Russian milbloggers to better understand the social\nand political narratives that circulated online in the months surrounding\nRussia's 2022 full-scale invasion of Ukraine. Overall, we found an 8,925%\nincrease (p<0.001) in the number of posts and a 5,352% increase (p<0.001) in\nthe number of images posted by these accounts in the two weeks prior to the\ninvasion. We also observed a similar increase in the number and intensity of\npolitically salient manipulated images that circulated on Telegram. Although\nthis paper does not evaluate malice or coordination in these activities, we do\nconclude with a call for further research into the role that manipulated visual\nmedia has in the lead-up to instability events and armed conflict.\n","authors":["William Theisen","Michael Yankoski","Kristina Hook","Ernesto Verdeja","Walter Scheirer","Tim Weninger"],"pdf_url":"https://arxiv.org/pdf/2402.14947v1.pdf","comment":"20 pages, 7 figures"},{"id":"http://arxiv.org/abs/2402.14326v1","updated":"2024-02-22T06:38:25Z","published":"2024-02-22T06:38:25Z","title":"Think before You Leap: Content-Aware Low-Cost Edge-Assisted Video\n  Semantic Segmentation","summary":"  Offloading computing to edge servers is a promising solution to support\ngrowing video understanding applications at resource-constrained IoT devices.\nRecent efforts have been made to enhance the scalability of such systems by\nreducing inference costs on edge servers. However, existing research is not\ndirectly applicable to pixel-level vision tasks such as video semantic\nsegmentation (VSS), partly due to the fluctuating VSS accuracy and segment\nbitrate caused by the dynamic video content. In response, we present Penance, a\nnew edge inference cost reduction framework. By exploiting softmax outputs of\nVSS models and the prediction mechanism of H.264/AVC codecs, Penance optimizes\nmodel selection and compression settings to minimize the inference cost while\nmeeting the required accuracy within the available bandwidth constraints. We\nimplement Penance in a commercial IoT device with only CPUs. Experimental\nresults show that Penance consumes a negligible 6.8% more computation resources\nthan the optimal strategy while satisfying accuracy and bandwidth constraints\nwith a low failure rate.\n","authors":["Mingxuan Yan","Yi Wang","Xuedou Xiao","Zhiqing Luo","Jianhua He","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2402.14326v1.pdf","comment":"Accepted by ACM Multimedia 2023"},{"id":"http://arxiv.org/abs/2307.09312v4","updated":"2024-02-22T06:17:39Z","published":"2023-07-18T14:57:12Z","title":"Multi-Modal Discussion Transformer: Integrating Text, Images and Graph\n  Transformers to Detect Hate Speech on Social Media","summary":"  We present the Multi-Modal Discussion Transformer (mDT), a novel methodfor\ndetecting hate speech in online social networks such as Reddit discussions. In\ncontrast to traditional comment-only methods, our approach to labelling a\ncomment as hate speech involves a holistic analysis of text and images grounded\nin the discussion context. This is done by leveraging graph transformers to\ncapture the contextual relationships in the discussion surrounding a comment\nand grounding the interwoven fusion layers that combine text and image\nembeddings instead of processing modalities separately. To evaluate our work,\nwe present a new dataset, HatefulDiscussions, comprising complete multi-modal\ndiscussions from multiple online communities on Reddit. We compare the\nperformance of our model to baselines that only process individual comments and\nconduct extensive ablation studies.\n","authors":["Liam Hebert","Gaurav Sahu","Yuxuan Guo","Nanda Kishore Sreenivas","Lukasz Golab","Robin Cohen"],"pdf_url":"https://arxiv.org/pdf/2307.09312v4.pdf","comment":"Accepted to AAAI 2024 (AI for Social Impact Track)"},{"id":"http://arxiv.org/abs/2305.06594v2","updated":"2024-02-22T05:58:36Z","published":"2023-05-11T06:26:41Z","title":"V2Meow: Meowing to the Visual Beat via Video-to-Music Generation","summary":"  Video-to-music generation demands both a temporally localized high-quality\nlistening experience and globally aligned video-acoustic signatures. While\nrecent music generation models excel at the former through advanced audio\ncodecs, the exploration of video-acoustic signatures has been confined to\nspecific visual scenarios. In contrast, our research confronts the challenge of\nlearning globally aligned signatures between video and music directly from\npaired music and videos, without explicitly modeling domain-specific rhythmic\nor semantic relationships. We propose V2Meow, a video-to-music generation\nsystem capable of producing high-quality music audio for a diverse range of\nvideo input types using a multi-stage autoregressive model. Trained on 5k hours\nof music audio clips paired with video frames mined from in-the-wild music\nvideos, V2Meow is competitive with previous domain-specific models when\nevaluated in a zero-shot manner. It synthesizes high-fidelity music audio\nwaveforms solely by conditioning on pre-trained general-purpose visual features\nextracted from video frames, with optional style control via text prompts.\nThrough both qualitative and quantitative evaluations, we demonstrate that our\nmodel outperforms various existing music generation systems in terms of\nvisual-audio correspondence and audio quality. Music samples are available at\ntinyurl.com/v2meow.\n","authors":["Kun Su","Judith Yue Li","Qingqing Huang","Dima Kuzmin","Joonseok Lee","Chris Donahue","Fei Sha","Aren Jansen","Yu Wang","Mauro Verzetti","Timo I. Denk"],"pdf_url":"https://arxiv.org/pdf/2305.06594v2.pdf","comment":"accepted at AAAI 2024, music samples available at\n  https://tinyurl.com/v2meow"},{"id":"http://arxiv.org/abs/2402.15444v1","updated":"2024-02-22T05:48:03Z","published":"2024-02-22T05:48:03Z","title":"Unleashing the Power of Imbalanced Modality Information for Multi-modal\n  Knowledge Graph Completion","summary":"  Multi-modal knowledge graph completion (MMKGC) aims to predict the missing\ntriples in the multi-modal knowledge graphs by incorporating structural,\nvisual, and textual information of entities into the discriminant models. The\ninformation from different modalities will work together to measure the triple\nplausibility. Existing MMKGC methods overlook the imbalance problem of modality\ninformation among entities, resulting in inadequate modal fusion and\ninefficient utilization of the raw modality information. To address the\nmentioned problems, we propose Adaptive Multi-modal Fusion and Modality\nAdversarial Training (AdaMF-MAT) to unleash the power of imbalanced modality\ninformation for MMKGC. AdaMF-MAT achieves multi-modal fusion with adaptive\nmodality weights and further generates adversarial samples by\nmodality-adversarial training to enhance the imbalanced modality information.\nOur approach is a co-design of the MMKGC model and training strategy which can\noutperform 19 recent MMKGC methods and achieve new state-of-the-art results on\nthree public MMKGC benchmarks. Our code and data have been released at\nhttps://github.com/zjukg/AdaMF-MAT.\n","authors":["Yichi Zhang","Zhuo Chen","Lei Liang","Huajun Chen","Wen Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.15444v1.pdf","comment":"Accepted by LREC-COLING 2024"}]},"2024-02-21T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2208.08011v2","updated":"2024-02-21T23:25:33Z","published":"2022-08-17T01:25:26Z","title":"Re4: Learning to Re-contrast, Re-attend, Re-construct for Multi-interest\n  Recommendation","summary":"  Effectively representing users lie at the core of modern recommender systems.\nSince users' interests naturally exhibit multiple aspects, it is of increasing\ninterest to develop multi-interest frameworks for recommendation, rather than\nrepresent each user with an overall embedding. Despite their effectiveness,\nexisting methods solely exploit the encoder (the forward flow) to represent\nmultiple aspects of interests. However, without explicit regularization, the\ninterest embeddings may not be distinct from each other nor semantically\nreflect representative historical items. Towards this end, we propose the Re4\nframework, which leverages the backward flow to reexamine each interest\nembedding. Specifically, Re4 encapsulates three backward flows, i.e., 1)\nRe-contrast, which drives each interest embedding to be distinct from other\ninterests using contrastive learning; 2) Re-attend, which ensures the\ninterest-item correlation estimation in the forward flow to be consistent with\nthe criterion used in final recommendation; and 3) Re-construct, which ensures\nthat each interest embedding can semantically reflect the information of\nrepresentative items that relate to the corresponding interest. We demonstrate\nthe novel forward-backward multi-interest paradigm on ComiRec, and perform\nextensive experiments on three real-world datasets. Empirical studies validate\nthat Re4 helps to learn learning distinct and effective multi-interest\nrepresentations.\n","authors":["Shengyu Zhang","Lingxiao Yang","Dong Yao","Yujie Lu","Fuli Feng","Zhou Zhao","Tat-seng Chua","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2208.08011v2.pdf","comment":"11 pages, 4 figures, accepted by WWW 2022"},{"id":"http://arxiv.org/abs/2311.01343v4","updated":"2024-02-21T23:13:10Z","published":"2023-11-02T15:52:35Z","title":"Collaborative Large Language Model for Recommender Systems","summary":"  Recently, there has been growing interest in developing the next-generation\nrecommender systems (RSs) based on pretrained large language models (LLMs).\nHowever, the semantic gap between natural language and recommendation tasks is\nstill not well addressed, leading to multiple issues such as spuriously\ncorrelated user/item descriptors, ineffective language modeling on user/item\ndata, inefficient recommendations via auto-regression, etc. In this paper, we\npropose CLLM4Rec, the first generative RS that tightly integrates the LLM\nparadigm and ID paradigm of RSs, aiming to address the above challenges\nsimultaneously. We first extend the vocabulary of pretrained LLMs with\nuser/item ID tokens to faithfully model user/item collaborative and content\nsemantics. Accordingly, a novel soft+hard prompting strategy is proposed to\neffectively learn user/item collaborative/content token embeddings via language\nmodeling on RS-specific corpora, where each document is split into a prompt\nconsisting of heterogeneous soft (user/item) tokens and hard (vocab) tokens and\na main text consisting of homogeneous item tokens or vocab tokens to facilitate\nstable and effective language modeling. In addition, a novel mutual\nregularization strategy is introduced to encourage CLLM4Rec to capture\nrecommendation-related information from noisy user/item content. Finally, we\npropose a novel recommendation-oriented finetuning strategy for CLLM4Rec, where\nan item prediction head with multinomial likelihood is added to the pretrained\nCLLM4Rec backbone to predict hold-out items based on soft+hard prompts\nestablished from masked user-item interaction history, where recommendations of\nmultiple items can be generated efficiently without hallucination. Codes are\nreleased at https://github.com/yaochenzhu/llm4rec.\n","authors":["Yaochen Zhu","Liang Wu","Qi Guo","Liangjie Hong","Jundong Li"],"pdf_url":"https://arxiv.org/pdf/2311.01343v4.pdf","comment":"Accepted by WWW2024"},{"id":"http://arxiv.org/abs/2402.14151v1","updated":"2024-02-21T22:22:30Z","published":"2024-02-21T22:22:30Z","title":"BIRCO: A Benchmark of Information Retrieval Tasks with Complex\n  Objectives","summary":"  We present the Benchmark of Information Retrieval (IR) tasks with Complex\nObjectives (BIRCO). BIRCO evaluates the ability of IR systems to retrieve\ndocuments given multi-faceted user objectives. The benchmark's complexity and\ncompact size make it suitable for evaluating large language model (LLM)-based\ninformation retrieval systems. We present a modular framework for investigating\nfactors that may influence LLM performance on retrieval tasks, and identify a\nsimple baseline model which matches or outperforms existing approaches and more\ncomplex alternatives. No approach achieves satisfactory performance on all\nbenchmark tasks, suggesting that stronger models and new retrieval protocols\nare necessary to address complex user needs.\n","authors":["Xiaoyue Wang","Jianyou Wang","Weili Cao","Kaicheng Wang","Ramamohan Paturi","Leon Bergen"],"pdf_url":"https://arxiv.org/pdf/2402.14151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03367v2","updated":"2024-02-21T21:19:39Z","published":"2024-01-31T22:06:07Z","title":"RAG-Fusion: a New Take on Retrieval-Augmented Generation","summary":"  Infineon has identified a need for engineers, account managers, and customers\nto rapidly obtain product information. This problem is traditionally addressed\nwith retrieval-augmented generation (RAG) chatbots, but in this study, I\nevaluated the use of the newly popularized RAG-Fusion method. RAG-Fusion\ncombines RAG and reciprocal rank fusion (RRF) by generating multiple queries,\nreranking them with reciprocal scores and fusing the documents and scores.\nThrough manually evaluating answers on accuracy, relevance, and\ncomprehensiveness, I found that RAG-Fusion was able to provide accurate and\ncomprehensive answers due to the generated queries contextualizing the original\nquery from various perspectives. However, some answers strayed off topic when\nthe generated queries' relevance to the original query is insufficient. This\nresearch marks significant progress in artificial intelligence (AI) and natural\nlanguage processing (NLP) applications and demonstrates transformations in a\nglobal and multi-industry context.\n","authors":["Zackary Rackauckas"],"pdf_url":"https://arxiv.org/pdf/2402.03367v2.pdf","comment":"8 pages, 2 figures, 8 pages"},{"id":"http://arxiv.org/abs/2402.14129v1","updated":"2024-02-21T20:53:29Z","published":"2024-02-21T20:53:29Z","title":"Combining Language and Graph Models for Semi-structured Information\n  Extraction on the Web","summary":"  Relation extraction is an efficient way of mining the extraordinary wealth of\nhuman knowledge on the Web. Existing methods rely on domain-specific training\ndata or produce noisy outputs. We focus here on extracting targeted relations\nfrom semi-structured web pages given only a short description of the relation.\nWe present GraphScholarBERT, an open-domain information extraction method based\non a joint graph and language model structure. GraphScholarBERT can generalize\nto previously unseen domains without additional data or training and produces\nonly clean extraction results matched to the search keyword. Experiments show\nthat GraphScholarBERT can improve extraction F1 scores by as much as 34.8\\%\ncompared to previous work in a zero-shot domain and zero-shot website setting.\n","authors":["Zhi Hong","Kyle Chard","Ian Foster"],"pdf_url":"https://arxiv.org/pdf/2402.14129v1.pdf","comment":"7 pages, 2 figures"},{"id":"http://arxiv.org/abs/2402.13973v1","updated":"2024-02-21T17:58:10Z","published":"2024-02-21T17:58:10Z","title":"Linear-Time Graph Neural Networks for Scalable Recommendations","summary":"  In an era of information explosion, recommender systems are vital tools to\ndeliver personalized recommendations for users. The key of recommender systems\nis to forecast users' future behaviors based on previous user-item\ninteractions. Due to their strong expressive power of capturing high-order\nconnectivities in user-item interaction data, recent years have witnessed a\nrising interest in leveraging Graph Neural Networks (GNNs) to boost the\nprediction performance of recommender systems. Nonetheless, classic Matrix\nFactorization (MF) and Deep Neural Network (DNN) approaches still play an\nimportant role in real-world large-scale recommender systems due to their\nscalability advantages. Despite the existence of GNN-acceleration solutions, it\nremains an open question whether GNN-based recommender systems can scale as\nefficiently as classic MF and DNN methods. In this paper, we propose a\nLinear-Time Graph Neural Network (LTGNN) to scale up GNN-based recommender\nsystems to achieve comparable scalability as classic MF approaches while\nmaintaining GNNs' powerful expressiveness for superior prediction accuracy.\nExtensive experiments and ablation studies are presented to validate the\neffectiveness and scalability of the proposed algorithm. Our implementation\nbased on PyTorch is available.\n","authors":["Jiahao Zhang","Rui Xue","Wenqi Fan","Xin Xu","Qing Li","Jian Pei","Xiaorui Liu"],"pdf_url":"https://arxiv.org/pdf/2402.13973v1.pdf","comment":"12 pages, 5 figures, accepted by The Web Conference 2024"},{"id":"http://arxiv.org/abs/2402.13959v1","updated":"2024-02-21T17:41:17Z","published":"2024-02-21T17:41:17Z","title":"Retention Induced Biases in a Recommendation System with Heterogeneous\n  Users","summary":"  I examine a conceptual model of a recommendation system (RS) with user inflow\nand churn dynamics. When inflow and churn balance out, the user distribution\nreaches a steady state. Changing the recommendation algorithm alters the steady\nstate and creates a transition period. During this period, the RS behaves\ndifferently from its new steady state. In particular, A/B experiment metrics\nobtained in transition periods are biased indicators of the RS's long term\nperformance. Scholars and practitioners, however, often conduct A/B tests\nshortly after introducing new algorithms to validate their effectiveness. This\nA/B experiment paradigm, widely regarded as the gold standard for assessing RS\nimprovements, may consequently yield false conclusions. I also briefly discuss\nthe data bias caused by the user retention dynamics.\n","authors":["Shichao Ma"],"pdf_url":"https://arxiv.org/pdf/2402.13959v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.11527v2","updated":"2024-02-21T16:52:52Z","published":"2023-05-19T08:51:11Z","title":"InstructIE: A Bilingual Instruction-based Information Extraction Dataset","summary":"  Traditional information extraction (IE) methodologies, constrained by\npre-defined classes and static training paradigms, often falter in\nadaptability, especially in the dynamic world. To bridge this gap, we explore\nan instruction-based IE paradigm in this paper, leveraging the substantial\ncross-task generalization capabilities of Large Language Models (LLMs). We\nobserve that most existing IE datasets tend to be overly redundant in their\nlabel sets, which leads to the inclusion of numerous labels not directly\nrelevant to the extraction content when constructing instructions. To tackle\nthis issue, we introduce a bilingual theme-centric IE instruction dataset\n(Chinese and English), InstructIE, and for the first time, incorporate a theme\nscheme design that effectively simplifies the label structure. Furthermore, we\ndevelop an innovative framework named KG2Instruction, which is specifically\ndesigned for the automatic generation of such datasets. Experimental\nevaluations based on InstructIE reveal that while current models show promise\nin Instruction-based IE tasks, opportunities for their potential optimization\nalso emerge. The dataset is available at\nhttps://huggingface.co/datasets/zjunlp/InstructIE.\n","authors":["Honghao Gui","Shuofei Qiao","Jintian Zhang","Hongbin Ye","Mengshu Sun","Lei Liang","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.11527v2.pdf","comment":"Work in progress; project homepage:\n  https://www.zjukg.org/project/InstructIE/ dataset:\n  https://huggingface.co/datasets/zjunlp/InstructIE"},{"id":"http://arxiv.org/abs/2402.13897v1","updated":"2024-02-21T16:09:25Z","published":"2024-02-21T16:09:25Z","title":"Science Checker Reloaded: A Bidirectional Paradigm for Transparency and\n  Logical Reasoning","summary":"  Information retrieval is a rapidly evolving field. However it still faces\nsignificant limitations in the scientific and industrial vast amounts of\ninformation, such as semantic divergence and vocabulary gaps in sparse\nretrieval, low precision and lack of interpretability in semantic search, or\nhallucination and outdated information in generative models. In this paper, we\nintroduce a two-block approach to tackle these hurdles for long documents. The\nfirst block enhances language understanding in sparse retrieval by query\nexpansion to retrieve relevant documents. The second block deepens the result\nby providing comprehensive and informative answers to the complex question\nusing only the information spread in the long document, enabling bidirectional\nengagement. At various stages of the pipeline, intermediate results are\npresented to users to facilitate understanding of the system's reasoning. We\nbelieve this bidirectional approach brings significant advancements in terms of\ntransparency, logical thinking, and comprehensive understanding in the field of\nscientific information retrieval.\n","authors":["Loïc Rakotoson","Sylvain Massip","Fréjus A. A. Laleye"],"pdf_url":"https://arxiv.org/pdf/2402.13897v1.pdf","comment":"6 pages, 3 figures"},{"id":"http://arxiv.org/abs/2402.07425v2","updated":"2024-02-21T15:12:47Z","published":"2024-02-12T06:02:24Z","title":"Debiasing Recommendation with Personal Popularity","summary":"  Global popularity (GP) bias is the phenomenon that popular items are\nrecommended much more frequently than they should be, which goes against the\ngoal of providing personalized recommendations and harms user experience and\nrecommendation accuracy. Many methods have been proposed to reduce GP bias but\nthey fail to notice the fundamental problem of GP, i.e., it considers\npopularity from a \\textit{global} perspective of \\textit{all users} and uses a\nsingle set of popular items, and thus cannot capture the interests of\nindividual users. As such, we propose a user-aware version of item popularity\nnamed \\textit{personal popularity} (PP), which identifies different popular\nitems for each user by considering the users that share similar interests. As\nPP models the preferences of individual users, it naturally helps to produce\npersonalized recommendations and mitigate GP bias. To integrate PP into\nrecommendation, we design a general \\textit{personal popularity aware\ncounterfactual} (PPAC) framework, which adapts easily to existing\nrecommendation models. In particular, PPAC recognizes that PP and GP have both\ndirect and indirect effects on recommendations and controls direct effects with\ncounterfactual inference techniques for unbiased recommendations. All codes and\ndatasets are available at \\url{https://github.com/Stevenn9981/PPAC}.\n","authors":["Wentao Ning","Reynold Cheng","Xiao Yan","Ben Kao","Nan Huo","Nur AI Hasan Haldar","Bo Tang"],"pdf_url":"https://arxiv.org/pdf/2402.07425v2.pdf","comment":"Accepted by WWW'24 as a research full paper"},{"id":"http://arxiv.org/abs/2402.13858v1","updated":"2024-02-21T15:09:51Z","published":"2024-02-21T15:09:51Z","title":"Diversity-Aware $k$-Maximum Inner Product Search Revisited","summary":"  The $k$-Maximum Inner Product Search ($k$MIPS) serves as a foundational\ncomponent in recommender systems and various data mining tasks. However, while\nmost existing $k$MIPS approaches prioritize the efficient retrieval of highly\nrelevant items for users, they often neglect an equally pivotal facet of search\nresults: \\emph{diversity}. To bridge this gap, we revisit and refine the\ndiversity-aware $k$MIPS (D$k$MIPS) problem by incorporating two well-known\ndiversity objectives -- minimizing the average and maximum pairwise item\nsimilarities within the results -- into the original relevance objective. This\nenhancement, inspired by Maximal Marginal Relevance (MMR), offers users a\ncontrollable trade-off between relevance and diversity. We introduce\n\\textsc{Greedy} and \\textsc{DualGreedy}, two linear scan-based algorithms\ntailored for D$k$MIPS. They both achieve data-dependent approximations and,\nwhen aiming to minimize the average pairwise similarity, \\textsc{DualGreedy}\nattains an approximation ratio of $1/4$ with an additive term for\nregularization. To further improve query efficiency, we integrate a lightweight\nBall-Cone Tree (BC-Tree) index with the two algorithms. Finally, comprehensive\nexperiments on ten real-world data sets demonstrate the efficacy of our\nproposed methods, showcasing their capability to efficiently deliver diverse\nand relevant search results to users.\n","authors":["Qiang Huang","Yanhao Wang","Yiqun Sun","Anthony K. H. Tung"],"pdf_url":"https://arxiv.org/pdf/2402.13858v1.pdf","comment":"14 pages, 9 figures, and 5 tables"},{"id":"http://arxiv.org/abs/2402.12994v2","updated":"2024-02-21T14:59:02Z","published":"2024-02-20T13:21:57Z","title":"Distributionally Robust Graph-based Recommendation System","summary":"  With the capacity to capture high-order collaborative signals, Graph Neural\nNetworks (GNNs) have emerged as powerful methods in Recommender Systems (RS).\nHowever, their efficacy often hinges on the assumption that training and\ntesting data share the same distribution (a.k.a. IID assumption), and exhibits\nsignificant declines under distribution shifts. Distribution shifts commonly\narises in RS, often attributed to the dynamic nature of user preferences or\nubiquitous biases during data collection in RS. Despite its significance,\nresearches on GNN-based recommendation against distribution shift are still\nsparse. To bridge this gap, we propose Distributionally Robust GNN (DR-GNN)\nthat incorporates Distributional Robust Optimization (DRO) into the GNN-based\nrecommendation. DR-GNN addresses two core challenges: 1) To enable DRO to cater\nto graph data intertwined with GNN, we reinterpret GNN as a graph smoothing\nregularizer, thereby facilitating the nuanced application of DRO; 2) Given the\ntypically sparse nature of recommendation data, which might impede robust\noptimization, we introduce slight perturbations in the training distribution to\nexpand its support. Notably, while DR-GNN involves complex optimization, it can\nbe implemented easily and efficiently. Our extensive experiments validate the\neffectiveness of DR-GNN against three typical distribution shifts. The code is\navailable at https://github.com/WANGBohaO-jpg/DR-GNN.\n","authors":["Bohao Wang","Jiawei Chen","Changdong Li","Sheng Zhou","Qihao Shi","Yang Gao","Yan Feng","Chun Chen","Can Wang"],"pdf_url":"https://arxiv.org/pdf/2402.12994v2.pdf","comment":"Accepted by WWW2024"},{"id":"http://arxiv.org/abs/2402.13840v1","updated":"2024-02-21T14:38:02Z","published":"2024-02-21T14:38:02Z","title":"LLM4SBR: A Lightweight and Effective Framework for Integrating Large\n  Language Models in Session-based Recommendation","summary":"  Traditional session-based recommendation (SBR) utilizes session behavior\nsequences from anonymous users for recommendation. Although this strategy is\nhighly efficient, it sacrifices the inherent semantic information of the items,\nmaking it difficult for the model to understand the true intent of the session\nand resulting in a lack of interpretability in the recommended results.\nRecently, large language models (LLMs) have flourished across various domains,\noffering a glimpse of hope in addressing the aforementioned challenges.\nInspired by the impact of LLMs, research exploring the integration of LLMs with\nthe Recommender system (RS) has surged like mushrooms after rain. However,\nconstrained by high time and space costs, as well as the brief and anonymous\nnature of session data, the first LLM recommendation framework suitable for\nindustrial deployment has yet to emerge in the field of SBR. To address the\naforementioned challenges, we have proposed the LLM Integration Framework for\nSBR (LLM4SBR). Serving as a lightweight and plug-and-play framework, LLM4SBR\nadopts a two-step strategy. Firstly, we transform session data into a bimodal\nform of text and behavior. In the first step, leveraging the inferential\ncapabilities of LLMs, we conduct inference on session text data from different\nperspectives and design the component for auxiliary enhancement. In the second\nstep, the SBR model is trained on behavior data, aligning and averaging two\nmodal session representations from different perspectives. Finally, we fuse\nsession representations from different perspectives and modalities as the\nultimate session representation for recommendation. We conducted experiments on\ntwo real-world datasets, and the results demonstrate that LLM4SBR significantly\nimproves the performance of traditional SBR models and is highly lightweight\nand efficient, making it suitable for industrial deployment.\n","authors":["Shutong Qiao","Chen Gao","Junhao Wen","Wei Zhou","Qun Luo","Peixuan Chen","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2402.13840v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13769v1","updated":"2024-02-21T12:44:21Z","published":"2024-02-21T12:44:21Z","title":"General Debiasing for Graph-based Collaborative Filtering via\n  Adversarial Graph Dropout","summary":"  Graph neural networks (GNNs) have shown impressive performance in recommender\nsystems, particularly in collaborative filtering (CF). The key lies in\naggregating neighborhood information on a user-item interaction graph to\nenhance user/item representations. However, we have discovered that this\naggregation mechanism comes with a drawback, which amplifies biases present in\nthe interaction graph. For instance, a user's interactions with items can be\ndriven by both unbiased true interest and various biased factors like item\npopularity or exposure. However, the current aggregation approach combines all\ninformation, both biased and unbiased, leading to biased representation\nlearning. Consequently, graph-based recommenders can learn distorted views of\nusers/items, hindering the modeling of their true preferences and\ngeneralizations. To address this issue, we introduce a novel framework called\nAdversarial Graph Dropout (AdvDrop). It differentiates between unbiased and\nbiased interactions, enabling unbiased representation learning. For each\nuser/item, AdvDrop employs adversarial learning to split the neighborhood into\ntwo views: one with bias-mitigated interactions and the other with bias-aware\ninteractions. After view-specific aggregation, AdvDrop ensures that the\nbias-mitigated and bias-aware representations remain invariant, shielding them\nfrom the influence of bias. We validate AdvDrop's effectiveness on five public\ndatasets that cover both general and specific biases, demonstrating significant\nimprovements. Furthermore, our method exhibits meaningful separation of\nsubgraphs and achieves unbiased representations for graph-based CF models, as\nrevealed by in-depth analysis. Our code is publicly available at\nhttps://github.com/Arthurma71/AdvDrop.\n","authors":["An Zhang","Wenchang Ma","Pengbo Wei","Leheng Sheng","Xiang Wang"],"pdf_url":"https://arxiv.org/pdf/2402.13769v1.pdf","comment":"Accepted to WWW 2024"},{"id":"http://arxiv.org/abs/2311.13277v2","updated":"2024-02-21T12:34:28Z","published":"2023-11-22T09:53:57Z","title":"Hierarchical Matrix Factorization for Interpretable Collaborative\n  Filtering","summary":"  Matrix factorization (MF) is a simple collaborative filtering technique that\nachieves superior recommendation accuracy by decomposing the user-item\ninteraction matrix into user and item latent matrices. Because the model\ntypically learns each interaction independently, it may overlook the underlying\nshared dependencies between users and items, resulting in less stable and\ninterpretable recommendations. Based on these insights, we propose\n\"Hierarchical Matrix Factorization\" (HMF), which incorporates clustering\nconcepts to capture the hierarchy, where leaf nodes and other nodes correspond\nto users/items and clusters, respectively. Central to our approach, called\nhierarchical embeddings, is the additional decomposition of the latent matrices\n(embeddings) into probabilistic connection matrices, which link the hierarchy,\nand a root cluster latent matrix. The embeddings are differentiable, allowing\nsimultaneous learning of interactions and clustering using a single gradient\ndescent method. Furthermore, the obtained cluster-specific interactions\nnaturally summarize user-item interactions and provide interpretability.\nExperimental results on ratings and ranking predictions show that HMF\noutperforms existing MF methods, in particular achieving a 1.37 point\nimprovement in RMSE for sparse interactions. Additionally, it was confirmed\nthat the clustering integration of HMF has the potential for faster learning\nconvergence and mitigation of overfitting compared to MF, and also provides\ninterpretability through a cluster-centered case study.\n","authors":["Kai Sugahara","Kazushi Okamoto"],"pdf_url":"https://arxiv.org/pdf/2311.13277v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13750v1","updated":"2024-02-21T12:22:01Z","published":"2024-02-21T12:22:01Z","title":"Breaking the Barrier: Utilizing Large Language Models for Industrial\n  Recommendation Systems through an Inferential Knowledge Graph","summary":"  Recommendation systems are widely used in e-commerce websites and online\nplatforms to address information overload. However, existing systems primarily\nrely on historical data and user feedback, making it difficult to capture user\nintent transitions. Recently, Knowledge Base (KB)-based models are proposed to\nincorporate expert knowledge, but it struggle to adapt to new items and the\nevolving e-commerce environment. To address these challenges, we propose a\nnovel Large Language Model based Complementary Knowledge Enhanced\nRecommendation System (LLM-KERec). It introduces an entity extractor that\nextracts unified concept terms from item and user information. To provide\ncost-effective and reliable prior knowledge, entity pairs are generated based\non entity popularity and specific strategies. The large language model\ndetermines complementary relationships in each entity pair, constructing a\ncomplementary knowledge graph. Furthermore, a new complementary recall module\nand an Entity-Entity-Item (E-E-I) weight decision model refine the scoring of\nthe ranking model using real complementary exposure-click samples. Extensive\nexperiments conducted on three industry datasets demonstrate the significant\nperformance improvement of our model compared to existing approaches.\nAdditionally, detailed analysis shows that LLM-KERec enhances users' enthusiasm\nfor consumption by recommending complementary items. In summary, LLM-KERec\naddresses the limitations of traditional recommendation systems by\nincorporating complementary knowledge and utilizing a large language model to\ncapture user intent transitions, adapt to new items, and enhance recommendation\nefficiency in the evolving e-commerce landscape.\n","authors":["Qian Zhao","Hao Qian","Ziqi Liu","Gong-Duo Zhang","Lihong Gu"],"pdf_url":"https://arxiv.org/pdf/2402.13750v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2402.10853v2","updated":"2024-02-21T10:51:12Z","published":"2024-02-16T17:47:11Z","title":"Discovering and exploring cases of educational source code plagiarism\n  with Dolos","summary":"  Source code plagiarism is a significant issue in educational practice, and\neducators need user-friendly tools to cope with such academic dishonesty. This\narticle introduces the latest version of Dolos, a state-of-the-art ecosystem of\ntools for detecting and preventing plagiarism in educational source code. In\nthis new version, the primary focus has been on enhancing the user experience.\nEducators can now run the entire plagiarism detection pipeline from a new web\napp in their browser, eliminating the need for any installation or\nconfiguration. Completely redesigned analytics dashboards provide an instant\nassessment of whether a collection of source files contains suspected cases of\nplagiarism and how widespread plagiarism is within the collection. The\ndashboards support hierarchically structured navigation to facilitate zooming\nin and out of suspect cases. Clusters are an essential new component of the\ndashboard design, reflecting the observation that plagiarism can occur among\nlarger groups of students. To meet various user needs, the Dolos software stack\nfor source code plagiarism detections now includes a web interface, a JSON\napplication programming interface (API), a command line interface (CLI), a\nJavaScript library and a preconfigured Docker container. Clear documentation\nand a free-to-use instance of the web app can be found at\nhttps://dolos.ugent.be. The source code is also available on GitHub.\n","authors":["Rien Maertens","Maarten Van Neyghem","Maxiem Geldhof","Charlotte Van Petegem","Niko Strijbol","Peter Dawyndt","Bart Mesuere"],"pdf_url":"https://arxiv.org/pdf/2402.10853v2.pdf","comment":"20 pages, 5 figures; minor corrections, reference added for section 2"},{"id":"http://arxiv.org/abs/2402.13576v1","updated":"2024-02-21T07:16:06Z","published":"2024-02-21T07:16:06Z","title":"Improving Video Corpus Moment Retrieval with Partial Relevance\n  Enhancement","summary":"  Video corpus moment retrieval~(VCMR) is a new video retrieval task aimed at\nretrieving a relevant moment from a large corpus of untrimmed videos using a\nnatural language text as query. The relevance between the video and query is\npartial, mainly evident in two aspects: (1) Scope: The untrimmed video contains\ninformation-rich frames, and not all are relevant to the query. Strong\ncorrelation is typically observed only within the relevant moment, emphasizing\nthe importance of capturing key content. (2) Modality: The relevance of query\nto different modalities varies; action descriptions align more with the visual\nelements, while character conversations are more related to textual\ninformation. Recognizing and addressing these modality-specific nuances is\ncrucial for effective retrieval in VCMR. However, existing methods often treat\nall video contents equally, leading to sub-optimal moment retrieval. We argue\nthat effectively capturing the partial relevance between the query and video is\nessential for the VCMR task. To this end, we propose a Partial Relevance\nEnhanced Model~(PREM) to improve VCMR. VCMR involves two sub-tasks: video\nretrieval and moment localization. To align with their distinct objectives, we\nimplement specialized partial relevance enhancement strategies. For video\nretrieval, we introduce a multi-modal collaborative video retriever, generating\ndistinct query representations tailored for different modalities by\nmodality-specific pooling, ensuring a more effective match. For moment\nlocalization, we propose the focus-then-fuse moment localizer, utilizing\nmodality-specific gates to capture essential content, followed by fusing\nmulti-modal information for moment localization. Experimental results on TVR\nand DiDeMo datasets show that the proposed model outperforms the baselines,\nachieving a new state-of-the-art of VCMR.\n","authors":["Danyang Hou","Liang Pang","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2402.13576v1.pdf","comment":"10 pages, 6 figures, 6 tables"},{"id":"http://arxiv.org/abs/2402.13566v1","updated":"2024-02-21T06:55:20Z","published":"2024-02-21T06:55:20Z","title":"Event-aware Video Corpus Moment Retrieval","summary":"  Video Corpus Moment Retrieval (VCMR) is a practical video retrieval task\nfocused on identifying a specific moment within a vast corpus of untrimmed\nvideos using the natural language query. Existing methods for VCMR typically\nrely on frame-aware video retrieval, calculating similarities between the query\nand video frames to rank videos based on maximum frame similarity.However, this\napproach overlooks the semantic structure embedded within the information\nbetween frames, namely, the event, a crucial element for human comprehension of\nvideos. Motivated by this, we propose EventFormer, a model that explicitly\nutilizes events within videos as fundamental units for video retrieval. The\nmodel extracts event representations through event reasoning and hierarchical\nevent encoding. The event reasoning module groups consecutive and visually\nsimilar frame representations into events, while the hierarchical event\nencoding encodes information at both the frame and event levels. We also\nintroduce anchor multi-head self-attenion to encourage Transformer to capture\nthe relevance of adjacent content in the video. The training of EventFormer is\nconducted by two-branch contrastive learning and dual optimization for two\nsub-tasks of VCMR. Extensive experiments on TVR, ANetCaps, and DiDeMo\nbenchmarks show the effectiveness and efficiency of EventFormer in VCMR,\nachieving new state-of-the-art results. Additionally, the effectiveness of\nEventFormer is also validated on partially relevant video retrieval task.\n","authors":["Danyang Hou","Liang Pang","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2402.13566v1.pdf","comment":"11 pages, 5 figures, 9 tables"},{"id":"http://arxiv.org/abs/2402.13542v1","updated":"2024-02-21T05:41:34Z","published":"2024-02-21T05:41:34Z","title":"ARL2: Aligning Retrievers for Black-box Large Language Models via\n  Self-guided Adaptive Relevance Labeling","summary":"  Retrieval-augmented generation enhances large language models (LLMs) by\nincorporating relevant information from external knowledge sources. This\nenables LLMs to adapt to specific domains and mitigate hallucinations in\nknowledge-intensive tasks. However, existing retrievers are often misaligned\nwith LLMs due to their separate training processes and the black-box nature of\nLLMs. To address this challenge, we propose ARL2, a retriever learning\ntechnique that harnesses LLMs as labelers. ARL2 leverages LLMs to annotate and\nscore relevant evidence, enabling learning the retriever from robust LLM\nsupervision. Furthermore, ARL2 uses an adaptive self-training strategy for\ncurating high-quality and diverse relevance data, which can effectively reduce\nthe annotation cost. Extensive experiments demonstrate the effectiveness of\nARL2, achieving accuracy improvements of 5.4% on NQ and 4.6% on MMLU compared\nto the state-of-the-art methods. Additionally, ARL2 exhibits robust transfer\nlearning capabilities and strong zero-shot generalization abilities. Our code\nwill be published at \\url{https://github.com/zhanglingxi-cs/ARL2}.\n","authors":["Lingxi Zhang","Yue Yu","Kuan Wang","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.13542v1.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2301.02998v2","updated":"2024-02-21T04:20:55Z","published":"2023-01-08T08:03:46Z","title":"InPars-Light: Cost-Effective Unsupervised Training of Efficient Rankers","summary":"  We carried out a reproducibility study of InPars, which is a method for\nunsupervised training of neural rankers (Bonifacio et al., 2022). As a\nby-product, we developed InPars-light, which is a simple-yet-effective\nmodification of InPars. Unlike InPars, InPars-light uses 7x-100x smaller\nranking models and only a freely available language model BLOOM, which -- as we\nfound out -- produced more accurate rankers compared to a proprietary GPT-3\nmodel. On all five English retrieval collections (used in the original InPars\nstudy) we obtained substantial (7%-30%) and statistically significant\nimprovements over BM25 (in nDCG and MRR) using only a 30M parameter six-layer\nMiniLM-30M ranker and a single three-shot prompt. In contrast, in the InPars\nstudy only a 100x larger monoT5-3B model consistently outperformed BM25,\nwhereas their smaller monoT5-220M model (which is still 7x larger than our\nMiniLM ranker) outperformed BM25 only on MS MARCO and TREC DL 2020. In the same\nthree-shot prompting scenario, our 435M parameter DeBERTA v3 ranker was at par\nwith the 7x larger monoT5-3B (average gain over BM25 of 1.3 vs 1.32): In fact,\non three out of five datasets, DeBERTA slightly outperformed monoT5-3B.\nFinally, these good results were achieved by re-ranking only 100 candidate\ndocuments compared to 1000 used by Bonifacio et al. (2022). We believe that\nInPars-light is the first truly cost-effective prompt-based unsupervised recipe\nto train and deploy neural ranking models that outperform BM25. Our code and\ndata is publicly available. https://github.com/searchivarius/inpars_light/\n","authors":["Leonid Boytsov","Preksha Patel","Vivek Sourabh","Riddhi Nisar","Sayani Kundu","Ramya Ramanathan","Eric Nyberg"],"pdf_url":"https://arxiv.org/pdf/2301.02998v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13500v1","updated":"2024-02-21T03:25:14Z","published":"2024-02-21T03:25:14Z","title":"Leveraging Translation For Optimal Recall: Tailoring LLM Personalization\n  With User Profiles","summary":"  This paper explores a novel technique for improving recall in cross-language\ninformation retrieval (CLIR) systems using iterative query refinement grounded\nin the user's lexical-semantic space. The proposed methodology combines\nmulti-level translation, semantic embedding-based expansion, and user\nprofile-centered augmentation to address the challenge of matching variance\nbetween user queries and relevant documents. Through an initial BM25 retrieval,\ntranslation into intermediate languages, embedding lookup of similar terms, and\niterative re-ranking, the technique aims to expand the scope of potentially\nrelevant results personalized to the individual user. Comparative experiments\non news and Twitter datasets demonstrate superior performance over baseline\nBM25 ranking for the proposed approach across ROUGE metrics. The translation\nmethodology also showed maintained semantic accuracy through the multi-step\nprocess. This personalized CLIR framework paves the path for improved\ncontext-aware retrieval attentive to the nuances of user language.\n","authors":["Karthik Ravichandran","Sarmistha Sarna Gomasta"],"pdf_url":"https://arxiv.org/pdf/2402.13500v1.pdf","comment":"This is just an initial idea and it's implementation. The results are\n  computed for the first 100 data points. Detailed results will be published\n  with the actual paper"},{"id":"http://arxiv.org/abs/2402.13495v1","updated":"2024-02-21T03:14:37Z","published":"2024-02-21T03:14:37Z","title":"Can One Embedding Fit All? A Multi-Interest Learning Paradigm Towards\n  Improving User Interest Diversity Fairness","summary":"  Recommender systems (RSs) have gained widespread applications across various\ndomains owing to the superior ability to capture users' interests. However, the\ncomplexity and nuanced nature of users' interests, which span a wide range of\ndiversity, pose a significant challenge in delivering fair recommendations. In\npractice, user preferences vary significantly; some users show a clear\npreference toward certain item categories, while others have a broad interest\nin diverse ones. Even though it is expected that all users should receive\nhigh-quality recommendations, the effectiveness of RSs in catering to this\ndisparate interest diversity remains under-explored.\n  In this work, we investigate whether users with varied levels of interest\ndiversity are treated fairly. Our empirical experiments reveal an inherent\ndisparity: users with broader interests often receive lower-quality\nrecommendations. To mitigate this, we propose a multi-interest framework that\nuses multiple (virtual) interest embeddings rather than single ones to\nrepresent users. Specifically, the framework consists of stacked multi-interest\nrepresentation layers, which include an interest embedding generator that\nderives virtual interests from shared parameters, and a center embedding\naggregator that facilitates multi-hop aggregation. Experiments demonstrate the\neffectiveness of the framework in achieving better trade-off between fairness\nand utility across various datasets and backbones.\n","authors":["Yuying Zhao","Minghua Xu","Huiyuan Chen","Yuzhong Chen","Yiwei Cai","Rashidul Islam","Yu Wang","Tyler Derr"],"pdf_url":"https://arxiv.org/pdf/2402.13495v1.pdf","comment":"Accepted by WWW'24"},{"id":"http://arxiv.org/abs/2402.13444v1","updated":"2024-02-21T00:37:16Z","published":"2024-02-21T00:37:16Z","title":"The Effectiveness of Graph Contrastive Learning on Mathematical\n  Information Retrieval","summary":"  This paper details an empirical investigation into using Graph Contrastive\nLearning (GCL) to generate mathematical equation representations, a critical\naspect of Mathematical Information Retrieval (MIR). Our findings reveal that\nthis simple approach consistently exceeds the performance of the current\nleading formula retrieval model, TangentCFT. To support ongoing research and\ndevelopment in this field, we have made our source code accessible to the\npublic at https://github.com/WangPeiSyuan/GCL-Formula-Retrieval/.\n","authors":["Pei-Syuan Wang","Hung-Hsuan Chen"],"pdf_url":"https://arxiv.org/pdf/2402.13444v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13435v1","updated":"2024-02-21T00:05:25Z","published":"2024-02-21T00:05:25Z","title":"Learning to Retrieve for Job Matching","summary":"  Web-scale search systems typically tackle the scalability challenge with a\ntwo-step paradigm: retrieval and ranking. The retrieval step, also known as\ncandidate selection, often involves extracting standardized entities, creating\nan inverted index, and performing term matching for retrieval. Such traditional\nmethods require manual and time-consuming development of query models. In this\npaper, we discuss applying learning-to-retrieve technology to enhance LinkedIns\njob search and recommendation systems. In the realm of promoted jobs, the key\nobjective is to improve the quality of applicants, thereby delivering value to\nrecruiter customers. To achieve this, we leverage confirmed hire data to\nconstruct a graph that evaluates a seeker's qualification for a job, and\nutilize learned links for retrieval. Our learned model is easy to explain,\ndebug, and adjust. On the other hand, the focus for organic jobs is to optimize\nseeker engagement. We accomplished this by training embeddings for personalized\nretrieval, fortified by a set of rules derived from the categorization of\nmember feedback. In addition to a solution based on a conventional inverted\nindex, we developed an on-GPU solution capable of supporting both KNN and term\nmatching efficiently.\n","authors":["Jianqiang Shen","Yuchin Juan","Shaobo Zhang","Ping Liu","Wen Pu","Sriram Vasudevan","Qingquan Song","Fedor Borisyuk","Kay Qianqi Shen","Haichao Wei","Yunxiang Ren","Yeou S. Chiou","Sicong Kuang","Yuan Yin","Ben Zheng","Muchen Wu","Shaghayegh Gharghabi","Xiaoqing Wang","Huichao Xue","Qi Guo","Daniel Hewlett","Luke Simon","Liangjie Hong","Wenjing Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.13435v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2110.08021v2","updated":"2024-02-21T21:48:55Z","published":"2021-10-15T11:32:17Z","title":"StreaMulT: Streaming Multimodal Transformer for Heterogeneous and\n  Arbitrary Long Sequential Data","summary":"  The increasing complexity of Industry 4.0 systems brings new challenges\nregarding predictive maintenance tasks such as fault detection and diagnosis. A\ncorresponding and realistic setting includes multi-source data streams from\ndifferent modalities, such as sensors measurements time series, machine images,\ntextual maintenance reports, etc. These heterogeneous multimodal streams also\ndiffer in their acquisition frequency, may embed temporally unaligned\ninformation and can be arbitrarily long, depending on the considered system and\ntask. Whereas multimodal fusion has been largely studied in a static setting,\nto the best of our knowledge, there exists no previous work considering\narbitrarily long multimodal streams alongside with related tasks such as\nprediction across time. Thus, in this paper, we first formalize this paradigm\nof heterogeneous multimodal learning in a streaming setting as a new one. To\ntackle this challenge, we propose StreaMulT, a Streaming Multimodal Transformer\nrelying on cross-modal attention and on a memory bank to process arbitrarily\nlong input sequences at training time and run in a streaming way at inference.\nStreaMulT improves the state-of-the-art metrics on CMU-MOSEI dataset for\nMultimodal Sentiment Analysis task, while being able to deal with much longer\ninputs than other multimodal models. The conducted experiments eventually\nhighlight the importance of the textual embedding layer, questioning recent\nimprovements in Multimodal Sentiment Analysis benchmarks.\n","authors":["Victor Pellegrain","Myriam Tami","Michel Batteux","Céline Hudelot"],"pdf_url":"https://arxiv.org/pdf/2110.08021v2.pdf","comment":"11 pages, 6 figures, 3 tables"},{"id":"http://arxiv.org/abs/2401.12264v2","updated":"2024-02-21T08:54:29Z","published":"2024-01-22T08:16:48Z","title":"CoAVT: A Cognition-Inspired Unified Audio-Visual-Text Pre-Training Model\n  for Multimodal Processing","summary":"  There has been a long-standing quest for a unified audio-visual-text model to\nenable various multimodal understanding tasks, which mimics the listening,\nseeing and reading process of human beings. Humans tends to represent knowledge\nusing two separate systems: one for representing verbal (textual) information\nand one for representing non-verbal (visual and auditory) information. These\ntwo systems can operate independently but can also interact with each other.\nMotivated by this understanding of human cognition, in this paper, we introduce\nCoAVT -- a novel cognition-inspired Correlated Audio-Visual-Text pre-training\nmodel to connect the three modalities. It contains a joint audio-visual encoder\nthat learns to encode audio-visual synchronization information together with\nthe audio and visual content for non-verbal information, and a text encoder to\nhandle textual input for verbal information. To bridge the gap between\nmodalities, CoAVT employs a query encoder, which contains a set of learnable\nquery embeddings, and extracts the most informative audiovisual features of the\ncorresponding text. Additionally, to leverage the correspondences between audio\nand vision with language respectively, we also establish the audio-text and\nvisual-text bi-modal alignments upon the foundational audiovisual-text\ntri-modal alignment to enhance the multimodal representation learning. Finally,\nwe jointly optimize CoAVT model with three multimodal objectives: contrastive\nloss, matching loss and language modeling loss. Extensive experiments show that\nCoAVT can learn strong multimodal correlations and be generalized to various\ndownstream tasks. CoAVT establishes new state-of-the-art performance on\ntext-video retrieval task on AudioCaps for both zero-shot and fine-tuning\nsettings, audio-visual event classification and audio-visual retrieval tasks on\nAudioSet and VGGSound.\n","authors":["Xianghu Yue","Xiaohai Tian","Lu Lu","Malu Zhang","Zhizheng Wu","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2401.12264v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.18010v2","updated":"2024-02-21T06:25:33Z","published":"2023-05-29T11:03:59Z","title":"Test-Time Adaptation with CLIP Reward for Zero-Shot Generalization in\n  Vision-Language Models","summary":"  One fascinating aspect of pre-trained vision-language models~(VLMs) learning\nunder language supervision is their impressive zero-shot generalization\ncapability. However, this ability is hindered by distribution shifts between\nthe training and testing data. Previous test time adaptation~(TTA) methods for\nVLMs in zero-shot classification rely on minimizing the entropy of model\noutputs, tending to be stuck in incorrect model predictions. In this work, we\npropose TTA with feedback to rectify the model output and prevent the model\nfrom becoming blindly confident. Specifically, a CLIP model is adopted as the\nreward model during TTA and provides feedback for the VLM. Given a single test\nsample, the VLM is forced to maximize the CLIP reward between the input and\nsampled results from the VLM output distribution. The proposed\n\\textit{reinforcement learning with CLIP feedback~(RLCF)} framework is highly\nflexible and universal. Beyond the classification task, with task-specific\nsampling strategies and a proper reward baseline choice, RLCF can be easily\nextended to not only discrimination tasks like retrieval but also\ngeneralization tasks like image captioning, improving the zero-shot\ngeneralization capacity of VLMs. According to the characteristics of these VL\ntasks, we build different fully TTA pipelines with RLCF to improve the\nzero-shot generalization ability of various VLMs. Extensive experiments along\nwith promising empirical results demonstrate the effectiveness of RLCF. The\ncode is available at https://github.com/mzhaoshuai/RLCF.\n","authors":["Shuai Zhao","Xiaohan Wang","Linchao Zhu","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2305.18010v2.pdf","comment":"accepted by ICLR 2024, project page at\n  https://mzhaoshuai.github.io/RLCF/, code is at\n  https://github.com/mzhaoshuai/RLCF"}]},"2024-02-20T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2402.09660v2","updated":"2024-02-20T23:43:20Z","published":"2024-02-15T02:06:06Z","title":"User Modeling and User Profiling: A Comprehensive Survey","summary":"  The integration of artificial intelligence (AI) into daily life, particularly\nthrough information retrieval and recommender systems, has necessitated\nadvanced user modeling and profiling techniques to deliver personalized\nexperiences. These techniques aim to construct accurate user representations\nbased on the rich amounts of data generated through interactions with these\nsystems. This paper presents a comprehensive survey of the current state,\nevolution, and future directions of user modeling and profiling research. We\nprovide a historical overview, tracing the development from early stereotype\nmodels to the latest deep learning techniques, and propose a novel taxonomy\nthat encompasses all active topics in this research area, including recent\ntrends. Our survey highlights the paradigm shifts towards more sophisticated\nuser profiling methods, emphasizing implicit data collection, multi-behavior\nmodeling, and the integration of graph data structures. We also address the\ncritical need for privacy-preserving techniques and the push towards\nexplainability and fairness in user modeling approaches. By examining the\ndefinitions of core terminology, we aim to clarify ambiguities and foster a\nclearer understanding of the field by proposing two novel encyclopedic\ndefinitions of the main terms. Furthermore, we explore the application of user\nmodeling in various domains, such as fake news detection, cybersecurity, and\npersonalized education. This survey serves as a comprehensive resource for\nresearchers and practitioners, offering insights into the evolution of user\nmodeling and profiling and guiding the development of more personalized,\nethical, and effective AI systems.\n","authors":["Erasmo Purificato","Ludovico Boratto","Ernesto William De Luca"],"pdf_url":"https://arxiv.org/pdf/2402.09660v2.pdf","comment":"71 pages"},{"id":"http://arxiv.org/abs/2402.13417v1","updated":"2024-02-20T23:04:06Z","published":"2024-02-20T23:04:06Z","title":"Unlocking the `Why' of Buying: Introducing a New Dataset and Benchmark\n  for Purchase Reason and Post-Purchase Experience","summary":"  Explanations are crucial for enhancing user trust and understanding within\nmodern recommendation systems. To build truly explainable systems, we need\nhigh-quality datasets that elucidate why users make choices. While previous\nefforts have focused on extracting users' post-purchase sentiment in reviews,\nthey ignore the reasons behind the decision to buy.\n  In our work, we propose a novel purchase reason explanation task. To this\nend, we introduce an LLM-based approach to generate a dataset that consists of\ntextual explanations of why real users make certain purchase decisions. We\ninduce LLMs to explicitly distinguish between the reasons behind purchasing a\nproduct and the experience after the purchase in a user review. An automated,\nLLM-driven evaluation, as well as a small scale human evaluation, confirms the\neffectiveness of our approach to obtaining high-quality, personalized\nexplanations. We benchmark this dataset on two personalized explanation\ngeneration tasks. We release the code and prompts to spur further research.\n","authors":["Tao Chen","Siqi Zuo","Cheng Li","Mingyang Zhang","Qiaozhu Mei","Michael Bendersky"],"pdf_url":"https://arxiv.org/pdf/2402.13417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.01610v2","updated":"2024-02-20T20:52:50Z","published":"2023-09-04T13:49:48Z","title":"Fair Ranking under Disparate Uncertainty","summary":"  Ranking is a ubiquitous method for focusing the attention of human evaluators\non a manageable subset of options. Its use as part of human decision-making\nprocesses ranges from surfacing potentially relevant products on an e-commerce\nsite to prioritizing college applications for human review. While ranking can\nmake human evaluation more effective by focusing attention on the most\npromising options, we argue that it can introduce unfairness if the uncertainty\nof the underlying relevance model differs between groups of options.\nUnfortunately, such disparity in uncertainty appears widespread, often to the\ndetriment of minority groups for which relevance estimates can have higher\nuncertainty due to a lack of data or appropriate features. To address this\nfairness issue, we propose Equal-Opportunity Ranking (EOR) as a new fairness\ncriterion for ranking and show that it corresponds to a group-wise fair lottery\namong the relevant options even in the presence of disparate uncertainty.\nFurthermore, EOR optimizes for an even cost burden on all groups, unlike the\nconventional Probability Ranking Principle. In contrast to affirmative action\ninterventions like proportional Rooney rule constraints, EOR does not require\nthe designation of a disadvantaged group. To make EOR ranking practical, we\npresent an efficient algorithm for computing it in time $O(n \\log(n))$ and\nprove its close approximation guarantee to the globally optimal solution. In a\ncomprehensive empirical evaluation on synthetic data, a US Census dataset, and\na real-world audit of Amazon search queries, we find that the algorithm\nreliably guarantees EOR fairness while providing effective rankings.\n","authors":["Richa Rastogi","Thorsten Joachims"],"pdf_url":"https://arxiv.org/pdf/2309.01610v2.pdf","comment":"A version of this paper was accepted as Spotlight (Oral) at UAI\n  workshop on Epistemic AI, 2023"},{"id":"http://arxiv.org/abs/2402.13364v1","updated":"2024-02-20T20:42:02Z","published":"2024-02-20T20:42:02Z","title":"A Simple but Effective Approach to Improve Structured Language Model\n  Output for Information Extraction","summary":"  Large language models (LLMs) have demonstrated impressive abilities in\ngenerating unstructured natural language according to instructions. However,\ntheir performance can be inconsistent when tasked with producing text that\nadheres to specific structured formats, which is crucial in applications like\nnamed entity recognition (NER) or relation extraction (RE). To address this\nissue, this paper introduces an efficient method, G&O, to enhance their\nstructured text generation capabilities. It breaks the generation into a\ntwo-step pipeline: initially, LLMs generate answers in natural language as\nintermediate responses. Subsequently, LLMs are asked to organize the output\ninto the desired structure, using the intermediate responses as context. G&O\neffectively separates the generation of content from the structuring process,\nreducing the pressure of completing two orthogonal tasks simultaneously. Tested\non zero-shot NER and RE, the results indicate a significant improvement in LLM\nperformance with minimal additional efforts. This straightforward and adaptable\nprompting technique can also be combined with other strategies, like\nself-consistency, to further elevate LLM capabilities in various structured\ntext generation tasks.\n","authors":["Yinghao Li","Rampi Ramprasad","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.13364v1.pdf","comment":"15 pages, 5 figures, 5 tables"},{"id":"http://arxiv.org/abs/2402.13234v1","updated":"2024-02-20T18:49:41Z","published":"2024-02-20T18:49:41Z","title":"Unlocking Insights: Semantic Search in Jupyter Notebooks","summary":"  Semantic search, a process aimed at delivering highly relevant search results\nby comprehending the searcher's intent and the contextual meaning of terms\nwithin a searchable dataspace, plays a pivotal role in information retrieval.\nIn this paper, we investigate the application of large language models to\nenhance semantic search capabilities, specifically tailored for the domain of\nJupyter Notebooks. Our objective is to retrieve generated outputs, such as\nfigures or tables, associated functions and methods, and other pertinent\ninformation.\n  We demonstrate a semantic search framework that achieves a comprehensive\nsemantic understanding of the entire notebook's contents, enabling it to\neffectively handle various types of user queries. Key components of this\nframework include:\n  1). A data preprocessor is designed to handle diverse types of cells within\nJupyter Notebooks, encompassing both markdown and code cells. 2). An innovative\nmethodology is devised to address token size limitations that arise with\ncode-type cells. We implement a finer-grained approach to data input,\ntransitioning from the cell level to the function level, effectively resolving\nthese issues.\n","authors":["Lan Li","Jinpeng Lv"],"pdf_url":"https://arxiv.org/pdf/2402.13234v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03190v3","updated":"2024-02-20T16:47:16Z","published":"2024-02-05T16:56:11Z","title":"Unified Hallucination Detection for Multimodal Large Language Models","summary":"  Despite significant strides in multimodal tasks, Multimodal Large Language\nModels (MLLMs) are plagued by the critical issue of hallucination. The reliable\ndetection of such hallucinations in MLLMs has, therefore, become a vital aspect\nof model evaluation and the safeguarding of practical application deployment.\nPrior research in this domain has been constrained by a narrow focus on\nsingular tasks, an inadequate range of hallucination categories addressed, and\na lack of detailed granularity. In response to these challenges, our work\nexpands the investigative horizons of hallucination detection. We present a\nnovel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate\nthe evaluation of advancements in hallucination detection methods.\nAdditionally, we unveil a novel unified multimodal hallucination detection\nframework, UNIHD, which leverages a suite of auxiliary tools to validate the\noccurrence of hallucinations robustly. We demonstrate the effectiveness of\nUNIHD through meticulous evaluation and comprehensive analysis. We also provide\nstrategic insights on the application of specific tools for addressing various\ncategories of hallucinations.\n","authors":["Xiang Chen","Chenxi Wang","Yida Xue","Ningyu Zhang","Xiaoyan Yang","Qiang Li","Yue Shen","Lei Liang","Jinjie Gu","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2402.03190v3.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2402.13079v1","updated":"2024-02-20T15:24:21Z","published":"2024-02-20T15:24:21Z","title":"Mode Estimation with Partial Feedback","summary":"  The combination of lightly supervised pre-training and online fine-tuning has\nplayed a key role in recent AI developments. These new learning pipelines call\nfor new theoretical frameworks. In this paper, we formalize core aspects of\nweakly supervised and active learning with a simple problem: the estimation of\nthe mode of a distribution using partial feedback. We show how entropy coding\nallows for optimal information acquisition from partial feedback, develop\ncoarse sufficient statistics for mode identification, and adapt bandit\nalgorithms to our new setting. Finally, we combine those contributions into a\nstatistically and computationally efficient solution to our problem.\n","authors":["Charles Arnal","Vivien Cabannes","Vianney Perchet"],"pdf_url":"https://arxiv.org/pdf/2402.13079v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13033v1","updated":"2024-02-20T14:18:43Z","published":"2024-02-20T14:18:43Z","title":"Enhancing Real-World Complex Network Representations with Hyperedge\n  Augmentation","summary":"  Graph augmentation methods play a crucial role in improving the performance\nand enhancing generalisation capabilities in Graph Neural Networks (GNNs).\nExisting graph augmentation methods mainly perturb the graph structures and are\nusually limited to pairwise node relations. These methods cannot fully address\nthe complexities of real-world large-scale networks that often involve\nhigher-order node relations beyond only being pairwise. Meanwhile, real-world\ngraph datasets are predominantly modelled as simple graphs, due to the scarcity\nof data that can be used to form higher-order edges. Therefore, reconfiguring\nthe higher-order edges as an integration into graph augmentation strategies\nlights up a promising research path to address the aforementioned issues. In\nthis paper, we present Hyperedge Augmentation (HyperAug), a novel graph\naugmentation method that constructs virtual hyperedges directly form the raw\ndata, and produces auxiliary node features by extracting from the virtual\nhyperedge information, which are used for enhancing GNN performances on\ndownstream tasks. We design three diverse virtual hyperedge construction\nstrategies to accompany the augmentation scheme: (1) via graph statistics, (2)\nfrom multiple data perspectives, and (3) utilising multi-modality. Furthermore,\nto facilitate HyperAug evaluation, we provide 23 novel real-world graph\ndatasets across various domains including social media, biology, and\ne-commerce. Our empirical study shows that HyperAug consistently and\nsignificantly outperforms GNN baselines and other graph augmentation methods,\nacross a variety of application contexts, which clearly indicates that it can\neffectively incorporate higher-order node relations into graph augmentation\nmethods for real-world complex networks.\n","authors":["Xiangyu Zhao","Zehui Li","Mingzhu Shen","Guy-Bart Stan","Pietro Liò","Yiren Zhao"],"pdf_url":"https://arxiv.org/pdf/2402.13033v1.pdf","comment":"Preprint. Under review. 17 pages, 4 figures, 14 tables. arXiv admin\n  note: text overlap with arXiv:2306.05108"},{"id":"http://arxiv.org/abs/2402.12993v1","updated":"2024-02-20T13:21:46Z","published":"2024-02-20T13:21:46Z","title":"An Autonomous Large Language Model Agent for Chemical Literature Data\n  Mining","summary":"  Chemical synthesis, which is crucial for advancing material synthesis and\ndrug discovery, impacts various sectors including environmental science and\nhealthcare. The rise of technology in chemistry has generated extensive\nchemical data, challenging researchers to discern patterns and refine synthesis\nprocesses. Artificial intelligence (AI) helps by analyzing data to optimize\nsynthesis and increase yields. However, AI faces challenges in processing\nliterature data due to the unstructured format and diverse writing style of\nchemical literature. To overcome these difficulties, we introduce an end-to-end\nAI agent framework capable of high-fidelity extraction from extensive chemical\nliterature. This AI agent employs large language models (LLMs) for prompt\ngeneration and iterative optimization. It functions as a chemistry assistant,\nautomating data collection and analysis, thereby saving manpower and enhancing\nperformance. Our framework's efficacy is evaluated using accuracy, recall, and\nF1 score of reaction condition data, and we compared our method with human\nexperts in terms of content correctness and time efficiency. The proposed\napproach marks a significant advancement in automating chemical literature\nextraction and demonstrates the potential for AI to revolutionize data\nmanagement and utilization in chemistry.\n","authors":["Kexin Chen","Hanqun Cao","Junyou Li","Yuyang Du","Menghao Guo","Xin Zeng","Lanqing Li","Jiezhong Qiu","Pheng Ann Heng","Guangyong Chen"],"pdf_url":"https://arxiv.org/pdf/2402.12993v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12784v1","updated":"2024-02-20T07:49:30Z","published":"2024-02-20T07:49:30Z","title":"Understanding and Mitigating the Threat of Vec2Text to Dense Retrieval\n  Systems","summary":"  The introduction of Vec2Text, a technique for inverting text embeddings, has\nraised serious privacy concerns within dense retrieval systems utilizing text\nembeddings, including those provided by OpenAI and Cohere. This threat comes\nfrom the ability for a malicious attacker with access to text embeddings to\nreconstruct the original text.\n  In this paper, we investigate various aspects of embedding models that could\ninfluence the recoverability of text using Vec2Text. Our exploration involves\nfactors such as distance metrics, pooling functions, bottleneck pre-training,\ntraining with noise addition, embedding quantization, and embedding dimensions\n-- aspects not previously addressed in the original Vec2Text paper. Through a\nthorough analysis of these factors, our aim is to gain a deeper understanding\nof the critical elements impacting the trade-offs between text recoverability\nand retrieval effectiveness in dense retrieval systems. This analysis provides\nvaluable insights for practitioners involved in designing privacy-aware dense\nretrieval systems. Additionally, we propose a straightforward fix for embedding\ntransformation that ensures equal ranking effectiveness while mitigating the\nrisk of text recoverability.\n  Furthermore, we extend the application of Vec2Text to the separate task of\ncorpus poisoning, where, theoretically, Vec2Text presents a more potent threat\ncompared to previous attack methods. Notably, Vec2Text does not require access\nto the dense retriever's model parameters and can efficiently generate numerous\nadversarial passages.\n  In summary, this study highlights the potential threat posed by Vec2Text to\nexisting dense retrieval systems, while also presenting effective methods to\npatch and strengthen such systems against such risks.\n","authors":["Shengyao Zhuang","Bevan Koopman","Xiaoran Chu","Guido Zuccon"],"pdf_url":"https://arxiv.org/pdf/2402.12784v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12774v1","updated":"2024-02-20T07:25:34Z","published":"2024-02-20T07:25:34Z","title":"Interpreting Conversational Dense Retrieval by Rewriting-Enhanced\n  Inversion of Session Embedding","summary":"  Conversational dense retrieval has shown to be effective in conversational\nsearch. However, a major limitation of conversational dense retrieval is their\nlack of interpretability, hindering intuitive understanding of model behaviors\nfor targeted improvements. This paper presents CONVINV, a simple yet effective\napproach to shed light on interpretable conversational dense retrieval models.\nCONVINV transforms opaque conversational session embeddings into explicitly\ninterpretable text while faithfully maintaining their original retrieval\nperformance as much as possible. Such transformation is achieved by training a\nrecently proposed Vec2Text model based on the ad-hoc query encoder, leveraging\nthe fact that the session and query embeddings share the same space in existing\nconversational dense retrieval. To further enhance interpretability, we propose\nto incorporate external interpretable query rewrites into the transformation\nprocess. Extensive evaluations on three conversational search benchmarks\ndemonstrate that CONVINV can yield more interpretable text and faithfully\npreserve original retrieval performance than baselines. Our work connects\nopaque session embeddings with transparent query rewriting, paving the way\ntoward trustworthy conversational search.\n","authors":["Yiruo Cheng","Kelong Mao","Zhicheng Dou"],"pdf_url":"https://arxiv.org/pdf/2402.12774v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.09234v3","updated":"2024-02-20T06:52:21Z","published":"2023-10-13T16:37:53Z","title":"ClickPrompt: CTR Models are Strong Prompt Generators for Adapting\n  Language Models to CTR Prediction","summary":"  Click-through rate (CTR) prediction has become increasingly indispensable for\nvarious Internet applications. Traditional CTR models convert the multi-field\ncategorical data into ID features via one-hot encoding, and extract the\ncollaborative signals among features. Such a paradigm suffers from the problem\nof semantic information loss. Another line of research explores the potential\nof pretrained language models (PLMs) for CTR prediction by converting input\ndata into textual sentences through hard prompt templates. Although semantic\nsignals are preserved, they generally fail to capture the collaborative\ninformation (e.g., feature interactions, pure ID features), not to mention the\nunacceptable inference overhead brought by the huge model size. In this paper,\nwe aim to model both the semantic knowledge and collaborative knowledge for\naccurate CTR estimation, and meanwhile address the inference inefficiency\nissue. To benefit from both worlds and close their gaps, we propose a novel\nmodel-agnostic framework (i.e., ClickPrompt), where we incorporate CTR models\nto generate interaction-aware soft prompts for PLMs. We design a\nprompt-augmented masked language modeling (PA-MLM) pretraining task, where PLM\nhas to recover the masked tokens based on the language context, as well as the\nsoft prompts generated by CTR model. The collaborative and semantic knowledge\nfrom ID and textual features would be explicitly aligned and interacted via the\nprompt interface. Then, we can either tune the CTR model with PLM for superior\nperformance, or solely tune the CTR model without PLM for inference efficiency.\nExperiments on four real-world datasets validate the effectiveness of\nClickPrompt compared with existing baselines.\n","authors":["Jianghao Lin","Bo Chen","Hangyu Wang","Yunjia Xi","Yanru Qu","Xinyi Dai","Kangning Zhang","Ruiming Tang","Yong Yu","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.09234v3.pdf","comment":"Accepted by WWW 2024"},{"id":"http://arxiv.org/abs/2401.07453v2","updated":"2024-02-20T06:24:03Z","published":"2024-01-15T03:57:15Z","title":"Model Editing at Scale leads to Gradual and Catastrophic Forgetting","summary":"  Editing knowledge in large language models is an attractive capability to\nhave which allows us to correct incorrectly learnt facts during pre-training,\nas well as update the model with an ever-growing list of new facts. While\nexisting model editing techniques have shown promise, they are usually\nevaluated using metrics for reliability, specificity and generalization over\none or few edits. We argue that for model editing to have practical utility, we\nmust be able to make multiple edits to the same model. With this in mind, we\nevaluate the current model editing methods at scale, focusing on two state of\nthe art methods: ROME and MEMIT. We find that as the model is edited\nsequentially with multiple facts, it continually forgets previously edited\nfacts and the ability to perform downstream tasks. This forgetting happens in\ntwo phases -- an initial gradual but progressive forgetting phase followed by\nabrupt or catastrophic forgetting phase. Both gradual and catastrophic\nforgetting limit the usefulness of model editing methods at scale -- the former\nmaking model editing less effective as multiple edits are made to the model\nwhile the latter caps the scalability of such model editing methods. Our\nanalysis also highlights other key limitations of ROME and MEMIT at scale. With\nour work, we push for the development and evaluation of model editing methods\nkeeping scalability in mind.\n","authors":["Akshat Gupta","Anurag Rao","Gopala Anumanchipalli"],"pdf_url":"https://arxiv.org/pdf/2401.07453v2.pdf","comment":"Added experiments with GPT-J and appendix for all samples and\n  ablations"},{"id":"http://arxiv.org/abs/2402.12733v1","updated":"2024-02-20T05:57:01Z","published":"2024-02-20T05:57:01Z","title":"BMLP: Behavior-aware MLP for Heterogeneous Sequential Recommendation","summary":"  In real recommendation scenarios, users often have different types of\nbehaviors, such as clicking and buying. Existing research methods show that it\nis possible to capture the heterogeneous interests of users through different\ntypes of behaviors. However, most multi-behavior approaches have limitations in\nlearning the relationship between different behaviors. In this paper, we\npropose a novel multilayer perceptron (MLP)-based heterogeneous sequential\nrecommendation method, namely behavior-aware multilayer perceptron (BMLP).\nSpecifically, it has two main modules, including a heterogeneous interest\nperception (HIP) module, which models behaviors at multiple granularities\nthrough behavior types and transition relationships, and a purchase intent\nperception (PIP) module, which adaptively fuses subsequences of auxiliary\nbehaviors to capture users' purchase intent. Compared with mainstream sequence\nmodels, MLP is competitive in terms of accuracy and has unique advantages in\nsimplicity and efficiency. Extensive experiments show that BMLP achieves\nsignificant improvement over state-of-the-art algorithms on four public\ndatasets. In addition, its pure MLP architecture leads to a linear time\ncomplexity.\n","authors":["Weixin Li","Yuhao Wu","Yang Liu","Weike Pan","Zhong Ming"],"pdf_url":"https://arxiv.org/pdf/2402.12733v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12728v1","updated":"2024-02-20T05:32:24Z","published":"2024-02-20T05:32:24Z","title":"Modality-Aware Integration with Large Language Models for\n  Knowledge-based Visual Question Answering","summary":"  Knowledge-based visual question answering (KVQA) has been extensively studied\nto answer visual questions with external knowledge, e.g., knowledge graphs\n(KGs). While several attempts have been proposed to leverage large language\nmodels (LLMs) as an implicit knowledge source, it remains challenging since\nLLMs may generate hallucinations. Moreover, multiple knowledge sources, e.g.,\nimages, KGs and LLMs, cannot be readily aligned for complex scenarios. To\ntackle these, we present a novel modality-aware integration with LLMs for KVQA\n(MAIL). It carefully leverages multimodal knowledge for both image\nunderstanding and knowledge reasoning. Specifically, (i) we propose a two-stage\nprompting strategy with LLMs to densely embody the image into a scene graph\nwith detailed visual features; (ii) We construct a coupled concept graph by\nlinking the mentioned entities with external facts. (iii) A tailored\npseudo-siamese graph medium fusion is designed for sufficient multimodal\nfusion. We utilize the shared mentioned entities in two graphs as mediums to\nbridge a tight inter-modal exchange, while maximally preserving insightful\nintra-modal learning by constraining the fusion within mediums. Extensive\nexperiments on two benchmark datasets show the superiority of MAIL with 24x\nless resources.\n","authors":["Junnan Dong","Qinggang Zhang","Huachi Zhou","Daochen Zha","Pai Zheng","Xiao Huang"],"pdf_url":"https://arxiv.org/pdf/2402.12728v1.pdf","comment":"8 pages,3 figures and 1 page appendix; The processed graphs and codes\n  will be avalibale"},{"id":"http://arxiv.org/abs/2401.14606v3","updated":"2024-02-20T04:07:04Z","published":"2024-01-26T02:34:41Z","title":"Challenging Low Homophily in Social Recommendation","summary":"  Social relations are leveraged to tackle the sparsity issue of user-item\ninteraction data in recommendation under the assumption of social homophily.\nHowever, social recommendation paradigms predominantly focus on homophily based\non user preferences. While social information can enhance recommendations, its\nalignment with user preferences is not guaranteed, thereby posing the risk of\nintroducing informational redundancy. We empirically discover that social\ngraphs in real recommendation data exhibit low preference-aware homophily,\nwhich limits the effect of social recommendation models. To comprehensively\nextract preference-aware homophily information latent in the social graph, we\npropose Social Heterophily-alleviating Rewiring (SHaRe), a data-centric\nframework for enhancing existing graph-based social recommendation models. We\nadopt Graph Rewiring technique to capture and add highly homophilic social\nrelations, and cut low homophilic (or heterophilic) relations. To better refine\nthe user representations from reliable social relations, we integrate a\ncontrastive learning method into the training of SHaRe, aiming to calibrate the\nuser representations for enhancing the result of Graph Rewiring. Experiments on\nreal-world datasets show that the proposed framework not only exhibits enhanced\nperformances across varying homophily ratios but also improves the performance\nof existing state-of-the-art (SOTA) social recommendation models.\n","authors":["Wei Jiang","Xinyi Gao","Guandong Xu","Tong Chen","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2401.14606v3.pdf","comment":"This paper has been accepted by The Web Conference (WWW) 2024"},{"id":"http://arxiv.org/abs/2402.11821v2","updated":"2024-02-20T03:23:49Z","published":"2024-02-19T04:29:45Z","title":"Microstructures and Accuracy of Graph Recall by Large Language Models","summary":"  Graphs data is crucial for many applications, and much of it exists in the\nrelations described in textual format. As a result, being able to accurately\nrecall and encode a graph described in earlier text is a basic yet pivotal\nability that LLMs need to demonstrate if they are to perform reasoning tasks\nthat involve graph-structured information. Human performance at graph recall\nhas been studied by cognitive scientists for decades, and has been found to\noften exhibit certain structural patterns of bias that align with human\nhandling of social relationships. To date, however, we know little about how\nLLMs behave in analogous graph recall tasks: do their recalled graphs also\nexhibit certain biased patterns, and if so, how do they compare with humans and\naffect other graph reasoning tasks? In this work, we perform the first\nsystematical study of graph recall by LLMs, investigating the accuracy and\nbiased microstructures (local structural patterns) in their recall. We find\nthat LLMs not only underperform often in graph recall, but also tend to favor\nmore triangles and alternating 2-paths. Moreover, we find that more advanced\nLLMs have a striking dependence on the domain that a real-world graph comes\nfrom -- by yielding the best recall accuracy when the graph is narrated in a\nlanguage style consistent with its original domain.\n","authors":["Yanbang Wang","Hejie Cui","Jon Kleinberg"],"pdf_url":"https://arxiv.org/pdf/2402.11821v2.pdf","comment":"16 pages, 7 tables, 5 figures"},{"id":"http://arxiv.org/abs/2402.12663v1","updated":"2024-02-20T02:23:15Z","published":"2024-02-20T02:23:15Z","title":"SoftQE: Learned Representations of Queries Expanded by LLMs","summary":"  We investigate the integration of Large Language Models (LLMs) into query\nencoders to improve dense retrieval without increasing latency and cost, by\ncircumventing the dependency on LLMs at inference time. SoftQE incorporates\nknowledge from LLMs by mapping embeddings of input queries to those of the\nLLM-expanded queries. While improvements over various strong baselines on\nin-domain MS-MARCO metrics are marginal, SoftQE improves performance by 2.83\nabsolute percentage points on average on five out-of-domain BEIR tasks.\n","authors":["Varad Pimpalkhute","John Heyer","Xusen Yin","Sameer Gupta"],"pdf_url":"https://arxiv.org/pdf/2402.12663v1.pdf","comment":"To be published in ECIR 2024 proceedings"},{"id":"http://arxiv.org/abs/2309.04761v3","updated":"2024-02-20T01:38:11Z","published":"2023-09-09T11:20:40Z","title":"A Comprehensive Survey on Deep Learning Techniques in Educational Data\n  Mining","summary":"  Educational Data Mining (EDM) has emerged as a vital field of research, which\nharnesses the power of computational techniques to analyze educational data.\nWith the increasing complexity and diversity of educational data, Deep Learning\ntechniques have shown significant advantages in addressing the challenges\nassociated with analyzing and modeling this data. This survey aims to\nsystematically review the state-of-the-art in EDM with Deep Learning. We begin\nby providing a brief introduction to EDM and Deep Learning, highlighting their\nrelevance in the context of modern education. Next, we present a detailed\nreview of Deep Learning techniques applied in four typical educational\nscenarios, including knowledge tracing, student behavior detection, performance\nprediction, and personalized recommendation. Furthermore, a comprehensive\noverview of public datasets and processing tools for EDM is provided. Finally,\nwe point out emerging trends and future directions in this research area.\n","authors":["Yuanguo Lin","Hong Chen","Wei Xia","Fan Lin","Zongyue Wang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2309.04761v3.pdf","comment":"19 pages, 6 figures"}],"Multimedia":[{"id":"http://arxiv.org/abs/2304.11783v2","updated":"2024-02-20T23:32:31Z","published":"2023-04-24T00:55:50Z","title":"Rip Current Detection in Nearshore Areas through UAV Video Analysis with\n  Almost Local-Isometric Embedding Techniques on Sphere","summary":"  Rip currents pose a significant danger to those who visit beaches, as they\ncan swiftly pull swimmers away from shore. Detecting these currents currently\nrelies on costly equipment and is challenging to implement on a larger scale.\nThe advent of unmanned aerial vehicles (UAVs) and camera technology, however,\nhas made monitoring near-shore regions more accessible and scalable. This paper\nproposes a new framework for detecting rip currents using video-based methods\nthat leverage optical flow estimation, offshore direction calculation, earth\ncamera projection with almost local-isometric embedding on the sphere, and\ntemporal data fusion techniques. Through the analysis of videos from multiple\nbeaches, including Palm Beach, Haulover, Ocean Reef Park, and South Beach, as\nwell as YouTube footage, we demonstrate the efficacy of our approach, which\naligns with human experts' annotations.\n","authors":["Anchen Sun","Kaiqi Yang"],"pdf_url":"https://arxiv.org/pdf/2304.11783v2.pdf","comment":"10 pages, 9 figures, 3 tables"},{"id":"http://arxiv.org/abs/2402.03190v3","updated":"2024-02-20T16:47:16Z","published":"2024-02-05T16:56:11Z","title":"Unified Hallucination Detection for Multimodal Large Language Models","summary":"  Despite significant strides in multimodal tasks, Multimodal Large Language\nModels (MLLMs) are plagued by the critical issue of hallucination. The reliable\ndetection of such hallucinations in MLLMs has, therefore, become a vital aspect\nof model evaluation and the safeguarding of practical application deployment.\nPrior research in this domain has been constrained by a narrow focus on\nsingular tasks, an inadequate range of hallucination categories addressed, and\na lack of detailed granularity. In response to these challenges, our work\nexpands the investigative horizons of hallucination detection. We present a\nnovel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate\nthe evaluation of advancements in hallucination detection methods.\nAdditionally, we unveil a novel unified multimodal hallucination detection\nframework, UNIHD, which leverages a suite of auxiliary tools to validate the\noccurrence of hallucinations robustly. We demonstrate the effectiveness of\nUNIHD through meticulous evaluation and comprehensive analysis. We also provide\nstrategic insights on the application of specific tools for addressing various\ncategories of hallucinations.\n","authors":["Xiang Chen","Chenxi Wang","Yida Xue","Ningyu Zhang","Xiaoyan Yang","Qiang Li","Yue Shen","Lei Liang","Jinjie Gu","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2402.03190v3.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2305.03582v3","updated":"2024-02-20T16:18:45Z","published":"2023-05-05T14:37:26Z","title":"A multimodal dynamical variational autoencoder for audiovisual speech\n  representation learning","summary":"  In this paper, we present a multimodal and dynamical VAE (MDVAE) applied to\nunsupervised audio-visual speech representation learning. The latent space is\nstructured to dissociate the latent dynamical factors that are shared between\nthe modalities from those that are specific to each modality. A static latent\nvariable is also introduced to encode the information that is constant over\ntime within an audiovisual speech sequence. The model is trained in an\nunsupervised manner on an audiovisual emotional speech dataset, in two stages.\nIn the first stage, a vector quantized VAE (VQ-VAE) is learned independently\nfor each modality, without temporal modeling. The second stage consists in\nlearning the MDVAE model on the intermediate representation of the VQ-VAEs\nbefore quantization. The disentanglement between static versus dynamical and\nmodality-specific versus modality-common information occurs during this second\ntraining stage. Extensive experiments are conducted to investigate how\naudiovisual speech latent factors are encoded in the latent space of MDVAE.\nThese experiments include manipulating audiovisual speech, audiovisual facial\nimage denoising, and audiovisual speech emotion recognition. The results show\nthat MDVAE effectively combines the audio and visual information in its latent\nspace. They also show that the learned static representation of audiovisual\nspeech can be used for emotion recognition with few labeled data, and with\nbetter accuracy compared with unimodal baselines and a state-of-the-art\nsupervised model based on an audiovisual transformer architecture.\n","authors":["Samir Sadok","Simon Leglaive","Laurent Girin","Xavier Alameda-Pineda","Renaud Séguier"],"pdf_url":"https://arxiv.org/pdf/2305.03582v3.pdf","comment":"14 figures, https://samsad35.github.io/site-mdvae/"},{"id":"http://arxiv.org/abs/2402.13022v1","updated":"2024-02-20T14:02:45Z","published":"2024-02-20T14:02:45Z","title":"SoMeLVLM: A Large Vision Language Model for Social Media Processing","summary":"  The growth of social media, characterized by its multimodal nature, has led\nto the emergence of diverse phenomena and challenges, which calls for an\neffective approach to uniformly solve automated tasks. The powerful Large\nVision Language Models make it possible to handle a variety of tasks\nsimultaneously, but even with carefully designed prompting methods, the general\ndomain models often fall short in aligning with the unique speaking style and\ncontext of social media tasks. In this paper, we introduce a Large Vision\nLanguage Model for Social Media Processing (SoMeLVLM), which is a cognitive\nframework equipped with five key capabilities including knowledge &\ncomprehension, application, analysis, evaluation, and creation. SoMeLVLM is\ndesigned to understand and generate realistic social media behavior. We have\ndeveloped a 654k multimodal social media instruction-tuning dataset to support\nour cognitive framework and fine-tune our model. Our experiments demonstrate\nthat SoMeLVLM achieves state-of-the-art performance in multiple social media\ntasks. Further analysis shows its significant advantages over baselines in\nterms of cognitive abilities.\n","authors":["Xinnong Zhang","Haoyu Kuang","Xinyi Mou","Hanjia Lyu","Kun Wu","Siming Chen","Jiebo Luo","Xuanjing Huang","Zhongyu Wei"],"pdf_url":"https://arxiv.org/pdf/2402.13022v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12760v1","updated":"2024-02-20T06:58:49Z","published":"2024-02-20T06:58:49Z","title":"A User-Friendly Framework for Generating Model-Preferred Prompts in\n  Text-to-Image Synthesis","summary":"  Well-designed prompts have demonstrated the potential to guide text-to-image\nmodels in generating amazing images. Although existing prompt engineering\nmethods can provide high-level guidance, it is challenging for novice users to\nachieve the desired results by manually entering prompts due to a discrepancy\nbetween novice-user-input prompts and the model-preferred prompts. To bridge\nthe distribution gap between user input behavior and model training datasets,\nwe first construct a novel Coarse-Fine Granularity Prompts dataset (CFP) and\npropose a novel User-Friendly Fine-Grained Text Generation framework (UF-FGTG)\nfor automated prompt optimization. For CFP, we construct a novel dataset for\ntext-to-image tasks that combines coarse and fine-grained prompts to facilitate\nthe development of automated prompt generation methods. For UF-FGTG, we propose\na novel framework that automatically translates user-input prompts into\nmodel-preferred prompts. Specifically, we propose a prompt refiner that\ncontinually rewrites prompts to empower users to select results that align with\ntheir unique needs. Meanwhile, we integrate image-related loss functions from\nthe text-to-image model into the training process of text generation to\ngenerate model-preferred prompts. Additionally, we propose an adaptive feature\nextraction module to ensure diversity in the generated results. Experiments\ndemonstrate that our approach is capable of generating more visually appealing\nand diverse images than previous state-of-the-art methods, achieving an average\nimprovement of 5% across six quality and aesthetic metrics.\n","authors":["Nailei Hei","Qianyu Guo","Zihao Wang","Yan Wang","Haofen Wang","Wenqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.12760v1.pdf","comment":"Accepted by The 38th Annual AAAI Conference on Artificial\n  Intelligence (AAAI 2024)"},{"id":"http://arxiv.org/abs/2402.12629v1","updated":"2024-02-20T01:20:31Z","published":"2024-02-20T01:20:31Z","title":"Television Discourse Decoded: Comprehensive Multimodal Analytics at\n  Scale","summary":"  In this paper, we tackle the complex task of analyzing televised debates,\nwith a focus on a prime time news debate show from India. Previous methods,\nwhich often relied solely on text, fall short in capturing the multimedia\nessence of these debates. To address this gap, we introduce a comprehensive\nautomated toolkit that employs advanced computer vision and speech-to-text\ntechniques for large-scale multimedia analysis. Utilizing state-of-the-art\ncomputer vision algorithms and speech-to-text methods, we transcribe, diarize,\nand analyze thousands of YouTube videos of prime-time television debates in\nIndia. These debates are a central part of Indian media but have been\ncriticized for compromised journalistic integrity and excessive dramatization.\nOur toolkit provides concrete metrics to assess bias and incivility, capturing\na comprehensive multimedia perspective that includes text, audio utterances,\nand video frames. Our findings reveal significant biases in topic selection and\npanelist representation, along with alarming levels of incivility. This work\noffers a scalable, automated approach for future research in multimedia\nanalysis, with profound implications for the quality of public discourse and\ndemocratic debate. We will make our data analysis pipeline and collected data\npublicly available to catalyze further research in this domain.\n","authors":["Anmol Agarwal","Pratyush Priyadarshi","Shiven Sinha","Shrey Gupta","Hitkul Jangra","Kiran Garimella","Ponnurangam Kumaraguru"],"pdf_url":"https://arxiv.org/pdf/2402.12629v1.pdf","comment":null}]}}