<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2024-02-27T00:00:00Z">2024-02-27</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">106</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17764v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17764v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research, such as BitNet, is paving the way for a new era of 1-bit
Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant,
namely BitNet b1.58, in which every single parameter (or weight) of the LLM is
ternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16)
Transformer LLM with the same model size and training tokens in terms of both
perplexity and end-task performance, while being significantly more
cost-effective in terms of latency, memory, throughput, and energy consumption.
More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for
training new generations of LLMs that are both high-performance and
cost-effective. Furthermore, it enables a new computation paradigm and opens
the door for designing specific hardware optimized for 1-bit LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Massive Activations in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingjie Sun, Xinlei Chen, J. Zico Kolter, Zhuang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We observe an empirical phenomenon in Large Language Models (LLMs) -- very
few activations exhibit significantly larger values than others (e.g., 100,000
times larger). We call them massive activations. First, we demonstrate the
widespread existence of massive activations across various LLMs and
characterize their locations. Second, we find their values largely stay
constant regardless of the input, and they function as indispensable bias terms
in LLMs. Third, these massive activations lead to the concentration of
attention probabilities to their corresponding tokens, and further, implicit
bias terms in the self-attention output. Last, we also study massive
activations in Vision Transformers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website at
  https://eric-mingjie.github.io/massive-activations/index.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Optimal Learning of Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxian Gu, Li Dong, Yaru Hao, Qingxiu Dong, Minlie Huang, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work studies the general principles of improving the learning of
language models (LMs), which aims at reducing the necessary training steps for
achieving superior performance. Specifically, we present a theory for the
optimal learning of LMs. We first propose an objective that optimizes LM
learning by maximizing the data compression ratio in an
"LM-training-as-lossless-compression" view. Then, we derive a theorem, named
Learning Law, to reveal the properties of the dynamics in the optimal learning
process under our objective. The theorem is then validated by experiments on a
linear classification and a real-world language modeling task. Finally, we
empirically verify that the optimal learning of LMs essentially stems from the
improvement of the coefficients in the scaling law of LMs, indicating great
promise and significance for designing practical learning acceleration methods.
Our code can be found at https://aka.ms/LearningLaw.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Very Long-Term Conversational Memory of LLM Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, Yuwei Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing works on long-term open-domain dialogues focus on evaluating model
responses within contexts spanning no more than five chat sessions. Despite
advancements in long-context large language models (LLMs) and retrieval
augmented generation (RAG) techniques, their efficacy in very long-term
dialogues remains unexplored. To address this research gap, we introduce a
machine-human pipeline to generate high-quality, very long-term dialogues by
leveraging LLM-based agent architectures and grounding their dialogues on
personas and temporal event graphs. Moreover, we equip each agent with the
capability of sharing and reacting to images. The generated conversations are
verified and edited by human annotators for long-range consistency and
grounding to the event graphs. Using this pipeline, we collect LoCoMo, a
dataset of very long-term conversations, each encompassing 300 turns and 9K
tokens on avg., over up to 35 sessions. Based on LoCoMo, we present a
comprehensive evaluation benchmark to measure long-term memory in models,
encompassing question answering, event summarization, and multi-modal dialogue
generation tasks. Our experimental results indicate that LLMs exhibit
challenges in understanding lengthy conversations and comprehending long-range
temporal and causal dynamics within dialogues. Employing strategies like
long-context LLMs or RAG can offer improvements but these models still
substantially lag behind human performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages; Project page: https://snap-research.github.io/locomo/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tower: An Open Multilingual Large Language Model for Translation-Related
  Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duarte M. Alves, José Pombal, Nuno M. Guerreiro, Pedro H. Martins, João Alves, Amin Farajian, Ben Peters, Ricardo Rei, Patrick Fernandes, Sweta Agrawal, Pierre Colombo, José G. C. de Souza, André F. T. Martins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While general-purpose large language models (LLMs) demonstrate proficiency on
multiple tasks within the domain of translation, approaches based on open LLMs
are competitive only when specializing on a single task. In this paper, we
propose a recipe for tailoring LLMs to multiple tasks present in translation
workflows. We perform continued pretraining on a multilingual mixture of
monolingual and parallel data, creating TowerBase, followed by finetuning on
instructions relevant for translation processes, creating TowerInstruct. Our
final model surpasses open alternatives on several tasks relevant to
translation workflows and is competitive with general-purpose closed LLMs. To
facilitate future research, we release the Tower models, our specialization
dataset, an evaluation framework for LLMs focusing on the translation
ecosystem, and a collection of model generations, including ours, on our
benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AmbigNLG: Addressing Task Ambiguity in Instruction for NLG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayana Niwa, Hayate Iso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we introduce AmbigNLG, a new task designed to tackle the
challenge of task ambiguity in instructions for Natural Language Generation
(NLG) tasks. Despite the impressive capabilities of Large Language Models
(LLMs) in understanding and executing a wide range of tasks through natural
language interaction, their performance is significantly hindered by the
ambiguity present in real-world instructions. To address this, AmbigNLG seeks
to identify and mitigate such ambiguities, aiming to refine instructions to
match user expectations better. We introduce a dataset, AmbigSNI-NLG,
consisting of 2,500 instances, and develop an ambiguity taxonomy for
categorizing and annotating instruction ambiguities. Our approach demonstrates
substantial improvements in text generation quality, highlighting the critical
role of clear and specific instructions in enhancing LLM performance in NLG
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Case-Based or Rule-Based: How Do <span class="highlight-title">Transformer</span>s Do the Math? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17709v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17709v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Hu, Xiaojuan Tang, Haotong Yang, Muhan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the impressive performance in a variety of complex tasks, modern
large language models (LLMs) still have trouble dealing with some math problems
that are simple and intuitive for humans, such as addition. While we can easily
learn basic rules of addition and apply them to new problems of any length,
LLMs struggle to do the same. Instead, they may rely on similar "cases" seen in
the training corpus for help. We define these two different reasoning
mechanisms as "rule-based reasoning" and "case-based reasoning". Since
rule-based reasoning is essential for acquiring the systematic generalization
ability, we aim to explore exactly whether transformers use rule-based or
case-based reasoning for math problems. Through carefully designed intervention
experiments on five math tasks, we confirm that transformers are performing
case-based reasoning, no matter whether scratchpad is used, which aligns with
the previous observations that transformers use subgraph matching/shortcut
learning to reason. To mitigate such problems, we propose a Rule-Following
Fine-Tuning (RFFT) technique to teach transformers to perform rule-based
reasoning. Specifically, we provide explicit rules in the input and then
instruct transformers to recite and follow the rules step by step. Through
RFFT, we successfully enable LLMs fine-tuned on 1-5 digit addition to
generalize to up to 12-digit addition with over 95% accuracy, which is over 40%
higher than scratchpad. The significant improvement demonstrates that teaching
LLMs to explicitly use rules helps them learn rule-based reasoning and
generalize better in length.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAVEL: Evaluating Interpretability Methods on Disentangling Language
  Model Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17700v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17700v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Huang, Zhengxuan Wu, Christopher Potts, Mor Geva, Atticus Geiger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Individual neurons participate in the representation of multiple high-level
concepts. To what extent can different interpretability methods successfully
disentangle these roles? To help address this question, we introduce RAVEL
(Resolving Attribute-Value Entanglements in Language Models), a dataset that
enables tightly controlled, quantitative comparisons between a variety of
existing interpretability methods. We use the resulting conceptual framework to
define the new method of Multi-task Distributed Alignment Search (MDAS), which
allows us to find distributed representations satisfying multiple causal
criteria. With Llama2-7B as the target language model, MDAS achieves
state-of-the-art results on RAVEL, demonstrating the importance of going beyond
neuron-level analyses to identify features distributed across activations. We
release our benchmark at https://github.com/explanare/ravel.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NextLevel<span class="highlight-title">BERT</span>: Investigating Masked Language Modeling with Higher-Level
  Representations for Long Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tamara Czinczoll, Christoph Hönes, Maximilian Schall, Gerard de Melo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While (large) language models have significantly improved over the last
years, they still struggle to sensibly process long sequences found, e.g., in
books, due to the quadratic scaling of the underlying attention mechanism. To
address this, we propose NextLevelBERT, a Masked Language Model operating not
on tokens, but on higher-level semantic representations in the form of text
embeddings. We pretrain NextLevelBERT to predict the vector representation of
entire masked text chunks and evaluate the effectiveness of the resulting
document vectors on three task types: 1) Semantic Textual Similarity via
zero-shot document embeddings, 2) Long document classification, 3)
Multiple-choice question answering. We find that next level Masked Language
Modeling is an effective technique to tackle long-document use cases and can
outperform much larger embedding models as long as the required level of detail
is not too high. We make model and code available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond <span class="highlight-title">prompt</span> brittleness: Evaluating the reliability and consistency of
  political worldviews in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanise Ceron, Neele Falk, Ana Barić, Dmitry Nikolaev, Sebastian Padó
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the widespread use of large language models (LLMs) in ubiquitous
systems, we need to understand whether they embed a specific worldview and what
these views reflect. Recent studies report that, prompted with political
questionnaires, LLMs show left-liberal leanings. However, it is as yet unclear
whether these leanings are reliable (robust to prompt variations) and whether
the leaning is consistent across policies and political leaning. We propose a
series of tests which assess the reliability and consistency of LLMs' stances
on political statements based on a dataset of voting-advice questionnaires
collected from seven EU countries and annotated for policy domains. We study
LLMs ranging in size from 7B to 70B parameters and find that their reliability
increases with parameter count. Larger models show overall stronger alignment
with left-leaning parties but differ among policy programs: They evince a
(left-wing) positive stance towards environment protection, social welfare but
also (right-wing) law and order, with no consistent preferences in foreign
policy, migration, and economy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SongComposer: A Large Language Model for Lyric and Melody Composition in
  Song Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17645v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17645v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuangrui Ding, Zihan Liu, Xiaoyi Dong, Pan Zhang, Rui Qian, Conghui He, Dahua Lin, Jiaqi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present SongComposer, an innovative LLM designed for song composition. It
could understand and generate melodies and lyrics in symbolic song
representations, by leveraging the capability of LLM. Existing music-related
LLM treated the music as quantized audio signals, while such implicit encoding
leads to inefficient encoding and poor flexibility. In contrast, we resort to
symbolic song representation, the mature and efficient way humans designed for
music, and enable LLM to explicitly compose songs like humans. In practice, we
design a novel tuple design to format lyric and three note attributes (pitch,
duration, and rest duration) in the melody, which guarantees the correct LLM
understanding of musical symbols and realizes precise alignment between lyrics
and melody. To impart basic music understanding to LLM, we carefully collected
SongCompose-PT, a large-scale song pretraining dataset that includes lyrics,
melodies, and paired lyrics-melodies in either Chinese or English. After
adequate pre-training, 10K carefully crafted QA pairs are used to empower the
LLM with the instruction-following capability and solve diverse tasks. With
extensive experiments, SongComposer demonstrates superior performance in
lyric-to-melody generation, melody-to-lyric generation, song continuation, and
text-to-song creation, outperforming advanced LLMs like GPT-4.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page: https://pjlab-songcomposer.github.io/ code:
  https://github.com/pjlab-songcomposer/songcomposer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are LLMs Capable of Data-based Statistical and Causal Reasoning?
  Benchmarking Advanced Quantitative Reasoning with Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17644v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17644v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Liu, Zirui Wu, Xueqing Wu, Pan Lu, Kai-Wei Chang, Yansong Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantitative reasoning is a critical skill to analyze data, yet the
assessment of such ability remains limited. To address this gap, we introduce
the Quantitative Reasoning with Data (QRData) benchmark, aiming to evaluate
Large Language Models' capability in statistical and causal reasoning with
real-world data. The benchmark comprises a carefully constructed dataset of 411
questions accompanied by data sheets from textbooks, online learning materials,
and academic papers. To compare models' quantitative reasoning abilities on
data and text, we enrich the benchmark with an auxiliary set of 290 text-only
questions, namely QRText. We evaluate natural language reasoning, program-based
reasoning, and agent reasoning methods including Chain-of-Thought,
Program-of-Thoughts, ReAct, and code interpreter assistants on diverse models.
The strongest model GPT-4 achieves an accuracy of 58%, which has a large room
for improvement. Among open-source models, Deepseek-coder-instruct, a code LLM
pretrained on 2T tokens, gets the highest accuracy of 37%. Analysis reveals
that models encounter difficulties in data analysis and causal reasoning, and
struggle in using causal knowledge and provided data simultaneously. Code and
data are in https://github.com/xxxiaol/QRData.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://xxxiaol.github.io/QRData/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Variational Learning is Effective for Large Deep Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17641v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17641v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuesong Shen, Nico Daheim, Bai Cong, Peter Nickl, Gian Maria Marconi, Clement Bazan, Rio Yokota, Iryna Gurevych, Daniel Cremers, Mohammad Emtiyaz Khan, Thomas Möllenhoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We give extensive empirical evidence against the common belief that
variational learning is ineffective for large neural networks. We show that an
optimizer called Improved Variational Online Newton (IVON) consistently matches
or outperforms Adam for training large networks such as GPT-2 and ResNets from
scratch. IVON's computational costs are nearly identical to Adam but its
predictive uncertainty is better. We show several new use cases of IVON where
we improve fine-tuning and model merging in Large Language Models, accurately
predict generalization error, and faithfully estimate sensitivity to data. We
find overwhelming evidence in support of effectiveness of variational learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally. Code is available here:
  https://github.com/team-approx-bayes/ivon</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Text Segmentation to Smart Chaptering: A Novel Benchmark for
  Structuring Video Transcriptions <span class="chip">EACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Retkowski, Alexander Waibel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text segmentation is a fundamental task in natural language processing, where
documents are split into contiguous sections. However, prior research in this
area has been constrained by limited datasets, which are either small in scale,
synthesized, or only contain well-structured documents. In this paper, we
address these limitations by introducing a novel benchmark YTSeg focusing on
spoken content that is inherently more unstructured and both topically and
structurally diverse. As part of this work, we introduce an efficient
hierarchical segmentation model MiniSeg, that outperforms state-of-the-art
baselines. Lastly, we expand the notion of text segmentation to a more
practical "smart chaptering" task that involves the segmentation of
unstructured content, the generation of meaningful segment titles, and a
potential real-time application of the models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-Grained Natural Language Inference Based Faithfulness Evaluation
  for Diverse Summarisation Tasks <span class="chip">EACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huajian Zhang, Yumo Xu, Laura Perez-Beltrachini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study existing approaches to leverage off-the-shelf Natural Language
Inference (NLI) models for the evaluation of summary faithfulness and argue
that these are sub-optimal due to the granularity level considered for premises
and hypotheses. That is, the smaller content unit considered as hypothesis is a
sentence and premises are made up of a fixed number of document sentences. We
propose a novel approach, namely InFusE, that uses a variable premise size and
simplifies summary sentences into shorter hypotheses. Departing from previous
studies which focus on single short document summarisation, we analyse NLI
based faithfulness evaluation for diverse summarisation tasks. We introduce
DiverSumm, a new benchmark comprising long form summarisation (long documents
and summaries) and diverse summarisation tasks (e.g., meeting and
multi-document summarisation). In experiments, InFusE obtains superior
performance across the different summarisation tasks. Our code and data are
available at https://github.com/HJZnlp/infuse.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Automated Writing Evaluation with Corrective Feedback <span class="chip">NAACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Izia Xiaoxiao Wang, Xihan Wu, Edith Coates, Min Zeng, Jiexin Kuang, Siliang Liu, Mengyang Qiu, Jungyeul Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The utilization of technology in second language learning and teaching has
become ubiquitous. For the assessment of writing specifically, automated
writing evaluation (AWE) and grammatical error correction (GEC) have become
immensely popular and effective methods for enhancing writing proficiency and
delivering instant and individualized feedback to learners. By leveraging the
power of natural language processing (NLP) and machine learning algorithms, AWE
and GEC systems have been developed separately to provide language learners
with automated corrective feedback and more accurate and unbiased scoring that
would otherwise be subject to examiners. In this paper, we propose an
integrated system for automated writing evaluation with corrective feedback as
a means of bridging the gap between AWE and GEC results for second language
learners. This system enables language learners to simulate the essay writing
tests: a student writes and submits an essay, and the system returns the
assessment of the writing along with suggested grammatical error corrections.
Given that automated scoring and grammatical correction are more efficient and
cost-effective than human grading, this integrated system would also alleviate
the burden of manually correcting innumerable essays.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the system demonstration track at NAACL-HLT 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Linguistic Knowledge Can Enhance Encoder-Decoder Models (If You Let It) <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessio Miaschi, Felice Dell'Orletta, Giulia Venturi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we explore the impact of augmenting pre-trained
Encoder-Decoder models, specifically T5, with linguistic knowledge for the
prediction of a target task. In particular, we investigate whether fine-tuning
a T5 model on an intermediate task that predicts structural linguistic
properties of sentences modifies its performance in the target task of
predicting sentence-level complexity. Our study encompasses diverse experiments
conducted on Italian and English datasets, employing both monolingual and
multilingual T5 models at various sizes. Results obtained for both languages
and in cross-lingual configurations show that linguistically motivated
intermediate fine-tuning has generally a positive impact on target task
performance, especially when applied to smaller models and in scenarios with
limited data availability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FaultProfIT: Hierarchical Fault Profiling of Incident Tickets in
  Large-scale Cloud Systems <span class="chip">ICSE</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Huang, Jinyang Liu, Zhuangbin Chen, Zhihan Jiang, Yichen LI, Jiazhen Gu, Cong Feng, Zengyin Yang, Yongqiang Yang, Michael R. Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Postmortem analysis is essential in the management of incidents within cloud
systems, which provides valuable insights to improve system's reliability and
robustness. At CloudA, fault pattern profiling is performed during the
postmortem phase, which involves the classification of incidents' faults into
unique categories, referred to as fault pattern. By aggregating and analyzing
these fault patterns, engineers can discern common faults, vulnerable
components and emerging fault trends. However, this process is currently
conducted by manual labeling, which has inherent drawbacks. On the one hand,
the sheer volume of incidents means only the most severe ones are analyzed,
causing a skewed overview of fault patterns. On the other hand, the complexity
of the task demands extensive domain knowledge, which leads to errors and
inconsistencies. To address these limitations, we propose an automated
approach, named FaultProfIT, for Fault pattern Profiling of Incident Tickets.
It leverages hierarchy-guided contrastive learning to train a hierarchy-aware
incident encoder and predicts fault patterns with enhanced incident
representations. We evaluate FaultProfIT using the production incidents from
CloudA. The results demonstrate that FaultProfIT outperforms state-of-the-art
methods. Our ablation study and analysis also verify the effectiveness of
hierarchy-guided contrastive learning. Additionally, we have deployed
FaultProfIT at CloudA for six months. To date, FaultProfIT has analyzed 10,000+
incidents from 30+ cloud services, successfully revealing several fault trends
that have informed system improvements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Proceedings of the 46th International Conference on
  Software Engineering: Software Engineering in Practice (ICSE SEIP 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agent-Pro: Learning to Evolve via Policy-Level Reflection and
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqi Zhang, Ke Tang, Hai Wu, Mengna Wang, Yongliang Shen, Guiyang Hou, Zeqi Tan, Peng Li, Yueting Zhuang, Weiming Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models exhibit robust problem-solving capabilities for diverse
tasks. However, most LLM-based agents are designed as specific task solvers
with sophisticated prompt engineering, rather than agents capable of learning
and evolving through interactions. These task solvers necessitate manually
crafted prompts to inform task rules and regulate LLM behaviors, inherently
incapacitating to address complex dynamic scenarios e.g., large interactive
games. In light of this, we propose Agent-Pro: an LLM-based Agent with
Policy-level Reflection and Optimization that can learn a wealth of expertise
from interactive experiences and progressively elevate its behavioral policy.
Specifically, it involves a dynamic belief generation and reflection process
for policy evolution. Rather than action-level reflection, Agent-Pro
iteratively reflects on past trajectories and beliefs, fine-tuning its
irrational beliefs for a better policy. Moreover, a depth-first search is
employed for policy optimization, ensuring continual enhancement in policy
payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em,
outperforming vanilla LLM and specialized models. Our results show Agent-Pro
can learn and evolve in complex and dynamic scenes, which also benefits
numerous LLM-based applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LLM-based Agent</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unleashing the Potential of Large Language Models as <span class="highlight-title">Prompt</span> Optimizers:
  An Analogical Analysis with Gradient-based Model Optimizers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Tang, Xiaolei Wang, Wayne Xin Zhao, Siyuan Lu, Yaliang Li, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic prompt optimization is an important approach to improving the
performance of large language models (LLMs). Recent research demonstrates the
potential of using LLMs as prompt optimizers, which can generate improved task
prompts via iterative refinement. In this paper, we propose a novel perspective
to investigate the design of LLM-based prompt optimizers, by drawing an analogy
with gradient-based model optimizers. To connect these two approaches, we
identify two pivotal factors in model parameter learning: update direction and
update method. Focused on the two aspects, we borrow the theoretical framework
and learning methods from gradient-based optimization to design improved
strategies for LLM-based prompt optimizers. By systematically analyzing a rich
set of improvement strategies, we further develop a capable Gradient-inspired
LLM-based Prompt Optimizer called GPO. At each step, it first retrieves
relevant prompts from the optimization trajectory as the update direction.
Then, it utilizes the generation-based refinement strategy to perform the
update, while controlling the edit distance through a cosine-based decay
strategy. Extensive experiments demonstrate the effectiveness and efficiency of
GPO. In particular, GPO brings an additional improvement of up to 56.8% on
Big-Bench Hard and 55.3% on MMLU compared to baseline methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OmniACT: A <span class="highlight-title">Dataset</span> and Benchmark for Enabling Multimodal Generalist
  Autonomous Agents for Desktop and Web 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17553v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17553v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran Kamble, Waseem Alshikh, Ruslan Salakhutdinov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For decades, human-computer interaction has fundamentally been manual. Even
today, almost all productive work done on the computer necessitates human input
at every step. Autonomous virtual agents represent an exciting step in
automating many of these menial tasks. Virtual agents would empower users with
limited technical proficiency to harness the full possibilities of computer
systems. They could also enable the efficient streamlining of numerous computer
tasks, ranging from calendar management to complex travel bookings, with
minimal human intervention. In this paper, we introduce OmniACT, the
first-of-a-kind dataset and benchmark for assessing an agent's capability to
generate executable programs to accomplish computer tasks. Our scope extends
beyond traditional web automation, covering a diverse range of desktop
applications. The dataset consists of fundamental tasks such as "Play the next
song", as well as longer horizon tasks such as "Send an email to John Doe
mentioning the time and place to meet". Specifically, given a pair of screen
image and a visually-grounded natural language task, the goal is to generate a
script capable of fully executing the task. We run several strong baseline
language model agents on our benchmark. The strongest baseline, GPT-4, performs
the best on our benchmark However, its performance level still reaches only 15%
of the human proficiency in generating executable scripts capable of completing
the task, demonstrating the challenge of our task for conventional web agents.
Our benchmark provides a platform to measure and evaluate the progress of
language model agents in automating computer tasks and motivates future work
towards building multimodal models that bridge large language models and the
visual grounding of computer screens.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ COCOA: CBT-based Conversational Counseling Agent using Memory
  Specialized in Cognitive Distortions and Dynamic <span class="highlight-title">Prompt</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17546v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17546v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suyeon Lee, Jieun Kang, Harim Kim, Kyoung-Mee Chung, Dongha Lee, Jinyoung Yeo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The demand for conversational agents that provide mental health care is
consistently increasing. In this work, we develop a psychological counseling
agent, referred to as CoCoA, that applies Cognitive Behavioral Therapy (CBT)
techniques to identify and address cognitive distortions inherent in the
client's statements. Specifically, we construct a memory system to efficiently
manage information necessary for counseling while extracting high-level
insights about the client from their utterances. Additionally, to ensure that
the counseling agent generates appropriate responses, we introduce dynamic
prompting to flexibly apply CBT techniques and facilitate the appropriate
retrieval of information. We conducted dialogues between CoCoA and characters
from Character.ai, creating a dataset for evaluation. Then, we asked GPT to
evaluate the constructed counseling dataset, and our model demonstrated a
statistically significant difference from other models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retrieval is Accurate Generation <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Cao, Deng Cai, Leyang Cui, Xuxin Cheng, Wei Bi, Yuexian Zou, Shuming Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Standard language models generate text by selecting tokens from a fixed,
finite, and standalone vocabulary. We introduce a novel method that selects
context-aware phrases from a collection of supporting documents. One of the
most significant challenges for this paradigm shift is determining the training
oracles, because a string of text can be segmented in various ways and each
segment can be retrieved from numerous possible documents. To address this, we
propose to initialize the training oracles using linguistic heuristics and,
more importantly, bootstrap the oracles through iterative self-reinforcement.
Extensive experiments show that our model not only outperforms standard
language models on a variety of knowledge-intensive tasks but also demonstrates
improved generation quality in open-ended text generation. For instance,
compared to the standard language model counterpart, our model raises the
accuracy from 23.47% to 36.27% on OpenbookQA, and improves the MAUVE score from
42.61% to 81.58% in open-ended text generation. Remarkably, our model also
achieves the best performance and the lowest latency among several
retrieval-augmented baselines. In conclusion, we assert that retrieval is more
accurate generation and hope that our work will encourage further research on
this new paradigm shift.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nissist: An Incident Mitigation Copilot based on Troubleshooting Guides 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17531v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17531v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaikai An, Fangkai Yang, Liqun Li, Zhixing Ren, Hao Huang, Lu Wang, Pu Zhao, Yu Kang, Hua Ding, Qingwei Lin, Saravan Rajmohan, Qi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective incident management is pivotal for the smooth operation of
enterprises-level cloud services. In order to expedite incident mitigation,
service teams compile troubleshooting knowledge into Troubleshooting Guides
(TSGs) accessible to on-call engineers (OCEs). While automated pipelines are
enabled to resolve the most frequent and easy incidents, there still exist
complex incidents that require OCEs' intervention. However, TSGs are often
unstructured and incomplete, which requires manual interpretation by OCEs,
leading to on-call fatigue and decreased productivity, especially among
new-hire OCEs. In this work, we propose Nissist which leverages TSGs and
incident mitigation histories to provide proactive suggestions, reducing human
intervention. Leveraging Large Language Models (LLM), Nissist extracts insights
from unstructured TSGs and historical incident mitigation discussions, forming
a comprehensive knowledge base. Its multi-agent system design enhances
proficiency in precisely discerning user queries, retrieving relevant
information, and delivering systematic plans consecutively. Through our user
case and experiment, we demonstrate that Nissist significant reduce Time to
Mitigate (TTM) in incident mitigation, alleviating operational burdens on OCEs
and improving service reliability. Our demo is available at
https://aka.ms/nissist_demo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predict the Next Word: <Humans exhibit uncertainty in this task and
  language models _____> <span class="chip">EACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evgenia Ilia, Wilker Aziz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs) are statistical models trained to assign probability to
human-generated text. As such, it is reasonable to question whether they
approximate linguistic variability exhibited by humans well. This form of
statistical assessment is difficult to perform at the passage level, for it
requires acceptability judgements (i.e., human evaluation) or a robust
automated proxy (which is non-trivial). At the word level, however, given some
context, samples from an LM can be assessed via exact matching against a
prerecorded dataset of alternative single-word continuations of the available
context. We exploit this fact and evaluate the LM's ability to reproduce
variability that humans (in particular, a population of English speakers)
exhibit in the 'next word prediction' task. This can be seen as assessing a
form of calibration, which, in the context of text classification, Baan et al.
(2022) termed calibration to human uncertainty. We assess GPT2, BLOOM and
ChatGPT and find that they exhibit fairly low calibration to human uncertainty.
We also verify the failure of expected calibration error (ECE) to reflect this,
and as such, advise the community against relying on it in this setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, EACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Latent Attention for Linear Time <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rares Dolga, Marius Cobzarenco, David Barber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The time complexity of the standard attention mechanism in a transformer
scales quadratically with the length of the sequence. We introduce a method to
reduce this to linear scaling with time, based on defining attention via latent
vectors. The method is readily usable as a drop-in replacement for the standard
attention mechanism. Our "Latte Transformer" model can be implemented for both
bidirectional and unidirectional tasks, with the causal version allowing a
recurrent implementation which is memory and time-efficient during inference of
language generation tasks. Whilst next token prediction scales linearly with
the sequence length for a standard transformer, a Latte Transformer requires
constant time to compute the next token. The empirical performance of our
method is comparable to standard attention, yet allows scaling to context
windows much larger than practical in standard attention.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extreme Miscalibration and the Illusion of Adversarial Robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17509v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17509v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vyas Raina, Samson Tan, Volkan Cevher, Aditya Rawal, Sheng Zha, George Karypis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning-based Natural Language Processing (NLP) models are vulnerable
to adversarial attacks, where small perturbations can cause a model to
misclassify. Adversarial Training (AT) is often used to increase model
robustness. However, we have discovered an intriguing phenomenon: deliberately
or accidentally miscalibrating models masks gradients in a way that interferes
with adversarial attack search methods, giving rise to an apparent increase in
robustness. We show that this observed gain in robustness is an illusion of
robustness (IOR), and demonstrate how an adversary can perform various forms of
test-time temperature calibration to nullify the aforementioned interference
and allow the adversarial attack to find adversarial examples. Hence, we urge
the NLP community to incorporate test-time temperature scaling into their
robustness evaluations to ensure that any observed gains are genuine. Finally,
we show how the temperature can be scaled during \textit{training} to improve
genuine robustness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BASES: Large-scale Web Search User Simulation with Large Language Model
  based Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17505v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17505v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiyang Ren, Peng Qiu, Yingqi Qu, Jing Liu, Wayne Xin Zhao, Hua Wu, Ji-Rong Wen, Haifeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the excellent capacities of large language models (LLMs), it becomes
feasible to develop LLM-based agents for reliable user simulation. Considering
the scarcity and limit (e.g., privacy issues) of real user data, in this paper,
we conduct large-scale user simulation for web search, to improve the analysis
and modeling of user search behavior. Specially, we propose BASES, a novel user
simulation framework with LLM-based agents, designed to facilitate
comprehensive simulations of web search user behaviors. Our simulation
framework can generate unique user profiles at scale, which subsequently leads
to diverse search behaviors. To demonstrate the effectiveness of BASES, we
conduct evaluation experiments based on two human benchmarks in both Chinese
and English, demonstrating that BASES can effectively simulate large-scale
human-like search behaviors. To further accommodate the research on web search,
we develop WARRIORS, a new large-scale dataset encompassing web search user
behaviors, including both Chinese and English versions, which can greatly
bolster research in the field of information retrieval. Our code and data will
be publicly released soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain
  Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Wang, Ruiyang Ren, Junyi Li, Wayne Xin Zhao, Jing Liu, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Considering the limited internal parametric knowledge, retrieval-augmented
generation (RAG) has been widely used to extend the knowledge scope of large
language models (LLMs). Despite the extensive efforts on RAG research, in
existing methods, LLMs cannot precisely assess the relevance of retrieved
documents, thus likely leading to misleading or even incorrect utilization of
external knowledge (i.e., retrieved documents). To address this issue, in this
paper, we propose REAR, a RElevance-Aware Retrieval-augmented approach for
open-domain question answering (QA). As the key motivation, we aim to enhance
the self-awareness of source relevance for LLMs, so as to adaptively utilize
external knowledge in RAG systems. Specially, we develop a new architecture for
LLM based RAG system, by incorporating a specially designed rank head that
precisely assesses the relevance of retrieved documents. Furthermore, we
propose an improved training method based on bi-granularity relevance fusion
and noise-resistant training. By combining the improvements in both
architecture and training, our proposed REAR can better utilize external
knowledge by effectively perceiving the relevance of retrieved documents.
Experiments on four open-domain QA tasks show that REAR significantly
outperforms previous a number of competitive RAG approaches. Our code and data
can be accessed at https://github.com/RUCAIBox/REAR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emotional Voice Messages (EMOVOME) database: emotion recognition in
  spontaneous voice messages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucía Gómez Zaragozá, Rocío del Amor, Elena Parra Vargas, Valery Naranjo, Mariano Alcañiz Raya, Javier Marín-Morales
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotional Voice Messages (EMOVOME) is a spontaneous speech dataset containing
999 audio messages from real conversations on a messaging app from 100 Spanish
speakers, gender balanced. Voice messages were produced in-the-wild conditions
before participants were recruited, avoiding any conscious bias due to
laboratory environment. Audios were labeled in valence and arousal dimensions
by three non-experts and two experts, which were then combined to obtain a
final label per dimension. The experts also provided an extra label
corresponding to seven emotion categories. To set a baseline for future
investigations using EMOVOME, we implemented emotion recognition models using
both speech and audio transcriptions. For speech, we used the standard eGeMAPS
feature set and support vector machines, obtaining 49.27% and 44.71% unweighted
accuracy for valence and arousal respectively. For text, we fine-tuned a
multilingual BERT model and achieved 61.15% and 47.43% unweighted accuracy for
valence and arousal respectively. This database will significantly contribute
to research on emotion recognition in the wild, while also providing a unique
natural and freely accessible resource for Spanish.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures, submitted to Scientific Data</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prescribing Large Language Models for Perioperative Care: What's The
  Right Dose for <span class="highlight-title">Pre-train</span>ed Models? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17493v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17493v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bing Xue, Charles Alba, Joanna Abraham, Thomas Kannampallil, Chenyang Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Postoperative risk predictions can inform effective perioperative care
management and planning. We aimed to assess whether clinical large language
models (LLMs) can predict postoperative risks using clinical texts with various
training strategies. The main cohort involved 84,875 records from Barnes Jewish
Hospital (BJH) system between 2018 and 2021. Methods were replicated on Beth
Israel Deaconess's MIMIC dataset. Both studies had mean duration of follow-up
based on the length of postoperative ICU stay less than 7 days. For the BJH
dataset, outcomes included 30-day mortality, pulmonary embolism (PE) and
pneumonia. Three domain adaptation and finetuning strategies were implemented
for BioGPT, ClinicalBERT and BioClinicalBERT: self-supervised objectives;
incorporating labels with semi-supervised fine-tuning; and foundational
modelling through multi-task learning. Model performance was compared using the
area under the receiver operating characteristic curve (AUROC) and the area
under the precision recall curve (AUPRC) for classification tasks, and mean
squared error (MSE) and R2 for regression tasks. Pre-trained LLMs outperformed
traditional word embeddings, with absolute maximal gains of 38.3% for AUROC and
14% for AUPRC. Adapting models further improved performance: (1)
self-supervised finetuning by 3.2% for AUROC and 1.5% for AUPRC; (2)
semi-supervised finetuning by 1.8% for AUROC and 2% for AUPRC, compared to
self-supervised finetuning; (3) foundational modelling by 3.6% for AUROC and
2.6% for AUPRC, compared to self-supervised finetuning. Pre-trained clinical
LLMs offer opportunities for postoperative risk predictions in unforeseen data,
with peaks in foundational models indicating the potential of task-agnostic
learning towards the generalizability of LLMs in perioperative care.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Supplemental file available at: http://tinyurl.com/mszmjna9; models
  publicly available at:
  https://huggingface.co/cja5553/BJH-perioperative-notes-bioGPT AND
  https://huggingface.co/cja5553/BJH-perioperative-notes-bioGPT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can <span class="highlight-title">GPT</span>-4 Identify Propaganda? Annotation and Detection of Propaganda
  Spans in News Articles <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17478v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17478v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maram Hasanain, Fatema Ahmed, Firoj Alam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of propaganda has spiked on mainstream and social media, aiming to
manipulate or mislead users. While efforts to automatically detect propaganda
techniques in textual, visual, or multimodal content have increased, most of
them primarily focus on English content. The majority of the recent initiatives
targeting medium to low-resource languages produced relatively small annotated
datasets, with a skewed distribution, posing challenges for the development of
sophisticated propaganda detection models. To address this challenge, we
carefully develop the largest propaganda dataset to date, ArPro, comprised of
8K paragraphs from newspaper articles, labeled at the text span level following
a taxonomy of 23 propagandistic techniques. Furthermore, our work offers the
first attempt to understand the performance of large language models (LLMs),
using GPT-4, for fine-grained propaganda detection from text. Results showed
that GPT-4's performance degrades as the task moves from simply classifying a
paragraph as propagandistic or not, to the fine-grained task of detecting
propaganda techniques and their manifestation in text. Compared to models
fine-tuned on the dataset for propaganda detection at different classification
granularities, GPT-4 is still far behind. Finally, we evaluate GPT-4 on a
dataset consisting of six other languages for span detection, and results
suggest that the model struggles with the task across languages. Our dataset
and resources will be released to the community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a full paper at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training-Free Long-Context Scaling of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17463v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17463v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxin An, Fei Huang, Jun Zhang, Shansan Gong, Xipeng Qiu, Chang Zhou, Lingpeng Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability of Large Language Models (LLMs) to process and generate coherent
text is markedly weakened when the number of input tokens exceeds their
pretraining length. Given the expensive overhead of finetuning large-scale
models with longer sequences, we propose Dual Chunk Attention (DCA), which
enables Llama2 70B to support context windows of more than 100k tokens without
continual training. By decomposing the attention computation for long sequences
into chunk-based modules, DCA manages to effectively capture the relative
positional information of tokens within the same chunk (Intra-Chunk) and across
distinct chunks (Inter-Chunk), as well as integrates seamlessly with Flash
Attention. In addition to its impressive extrapolation capability, DCA achieves
performance on practical long-context tasks that is comparable to or even
better than that of finetuned models. When compared with proprietary models,
our training-free 70B model attains 94% of the performance of gpt-3.5-16k,
indicating it is a viable open-source alternative. All code and data used in
this work are released at \url{https://github.com/HKUNLP/ChunkLlama}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Piece of Theatre: Investigating How Teachers Design LLM Chatbots to
  Assist Adolescent Cyberbullying Education 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17456v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17456v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael A. Hedderich, Natalie N. Bazarova, Wenting Zou, Ryun Shim, Xinda Ma, Qian Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cyberbullying harms teenagers' mental health, and teaching them upstanding
intervention is crucial. Wizard-of-Oz studies show chatbots can scale up
personalized and interactive cyberbullying education, but implementing such
chatbots is a challenging and delicate task. We created a no-code chatbot
design tool for K-12 teachers. Using large language models and prompt chaining,
our tool allows teachers to prototype bespoke dialogue flows and chatbot
utterances. In offering this tool, we explore teachers' distinctive needs when
designing chatbots to assist their teaching, and how chatbot design tools might
better support them. Our findings reveal that teachers welcome the tool
enthusiastically. Moreover, they see themselves as playwrights guiding both the
students' and the chatbot's behaviors, while allowing for some improvisation.
Their goal is to enable students to rehearse both desirable and undesirable
reactions to cyberbullying in a safe environment. We discuss the design
opportunities LLM-Chains offer for empowering teachers and the research
opportunities this work opens up.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning Based Named Entity Recognition Models for Recipes <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mansi Goel, Ayush Agarwal, Shubham Agrawal, Janak Kapuriya, Akhil Vamshi Konam, Rishabh Gupta, Shrey Rastogi,  Niharika, Ganesh Bagler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Food touches our lives through various endeavors, including flavor,
nourishment, health, and sustainability. Recipes are cultural capsules
transmitted across generations via unstructured text. Automated protocols for
recognizing named entities, the building blocks of recipe text, are of immense
value for various applications ranging from information extraction to novel
recipe generation. Named entity recognition is a technique for extracting
information from unstructured or semi-structured data with known labels.
Starting with manually-annotated data of 6,611 ingredient phrases, we created
an augmented dataset of 26,445 phrases cumulatively. Simultaneously, we
systematically cleaned and analyzed ingredient phrases from RecipeDB, the
gold-standard recipe data repository, and annotated them using the Stanford
NER. Based on the analysis, we sampled a subset of 88,526 phrases using a
clustering-based approach while preserving the diversity to create the
machine-annotated dataset. A thorough investigation of NER approaches on these
three datasets involving statistical, fine-tuning of deep learning-based
language models and few-shot prompting on large language models (LLMs) provides
deep insights. We conclude that few-shot prompting on LLMs has abysmal
performance, whereas the fine-tuned spaCy-transformer emerges as the best model
with macro-F1 scores of 95.9%, 96.04%, and 95.71% for the manually-annotated,
augmented, and machine-annotated datasets, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 main figures and 2 in appendices, and 3 main tables;
  Accepted for publication in LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploiting Emotion-Semantic Correlations for Empathetic Response
  Generation <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17437v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17437v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhou Yang, Zhaochun Ren, Yufeng Wang, Xiaofei Zhu, Zhihao Chen, Tiecheng Cai, Yunbing Wu, Yisong Su, Sibo Ju, Xiangwen Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Empathetic response generation aims to generate empathetic responses by
understanding the speaker's emotional feelings from the language of dialogue.
Recent methods capture emotional words in the language of communicators and
construct them as static vectors to perceive nuanced emotions. However,
linguistic research has shown that emotional words in language are dynamic and
have correlations with other grammar semantic roles, i.e., words with semantic
meanings, in grammar. Previous methods overlook these two characteristics,
which easily lead to misunderstandings of emotions and neglect of key
semantics. To address this issue, we propose a dynamical Emotion-Semantic
Correlation Model (ESCM) for empathetic dialogue generation tasks. ESCM
constructs dynamic emotion-semantic vectors through the interaction of context
and emotions. We introduce dependency trees to reflect the correlations between
emotions and semantics. Based on dynamic emotion-semantic vectors and
dependency trees, we propose a dynamic correlation graph convolutional network
to guide the model in learning context meanings in dialogue and generating
empathetic responses. Experimental results on the EMPATHETIC-DIALOGUES dataset
show that ESCM understands semantics and emotions more accurately and expresses
fluent and informative empathetic responses. Our analysis results also indicate
that the correlations between emotions and semantics are frequently used in
dialogues, which is of great significance for empathetic perception and
expression.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures, Findings of EMNLP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing EEG-to-Text Decoding through Transferable Representations from
  <span class="highlight-title">Pre-train</span>ed Contrastive EEG-Text Masked Autoencoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17433v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17433v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Wang, Zhenxi Song, Zhengyu Ma, Xipeng Qiu, Min Zhang, Zhiguo Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing natural language from non-invasive electroencephalography
(EEG) holds great promise as a language decoding technology for brain-computer
interfaces (BCIs). However, EEG-based language decoding is still in its nascent
stages, facing several technical issues such as: 1) Absence of a hybrid
strategy that can effectively integrate cross-modality (between EEG and text)
self-learning with intra-modality self-reconstruction of EEG features or
textual sequences; 2) Under-utilization of large language models (LLMs) to
enhance EEG-based language decoding. To address above issues, we propose the
Contrastive EEG-Text Masked Autoencoder (CET-MAE), a novel model that
orchestrates compound self-supervised learning across and within EEG and text
through a dedicated multi-stream encoder. Furthermore, we develop a framework
called E2T-PTR (EEG-to-Text decoding using Pretrained Transferable
Representations), which leverages pre-trained modules alongside the EEG stream
from CET-MAE and further enables an LLM (specifically BART) to decode text from
EEG sequences. Comprehensive experiments conducted on the popular text-evoked
EEG database, ZuCo, demonstrate the superiority of E2T-PTR, which outperforms
the state-of-the-art in ROUGE-1 F1 and BLEU-4 scores by 8.34% and 32.21%,
respectively. These results indicate significant advancements in the field and
underscores the proposed framework's potential to enable more powerful and
widespread BCI applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Consistency Matters: Explore LLMs Consistency From a Black-Box
  Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17411v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17411v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fufangchen Zhao, Guoqiang Jin, Jiaheng Huang, Rui Zhao, Fei Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays both commercial and open-source academic LLM have become the
mainstream models of NLP. However, there is still a lack of research on LLM
consistency, meaning that throughout the various stages of LLM research and
deployment, its internal parameters and capabilities should remain unchanged.
This issue exists in both the industrial and academic sectors. The solution to
this problem is often time-consuming and labor-intensive, and there is also an
additional cost of secondary deployment, resulting in economic and time losses.
To fill this gap, we build an LLM consistency task dataset and design several
baselines. Additionally, we choose models of diverse scales for the main
experiments. Specifically, in the LightGBM experiment, we used traditional NLG
metrics (i.e., ROUGE, BLEU, METEOR) as the features needed for model training.
The final result exceeds the manual evaluation and GPT3.5 as well as other
models in the main experiment, achieving the best performance. In the end, we
use the best performing LightGBM model as the base model to build the
evaluation tool, which can effectively assist in the deployment of business
models. Our code and tool demo are available at
https://github.com/heavenhellchen/Consistency.git
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Neural Rewriting System to Solve Algorithmic Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Flavio Petruzzellis, Alberto Testolin, Alessandro Sperduti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern neural network architectures still struggle to learn algorithmic
procedures that require to systematically apply compositional rules to solve
out-of-distribution problem instances. In this work, we propose an original
approach to learn algorithmic tasks inspired by rewriting systems, a classic
framework in symbolic artificial intelligence. We show that a rewriting system
can be implemented as a neural architecture composed by specialized modules:
the Selector identifies the target sub-expression to process, the Solver
simplifies the sub-expression by computing the corresponding result, and the
Combiner produces a new version of the original expression by replacing the
sub-expression with the solution provided. We evaluate our model on three types
of algorithmic tasks that require simplifying symbolic formulas involving
lists, arithmetic, and algebraic expressions. We test the extrapolation
capabilities of the proposed architecture using formulas involving a higher
number of operands and nesting levels than those seen during training, and we
benchmark its performance against the Neural Data Router, a recent model
specialized for systematic generalization, and a state-of-the-art large
language model (GPT-4) probed with advanced prompting strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating Continual <span class="highlight-title">Pretrain</span>ing in Large Language Models: Insights
  and Implications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17400v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17400v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Çağatay Yıldız, Nishaanth Kanna Ravichandran, Prishruit Punia, Matthias Bethge, Beyza Ermis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the evolving domain of Continual Learning (CL) in large
language models (LLMs), with a focus on developing strategies for efficient and
sustainable training. Our primary emphasis is on continual domain-adaptive
pretraining, a process designed to equip LLMs with the ability to integrate new
information from various domains while retaining previously learned knowledge
and enhancing cross-domain knowledge transfer without relying on
domain-specific identification. Unlike previous studies, which mostly
concentrate on a limited selection of tasks or domains and primarily aim to
address the issue of forgetting, our research evaluates the adaptability and
capabilities of LLMs to changing data landscapes in practical scenarios. To
this end, we introduce a new benchmark designed to measure the adaptability of
LLMs to these evolving data environments, offering a comprehensive framework
for evaluation. We examine the impact of model size on learning efficacy and
forgetting, as well as how the progression and similarity of emerging domains
affect the knowledge transfer within these models. Our findings uncover several
key insights: (i) when the sequence of domains shows semantic similarity,
continual pretraining enables LLMs to better specialize in the current domain
compared to stand-alone fine-tuning, (ii) training across a diverse range of
domains enhances both backward and forward knowledge transfer, and (iii)
smaller models are particularly sensitive to continual pretraining, showing the
most significant rates of both forgetting and learning. We posit that our
research marks a shift towards establishing a more realistic benchmark for
investigating CL in LLMs, and has the potential to play a key role in guiding
the direction of future research in the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking <span class="highlight-title">GPT</span>-4 on Algorithmic Problems: A Systematic Evaluation of
  <span class="highlight-title">Prompt</span>ing Strategies <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17396v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17396v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Flavio Petruzzellis, Alberto Testolin, Alessandro Sperduti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have revolutionized the field of Natural
Language Processing thanks to their ability to reuse knowledge acquired on
massive text corpora on a wide variety of downstream tasks, with minimal (if
any) tuning steps. At the same time, it has been repeatedly shown that LLMs
lack systematic generalization, which allows to extrapolate the learned
statistical regularities outside the training distribution. In this work, we
offer a systematic benchmarking of GPT-4, one of the most advanced LLMs
available, on three algorithmic tasks characterized by the possibility to
control the problem difficulty with two parameters. We compare the performance
of GPT-4 with that of its predecessor (GPT-3.5) and with a variant of the
Transformer-Encoder architecture recently introduced to solve similar tasks,
the Neural Data Router. We find that the deployment of advanced prompting
techniques allows GPT-4 to reach superior accuracy on all tasks, demonstrating
that state-of-the-art LLMs constitute a very strong baseline also in
challenging tasks that require systematic generalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spot the bot: Coarse-Grained Partition of Semantic Paths for Bots and
  Humans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vasilii A. Gromov, Alexandra S. Kogan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, technology is rapidly advancing: bots are writing comments,
articles, and reviews. Due to this fact, it is crucial to know if the text was
written by a human or by a bot. This paper focuses on comparing structures of
the coarse-grained partitions of semantic paths for human-written and
bot-generated texts. We compare the clusterizations of datasets of n-grams from
literary texts and texts generated by several bots. The hypothesis is that the
structures and clusterizations are different. Our research supports the
hypothesis. As the semantic structure may be different for different languages,
we investigate Russian, English, German, and Vietnamese languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FairBelief - Assessing Harmful Beliefs in Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17389v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17389v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mattia Setzu, Marta Marchiori Manerba, Pasquale Minervini, Debora Nozza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language Models (LMs) have been shown to inherit undesired biases that might
hurt minorities and underrepresented groups if such systems were integrated
into real-world applications without careful fairness auditing. This paper
proposes FairBelief, an analytical approach to capture and assess beliefs,
i.e., propositions that an LM may embed with different degrees of confidence
and that covertly influence its predictions. With FairBelief, we leverage
prompting to study the behavior of several state-of-the-art LMs across
different previously neglected axes, such as model scale and likelihood,
assessing predictions on a fairness dataset specifically designed to quantify
LMs' outputs' hurtfulness. Finally, we conclude with an in-depth qualitative
assessment of the beliefs emitted by the models. We apply FairBelief to English
LMs, revealing that, although these architectures enable high performances on
diverse natural language processing tasks, they show hurtful beliefs about
specific genders. Interestingly, training procedure and dataset, model scale,
and architecture induce beliefs of different degrees of hurtfulness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KoDialogBench: Evaluating Conversational Understanding of Language
  Models with Korean Dialogue Benchmark <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seongbo Jang, Seonghyeon Lee, Hwanjo Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As language models are often deployed as chatbot assistants, it becomes a
virtue for models to engage in conversations in a user's first language. While
these models are trained on a wide range of languages, a comprehensive
evaluation of their proficiency in low-resource languages such as Korean has
been lacking. In this work, we introduce KoDialogBench, a benchmark designed to
assess language models' conversational capabilities in Korean. To this end, we
collect native Korean dialogues on daily topics from public sources, or
translate dialogues from other languages. We then structure these conversations
into diverse test datasets, spanning from dialogue comprehension to response
selection tasks. Leveraging the proposed benchmark, we conduct extensive
evaluations and analyses of various language models to measure a foundational
understanding of Korean dialogues. Experimental results indicate that there
exists significant room for improvement in models' conversation skills.
Furthermore, our in-depth comparisons across different language models
highlight the effectiveness of recent training techniques in enhancing
conversational proficiency. We anticipate that KoDialogBench will promote the
progress towards conversation-aware Korean language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Dataset</span> for Metaphor Detection in Early Medieval Hebrew Poetry <span class="chip">EACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17371v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17371v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Toker, Oren Mishali, Ophir Münz-Manor, Benny Kimelfeld, Yonatan Belinkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a large volume of late antique and medieval Hebrew texts. They
represent a crucial linguistic and cultural bridge between Biblical and modern
Hebrew. Poetry is prominent in these texts and one of its main haracteristics
is the frequent use of metaphor. Distinguishing figurative and literal language
use is a major task for scholars of the Humanities, especially in the fields of
literature, linguistics, and hermeneutics. This paper presents a new,
challenging dataset of late antique and medieval Hebrew poetry with expert
annotations of metaphor, as well as some baseline results, which we hope will
facilitate further research in this area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EACL 2024. Project webpage: https://tokeron.github.io/metaphor/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SoFA: Shielded On-the-fly Alignment via Priority Rule Following 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17358v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17358v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Lu, Bowen Yu, Yaojie Lu, Hongyu Lin, Haiyang Yu, Le Sun, Xianpei Han, Yongbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The alignment problem in Large Language Models (LLMs) involves adapting them
to the broad spectrum of human values. This requirement challenges existing
alignment methods due to diversity of preferences and regulatory standards.
This paper introduces a novel alignment paradigm, priority rule following,
which defines rules as the primary control mechanism in each dialog,
prioritizing them over user instructions. Our preliminary analysis reveals that
even the advanced LLMs, such as GPT-4, exhibit shortcomings in understanding
and prioritizing the rules. Therefore, we present PriorityDistill, a
semi-automated approach for distilling priority following signals from LLM
simulations to ensure robust rule integration and adherence. Our experiments
show that this method not only effectively minimizes misalignments utilizing
only one general rule but also adapts smoothly to various unseen rules,
ensuring they are shielded from hijacking and that the model responds
appropriately.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RECOST: External Knowledge Guided Data-efficient Instruction Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17355v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17355v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Zhang, Yiming Zhang, Haobo Wang, Junbo Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the current landscape of large language models (LLMs), the process of
instruction tuning serves as an essential step. Considering the high computing
power overhead, data-efficient instruction tuning was proposed to reduce the
training data size in this process, aiming at selecting high-quality
instructional data. Nevertheless, we argue that most current data-efficient
instruction-tuning methods are highly dependent on the quality of the original
instruction-tuning dataset. When it comes to datasets synthesized by LLMs, a
common scenario in this field, dirty samples will even be selected with a
higher probability than other samples. To address these challenges, we utilized
external knowledge (relevant examples or paragraphs) to evaluate those samples
synthesized by LLMs with an in-context-based relative predictive entropy. Based
on the new metric, we proposed a framework, dubbed as \textbf{RECOST}, which
integrates external-knowledge-base re-ranking and diversity-consistent sampling
into a single pipeline. Through extensive experiments on several synthetic
datasets (Alpaca and Alpaca-gpt4), we demonstrate the effectiveness of our
method and achieve even better results with only \textbf{1\%} of the full
dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised multiple choices question answering via universal corpus <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17333v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17333v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qin Zhang, Hao Ge, Xiaojun Chen, Meng Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised question answering is a promising yet challenging task, which
alleviates the burden of building large-scale annotated data in a new domain.
It motivates us to study the unsupervised multiple-choice question answering
(MCQA) problem. In this paper, we propose a novel framework designed to
generate synthetic MCQA data barely based on contexts from the universal domain
without relying on any form of manual annotation. Possible answers are
extracted and used to produce related questions, then we leverage both named
entities (NE) and knowledge graphs to discover plausible distractors to form
complete synthetic samples. Experiments on multiple MCQA datasets demonstrate
the effectiveness of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figures, published to ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SKT5SciSumm - A Hybrid Generative Approach for Multi-Document Scientific
  Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17311v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17311v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huy Quoc To, Hung-Nghiep Tran, Andr'e Greiner-Petter, Felix Beierle, Akiko Aizawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Summarization for scientific text has shown significant benefits both for the
research community and human society. Given the fact that the nature of
scientific text is distinctive and the input of the multi-document
summarization task is substantially long, the task requires sufficient
embedding generation and text truncation without losing important information.
To tackle these issues, in this paper, we propose SKT5SciSumm - a hybrid
framework for multi-document scientific summarization (MDSS). We leverage the
Sentence-Transformer version of Scientific Paper Embeddings using
Citation-Informed Transformers (SPECTER) to encode and represent textual
sentences, allowing for efficient extractive summarization using k-means
clustering. We employ the T5 family of models to generate abstractive summaries
using extracted sentences. SKT5SciSumm achieves state-of-the-art performance on
the Multi-XScience dataset. Through extensive experiments and evaluation, we
showcase the benefits of our model by using less complicated models to achieve
remarkable results, thereby highlighting its potential in advancing the field
of multi-document summarization for scientific text.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Probing Multimodal Large Language Models for Global and Local Semantic
  Representation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingxu Tao, Quzhe Huang, Kun Xu, Liwei Chen, Yansong Feng, Dongyan Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of large language models has inspired researchers to transfer
their exceptional representing ability to other modalities. Several recent
works leverage image-caption alignment datasets to train multimodal large
language models (MLLMs), which achieve state-of-the-art performance on
image-to-text tasks. However, there are very few studies exploring whether
MLLMs truly understand the complete image information, i.e., global
information, or if they can only capture some local object information. In this
study, we find that the intermediate layers of models can encode more global
semantic information, whose representation vectors perform better on
visual-language entailment tasks, rather than the topmost layers. We further
probe models for local semantic representation through object detection tasks.
And we draw a conclusion that the topmost layers may excessively focus on local
information, leading to a diminished ability to encode global information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-COLING 2024 as a short paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in
  Indonesian and Sundanese 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rifki Afina Putri, Faiz Ghifari Haznitrama, Dea Adhista, Alice Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators. Our experiments show that the current best-performing LLM, GPT-4
Turbo, is capable of generating questions with adequate knowledge in Indonesian
but not in Sundanese, highlighting the performance discrepancy between medium-
and lower-resource languages. We also benchmark various LLMs on our generated
datasets and find that they perform better on the LLM-generated datasets
compared to those created by humans.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17263v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17263v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengjie Ren, Chengshun Shi, Shiguang Wu, Mengqi Zhang, Zhaochun Ren, Maarten de Rijke, Zhumin Chen, Jiahuan Pei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-efficient fine-tuning (PEFT) is a popular method for tailoring
pre-trained large language models (LLMs), especially as the models' scale and
the diversity of tasks increase. Low-rank adaptation (LoRA) is based on the
idea that the adaptation process is intrinsically low-dimensional, i.e.,
significant model changes can be represented with relatively few parameters.
However, decreasing the rank encounters challenges with generalization errors
for specific tasks when compared to full-parameter fine-tuning. We present
MELoRA, a mini-ensemble low-rank adapters that uses fewer trainable parameters
while maintaining a higher rank, thereby offering improved performance
potential. The core idea is to freeze original pretrained weights and train a
group of mini LoRAs with only a small number of parameters. This can capture a
significant degree of diversity among mini LoRAs, thus promoting better
generalization ability. We conduct a theoretical analysis and empirical studies
on various NLP tasks. Our experimental results show that, compared to LoRA,
MELoRA achieves better performance with 8 times fewer trainable parameters on
natural language understanding tasks and 36 times fewer trainable parameters on
instruction following tasks, which demonstrates the effectiveness of MELoRA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Speak Out of Turn: Safety Vulnerability of Large Language Models in
  Multi-turn Dialogue 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17262v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17262v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenhong Zhou, Jiuyang Xiang, Haopeng Chen, Quan Liu, Zherui Li, Sen Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have been demonstrated to generate illegal or
unethical responses, particularly when subjected to "jailbreak." Research on
jailbreak has highlighted the safety issues of LLMs. However, prior studies
have predominantly focused on single-turn dialogue, ignoring the potential
complexities and risks presented by multi-turn dialogue, a crucial mode through
which humans derive information from LLMs. In this paper, we argue that humans
could exploit multi-turn dialogue to induce LLMs into generating harmful
information. LLMs may not intend to reject cautionary or borderline unsafe
queries, even if each turn is closely served for one malicious purpose in a
multi-turn dialogue. Therefore, by decomposing an unsafe query into several
sub-queries for multi-turn dialogue, we induced LLMs to answer harmful
sub-questions incrementally, culminating in an overall harmful response. Our
experiments, conducted across a wide range of LLMs, indicate current
inadequacies in the safety mechanisms of LLMs in multi-turn dialogue. Our
findings expose vulnerabilities of LLMs in complex scenarios involving
multi-turn dialogue, presenting new challenges for the safety of LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>working in progress 23pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond the Known: Investigating LLMs Performance on Out-of-Domain Intent
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pei Wang, Keqing He, Yejie Wang, Xiaoshuai Song, Yutao Mou, Jingang Wang, Yunsen Xian, Xunliang Cai, Weiran Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-domain (OOD) intent detection aims to examine whether the user's query
falls outside the predefined domain of the system, which is crucial for the
proper functioning of task-oriented dialogue (TOD) systems. Previous methods
address it by fine-tuning discriminative models. Recently, some studies have
been exploring the application of large language models (LLMs) represented by
ChatGPT to various downstream tasks, but it is still unclear for their ability
on OOD detection task.This paper conducts a comprehensive evaluation of LLMs
under various experimental settings, and then outline the strengths and
weaknesses of LLMs. We find that LLMs exhibit strong zero-shot and few-shot
capabilities, but is still at a disadvantage compared to models fine-tuned with
full resource. More deeply, through a series of additional analysis
experiments, we discuss and summarize the challenges faced by LLMs and provide
guidance for future work including injecting domain knowledge, strengthening
knowledge transfer from IND(In-domain) to OOD, and understanding long
instructions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image-Text Matching with Multi-View Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17237v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17237v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Cheng, Wanqing Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing two-stream models for image-text matching show good performance
while ensuring retrieval speed and have received extensive attention from
industry and academia. These methods use a single representation to encode
image and text separately and get a matching score with cosine similarity or
the inner product of vectors. However, the performance of the two-stream model
is often sub-optimal. On the one hand, a single representation is challenging
to cover complex content comprehensively. On the other hand, in this framework
of lack of interaction, it is challenging to match multiple meanings which
leads to information being ignored. To address the problems mentioned above and
facilitate the performance of the two-stream model, we propose a multi-view
attention approach for two-stream image-text matching MVAM
(\textbf{M}ulti-\textbf{V}iew \textbf{A}ttention \textbf{M}odel). It first
learns multiple image and text representations by diverse attention heads with
different view codes. And then concatenate these representations into one for
matching. A diversity objective is also used to promote diversity between
attention heads. With this method, models are able to encode images and text
from different views and attend to more key points. So we can get
representations that contain more information. When doing retrieval tasks, the
matching scores between images and texts can be calculated from different
aspects, leading to better matching performance. Experiment results on MSCOCO
and Flickr30K show that our proposed model brings improvements over existing
models. Further case studies show that different attention heads can focus on
different contents and finally obtain a more comprehensive representation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17231v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17231v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debrup Das, Debopriyo Banerjee, Somak Aditya, Ashish Kulkarni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tool-augmented Large Language Models (TALM) are known to enhance the skillset
of large language models (LLM), thereby, leading to their improved reasoning
abilities across many tasks. While, TALMs have been successfully employed in
different question-answering benchmarks, their efficacy on complex mathematical
reasoning benchmarks, and the potential complimentary benefits offered by tools
for knowledge retrieval and mathematical equation solving, are open research
questions. In this work, we present MATHSENSEI, a tool-augmented large language
model for mathematical reasoning. Augmented with tools for knowledge retrieval
(Bing Web Search), program execution (Python), and symbolic equation solving
(Wolfram-Alpha), we study the complimentary benefits of these tools through
evaluations on mathematical reasoning datasets. We perform exhaustive ablations
on MATH,a popular dataset for evaluating mathematical reasoning on diverse
mathematical disciplines. We also conduct experiments involving well-known tool
planners to study the impact of tool sequencing on the model performance.
MATHSENSEI achieves 13.5% better accuracy over gpt-3.5-turbo with
chain-of-thought on the MATH dataset. We further observe that TALMs are not as
effective for simpler math word problems (in GSM-8k), and the benefit increases
as the complexity and required knowledge increases (progressively over AQuA,
MMLU-Math, and higher level complex questions in MATH). The code and data are
available at https://github.com/Debrup-61/MathSensei.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reasoning in Conversation: Solving Subjective Tasks through Dialogue
  Simulation for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17226v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17226v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaolong Wang, Yile Wang, Yuanchi Zhang, Fuwen Luo, Peng Li, Maosong Sun, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have achieved remarkable performance in
objective tasks such as open-domain question answering and mathematical
reasoning, which can often be solved through recalling learned factual
knowledge or chain-of-thought style reasoning. However, we find that the
performance of LLMs in subjective tasks is still unsatisfactory, such as
metaphor recognition, dark humor detection, etc. Compared to objective tasks,
subjective tasks focus more on interpretation or emotional response rather than
a universally accepted reasoning pathway. Based on the characteristics of the
tasks and the strong dialogue-generation capabilities of LLMs, we propose RiC
(Reasoning in Conversation), a method that focuses on solving subjective tasks
through dialogue simulation. The motivation of RiC is to mine useful contextual
information by simulating dialogues instead of supplying chain-of-thought style
rationales, thereby offering potential useful knowledge behind dialogues for
giving the final answers. We evaluate both API-based and open-source LLMs
including GPT-4, ChatGPT, and OpenChat across twelve tasks. Experimental
results show that RiC can yield significant improvement compared with various
baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Measuring Vision-Language STEM Skills of Neural Models <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17205v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17205v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianhao Shen, Ye Yuan, Srbuhi Mirzoyan, Ming Zhang, Chenguang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new challenge to test the STEM skills of neural models. The
problems in the real world often require solutions, combining knowledge from
STEM (science, technology, engineering, and math). Unlike existing datasets,
our dataset requires the understanding of multimodal vision-language
information of STEM. Our dataset features one of the largest and most
comprehensive datasets for the challenge. It includes 448 skills and 1,073,146
questions spanning all STEM subjects. Compared to existing datasets that often
focus on examining expert-level ability, our dataset includes fundamental
skills and questions designed based on the K-12 curriculum. We also add
state-of-the-art foundation models such as CLIP and GPT-3.5-Turbo to our
benchmark. Results show that the recent model advances only help master a very
limited number of lower grade-level skills (2.5% in the third grade) in our
dataset. In fact, these models are still well below (averaging 54.7%) the
performance of elementary students, not to mention near expert-level
performance. To understand and increase the performance on our dataset, we
teach the models on a training split of our dataset. Even though we observe
improved performance, the model performance remains relatively low compared to
average elementary students. To solve STEM problems, we will need novel
algorithmic innovations from the community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When Scaling Meets LLM Finetuning: The Effect of Data, Model and
  Finetuning Method <span class="chip">ICLR24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17193v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17193v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Biao Zhang, Zhongtao Liu, Colin Cherry, Orhan Firat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) often adopt finetuning to unlock their
capabilities for downstream applications, our understanding on the inductive
biases (especially the scaling properties) of different finetuning methods is
still limited. To fill this gap, we conduct systematic experiments studying
whether and how different scaling factors, including LLM model size,
pretraining data size, new finetuning parameter size and finetuning data size,
affect the finetuning performance. We consider two types of finetuning --
full-model tuning (FMT) and parameter efficient tuning (PET, including prompt
tuning and LoRA), and explore their scaling behaviors in the data-limited
regime where the LLM model size substantially outweighs the finetuning data
size. Based on two sets of pretrained bilingual LLMs from 1B to 16B and
experiments on bilingual machine translation and multilingual summarization
benchmarks, we find that 1) LLM finetuning follows a powerbased multiplicative
joint scaling law between finetuning data size and each other scaling factor;
2) LLM finetuning benefits more from LLM model scaling than pretraining data
scaling, and PET parameter scaling is generally ineffective; and 3) the optimal
finetuning method is highly task- and finetuning data-dependent. We hope our
findings could shed light on understanding, selecting and developing LLM
finetuning methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Effective Mixture-Of-Experts Approach For Code-Switching Speech
  Recognition Leveraging Encoder Disentanglement <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17189v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17189v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tzu-Ting Yang, Hsin-Wei Wang, Yi-Cheng Wang, Chi-Han Lin, Berlin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the massive developments of end-to-end (E2E) neural networks, recent
years have witnessed unprecedented breakthroughs in automatic speech
recognition (ASR). However, the codeswitching phenomenon remains a major
obstacle that hinders ASR from perfection, as the lack of labeled data and the
variations between languages often lead to degradation of ASR performance. In
this paper, we focus exclusively on improving the acoustic encoder of E2E ASR
to tackle the challenge caused by the codeswitching phenomenon. Our main
contributions are threefold: First, we introduce a novel disentanglement loss
to enable the lower-layer of the encoder to capture inter-lingual acoustic
information while mitigating linguistic confusion at the higher-layer of the
encoder. Second, through comprehensive experiments, we verify that our proposed
method outperforms the prior-art methods using pretrained dual-encoders,
meanwhile having access only to the codeswitching corpus and consuming half of
the parameterization. Third, the apparent differentiation of the encoders'
output features also corroborates the complementarity between the
disentanglement loss and the mixture-of-experts (MoE) architecture.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extreme Encoder Output Frame Rate Reduction: Improving Computational
  Latencies of Large End-to-End Models <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohit Prabhavalkar, Zhong Meng, Weiran Wang, Adam Stooke, Xingyu Cai, Yanzhang He, Arun Narayanan, Dongseong Hwang, Tara N. Sainath, Pedro J. Moreno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The accuracy of end-to-end (E2E) automatic speech recognition (ASR) models
continues to improve as they are scaled to larger sizes, with some now reaching
billions of parameters. Widespread deployment and adoption of these models,
however, requires computationally efficient strategies for decoding. In the
present work, we study one such strategy: applying multiple frame reduction
layers in the encoder to compress encoder outputs into a small number of output
frames. While similar techniques have been investigated in previous work, we
achieve dramatically more reduction than has previously been demonstrated
through the use of multiple funnel reduction layers. Through ablations, we
study the impact of various architectural choices in the encoder to identify
the most effective strategies. We demonstrate that we can generate one encoder
output frame for every 2.56 sec of input speech, without significantly
affecting word error rate on a large-scale voice search task, while improving
encoder and decoder latencies by 48% and 92% respectively, relative to a strong
but computationally expensive baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 2024 IEEE International Conference on Acoustics, Speech,
  and Signal Processing (ICASSP 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Data Science Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17168v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17168v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuge Zhang, Qiyang Jiang, Xingyu Han, Nan Chen, Yuqing Yang, Kan Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of data-driven decision-making, the complexity of data analysis
necessitates advanced expertise and tools of data science, presenting
significant challenges even for specialists. Large Language Models (LLMs) have
emerged as promising aids as data science agents, assisting humans in data
analysis and processing. Yet their practical efficacy remains constrained by
the varied demands of real-world applications and complicated analytical
process. In this paper, we introduce DSEval -- a novel evaluation paradigm, as
well as a series of innovative benchmarks tailored for assessing the
performance of these agents throughout the entire data science lifecycle.
Incorporating a novel bootstrapped annotation method, we streamline dataset
preparation, improve the evaluation coverage, and expand benchmarking
comprehensiveness. Our findings uncover prevalent obstacles and provide
critical insights to inform future advancements in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Source code and data are available at
  https://github.com/MetaCopilot/dseval</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clustering Document Parts: Detecting and Characterizing Influence
  Campaigns From Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengxiang Wang, Owen Rambow
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel clustering pipeline to detect and characterize influence
campaigns from documents. This approach clusters parts of document, detects
clusters that likely reflect an influence campaign, and then identifies
documents linked to an influence campaign via their association with the
high-influence clusters. Our approach outperforms both the direct
document-level classification and the direct document-level clustering approach
in predicting if a document is part of an influence campaign. We propose
various novel techniques to enhance our pipeline, including using an existing
event factuality prediction system to obtain document parts, and aggregating
multiple clustering experiments to improve the performance of both cluster and
document classification. Classifying documents on the top of clustering not
only accurately extracts the parts of the documents that are relevant to
influence campaigns, but also capture influence campaigns as a coordinated and
holistic phenomenon. Our approach makes possible more fine-grained and
interpretable characterizations of influence campaigns from documents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 2 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fact-and-Reflection (FaR) Improves Confidence Calibration of Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17124v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17124v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinran Zhao, Hongming Zhang, Xiaoman Pan, Wenlin Yao, Dong Yu, Tongshuang Wu, Jianshu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For a LLM to be trustworthy, its confidence level should be well-calibrated
with its actual performance. While it is now common sense that LLM performances
are greatly impacted by prompts, the confidence calibration in prompting LLMs
has yet to be thoroughly explored. In this paper, we explore how different
prompting strategies influence LLM confidence calibration and how it could be
improved. We conduct extensive experiments on six prompting methods in the
question-answering context and we observe that, while these methods help
improve the expected LLM calibration, they also trigger LLMs to be
over-confident when responding to some instances. Inspired by human cognition,
we propose Fact-and-Reflection (FaR) prompting, which improves the LLM
calibration in two steps. First, FaR elicits the known "facts" that are
relevant to the input prompt from the LLM. And then it asks the model to
"reflect" over them to generate the final answer. Experiments show that FaR
prompting achieves significantly better calibration; it lowers the Expected
Calibration Error by 23.5% on our multi-purpose QA tasks. Notably, FaR
prompting even elicits the capability of verbally expressing concerns in less
confident scenarios, which helps trigger retrieval augmentation for solving
these harder instances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Creating Suspenseful Stories: Iterative Planning with Large Language
  Models <span class="chip">EACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17119v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17119v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaige Xie, Mark Riedl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated story generation has been one of the long-standing challenges in
NLP. Among all dimensions of stories, suspense is very common in human-written
stories but relatively under-explored in AI-generated stories. While recent
advances in large language models (LLMs) have greatly promoted language
generation in general, state-of-the-art LLMs are still unreliable when it comes
to suspenseful story generation. We propose a novel iterative-prompting-based
planning method that is grounded in two theoretical foundations of story
suspense from cognitive psychology and narratology. This theory-grounded method
works in a fully zero-shot manner and does not rely on any supervised story
corpora. To the best of our knowledge, this paper is the first attempt at
suspenseful story generation with LLMs. Extensive human evaluations of the
generated suspenseful stories demonstrate the effectiveness of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Re-Ex: Revising after Explanation Reduces the Factual Errors in LLM
  Responses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17097v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17097v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juyeon Kim, Jeongeun Lee, Yoonho Chang, Chanyeol Choi, Junseong Kim, Jy-yong Sohn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mitigating hallucination issues is one of the main challenges of LLMs we need
to overcome, in order to reliably use them in real-world scenarios. Recently,
various methods are proposed to check the factual errors in the LLM-generated
texts and revise them accordingly, to reduce the hallucination issue. In this
paper, we propose Re-Ex, a method of revising LLM-generated texts, which
introduces a novel step dubbed as the factual error explanation step. Re-Ex
revises the initial response of LLMs using 3-steps: first, external tools are
used to get the evidences on the factual errors in the response; second, LLMs
are instructed to explain the problematic parts of the response based on the
evidences gathered in the first step; finally, LLMs revise the response using
the explanation obtained in the second step. In addition to the explanation
step, we propose new prompting techniques to reduce the amount of tokens and
wall-clock time required for the response revision process. Compared with
existing methods including Factool, CoVE, and RARR, Re-Ex provides better
revision performance with less time and fewer tokens in multiple benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preference Ranking Optimization for Human Alignment <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.17492v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.17492v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, Houfeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often contain misleading content, emphasizing
the need to align them with human values to ensure secure AI systems.
Reinforcement learning from human feedback (RLHF) has been employed to achieve
this alignment. However, it encompasses two main drawbacks: (1) RLHF exhibits
complexity, instability, and sensitivity to hyperparameters in contrast to SFT.
(2) Despite massive trial-and-error, multiple sampling is reduced to pair-wise
contrast, thus lacking contrasts from a macro perspective. In this paper, we
propose Preference Ranking Optimization (PRO) as an efficient SFT algorithm to
directly fine-tune LLMs for human alignment. PRO extends the pair-wise contrast
to accommodate preference rankings of any length. By iteratively contrasting
candidates, PRO instructs the LLM to prioritize the best response while
progressively ranking the rest responses. In this manner, PRO effectively
transforms human alignment into aligning the probability ranking of n responses
generated by LLM with the preference ranking of humans towards these responses.
Experiments have shown that PRO outperforms baseline algorithms, achieving
comparable results to ChatGPT and human responses through automatic-based,
reward-based, GPT-4, and human evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advancing Translation Preference Modeling with RLHF: A Step Towards
  Cost-Effective Solution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11525v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11525v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nuo Xu, Jun Zhao, Can Zu, Sixian Li, Lu Chen, Zhihao Zhang, Rui Zheng, Shihan Dou, Wenjuan Qin, Tao Gui, Qi Zhang, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Faithfulness, expressiveness, and elegance is the constant pursuit in machine
translation. However, traditional metrics like \textit{BLEU} do not strictly
align with human preference of translation quality. In this paper, we explore
leveraging reinforcement learning with human feedback (\textit{RLHF}) to
improve translation quality. It is non-trivial to collect a large high-quality
dataset of human comparisons between translations, especially for low-resource
languages. To address this issue, we propose a cost-effective preference
learning strategy, optimizing reward models by distinguishing between human and
machine translations. In this manner, the reward model learns the deficiencies
of machine translation compared to human and guides subsequent improvements in
machine translation. Experimental results demonstrate that \textit{RLHF} can
effectively enhance translation quality and this improvement benefits other
translation directions not trained with \textit{RLHF}. Further analysis
indicates that the model's language capabilities play a crucial role in
preference learning. A reward model with strong language capabilities can more
sensitively learn the subtle differences in translation quality and align
better with real human translation preferences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EvalLM: Interactive Evaluation of Large Language Model <span class="highlight-title">Prompt</span>s on
  User-Defined Criteria 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13633v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13633v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tae Soo Kim, Yoonjoo Lee, Jamin Shin, Young-Ho Kim, Juho Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By simply composing prompts, developers can prototype novel generative
applications with Large Language Models (LLMs). To refine prototypes into
products, however, developers must iteratively revise prompts by evaluating
outputs to diagnose weaknesses. Formative interviews (N=8) revealed that
developers invest significant effort in manually evaluating outputs as they
assess context-specific and subjective criteria. We present EvalLM, an
interactive system for iteratively refining prompts by evaluating multiple
outputs on user-defined criteria. By describing criteria in natural language,
users can employ the system's LLM-based evaluator to get an overview of where
prompts excel or fail, and improve these based on the evaluator's feedback. A
comparative study (N=12) showed that EvalLM, when compared to manual
evaluation, helped participants compose more diverse criteria, examine twice as
many outputs, and reach satisfactory prompts with 59% fewer revisions. Beyond
prompts, our work can be extended to augment model evaluation and alignment in
specific application contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CHI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large
  Language Models in Knowledge Conflicts <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.13300v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.13300v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, Yu Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By providing external information to large language models (LLMs), tool
augmentation (including retrieval augmentation) has emerged as a promising
solution for addressing the limitations of LLMs' static parametric memory.
However, how receptive are LLMs to such external evidence, especially when the
evidence conflicts with their parametric memory? We present the first
comprehensive and controlled investigation into the behavior of LLMs when
encountering knowledge conflicts. We propose a systematic framework to elicit
high-quality parametric memory from LLMs and construct the corresponding
counter-memory, which enables us to conduct a series of controlled experiments.
Our investigation reveals seemingly contradicting behaviors of LLMs. On the one
hand, different from prior wisdom, we find that LLMs can be highly receptive to
external evidence even when that conflicts with their parametric memory, given
that the external evidence is coherent and convincing. On the other hand, LLMs
also demonstrate a strong confirmation bias when the external evidence contains
some information that is consistent with their parametric memory, despite being
presented with conflicting evidence at the same time. These results pose
important implications that are worth careful consideration for the further
development and deployment of tool- and retrieval-augmented LLMs. Resources are
available at https://github.com/OSU-NLP-Group/LLM-Knowledge-Conflict.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Looking Right is Sometimes Right: Investigating the Capabilities of
  Decoder-only LLMs for Sequence Labeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.14556v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.14556v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Dukić, Jan Šnajder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained language models based on masked language modeling (MLM) excel in
natural language understanding (NLU) tasks. While fine-tuned MLM-based encoders
consistently outperform causal language modeling decoders of comparable size,
recent decoder-only large language models (LLMs) perform on par with smaller
MLM-based encoders. Although their performance improves with scale, LLMs fall
short of achieving state-of-the-art results in information extraction (IE)
tasks, many of which are formulated as sequence labeling (SL). We hypothesize
that LLMs' poor SL performance stems from causal masking, which prevents the
model from attending to tokens on the right of the current token. Yet, how
exactly and to what extent LLMs' performance on SL can be improved remains
unclear. We explore techniques for improving the SL performance of open LLMs on
IE tasks by applying layer-wise removal of the causal mask (CM) during LLM
fine-tuning. This approach yields performance gains competitive with
state-of-the-art SL models, matching or outperforming the results of CM removal
from all blocks. Our findings hold for diverse SL tasks, demonstrating that
open LLMs with layer-dependent CM removal outperform strong MLM-based encoders
and even instruction-tuned LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Anatomy of Neural Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03797v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03797v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Majd Saleh, Stéphane Paquelet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The fields of generative AI and transfer learning have experienced remarkable
advancements in recent years especially in the domain of Natural Language
Processing (NLP). Transformers have been at the heart of these advancements
where the cutting-edge transformer-based Language Models (LMs) have led to new
state-of-the-art results in a wide spectrum of applications. While the number
of research works involving neural LMs is exponentially increasing, their vast
majority are high-level and far from self-contained. Consequently, a deep
understanding of the literature in this area is a tough task especially in the
absence of a unified mathematical framework explaining the main types of neural
LMs. We address the aforementioned problem in this tutorial where the objective
is to explain neural LMs in a detailed, simplified and unambiguous mathematical
framework accompanied by clear graphical illustrations. Concrete examples on
widely used models like BERT and GPT2 are explored. Finally, since transformers
pretrained on language-modeling-like tasks have been widely adopted in computer
vision and time series applications, we briefly explore some examples of such
solutions in order to enable readers to understand how transformers work in the
aforementioned domains and compare this use with the original one in NLP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 Pages; 25 Figures; some typos and notation errors are corrected in
  this version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nemotron-4 15B Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16819v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16819v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jupinder Parmar, Shrimai Prabhumoye, Joseph Jennings, Mostofa Patwary, Sandeep Subramanian, Dan Su, Chen Zhu, Deepak Narayanan, Aastha Jhunjhunwala, Ayush Dattagupta, Vibhu Jawa, Jiwei Liu, Ameya Mahabaleshwarkar, Osvald Nitski, Annika Brundyn, James Maki, Miguel Martinez, Jiaxuan You, John Kamalu, Patrick LeGresley, Denys Fridman, Jared Casper, Ashwath Aithal, Oleksii Kuchaiev, Mohammad Shoeybi, Jonathan Cohen, Bryan Catanzaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Nemotron-4 15B, a 15-billion-parameter large multilingual
language model trained on 8 trillion text tokens. Nemotron-4 15B demonstrates
strong performance when assessed on English, multilingual, and coding tasks: it
outperforms all existing similarly-sized open models on 4 out of 7 downstream
evaluation areas and achieves competitive performance to the leading open
models in the remaining ones. Specifically, Nemotron-4 15B exhibits the best
multilingual capabilities of all similarly-sized models, even outperforming
models over four times larger and those explicitly specialized for multilingual
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LinkNER: Linking Local Named Entity Recognition Models to Large Language
  Models using Uncertainty <span class="chip">WWW'2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10573v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10573v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Zhang, Yuhua Zhao, Hang Gao, Mengting Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Named Entity Recognition (NER) serves as a fundamental task in natural
language understanding, bearing direct implications for web content analysis,
search engines, and information retrieval systems. Fine-tuned NER models
exhibit satisfactory performance on standard NER benchmarks. However, due to
limited fine-tuning data and lack of knowledge, it performs poorly on unseen
entity recognition. As a result, the usability and reliability of NER models in
web-related applications are compromised. Instead, Large Language Models (LLMs)
like GPT-4 possess extensive external knowledge, but research indicates that
they lack specialty for NER tasks. Furthermore, non-public and large-scale
weights make tuning LLMs difficult. To address these challenges, we propose a
framework that combines small fine-tuned models with LLMs (LinkNER) and an
uncertainty-based linking strategy called RDC that enables fine-tuned models to
complement black-box LLMs, achieving better performance. We experiment with
both standard NER test sets and noisy social media datasets. LinkNER enhances
NER task performance, notably surpassing SOTA models in robustness tests. We
also quantitatively analyze the influence of key components like uncertainty
estimation methods, LLMs, and in-context learning on diverse NER tasks,
offering specific web-related recommendations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WebConf (WWW'2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatic <span class="highlight-title">Prompt</span> Augmentation and Selection with Chain-of-Thought from
  Labeled Data <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.12822v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.12822v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        KaShun Shum, Shizhe Diao, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-thought (CoT) advances the reasoning abilities of large language
models (LLMs) and achieves superior performance in complex reasoning tasks.
However, most CoT studies rely on carefully designed human-annotated rational
chains to prompt LLMs, posing challenges for real-world applications where
labeled data is available without rational chains. This paper proposes a new
strategy, Automate-CoT (Automatic Prompt Augmentation and Selection with
Chain-of-Thought), that can bypass human engineering of CoT by automatically
augmenting rational chains from a small labeled dataset, and then pruning
low-quality chains to construct a candidate pool of machine-generated rationale
chains based on the labels. Finally, it selects the optimal combination of
several rationale chains from the pool for CoT prompting by employing a
variance-reduced policy gradient strategy to estimate the significance of each
example. Automate-CoT enables a quick adaptation of the CoT technique to
different tasks. Experimental results demonstrate the effectiveness of our
method, where competitive results are achieved on arithmetic reasoning (+2.7%),
commonsense reasoning (+3.4%), symbolic reasoning (+3.2%), and non-reasoning
tasks (+2.5%). The code is available at
https://github.com/SHUMKASHUN/Automate-CoT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ask Again, Then Fail: Large Language Models' Vacillations in Judgement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02174v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02174v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiming Xie, Zengzhi Wang, Yi Feng, Rui Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the emergence of generative conversational large language models (LLMs)
like ChatGPT, serving as virtual assistants in various fields, the stability
and reliability of their responses have become crucial. However, during usage,
it has been observed that these models tend to waver in their judgements when
confronted with follow-up questions from users expressing skepticism or
disagreement. In this work, we draw inspiration from questioning strategies in
education and propose a \textsc{Follow-up Questioning Mechanism} along with two
evaluation metrics to assess the judgement consistency of LLMs before and after
exposure to disturbances. We evaluate the judgement consistency of ChatGPT,
PaLM2-Bison, and Vicuna-13B under this mechanism across eight reasoning
benchmarks. Empirical results show that even when the initial answers are
correct, judgement consistency sharply decreases when LLMs face disturbances
such as questioning, negation, or misleading. Additionally, we study these
models' judgement consistency under various settings (sampling temperature and
prompts) to validate this issue further, observing the impact of prompt tone
and conducting an in-depth error analysis for deeper behavioral insights.
Furthermore, we also explore several prompting methods to mitigate this issue
and demonstrate their
effectiveness\footnote{\url{https://github.com/NUSTM/LLMs-Waver-In-Judgements}}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Update mitigation results of fine-tuning the model on synthesized
  high-quality preference data with DPO algorithm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HeySQuAD: A Spoken Question Answering <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.13689v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.13689v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijing Wu, SaiKrishna Rallabandi, Ravisutha Srinivasamurthy, Parag Pravin Dakle, Alolika Gon, Preethi Raghavan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spoken question answering (SQA) systems are critical for digital assistants
and other real-world use cases, but evaluating their performance is a challenge
due to the importance of human-spoken questions. This study presents a new
large-scale community-shared SQA dataset called HeySQuAD, which includes 76k
human-spoken questions, 97k machine-generated questions, and their
corresponding textual answers from the SQuAD QA dataset. Our goal is to measure
the ability of machines to accurately understand noisy spoken questions and
provide reliable answers. Through extensive testing, we demonstrate that
training with transcribed human-spoken and original SQuAD questions leads to a
significant improvement (12.51%) in answering human-spoken questions compared
to training with only the original SQuAD textual questions. Moreover,
evaluating with a higher-quality transcription can lead to a further
improvement of 2.03%. This research has significant implications for the
development of SQA systems and their ability to meet the needs of users in
real-world scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak <span class="highlight-title">Prompt</span>s
  Against Open-source LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14872v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14872v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoxia Li, Siyuan Liang, Jiyi Zhang, Han Fang, Aishan Liu, Ee-Chien Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs), used in creative writing, code generation, and
translation, generate text based on input sequences but are vulnerable to
jailbreak attacks, where crafted prompts induce harmful outputs. Most jailbreak
prompt methods use a combination of jailbreak templates followed by questions
to ask to create jailbreak prompts. However, existing jailbreak prompt designs
generally suffer from excessive semantic differences, resulting in an inability
to resist defenses that use simple semantic metrics as thresholds. Jailbreak
prompts are semantically more varied than the original questions used for
queries. In this paper, we introduce a Semantic Mirror Jailbreak (SMJ) approach
that bypasses LLMs by generating jailbreak prompts that are semantically
similar to the original question. We model the search for jailbreak prompts
that satisfy both semantic similarity and jailbreak validity as a
multi-objective optimization problem and employ a standardized set of genetic
algorithms for generating eligible prompts. Compared to the baseline
AutoDAN-GA, SMJ achieves attack success rates (ASR) that are at most 35.4%
higher without ONION defense and 85.2% higher with ONION defense. SMJ's better
performance in all three semantic meaningfulness metrics of Jailbreak Prompt,
Similarity, and Outlier, also means that SMJ is resistant to defenses that use
those metrics as thresholds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Small Language Models Fine-tuned to Coordinate Larger Language Models
  improve Complex Reasoning <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.18338v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.18338v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gurusha Juneja, Subhabrata Dutta, Soumen Chakrabarti, Sunny Manchanda, Tanmoy Chakraborty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) prompted to generate chain-of-thought (CoT)
exhibit impressive reasoning capabilities. Recent attempts at prompt
decomposition toward solving complex, multi-step reasoning problems depend on
the ability of the LLM to simultaneously decompose and solve the problem. A
significant disadvantage is that foundational LLMs are typically not available
for fine-tuning, making adaptation computationally prohibitive. We believe (and
demonstrate) that problem decomposition and solution generation are distinct
capabilites, better addressed in separate modules, than by one monolithic LLM.
We introduce DaSLaM, which uses a decomposition generator to decompose complex
problems into subproblems that require fewer reasoning steps. These subproblems
are answered by a solver. We use a relatively small (13B parameters) LM as the
decomposition generator, which we train using policy gradient optimization to
interact with a solver LM (regarded as black-box) and guide it through
subproblems, thereby rendering our method solver-agnostic. Evaluation on
multiple different reasoning datasets reveal that with our method, a 175
billion parameter LM (text-davinci-003) can produce competitive or even better
performance, compared to its orders-of-magnitude larger successor, GPT-4.
Additionally, we show that DaSLaM is not limited by the solver's capabilities
as a function of scale; e.g., solver LMs with diverse sizes give significant
performance improvement with our solver-agnostic decomposition technique.
Exhaustive ablation studies evince the superiority of our modular finetuning
technique over exorbitantly large decomposer LLMs, based on prompting alone.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2023 (Typos corrected)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-lingual Text-To-Speech with Flow-based Voice Conversion for
  Improved Pronunciation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.17264v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.17264v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Ellinas, Georgios Vamvoukakis, Konstantinos Markopoulos, Georgia Maniati, Panos Kakoulidis, June Sig Sung, Inchul Hwang, Spyros Raptis, Aimilios Chalamandaris, Pirros Tsiakoulis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a method for end-to-end cross-lingual text-to-speech
(TTS) which aims to preserve the target language's pronunciation regardless of
the original speaker's language. The model used is based on a non-attentive
Tacotron architecture, where the decoder has been replaced with a normalizing
flow network conditioned on the speaker identity, allowing both TTS and voice
conversion (VC) to be performed by the same model due to the inherent
linguistic content and speaker identity disentanglement. When used in a
cross-lingual setting, acoustic features are initially produced with a native
speaker of the target language and then voice conversion is applied by the same
model in order to convert these features to the target speaker's voice. We
verify through objective and subjective evaluations that our method can have
benefits compared to baseline cross-lingual synthesis. By including speakers
averaging 7.5 minutes of speech, we also present positive results on
low-resource scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Fundamental changes to the model described and experimental procedure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ANLS* -- A Universal Document Processing Metric for Generative Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03848v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03848v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Peer, Philemon Schöpf, Volckmar Nebendahl, Alexander Rietzler, Sebastian Stabinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditionally, discriminative models have been the predominant choice for
tasks like document classification and information extraction. These models
make predictions that fall into a limited number of predefined classes,
facilitating a binary true or false evaluation and enabling the direct
calculation of metrics such as the F1 score. However, recent advancements in
generative large language models (GLLMs) have prompted a shift in the field due
to their enhanced zero-shot capabilities, which eliminate the need for a
downstream dataset and computationally expensive fine-tuning. However,
evaluating GLLMs presents a challenge as the binary true or false evaluation
used for discriminative models is not applicable to the predictions made by
GLLMs. This paper introduces a new metric for generative models called ANLS*
for evaluating a wide variety of tasks, including information extraction and
classification tasks. The ANLS* metric extends existing ANLS metrics as a
drop-in-replacement and is still compatible with previously reported ANLS
scores. An evaluation of 7 different datasets and 3 different GLLMs using the
ANLS* metric is also provided, demonstrating the importance of the proposed
metric. We also benchmark a novel approach to generate prompts for documents,
called SFT, against other prompting techniques such as LATIN. In 15 out of 21
cases, SFT outperforms other techniques and improves the state-of-the-art,
sometimes by as much as 15 percentage points.
  Sources are available at https://github.com/deepopinion/anls_star_metric
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Few-shot Transfer Learning for Knowledge Base Question Answering: Fusing
  Supervised Models with In-Context Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08894v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08894v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mayur Patidar, Riya Sawhney, Avinash Singh, Biswajit Chatterjee,  Mausam, Indrajit Bhattacharya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing Knowledge Base Question Answering (KBQA) architectures are hungry
for annotated data, which make them costly and time-consuming to deploy. We
introduce the problem of few-shot transfer learning for KBQA, where the target
domain offers only a few labeled examples, but a large labeled training dataset
is available in a source domain. We propose a novel KBQA architecture called
FuSIC-KBQA that performs KB-retrieval using multiple source-trained retrievers,
re-ranks using an LLM and uses this as input for LLM few-shot in-context
learning to generate logical forms, which are further refined using
execution-guided feedback. Experiments over four source-target KBQA pairs of
varying complexity show that FuSIC-KBQA significantly outperforms adaptations
of SoTA KBQA models for this setting. Additional experiments in the in-domain
setting show that FuSIC-KBQA also outperforms SoTA KBQA models when training
data is limited.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ID-XCB: Data-independent Debiasing for Fair and Accurate
  <span class="highlight-title">Transformer</span>-based Cyberbullying Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16458v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16458v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiling Yi, Arkaitz Zubiaga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Swear words are a common proxy to collect datasets with cyberbullying
incidents. Our focus is on measuring and mitigating biases derived from
spurious associations between swear words and incidents occurring as a result
of such data collection strategies. After demonstrating and quantifying these
biases, we introduce ID-XCB, the first data-independent debiasing technique
that combines adversarial training, bias constraints and debias fine-tuning
approach aimed at alleviating model attention to bias-inducing words without
impacting overall model performance. We explore ID-XCB on two popular
session-based cyberbullying datasets along with comprehensive ablation and
generalisation studies. We show that ID-XCB learns robust cyberbullying
detection capabilities while mitigating biases, outperforming state-of-the-art
debiasing methods in both performance and bias mitigation. Our quantitative and
qualitative analyses demonstrate its generalisability to unseen data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FAIR Enough: How Can We Develop and Assess a FAIR-Compliant <span class="highlight-title">Dataset</span> for
  Large Language Models' Training? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11033v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11033v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaina Raza, Shardul Ghuge, Chen Ding, Elham Dolatabadi, Deval Pandya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid evolution of Large Language Models (LLMs) underscores the critical
importance of ethical considerations and data integrity in AI development,
emphasizing the role of FAIR (Findable, Accessible, Interoperable, Reusable)
data principles. While these principles have long been a cornerstone of ethical
data stewardship, their application in LLM training data is less prevalent, an
issue our research aims to address. Our study begins with a review of existing
literature, highlighting the significance of FAIR principles in data management
for model training. Building on this foundation, we introduce a novel framework
that incorporates FAIR principles into the LLM training process. A key aspect
of this approach is a comprehensive checklist, designed to assist researchers
and developers in consistently applying FAIR data principles throughout the
model development lifecycle. The practicality and effectiveness of our
framework are demonstrated through a case study that involves creating a
FAIR-compliant dataset to detect and reduce biases. This case study not only
validates the usefulness of our framework but also establishes new benchmarks
for more equitable, transparent, and ethical practices in LLM training. We
offer this framework to the community as a means to promote technologically
advanced, ethically sound, and socially responsible AI models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Connecting Large Language Models with Evolutionary Algorithms Yields
  Powerful <span class="highlight-title">Prompt</span> Optimizers <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08532v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08532v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, Yujiu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) excel in various tasks, but they rely on
carefully crafted prompts that often demand substantial human effort. To
automate this process, in this paper, we propose a novel framework for discrete
prompt optimization, called EvoPrompt, which borrows the idea of evolutionary
algorithms (EAs) as they exhibit good performance and fast convergence. To
enable EAs to work on discrete prompts, which are natural language expressions
that need to be coherent and human-readable, we connect LLMs with EAs. This
approach allows us to simultaneously leverage the powerful language processing
capabilities of LLMs and the efficient optimization performance of EAs.
Specifically, abstaining from any gradients or parameters, EvoPrompt starts
from a population of prompts and iteratively generates new prompts with LLMs
based on the evolutionary operators, improving the population based on the
development set. We optimize prompts for both closed- and open-source LLMs
including GPT-3.5 and Alpaca, on 31 datasets covering language understanding,
generation tasks, as well as BIG-Bench Hard (BBH) tasks. EvoPrompt
significantly outperforms human-engineered prompts and existing methods for
automatic prompt generation (e.g., up to 25% on BBH). Furthermore, EvoPrompt
demonstrates that connecting LLMs with EAs creates synergies, which could
inspire further research on the combination of LLMs and conventional
algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Learning Representations (ICLR) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling the Authoring of AutoTutors with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09216v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09216v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sankalan Pal Chowdhury, Vilém Zouhar, Mrinmaya Sachan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have found several use cases in education,
ranging from automatic question generation to essay evaluation. In this paper,
we explore the potential of using Large Language Models (LLMs) to author
Intelligent Tutoring Systems. A common pitfall of LLMs is their straying from
desired pedagogical strategies such as leaking the answer to the student, and
in general, providing no guarantees. We posit that while LLMs with certain
guardrails can take the place of subject experts, the overall pedagogical
design still needs to be handcrafted for the best learning results. Based on
this principle, we create a sample end-to-end tutoring system named MWPTutor,
which uses LLMs to fill in the state space of a pre-defined finite state
transducer. This approach retains the structure and the pedagogy of traditional
tutoring systems that has been developed over the years by learning scientists
but brings in additional flexibility of LLM-based approaches. Through a human
evaluation study on two datasets based on math word problems, we show that our
hybrid approach achieves a better overall tutoring score than an instructed,
but otherwise free-form, GPT-4. MWPTutor is completely modular and opens up the
scope for the community to improve its performance by improving individual
modules or using different teaching strategies that it can follow
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Citation-Enhanced Generation for LLM-based Chatbot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16063v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16063v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weitao Li, Junkai Li, Weizhi Ma, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) exhibit powerful general intelligence across
diverse scenarios, including their integration into chatbots. However, a vital
challenge of LLM-based chatbots is that they may produce hallucinated content
in responses, which significantly limits their applicability. Various efforts
have been made to alleviate hallucination, such as retrieval augmented
generation and reinforcement learning with human feedback, but most of them
require additional training and data annotation. In this paper, we propose a
novel post-hoc Citation-Enhanced Generation (CEG) approach combined with
retrieval argumentation. Unlike previous studies that focus on preventing
hallucinations during generation, our method addresses this issue in a post-hoc
way. It incorporates a retrieval module to search for supporting documents
relevant to the generated content, and employs a natural language
inference-based citation generation module. Once the statements in the
generated content lack of reference, our model can regenerate responses until
all statements are supported by citations. Note that our method is a
training-free plug-and-play plugin that is capable of various LLMs. Experiments
on various hallucination-related datasets show our framework outperforms
state-of-the-art methods in both hallucination detection and response
regeneration on three benchmarks. Our codes and dataset will be publicly
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Agents as Optimizable Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16823v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16823v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, Jürgen Schmidhuber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Various human-designed prompt engineering techniques have been proposed to
improve problem solvers based on Large Language Models (LLMs), yielding many
disparate code bases. We unify these approaches by describing LLM-based agents
as computational graphs. The nodes implement functions to process multimodal
data or query LLMs, and the edges describe the information flow between
operations. Graphs can be recursively combined into larger composite graphs
representing hierarchies of inter-agent collaboration (where edges connect
operations of different agents). Our novel automatic graph optimizers (1)
refine node-level LLM prompts (node optimization) and (2) improve agent
orchestration by changing graph connectivity (edge optimization). Experiments
demonstrate that our framework can be used to efficiently develop, integrate,
and automatically improve various LLM agents. The code can be found at
https://github.com/metauto-ai/gptswarm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Website: https://gptswarm.org ; Github Repo:
  https://github.com/metauto-ai/gptswarm ; Replace to fix typos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Know<span class="highlight-title">GPT</span>: Knowledge Injection for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06185v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06185v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinggang Zhang, Junnan Dong, Hao Chen, Daochen Zha, Zailiang Yu, Xiao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Large Language Models (LLMs), such as ChatGPT, offer interactive
APIs that can answer common questions at a human-expert level. However, these
models often give inaccurate or incorrect responses when faced with questions
requiring domain-specific or professional-specific knowledge not covered in
their training corpus. Furthermore, many state-of-the-art LLMs are not
open-source, making it challenging to inject knowledge with model APIs only. In
this work, we introduce KnowGPT, a black-box knowledge injection framework for
LLMs in question answering. KnowGPT leverages deep reinforcement learning (RL)
to extract relevant knowledge from Knowledge Graphs (KGs) and use Multi-Armed
Bandit (MAB) to construct the most suitable prompt for each question. Our
extensive experiments on three benchmark datasets showcase that KnowGPT
significantly enhances the existing methods. Notably, KnowGPT achieves an
average improvement of 23.7% over ChatGPT and an average improvement of 2.9%
over GPT-4. Additionally, KnowGPT attains a 91.6% accuracy on the OpenbookQA
official leaderboard, which is comparable to human-level performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-domain Chinese Sentence Pattern Parsing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16311v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16311v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingsi Yu, Cunliang Kong, Liner Yang, Meishan Zhang, Lin Zhu, Yujie Wang, Haozhe Lin, Maosong Sun, Erhong Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sentence Pattern Structure (SPS) parsing is a syntactic analysis method
primarily employed in language teaching.Existing SPS parsers rely heavily on
textbook corpora for training, lacking cross-domain capability.To overcome this
constraint, this paper proposes an innovative approach leveraging large
language models (LLMs) within a self-training framework. Partial syntactic
rules from a source domain are combined with target domain sentences to
dynamically generate training data, enhancing the adaptability of the parser to
diverse domains.Experiments conducted on textbook and news domains demonstrate
the effectiveness of the proposed method, outperforming rule-based baselines by
1.68 points on F1 metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distinguishing the Knowable from the Unknowable with Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03563v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03563v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gustaf Ahdritz, Tian Qin, Nikhil Vyas, Boaz Barak, Benjamin L. Edelman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the feasibility of identifying epistemic uncertainty (reflecting a
lack of knowledge), as opposed to aleatoric uncertainty (reflecting entropy in
the underlying distribution), in the outputs of large language models (LLMs)
over free-form text. In the absence of ground-truth probabilities, we explore a
setting where, in order to (approximately) disentangle a given LLM's
uncertainty, a significantly larger model stands in as a proxy for the ground
truth. We show that small linear probes trained on the embeddings of frozen,
pretrained models accurately predict when larger models will be more confident
at the token level and that probes trained on one text domain generalize to
others. Going further, we propose a fully unsupervised method that achieves
non-trivial accuracy on the same task. Taken together, we interpret these
results as evidence that LLMs naturally contain internal representations of
different types of uncertainty that could potentially be leveraged to devise
more informative indicators of model confidence in diverse practical settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity
  within Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13516v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13516v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyang Song, Xu Han, Zhengyan Zhang, Shengding Hu, Xiyu Shi, Kuai Li, Chen Chen, Zhiyuan Liu, Guangli Li, Tao Yang, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Activation sparsity refers to the existence of considerable
weakly-contributed elements among activation outputs. As a prevalent property
of the models using the ReLU activation function, it has been proven a
promising paradigm to boost model inference efficiency. Nevertheless, most
large language models (LLMs) adopt activation functions without intrinsic
activation sparsity (e.g., GELU and Swish). Some recent efforts have explored
introducing ReLU or its variants as the substitutive activation function to
help LLMs achieve activation sparsity and inference acceleration, but few can
simultaneously obtain high sparsity and comparable model performance. This
paper introduces an effective sparsification method named "ProSparse" to push
LLMs for higher activation sparsity without decreasing model performance.
Specifically, after substituting the activation function of LLMs with ReLU,
ProSparse adopts progressive sparsity regularization with a factor smoothly
increasing along sine curves in multiple stages. This can enhance activation
sparsity and alleviate performance degradation by avoiding radical shifts in
activation distribution. With ProSparse, we obtain high sparsity of 89.32% and
88.80% for LLaMA2-7B and LLaMA2-13B, respectively, achieving comparable
performance to their original Swish-activated versions. Our inference
acceleration experiments further demonstrate the practical acceleration brought
by higher activation sparsity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 3 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Frustratingly Simple Decoding Method for Neural Text Generation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.12675v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.12675v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Yang, Deng Cai, Huayang Li, Wei Bi, Wai Lam, Shuming Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a frustratingly simple, super efficient and surprisingly
effective decoding method, which we call Frustratingly Simple Decoding (FSD),
for neural text generation. The idea behind FSD is straightforward: we build an
anti-LM based on previously generated text and use this anti-LM to penalize
future generation of what has been generated. The anti-LM can be implemented as
simple as an n-gram language model or a vectorized variant. In this way, FSD
introduces no extra model parameters and negligible computational overhead (FSD
can be as fast as greedy search). Despite the simplicity, FSD is surprisingly
effective; Experiments show that FSD can outperform the canonical methods to
date (i.e., nucleus sampling) as well as several strong baselines that were
proposed recently.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-Coling 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12563v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12563v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Loka Li, Guangyi Chen, Yusheng Su, Zhenhao Chen, Yixuan Zhang, Eric Xing, Kun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent success of Large Language Models (LLMs) has catalyzed an
increasing interest in their self-correction capabilities. This paper presents
a comprehensive investigation into the intrinsic self-correction of LLMs,
attempting to address the ongoing debate about its feasibility. Our research
has identified an important latent factor - the "confidence" of LLMs - during
the self-correction process. Overlooking this factor may cause the models to
over-criticize themselves, resulting in unreliable conclusions regarding the
efficacy of self-correction. We have experimentally observed that LLMs possess
the capability to understand the "confidence" in their own responses. It
motivates us to develop an "If-or-Else" (IoE) prompting framework, designed to
guide LLMs in assessing their own "confidence", facilitating intrinsic
self-corrections. We conduct extensive experiments and demonstrate that our
IoE-based Prompt can achieve a consistent improvement regarding the accuracy of
self-corrected responses over the initial answers. Our study not only sheds
light on the underlying factors affecting self-correction in LLMs, but also
introduces a practical framework that utilizes the IoE prompting principle to
efficiently improve self-correction capabilities with "confidence". The code is
available at https://github.com/MBZUAI-CLeaR/IoE-Prompting.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EHRNoteQA: A Patient-Specific Question Answering Benchmark for
  Evaluating Large Language Models in Clinical Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16040v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16040v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunjun Kweon, Jiyoun Kim, Heeyoung Kwak, Dongchul Cha, Hangyul Yoon, Kwanghyun Kim, Seunghyun Won, Edward Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study introduces EHRNoteQA, a novel patient-specific question answering
benchmark tailored for evaluating Large Language Models (LLMs) in clinical
environments. Based on MIMIC-IV Electronic Health Record (EHR), a team of three
medical professionals has curated the dataset comprising 962 unique questions,
each linked to a specific patient's EHR clinical notes. What makes EHRNoteQA
distinct from existing EHR-based benchmarks is as follows: Firstly, it is the
first dataset to adopt a multi-choice question answering format, a design
choice that effectively evaluates LLMs with reliable scores in the context of
automatic evaluation, compared to other formats. Secondly, it requires an
analysis of multiple clinical notes to answer a single question, reflecting the
complex nature of real-world clinical decision-making where clinicians review
extensive records of patient histories. Our comprehensive evaluation on various
large language models showed that their scores on EHRNoteQA correlate more
closely with their performance in addressing real-world medical questions
evaluated by clinicians than their scores from other LLM benchmarks. This
underscores the significance of EHRNoteQA in evaluating LLMs for medical
applications and highlights its crucial role in facilitating the integration of
LLMs into healthcare systems. The dataset will be made available to the public
under PhysioNet credential access, promoting further research in this vital
field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cascade Speculative Drafting for Even Faster LLM Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11462v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11462v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyi Chen, Xiaocong Yang, Jiacheng Lin, Chenkai Sun, Kevin Chen-Chuan Chang, Jie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Introduced to enhance the efficiency of large language model (LLM) inference,
speculative decoding operates by having a smaller model generate a draft. A
larger target model then reviews this draft to align with its output, and any
acceptance by the target model results in a reduction of the number of the
target model runs, ultimately improving efficiency. However, the drafting
process in speculative decoding includes slow autoregressive generation and
allocates equal time to generating tokens, irrespective of their importance.
These inefficiencies collectively contribute to the suboptimal performance of
speculative decoding. To further improve LLM inference, we introduce Cascade
Speculative Drafting (CS Drafting), a speculative execution algorithm that
incorporates two types of cascades. The Vertical Cascade eliminates
autoregressive generation from neural models, while the Horizontal Cascade
optimizes time allocation in drafting for improved efficiency. Combining both
cascades, CS Drafting achieves up to an 81 percent additional speedup over
speculative decoding in our experiments, while maintaining the same output
distribution as the target model. Our code is publicly available at
https://github.com/lfsszd/CS-Drafting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine Unlearning of <span class="highlight-title">Pre-train</span>ed Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15159v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15159v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Yao, Eli Chien, Minxin Du, Xinyao Niu, Tianhao Wang, Zezhou Cheng, Xiang Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the concept of the `right to be forgotten' within the
context of large language models (LLMs). We explore machine unlearning as a
pivotal solution, with a focus on pre-trained models--a notably
under-researched area. Our research delineates a comprehensive framework for
machine unlearning in pre-trained LLMs, encompassing a critical analysis of
seven diverse unlearning methods. Through rigorous evaluation using curated
datasets from arXiv, books, and GitHub, we establish a robust benchmark for
unlearning performance, demonstrating that these methods are over $10^5$ times
more computationally efficient than retraining. Our results show that
integrating gradient ascent with gradient descent on in-distribution data
improves hyperparameter robustness. We also provide detailed guidelines for
efficient hyperparameter tuning in the unlearning process. Our findings advance
the discourse on ethical AI practices, offering substantive insights into the
mechanics of machine unlearning for pre-trained LLMs and underscoring the
potential for responsible AI development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/yaojin17/Unlearning_LLM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FuseChat: Knowledge Fusion of Chat Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16107v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16107v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fanqi Wan, Ziyi Yang, Longguang Zhong, Xiaojun Quan, Xinting Huang, Wei Bi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While training large language models (LLMs) from scratch can indeed lead to
models with distinct capabilities and strengths, this approach incurs
substantial costs and may lead to potential redundancy in competencies. An
alternative strategy is to combine existing LLMs into a more robust LLM,
thereby diminishing the necessity for expensive pre-training. However, due to
the diverse architectures of LLMs, direct parameter blending proves to be
unfeasible. Recently, \textsc{FuseLLM} introduced the concept of knowledge
fusion to transfer the collective knowledge of multiple structurally varied
LLMs into a target LLM through lightweight continual training. In this report,
we extend the scalability and flexibility of the \textsc{FuseLLM} framework to
realize the fusion of chat LLMs, resulting in \textsc{FuseChat}.
\textsc{FuseChat} comprises two main stages. Firstly, we undertake knowledge
fusion for structurally and scale-varied source LLMs to derive multiple target
LLMs of identical structure and size via lightweight fine-tuning. Then, these
target LLMs are merged within the parameter space, wherein we propose a novel
method for determining the merging weights based on the variation ratio of
parameter matrices before and after fine-tuning. We validate our approach using
three prominent chat LLMs with diverse architectures and scales, namely
\texttt{NH2-Mixtral-8x7B}, \texttt{NH2-Solar-10.7B}, and
\texttt{OpenChat-3.5-7B}. Experimental results spanning various chat domains
demonstrate the superiority of \texttt{\textsc{FuseChat}-7B} across a broad
spectrum of chat LLMs at 7B and 34B scales, even surpassing \texttt{GPT-3.5
(March)} and approaching \texttt{Mixtral-8x7B-Instruct}. Our code, model
weights, and data are openly accessible at
\url{https://github.com/fanqiwan/FuseLLM}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report, work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HarmBench: A Standardized Evaluation Framework for Automated Red Teaming
  and Robust Refusal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04249v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04249v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo Li, David Forsyth, Dan Hendrycks
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated red teaming holds substantial promise for uncovering and mitigating
the risks associated with the malicious use of large language models (LLMs),
yet the field lacks a standardized evaluation framework to rigorously assess
new methods. To address this issue, we introduce HarmBench, a standardized
evaluation framework for automated red teaming. We identify several desirable
properties previously unaccounted for in red teaming evaluations and
systematically design HarmBench to meet these criteria. Using HarmBench, we
conduct a large-scale comparison of 18 red teaming methods and 33 target LLMs
and defenses, yielding novel insights. We also introduce a highly efficient
adversarial training method that greatly enhances LLM robustness across a wide
range of attacks, demonstrating how HarmBench enables codevelopment of attacks
and defenses. We open source HarmBench at
https://github.com/centerforaisafety/HarmBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: https://www.harmbench.org</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Where Do People Tell Stories Online? Story Detection Across Online
  Communities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09675v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09675v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Antoniak, Joel Mire, Maarten Sap, Elliott Ash, Andrew Piper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Story detection in online communities is a challenging task as stories are
scattered across communities and interwoven with non-storytelling spans within
a single text. We address this challenge by building and releasing the
StorySeeker toolkit, including a richly annotated dataset of 502 Reddit posts
and comments, a detailed codebook adapted to the social media context, and
models to predict storytelling at the document and span level. Our dataset is
sampled from hundreds of popular English-language Reddit communities ranging
across 33 topic categories, and it contains fine-grained expert annotations,
including binary story labels, story spans, and event spans. We evaluate a
range of detection methods using our data, and we identify the distinctive
textual features of online storytelling, focusing on storytelling span
detection, which we introduce as a new task. We illuminate distributional
characteristics of storytelling on a large community-centric social media
platform, and we also conduct a case study on r/ChangeMyView, where
storytelling is used as one of many persuasive strategies, illustrating that
our data and models can be used for both inter- and intra-community research.
Finally, we discuss implications of our tools and analyses for narratology and
the study of online communities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Acquiring Clean Language Models from Backdoor Poisoned <span class="highlight-title">Dataset</span>s by
  Downscaling Frequency Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12026v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12026v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongru Wu, Zhuosheng Zhang, Pengzhou Cheng, Gongshen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the notable success of language models (LMs) in various natural
language processing (NLP) tasks, the reliability of LMs is susceptible to
backdoor attacks. Prior research attempts to mitigate backdoor learning while
training the LMs on the poisoned dataset, yet struggles against complex
backdoor attacks in real-world scenarios. In this paper, we investigate the
learning mechanisms of backdoor LMs in the frequency space by Fourier analysis.
Our findings indicate that the backdoor mapping presented on the poisoned
datasets exhibits a more discernible inclination towards lower frequency
compared to clean mapping, resulting in the faster convergence of backdoor
mapping. To alleviate this dilemma, we propose Multi-Scale Low-Rank Adaptation
(MuScleLoRA), which deploys multiple radial scalings in the frequency space
with low-rank adaptation to the target model and further aligns the gradients
when updating parameters. Through downscaling in the frequency space,
MuScleLoRA encourages the model to prioritize the learning of relatively
high-frequency clean mapping, consequently mitigating backdoor learning.
Experimental results demonstrate that MuScleLoRA outperforms baselines
significantly. Notably, MuScleLoRA reduces the average success rate of diverse
backdoor attacks to below 15\% across multiple datasets and generalizes to
various backbone LMs, including BERT, RoBERTa, and Llama2. The codes are
available at https://github.com/ZrW00/MuScleLoRA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Few-Shot Dialogue Summarization via Skeleton-Assisted <span class="highlight-title">Prompt</span> Transfer in
  <span class="highlight-title">Prompt</span> Tuning <span class="chip">EACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.12077v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.12077v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaige Xie, Tong Yu, Haoliang Wang, Junda Wu, Handong Zhao, Ruiyi Zhang, Kanak Mahadik, Ani Nenkova, Mark Riedl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world scenarios, labeled samples for dialogue summarization are
usually limited (i.e., few-shot) due to high annotation costs for high-quality
dialogue summaries. To efficiently learn from few-shot samples, previous works
have utilized massive annotated data from other downstream tasks and then
performed prompt transfer in prompt tuning so as to enable cross-task knowledge
transfer. However, existing general-purpose prompt transfer techniques lack
consideration for dialogue-specific information. In this paper, we focus on
improving the prompt transfer from dialogue state tracking to dialogue
summarization and propose Skeleton-Assisted Prompt Transfer (SAPT), which
leverages skeleton generation as extra supervision that functions as a medium
connecting the distinct source and target task and resulting in the model's
better consumption of dialogue state information. To automatically extract
dialogue skeletons as supervised training data for skeleton generation, we
design a novel approach with perturbation-based probes requiring neither
annotation effort nor domain knowledge. Training the model on such skeletons
can also help preserve model capability during prompt transfer. Our method
significantly outperforms existing baselines. In-depth analyses demonstrate the
effectiveness of our method in facilitating cross-task knowledge transfer in
few-shot dialogue summarization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks
  with Self-Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15180v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15180v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heegyu Kim, Sehyun Yuk, Hyunsouk Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Caution: This paper includes offensive words that could potentially cause
unpleasantness. Language models (LMs) are vulnerable to exploitation for
adversarial misuse. Training LMs for safety alignment is extensive and makes it
hard to respond to fast-developing attacks immediately, such as jailbreaks. We
propose self-refine with formatting that achieves outstanding safety even in
non-safety-aligned LMs and evaluate our method alongside several defense
baselines, demonstrating that it is the safest training-free method against
jailbreak attacks. Additionally, we proposed a formatting method that improves
the efficiency of the self-refine process while reducing attack success rates
in fewer iterations. We've also observed that non-safety-aligned LMs outperform
safety-aligned LMs in safety tasks by giving more helpful and safe responses.
In conclusion, our findings can achieve less safety risk with fewer
computational costs, allowing non-safety LM to be easily utilized in real-world
service.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Robust Cybersecurity Topic Classification Tool 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.02473v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.02473v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elijah Pelofske, Lorie M. Liebrock, Vincent Urias
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this research, we use user defined labels from three internet text sources
(Reddit, Stackexchange, Arxiv) to train 21 different machine learning models
for the topic classification task of detecting cybersecurity discussions in
natural text. We analyze the false positive and false negative rates of each of
the 21 model's in a cross validation experiment. Then we present a
Cybersecurity Topic Classification (CTC) tool, which takes the majority vote of
the 21 trained machine learning models as the decision mechanism for detecting
cybersecurity related text. We also show that the majority vote mechanism of
the CTC tool provides lower false negative and false positive rates on average
than any of the 21 individual models. We show that the CTC tool is scalable to
the hundreds of thousands of documents with a wall clock time on the order of
hours.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Improved formatting</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GUARD: Role-playing to Generate Natural-language Jailbreakings to Test
  Guideline Adherence of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03299v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03299v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haibo Jin, Ruoxi Chen, Andy Zhou, Jinyin Chen, Yang Zhang, Haohan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The discovery of "jailbreaks" to bypass safety filters of Large Language
Models (LLMs) and harmful responses have encouraged the community to implement
safety measures. One major safety measure is to proactively test the LLMs with
jailbreaks prior to the release. Therefore, such testing will require a method
that can generate jailbreaks massively and efficiently. In this paper, we
follow a novel yet intuitive strategy to generate jailbreaks in the style of
the human generation. We propose a role-playing system that assigns four
different roles to the user LLMs to collaborate on new jailbreaks. Furthermore,
we collect existing jailbreaks and split them into different independent
characteristics using clustering frequency and semantic patterns sentence by
sentence. We organize these characteristics into a knowledge graph, making them
more accessible and easier to retrieve. Our system of different roles will
leverage this knowledge graph to generate new jailbreaks, which have proved
effective in inducing LLMs to generate unethical or guideline-violating
responses. In addition, we also pioneer a setting in our system that will
automatically follow the government-issued guidelines to generate jailbreaks to
test whether LLMs follow the guidelines accordingly. We refer to our system as
GUARD (Guideline Upholding through Adaptive Role-play Diagnostics). We have
empirically validated the effectiveness of GUARD on three cutting-edge
open-sourced LLMs (Vicuna-13B, LongChat-7B, and Llama-2-7B), as well as a
widely-utilized commercial LLM (ChatGPT). Moreover, our work extends to the
realm of vision language models (MiniGPT-v2 and Gemini Vision Pro), showcasing
GUARD's versatility and contributing valuable insights for the development of
safer, more reliable LLM-based applications across diverse modalities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 papges</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Systematic <span class="highlight-title">Review</span> of Data-to-Text NLG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08496v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08496v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chinonso Cynthia Osuji, Thiago Castro Ferreira, Brian Davis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This systematic review undertakes a comprehensive analysis of current
research on data-to-text generation, identifying gaps, challenges, and future
directions within the field. Relevant literature in this field on datasets,
evaluation metrics, application areas, multilingualism, language models, and
hallucination mitigation methods is reviewed. Various methods for producing
high-quality text are explored, addressing the challenge of hallucinations in
data-to-text generation. These methods include re-ranking, traditional and
neural pipeline architecture, planning architectures, data cleaning, controlled
generation, and modification of models and training techniques. Their
effectiveness and limitations are assessed, highlighting the need for
universally applicable strategies to mitigate hallucinations. The review also
examines the usage, popularity, and impact of datasets, alongside evaluation
metrics, with an emphasis on both automatic and human assessment. Additionally,
the evolution of data-to-text models, particularly the widespread adoption of
transformer models, is discussed. Despite advancements in text quality, the
review emphasizes the importance of research in low-resourced languages and the
engineering of datasets in these languages to promote inclusivity. Finally,
several application domains of data-to-text are highlighted, emphasizing their
relevance in such domains. Overall, this review serves as a guiding framework
for fostering innovation and advancing data-to-text generation.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion Meets DAgger: Supercharging Eye-in-hand Imitation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17768v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17768v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Zhang, Matthew Chang, Pranav Kumar, Saurabh Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A common failure mode for policies trained with imitation is compounding
execution errors at test time. When the learned policy encounters states that
were not present in the expert demonstrations, the policy fails, leading to
degenerate behavior. The Dataset Aggregation, or DAgger approach to this
problem simply collects more data to cover these failure states. However, in
practice, this is often prohibitively expensive. In this work, we propose
Diffusion Meets DAgger (DMD), a method to reap the benefits of DAgger without
the cost for eye-in-hand imitation learning problems. Instead of collecting new
samples to cover out-of-distribution states, DMD uses recent advances in
diffusion models to create these samples with diffusion models. This leads to
robust performance from few demonstrations. In experiments conducted for
non-prehensile pushing on a Franka Research 3, we show that DMD can achieve a
success rate of 80% with as few as 8 expert demonstrations, where naive
behavior cloning reaches only 20%. DMD also outperform competing NeRF-based
augmentation schemes by 50%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>for project website with video, see
  https://sites.google.com/view/diffusion-meets-dagger</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Opening Cabinets and Drawers in the Real World using a Commodity Mobile
  Manipulator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17767v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17767v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arjun Gupta, Michelle Zhang, Rishik Sathua, Saurabh Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pulling open cabinets and drawers presents many difficult technical
challenges in perception (inferring articulation parameters for objects from
onboard sensors), planning (producing motion plans that conform to tight task
constraints), and control (making and maintaining contact while applying forces
on the environment). In this work, we build an end-to-end system that enables a
commodity mobile manipulator (Stretch RE2) to pull open cabinets and drawers in
diverse previously unseen real world environments. We conduct 4 days of real
world testing of this system spanning 31 different objects from across 13
different real world environments. Our system achieves a success rate of 61% on
opening novel cabinets and drawers in unseen environments zero-shot. An
analysis of the failure modes suggests that errors in perception are the most
significant challenge for our system. We will open source code and models for
others to replicate and build upon our system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project webpage:
  https://arjung128.github.io/opening-cabinets-and-drawers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ShapeLLM: Universal 3D Object Understanding for Embodied Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zekun Qi, Runpei Dong, Shaochen Zhang, Haoran Geng, Chunrui Han, Zheng Ge, Li Yi, Kaisheng Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents ShapeLLM, the first 3D Multimodal Large Language Model
(LLM) designed for embodied interaction, exploring a universal 3D object
understanding with 3D point clouds and languages. ShapeLLM is built upon an
improved 3D encoder by extending ReCon to ReCon++ that benefits from multi-view
image distillation for enhanced geometry understanding. By utilizing ReCon++ as
the 3D point cloud input encoder for LLMs, ShapeLLM is trained on constructed
instruction-following data and tested on our newly human-curated evaluation
benchmark, 3D MM-Vet. ReCon++ and ShapeLLM achieve state-of-the-art performance
in 3D geometry understanding and language-unified 3D interaction tasks, such as
embodied visual grounding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ADL4D: Towards A Contextually Rich <span class="highlight-title">Dataset</span> for 4D Activities of Daily
  Living 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17758v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17758v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marsil Zakour, Partha Pratim Nath, Ludwig Lohmer, Emre Faik Gökçe, Martin Piccolrovazzi, Constantin Patsch, Yuankai Wu, Rahul Chaudhari, Eckehard Steinbach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hand-Object Interactions (HOIs) are conditioned on spatial and temporal
contexts like surrounding objects, pre- vious actions, and future intents (for
example, grasping and handover actions vary greatly based on objects proximity
and trajectory obstruction). However, existing datasets for 4D HOI (3D HOI over
time) are limited to one subject inter- acting with one object only. This
restricts the generalization of learning-based HOI methods trained on those
datasets. We introduce ADL4D, a dataset of up to two subjects inter- acting
with different sets of objects performing Activities of Daily Living (ADL) like
breakfast or lunch preparation ac- tivities. The transition between multiple
objects to complete a certain task over time introduces a unique context
lacking in existing datasets. Our dataset consists of 75 sequences with a total
of 1.1M RGB-D frames, hand and object poses, and per-hand fine-grained action
annotations. We develop an automatic system for multi-view multi-hand 3D pose
an- notation capable of tracking hand poses over time. We inte- grate and test
it against publicly available datasets. Finally, we evaluate our dataset on the
tasks of Hand Mesh Recov- ery (HMR) and Hand Action Segmentation (HAS).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LoDIP: Low light phase retrieval with deep image prior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17745v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17745v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raunak Manekar, Elisa Negrini, Minh Pham, Daniel Jacobs, Jaideep Srivastava
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Phase retrieval (PR) is a fundamental challenge in scientific imaging,
enabling nanoscale techniques like coherent diffractive imaging (CDI). Imaging
at low radiation doses becomes important in applications where samples are
susceptible to radiation damage. However, most PR methods struggle in low dose
scenario due to the presence of very high shot noise. Advancements in the
optical data acquisition setup, exemplified by in-situ CDI, have shown
potential for low-dose imaging. But these depend on a time series of
measurements, rendering them unsuitable for single-image applications.
Similarly, on the computational front, data-driven phase retrieval techniques
are not readily adaptable to the single-image context. Deep learning based
single-image methods, such as deep image prior, have been effective for various
imaging tasks but have exhibited limited success when applied to PR. In this
work, we propose LoDIP which combines the in-situ CDI setup with the power of
implicit neural priors to tackle the problem of single-image low-dose phase
retrieval. Quantitative evaluations demonstrate the superior performance of
LoDIP on this task as well as applicability to real experimental scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analyzing Regional Organization of the Human Hippocampus in 3D-PLI Using
  Contrastive Learning and Geometric Unfolding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Oberstrass, Jordan DeKraker, Nicola Palomero-Gallagher, Sascha E. A. Muenzing, Alan C. Evans, Markus Axer, Katrin Amunts, Timo Dickscheid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the cortical organization of the human brain requires
interpretable descriptors for distinct structural and functional imaging data.
3D polarized light imaging (3D-PLI) is an imaging modality for visualizing
fiber architecture in postmortem brains with high resolution that also captures
the presence of cell bodies, for example, to identify hippocampal subfields.
The rich texture in 3D-PLI images, however, makes this modality particularly
difficult to analyze and best practices for characterizing architectonic
patterns still need to be established. In this work, we demonstrate a novel
method to analyze the regional organization of the human hippocampus in 3D-PLI
by combining recent advances in unfolding methods with deep texture features
obtained using a self-supervised contrastive learning approach. We identify
clusters in the representations that correspond well with classical
descriptions of hippocampal subfields, lending validity to the developed
methodology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ISBI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Fairness-Aware Adversarial Learning <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanghao Zhang, Tianle Zhang, Ronghui Mu, Xiaowei Huang, Wenjie Ruan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although adversarial training (AT) has proven effective in enhancing the
model's robustness, the recently revealed issue of fairness in robustness has
not been well addressed, i.e. the robust accuracy varies significantly among
different categories. In this paper, instead of uniformly evaluating the
model's average class performance, we delve into the issue of robust fairness,
by considering the worst-case distribution across various classes. We propose a
novel learning paradigm, named Fairness-Aware Adversarial Learning (FAAL). As a
generalization of conventional AT, we re-define the problem of adversarial
training as a min-max-max framework, to ensure both robustness and fairness of
the trained model. Specifically, by taking advantage of distributional robust
optimization, our method aims to find the worst distribution among different
categories, and the solution is guaranteed to obtain the upper bound
performance with high probability. In particular, FAAL can fine-tune an unfair
robust model to be fair within only two epochs, without compromising the
overall clean and robust accuracies. Extensive experiments on various image
datasets validate the superior performance and efficiency of the proposed FAAL
compared to other state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work will appear in the CVPR 2024 conference proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VRP-SAM: SAM with Visual Reference <span class="highlight-title">Prompt</span> <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanpeng Sun, Jiahui Chen, Shan Zhang, Xinyu Zhang, Qiang Chen, Gang Zhang, Errui Ding, Jingdong Wang, Zechao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel Visual Reference Prompt (VRP) encoder that
empowers the Segment Anything Model (SAM) to utilize annotated reference images
as prompts for segmentation, creating the VRP-SAM model. In essence, VRP-SAM
can utilize annotated reference images to comprehend specific objects and
perform segmentation of specific objects in target image. It is note that the
VRP encoder can support a variety of annotation formats for reference images,
including \textbf{point}, \textbf{box}, \textbf{scribble}, and \textbf{mask}.
VRP-SAM achieves a breakthrough within the SAM framework by extending its
versatility and applicability while preserving SAM's inherent strengths, thus
enhancing user-friendliness. To enhance the generalization ability of VRP-SAM,
the VRP encoder adopts a meta-learning strategy. To validate the effectiveness
of VRP-SAM, we conducted extensive empirical studies on the Pascal and COCO
datasets. Remarkably, VRP-SAM achieved state-of-the-art performance in visual
reference segmentation with minimal learnable parameters. Furthermore, VRP-SAM
demonstrates strong generalization capabilities, allowing it to perform
segmentation of unseen objects and enabling cross-domain segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024; It is not the camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MedContext: Learning Contextual Cues for Efficient Volumetric Medical
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanan Gani, Muzammal Naseer, Fahad Khan, Salman Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Volumetric medical segmentation is a critical component of 3D medical image
analysis that delineates different semantic regions. Deep neural networks have
significantly improved volumetric medical segmentation, but they generally
require large-scale annotated data to achieve better performance, which can be
expensive and prohibitive to obtain. To address this limitation, existing works
typically perform transfer learning or design dedicated pretraining-finetuning
stages to learn representative features. However, the mismatch between the
source and target domain can make it challenging to learn optimal
representation for volumetric data, while the multi-stage training demands
higher compute as well as careful selection of stage-specific design choices.
In contrast, we propose a universal training framework called MedContext that
is architecture-agnostic and can be incorporated into any existing training
framework for 3D medical segmentation. Our approach effectively learns self
supervised contextual cues jointly with the supervised voxel segmentation task
without requiring large-scale annotated volumetric medical data or dedicated
pretraining-finetuning stages. The proposed approach induces contextual
knowledge in the network by learning to reconstruct the missing organ or parts
of an organ in the output segmentation space. The effectiveness of MedContext
is validated across multiple 3D medical datasets and four state-of-the-art
model architectures. Our approach demonstrates consistent gains in segmentation
performance across datasets and different architectures even in few-shot data
scenarios. Our code and pretrained models are available at
https://github.com/hananshafi/MedContext
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at https://github.com/hananshafi/MedContext</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion
  Latent Aligners <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17723v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17723v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yazhou Xing, Yingqing He, Zeyue Tian, Xintao Wang, Qifeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video and audio content creation serves as the core technique for the movie
industry and professional users. Recently, existing diffusion-based methods
tackle video and audio generation separately, which hinders the technique
transfer from academia to industry. In this work, we aim at filling the gap,
with a carefully designed optimization-based framework for cross-visual-audio
and joint-visual-audio generation. We observe the powerful generation ability
of off-the-shelf video or audio generation models. Thus, instead of training
the giant models from scratch, we propose to bridge the existing strong models
with a shared latent representation space. Specifically, we propose a
multimodality latent aligner with the pre-trained ImageBind model. Our latent
aligner shares a similar core as the classifier guidance that guides the
diffusion denoising process during inference time. Through carefully designed
optimization strategy and loss functions, we show the superior performance of
our method on joint video-audio generation, visual-steered audio generation,
and audio-steered visual generation tasks. The project website can be found at
https://yzxing87.github.io/Seeing-and-Hearing/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024. Project website:
  https://yzxing87.github.io/Seeing-and-Hearing/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive quantization with mixed-precision based on low-cost proxy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junzhe Chen, Qiao Yang, Senmao Tian, Shunli Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is critical to deploy complicated neural network models on hardware with
limited resources. This paper proposes a novel model quantization method, named
the Low-Cost Proxy-Based Adaptive Mixed-Precision Model Quantization (LCPAQ),
which contains three key modules. The hardware-aware module is designed by
considering the hardware limitations, while an adaptive mixed-precision
quantization module is developed to evaluate the quantization sensitivity by
using the Hessian matrix and Pareto frontier techniques. Integer linear
programming is used to fine-tune the quantization across different layers. Then
the low-cost proxy neural architecture search module efficiently explores the
ideal quantization hyperparameters. Experiments on the ImageNet demonstrate
that the proposed LCPAQ achieves comparable or superior quantization accuracy
to existing mixed-precision models. Notably, LCPAQ achieves 1/200 of the search
time compared with existing methods, which provides a shortcut in practical
quantization use for resource-limited devices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by icassp2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MCF-VC: Mitigate Catastrophic Forgetting in Class-Incremental Learning
  for Multimodal Video Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huiyu Xiong, Lanxiao Wang, Heqian Qiu, Taijin Zhao, Benliu Qiu, Hongliang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To address the problem of catastrophic forgetting due to the invisibility of
old categories in sequential input, existing work based on relatively simple
categorization tasks has made some progress. In contrast, video captioning is a
more complex task in multimodal scenario, which has not been explored in the
field of incremental learning. After identifying this stability-plasticity
problem when analyzing video with sequential input, we originally propose a
method to Mitigate Catastrophic Forgetting in class-incremental learning for
multimodal Video Captioning (MCF-VC). As for effectively maintaining good
performance on old tasks at the macro level, we design Fine-grained Sensitivity
Selection (FgSS) based on the Mask of Linear's Parameters and Fisher
Sensitivity to pick useful knowledge from old tasks. Further, in order to
better constrain the knowledge characteristics of old and new tasks at the
specific feature level, we have created the Two-stage Knowledge Distillation
(TsKD), which is able to learn the new task well while weighing the old task.
Specifically, we design two distillation losses, which constrain the cross
modal semantic information of semantic attention feature map and the textual
information of the final outputs respectively, so that the inter-model and
intra-model stylized knowledge of the old class is retained while learning the
new class. In order to illustrate the ability of our model to resist
forgetting, we designed a metric CIDER_t to detect the stage forgetting rate.
Our experiments on the public dataset MSR-VTT show that the proposed method
significantly resists the forgetting of previous tasks without replaying old
samples, and performs well on the new task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAD-SIGNet: CAD Language Inference from Point Clouds using Layer-wise
  Sketch Instance Guided Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Sadil Khan, Elona Dupont, Sk Aziz Ali, Kseniya Cherenkova, Anis Kacem, Djamila Aouada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reverse engineering in the realm of Computer-Aided Design (CAD) has been a
longstanding aspiration, though not yet entirely realized. Its primary aim is
to uncover the CAD process behind a physical object given its 3D scan. We
propose CAD-SIGNet, an end-to-end trainable and auto-regressive architecture to
recover the design history of a CAD model represented as a sequence of
sketch-and-extrusion from an input point cloud. Our model learns
visual-language representations by layer-wise cross-attention between point
cloud and CAD language embedding. In particular, a new Sketch instance Guided
Attention (SGA) module is proposed in order to reconstruct the fine-grained
details of the sketches. Thanks to its auto-regressive nature, CAD-SIGNet not
only reconstructs a unique full design history of the corresponding CAD model
given an input point cloud but also provides multiple plausible design choices.
This allows for an interactive reverse engineering scenario by providing
designers with multiple next-step choices along with the design process.
Extensive experiments on publicly available CAD datasets showcase the
effectiveness of our approach against existing baseline models in two settings,
namely, full design history recovery and conditional auto-completion from point
clouds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SDF2Net: Shallow to Deep Feature Fusion Network for PolSAR Image
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammed Q. Alkhatib, M. Sami Zitouni, Mina Al-Saad, Nour Aburaed, Hussain Al-Ahmad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Polarimetric synthetic aperture radar (PolSAR) images encompass valuable
information that can facilitate extensive land cover interpretation and
generate diverse output products. Extracting meaningful features from PolSAR
data poses challenges distinct from those encountered in optical imagery. Deep
learning (DL) methods offer effective solutions for overcoming these challenges
in PolSAR feature extraction. Convolutional neural networks (CNNs) play a
crucial role in capturing PolSAR image characteristics by leveraging kernel
capabilities to consider local information and the complex-valued nature of
PolSAR data. In this study, a novel three-branch fusion of complex-valued CNN,
named the Shallow to Deep Feature Fusion Network (SDF2Net), is proposed for
PolSAR image classification. To validate the performance of the proposed
method, classification results are compared against multiple state-of-the-art
approaches using the airborne synthetic aperture radar (AIRSAR) datasets of
Flevoland and San Francisco, as well as the ESAR Oberpfaffenhofen dataset. The
results indicate that the proposed approach demonstrates improvements in
overallaccuracy, with a 1.3% and 0.8% enhancement for the AIRSAR datasets and a
0.5% improvement for the ESAR dataset. Analyses conducted on the Flevoland data
underscore the effectiveness of the SDF2Net model, revealing a promising
overall accuracy of 96.01% even with only a 1% sampling ratio.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian Differentiable Physics for Cloth Digitalization <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deshan Gong, Ningtao Mao, He Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new method for cloth digitalization. Deviating from existing
methods which learn from data captured under relatively casual settings, we
propose to learn from data captured in strictly tested measuring protocols, and
find plausible physical parameters of the cloths. However, such data is
currently absent, so we first propose a new dataset with accurate cloth
measurements. Further, the data size is considerably smaller than the ones in
current deep learning, due to the nature of the data capture process. To learn
from small data, we propose a new Bayesian differentiable cloth model to
estimate the complex material heterogeneity of real cloths. It can provide
highly accurate digitalization from very limited data samples. Through
exhaustive evaluation and comparison, we show our method is accurate in cloth
digitalization, efficient in learning from limited data samples, and general in
capturing material variations. Code and data are available
https://github.com/realcrane/Bayesian-Differentiable-Physics-for-Cloth-Digitalization
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 8 figures, to be published in CVPR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Distributional Shift in Semantic Segmentation via Uncertainty
  Estimation from Unlabelled Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17653v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17653v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David S. W. Williams, Daniele De Martini, Matthew Gadd, Paul Newman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowing when a trained segmentation model is encountering data that is
different to its training data is important. Understanding and mitigating the
effects of this play an important part in their application from a performance
and assurance perspective - this being a safety concern in applications such as
autonomous vehicles (AVs). This work presents a segmentation network that can
detect errors caused by challenging test domains without any additional
annotation in a single forward pass. As annotation costs limit the diversity of
labelled datasets, we use easy-to-obtain, uncurated and unlabelled data to
learn to perform uncertainty estimation by selectively enforcing consistency
over data augmentation. To this end, a novel segmentation benchmark based on
the SAX Dataset is used, which includes labelled test data spanning three
autonomous-driving domains, ranging in appearance from dense urban to off-road.
The proposed method, named Gamma-SSL, consistently outperforms uncertainty
estimation and Out-of-Distribution (OoD) techniques on this difficult benchmark
- by up to 10.7% in area under the receiver operating characteristic (ROC)
curve and 19.2% in area under the precision-recall (PR) curve in the most
challenging of the three scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in IEEE Transactions on Robotics (T-RO)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CustomSketching: Sketch Concept Extraction for Sketch-based Image
  Synthesis and Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chufeng Xiao, Hongbo Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalization techniques for large text-to-image (T2I) models allow users
to incorporate new concepts from reference images. However, existing methods
primarily rely on textual descriptions, leading to limited control over
customized images and failing to support fine-grained and local editing (e.g.,
shape, pose, and details). In this paper, we identify sketches as an intuitive
and versatile representation that can facilitate such control, e.g., contour
lines capturing shape information and flow lines representing texture. This
motivates us to explore a novel task of sketch concept extraction: given one or
more sketch-image pairs, we aim to extract a special sketch concept that
bridges the correspondence between the images and sketches, thus enabling
sketch-based image synthesis and editing at a fine-grained level. To accomplish
this, we introduce CustomSketching, a two-stage framework for extracting novel
sketch concepts. Considering that an object can often be depicted by a contour
for general shapes and additional strokes for internal details, we introduce a
dual-sketch representation to reduce the inherent ambiguity in sketch
depiction. We employ a shape loss and a regularization loss to balance fidelity
and editability during optimization. Through extensive experiments, a user
study, and several applications, we show our method is effective and superior
to the adapted baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Masked Gamma-SSL: Learning Uncertainty Estimation via Masked Image
  Modeling <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17622v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17622v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David S. W. Williams, Matthew Gadd, Paul Newman, Daniele De Martini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work proposes a semantic segmentation network that produces high-quality
uncertainty estimates in a single forward pass. We exploit general
representations from foundation models and unlabelled datasets through a Masked
Image Modeling (MIM) approach, which is robust to augmentation hyper-parameters
and simpler than previous techniques. For neural networks used in
safety-critical applications, bias in the training data can lead to errors;
therefore it is crucial to understand a network's limitations at run time and
act accordingly. To this end, we test our proposed method on a number of test
domains including the SAX Segmentation benchmark, which includes labelled test
data from dense urban, rural and off-road driving domains. The proposed method
consistently outperforms uncertainty estimation and Out-of-Distribution (OoD)
techniques on this difficult benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at 2024 IEEE International Conference on
  Robotics and Automation (ICRA)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adapt Before Comparison: A New Perspective on Cross-Domain Few-Shot
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Herzog
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot segmentation performance declines substantially when facing images
from a domain different than the training domain, effectively limiting
real-world use cases. To alleviate this, recently cross-domain few-shot
segmentation (CD-FSS) has emerged. Works that address this task mainly
attempted to learn segmentation on a source domain in a manner that generalizes
across domains. Surprisingly, we can outperform these approaches while
eliminating the training stage and removing their main segmentation network. We
show test-time task-adaption is the key for successful CD-FSS instead.
Task-adaption is achieved by appending small networks to the feature pyramid of
a conventionally classification-pretrained backbone. To avoid overfitting to
the few labeled samples in supervised fine-tuning, consistency across augmented
views of input images serves as guidance while learning the parameters of the
attached layers. Despite our self-restriction not to use any images other than
the few labeled samples at test time, we achieve new state-of-the-art
performance in CD-FSS, evidencing the need to rethink approaches for the task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Large-scale Evaluation of <span class="highlight-title">Pretrain</span>ing Paradigms for the Detection of
  Defects in Electroluminescence Solar Cell Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Torpey, Lawrence Pratt, Richard Klein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretraining has been shown to improve performance in many domains, including
semantic segmentation, especially in domains with limited labelled data. In
this work, we perform a large-scale evaluation and benchmarking of various
pretraining methods for Solar Cell Defect Detection (SCDD) in
electroluminescence images, a field with limited labelled datasets. We cover
supervised training with semantic segmentation, semi-supervised learning, and
two self-supervised techniques. We also experiment with both in-distribution
and out-of-distribution (OOD) pretraining and observe how this affects
downstream performance. The results suggest that supervised training on a large
OOD dataset (COCO), self-supervised pretraining on a large OOD dataset
(ImageNet), and semi-supervised pretraining (CCT) all yield statistically
equivalent performance for mean Intersection over Union (mIoU). We achieve a
new state-of-the-art for SCDD and demonstrate that certain pretraining schemes
result in superior performance on underrepresented classes. Additionally, we
provide a large-scale unlabelled EL image dataset of $22000$ images, and a
$642$-image labelled semantic segmentation EL dataset, for further research in
developing self- and semi-supervised training techniques in this domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PLReMix: Combating Noisy Labels with Pseudo-Label Relaxed Contrastive
  Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Liu, Beitong Zhou, Cheng Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the application of Contrastive Representation Learning (CRL) in
learning with noisy labels (LNL) has shown promising advancements due to its
remarkable ability to learn well-distributed representations for better
distinguishing noisy labels. However, CRL is mainly used as a pre-training
technique, leading to a complicated multi-stage training pipeline. We also
observed that trivially combining CRL with supervised LNL methods decreases
performance. Using different images from the same class as negative pairs in
CRL creates optimization conflicts between CRL and the supervised loss. To
address these two issues, we propose an end-to-end PLReMix framework that
avoids the complicated pipeline by introducing a Pseudo-Label Relaxed (PLR)
contrastive loss to alleviate the conflicts between losses. This PLR loss
constructs a reliable negative set of each sample by filtering out its
inappropriate negative pairs that overlap at the top k indices of prediction
probabilities, leading to more compact semantic clusters than vanilla CRL.
Furthermore, a two-dimensional Gaussian Mixture Model (GMM) is adopted to
distinguish clean and noisy samples by leveraging semantic information and
model outputs simultaneously, which is expanded on the previously widely used
one-dimensional form. The PLR loss and a semi-supervised loss are
simultaneously applied to train on the GMM divided clean and noisy samples.
Experiments on multiple benchmark datasets demonstrate the effectiveness of the
proposed method. Our proposed PLR loss is scalable, which can be easily
integrated into other LNL methods and boost their performance. Codes will be
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Structure-Guided Adversarial Training of Diffusion Models <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ling Yang, Haotian Qian, Zhilong Zhang, Jingwei Liu, Bin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated exceptional efficacy in various generative
applications. While existing models focus on minimizing a weighted sum of
denoising score matching losses for data distribution modeling, their training
primarily emphasizes instance-level optimization, overlooking valuable
structural information within each mini-batch, indicative of pair-wise
relationships among samples. To address this limitation, we introduce
Structure-guided Adversarial training of Diffusion Models (SADM). In this
pioneering approach, we compel the model to learn manifold structures between
samples in each training batch. To ensure the model captures authentic manifold
structures in the data distribution, we advocate adversarial training of the
diffusion generator against a novel structure discriminator in a minimax game,
distinguishing real manifold structures from the generated ones. SADM
substantially improves existing diffusion transformers (DiT) and outperforms
existing methods in image generation and cross-domain fine-tuning tasks across
12 datasets, establishing a new state-of-the-art FID of 1.58 and 2.11 on
ImageNet for class-conditional image generation at resolutions of 256x256 and
512x512, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Empirical Study of the Generalization Ability of Lidar 3D Object
  Detectors to Unseen Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George Eskandar, Chongzhe Zhang, Abhishek Kaushik, Karim Guirguis, Mohamed Sayed, Bin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Object Detectors (3D-OD) are crucial for understanding the environment in
many robotic tasks, especially autonomous driving. Including 3D information via
Lidar sensors improves accuracy greatly. However, such detectors perform poorly
on domains they were not trained on, i.e. different locations, sensors,
weather, etc., limiting their reliability in safety-critical applications.
There exist methods to adapt 3D-ODs to these domains; however, these methods
treat 3D-ODs as a black box, neglecting underlying architectural decisions and
source-domain training strategies. Instead, we dive deep into the details of
3D-ODs, focusing our efforts on fundamental factors that influence robustness
prior to domain adaptation.
  We systematically investigate four design choices (and the interplay between
them) often overlooked in 3D-OD robustness and domain adaptation: architecture,
voxel encoding, data augmentations, and anchor strategies. We assess their
impact on the robustness of nine state-of-the-art 3D-ODs across six benchmarks
encompassing three types of domain gaps - sensor type, weather, and location.
  Our main findings are: (1) transformer backbones with local point features
are more robust than 3D CNNs, (2) test-time anchor size adjustment is crucial
for adaptation across geographical locations, significantly boosting scores
without retraining, (3) source-domain augmentations allow the model to
generalize to low-resolution sensors, and (4) surprisingly, robustness to bad
weather is improved when training directly on more clean weather data than on
training with bad weather data. We outline our main conclusions and findings to
provide practical guidance on developing more robust 3D-ODs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PHNet: Patch-based Normalization for Portrait Harmonization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karen Efremyan, Elizaveta Petrova, Evgeny Kaskov, Alexander Kapitanov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A common problem for composite images is the incompatibility of their
foreground and background components. Image harmonization aims to solve this
problem, making the whole image look more authentic and coherent. Most existing
solutions predict lookup tables (LUTs) or reconstruct images, utilizing various
attributes of composite images. Recent approaches have primarily focused on
employing global transformations like normalization and color curve rendering
to achieve visual consistency, and they often overlook the importance of local
visual coherence. We present a patch-based harmonization network consisting of
novel Patch-based normalization (PN) blocks and a feature extractor based on
statistical color transfer. Extensive experiments demonstrate the network's
high generalization capability for different domains. Our network achieves
state-of-the-art results on the iHarmony4 dataset. Also, we created a new human
portrait harmonization dataset based on FFHQ and checked the proposed method to
show the generalization ability by achieving the best metrics on it. The
benchmark experiments confirm that the suggested patch-based normalization
block and feature extractor effectively improve the network's capability to
harmonize portraits. Our code and model baselines are publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Image harmonization, Patch-based normalization, Portrait
  harmonization</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scribble Hides Class: Promoting Scribble-Based Weakly-Supervised
  Semantic Segmentation with Its Class Label 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinliang Zhang, Lei Zhu, Hangzhou He, Lujia Jin, Yanye Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scribble-based weakly-supervised semantic segmentation using sparse scribble
supervision is gaining traction as it reduces annotation costs when compared to
fully annotated alternatives. Existing methods primarily generate pseudo-labels
by diffusing labeled pixels to unlabeled ones with local cues for supervision.
However, this diffusion process fails to exploit global semantics and
class-specific cues, which are important for semantic segmentation. In this
study, we propose a class-driven scribble promotion network, which utilizes
both scribble annotations and pseudo-labels informed by image-level classes and
global semantics for supervision. Directly adopting pseudo-labels might
misguide the segmentation model, thus we design a localization rectification
module to correct foreground representations in the feature space. To further
combine the advantages of both supervisions, we also introduce a distance
entropy loss for uncertainty reduction, which adapts per-pixel confidence
weights according to the reliable region determined by the scribble and
pseudo-label's boundary. Experiments on the ScribbleSup dataset with different
qualities of scribble annotations outperform all the previous methods,
demonstrating the superiority and robustness of our method.The code is
available at
https://github.com/Zxl19990529/Class-driven-Scribble-Promotion-Network.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OmniACT: A <span class="highlight-title">Dataset</span> and Benchmark for Enabling Multimodal Generalist
  Autonomous Agents for Desktop and Web 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17553v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17553v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran Kamble, Waseem Alshikh, Ruslan Salakhutdinov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For decades, human-computer interaction has fundamentally been manual. Even
today, almost all productive work done on the computer necessitates human input
at every step. Autonomous virtual agents represent an exciting step in
automating many of these menial tasks. Virtual agents would empower users with
limited technical proficiency to harness the full possibilities of computer
systems. They could also enable the efficient streamlining of numerous computer
tasks, ranging from calendar management to complex travel bookings, with
minimal human intervention. In this paper, we introduce OmniACT, the
first-of-a-kind dataset and benchmark for assessing an agent's capability to
generate executable programs to accomplish computer tasks. Our scope extends
beyond traditional web automation, covering a diverse range of desktop
applications. The dataset consists of fundamental tasks such as "Play the next
song", as well as longer horizon tasks such as "Send an email to John Doe
mentioning the time and place to meet". Specifically, given a pair of screen
image and a visually-grounded natural language task, the goal is to generate a
script capable of fully executing the task. We run several strong baseline
language model agents on our benchmark. The strongest baseline, GPT-4, performs
the best on our benchmark However, its performance level still reaches only 15%
of the human proficiency in generating executable scripts capable of completing
the task, demonstrating the challenge of our task for conventional web agents.
Our benchmark provides a platform to measure and evaluate the progress of
language model agents in automating computer tasks and motivates future work
towards building multimodal models that bridge large language models and the
visual grounding of computer screens.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adapting Learned Image Codecs to Screen Content via Adjustable
  Transformations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17544v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17544v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        H. Burak Dogaroglu, A. Burakhan Koyuncu, Atanas Boev, Elena Alshina, Eckehard Steinbach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As learned image codecs (LICs) become more prevalent, their low coding
efficiency for out-of-distribution data becomes a bottleneck for some
applications. To improve the performance of LICs for screen content (SC) images
without breaking backwards compatibility, we propose to introduce parameterized
and invertible linear transformations into the coding pipeline without changing
the underlying baseline codec's operation flow. We design two neural networks
to act as prefilters and postfilters in our setup to increase the coding
efficiency and help with the recovery from coding artifacts. Our end-to-end
trained solution achieves up to 10% bitrate savings on SC compression compared
to the baseline LICs while introducing only 1% extra parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Learned Sparse Retrieval with Probabilistic Expansion Control <span class="chip">ECIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17535v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17535v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thong Nguyen, Mariya Hendriksen, Andrew Yates, Maarten de Rijke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learned sparse retrieval (LSR) is a family of neural methods that encode
queries and documents into sparse lexical vectors that can be indexed and
retrieved efficiently with an inverted index. We explore the application of LSR
to the multi-modal domain, with a focus on text-image retrieval. While LSR has
seen success in text retrieval, its application in multimodal retrieval remains
underexplored. Current approaches like LexLIP and STAIR require complex
multi-step training on massive datasets. Our proposed approach efficiently
transforms dense vectors from a frozen dense model into sparse lexical vectors.
We address issues of high dimension co-activation and semantic deviation
through a new training algorithm, using Bernoulli random variables to control
query expansion. Experiments with two dense models (BLIP, ALBEF) and two
datasets (MSCOCO, Flickr30k) show that our proposed algorithm effectively
reduces co-activation and semantic deviation. Our best-performing sparsified
model outperforms state-of-the-art text-image LSR models with a shorter
training time and lower GPU memory requirements. Our approach offers an
effective solution for training LSR retrieval models in multimodal settings.
Our code and model checkpoints are available at
github.com/thongnt99/lsr-multimodal
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, accepted as a full paper at ECIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Black-box Adversarial Attacks Against Image Quality Assessment Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Ran, Ao-Xiang Zhang, Mingjie Li, Weixuan Tang, Yuan-Gen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of No-Reference Image Quality Assessment (NR-IQA) is to predict the
perceptual quality of an image in line with its subjective evaluation. To put
the NR-IQA models into practice, it is essential to study their potential
loopholes for model refinement. This paper makes the first attempt to explore
the black-box adversarial attacks on NR-IQA models. Specifically, we first
formulate the attack problem as maximizing the deviation between the estimated
quality scores of original and perturbed images, while restricting the
perturbed image distortions for visual quality preservation. Under such
formulation, we then design a Bi-directional loss function to mislead the
estimated quality scores of adversarial examples towards an opposite direction
with maximum deviation. On this basis, we finally develop an efficient and
effective black-box attack method against NR-IQA models. Extensive experiments
reveal that all the evaluated NR-IQA models are vulnerable to the proposed
attack method. And the generated perturbations are not transferable, enabling
them to serve the investigation of specialities of disparate IQA models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion Model-Based Image Editing: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Huang, Jiancheng Huang, Yifan Liu, Mingfu Yan, Jiaxi Lv, Jianzhuang Liu, Wei Xiong, He Zhang, Shifeng Chen, Liangliang Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Denoising diffusion models have emerged as a powerful tool for various image
generation and editing tasks, facilitating the synthesis of visual content in
an unconditional or input-conditional manner. The core idea behind them is
learning to reverse the process of gradually adding noise to images, allowing
them to generate high-quality samples from a complex distribution. In this
survey, we provide an exhaustive overview of existing methods using diffusion
models for image editing, covering both theoretical and practical aspects in
the field. We delve into a thorough analysis and categorization of these works
from multiple perspectives, including learning strategies, user-input
conditions, and the array of specific editing tasks that can be accomplished.
In addition, we pay special attention to image inpainting and outpainting, and
explore both earlier traditional context-driven and current multimodal
conditional methods, offering a comprehensive analysis of their methodologies.
To further evaluate the performance of text-guided image editing algorithms, we
propose a systematic benchmark, EditEval, featuring an innovative metric, LMM
Score. Finally, we address current limitations and envision some potential
directions for future research. The accompanying repository is released at
https://github.com/SiatMMLab/Awesome-Diffusion-Model-Based-Image-Editing-Methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AVS-Net: Point Sampling with Adaptive Voxel Size for 3D Point Cloud
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17521v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17521v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongcheng Yang, Dingkang Liang, Dingyuan Zhang, Xingyu Jiang, Zhe Liu, Zhikang Zou, Yingying Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient downsampling plays a crucial role in point cloud learning,
particularly for large-scale 3D scenes. Existing downsampling methods either
require a huge computational burden or sacrifice fine-grained geometric
information. This paper presents an advanced sampler that achieves both high
accuracy and efficiency. The proposed method utilizes voxel-based sampling as a
foundation, but effectively addresses the challenges regarding voxel size
determination and the preservation of critical geometric cues. Specifically, we
propose a Voxel Adaptation Module that adaptively adjusts voxel sizes with the
reference of point-based downsampling ratio. This ensures the sampling results
exhibit a favorable distribution for comprehending various 3D objects or
scenes. Additionally, we introduce a network compatible with arbitrary voxel
sizes for sampling and feature extraction while maintaining high efficiency.
Our method achieves state-of-the-art accuracy on the ShapeNetPart and ScanNet
benchmarks with promising efficiency. Code will be available at
https://github.com/yhc2021/AVS-Net.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Unsupervised Crowd Counting and Localization with Adaptive
  Resolution SAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17514v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17514v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia Wan, Qiangqiang Wu, Wei Lin, Antoni B. Chan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The existing crowd counting models require extensive training data, which is
time-consuming to annotate. To tackle this issue, we propose a simple yet
effective crowd counting method by utilizing the Segment-Everything-Everywhere
Model (SEEM), an adaptation of the Segmentation Anything Model (SAM), to
generate pseudo-labels for training crowd counting models. However, our initial
investigation reveals that SEEM's performance in dense crowd scenes is limited,
primarily due to the omission of many persons in high-density areas. To
overcome this limitation, we propose an adaptive resolution SEEM to handle the
scale variations, occlusions, and overlapping of people within crowd scenes.
Alongside this, we introduce a robust localization method, based on Gaussian
Mixture Models, for predicting the head positions in the predicted people
masks. Given the mask and point pseudo-labels, we propose a robust loss
function, which is designed to exclude uncertain regions based on SEEM's
predictions, thereby enhancing the training process of the counting networks.
Finally, we propose an iterative method for generating pseudo-labels. This
method aims at improving the quality of the segmentation masks by identifying
more tiny persons in high-density regions, which are often missed in the first
pseudo-labeling stage. Overall, our proposed method achieves the best
unsupervised performance in crowd counting, while also being comparable results
to some supervised methods. This makes it a highly effective and versatile tool
for crowd counting, especially in situations where labeled data is not
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Demonstrating and Reducing Shortcuts in Vision-Language Representation
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17510v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17510v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maurits Bleeker, Mariya Hendriksen, Andrew Yates, Maarten de Rijke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) mainly rely on contrastive training to learn
general-purpose representations of images and captions. We focus on the
situation when one image is associated with several captions, each caption
containing both information shared among all captions and unique information
per caption about the scene depicted in the image. In such cases, it is unclear
whether contrastive losses are sufficient for learning task-optimal
representations that contain all the information provided by the captions or
whether the contrastive learning setup encourages the learning of a simple
shortcut that minimizes contrastive loss. We introduce synthetic shortcuts for
vision-language: a training and evaluation framework where we inject synthetic
shortcuts into image-text data. We show that contrastive VLMs trained from
scratch or fine-tuned with data containing these synthetic shortcuts mainly
learn features that represent the shortcut. Hence, contrastive losses are not
sufficient to learn task-optimal representations, i.e., representations that
contain all task-relevant information shared between the image and associated
captions. We examine two methods to reduce shortcut learning in our training
and evaluation framework: (i) latent target decoding and (ii) implicit feature
modification. We show empirically that both methods improve performance on the
evaluation task, but only partly reduce shortcut learning when training and
evaluating with our shortcut learning framework. Hence, we show the difficulty
and challenge of our shortcut learning framework for contrastive
vision-language representation learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interactive Multi-Head Self-Attention with Linear Complexity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hankyul Kang, Ming-Hsuan Yang, Jongbin Ryu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an efficient interactive method for multi-head self-attention via
decomposition. For existing methods using multi-head self-attention, the
attention operation of each head is computed independently. However, we show
that the interactions between cross-heads of the attention matrix enhance the
information flow of the attention operation. Considering that the attention
matrix of each head can be seen as a feature of networks, it is beneficial to
establish connectivity between them to capture interactions better. However, a
straightforward approach to capture the interactions between the cross-heads is
computationally prohibitive as the complexity grows substantially with the high
dimension of an attention matrix. In this work, we propose an effective method
to decompose the attention operation into query- and key-less components. This
will result in a more manageable size for the attention matrix, specifically
for the cross-head interactions. Expensive experimental results show that the
proposed cross-head interaction approach performs favorably against existing
efficient attention methods and state-of-the-art backbone models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedLPPA: Learning Personalized <span class="highlight-title">Prompt</span> and Aggregation for Federated
  Weakly-supervised Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Lin, Yixiang Liu, Jiewei Wu, Pujin Cheng, Zhiyuan Cai, Kenneth K. Y. Wong, Xiaoying Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) effectively mitigates the data silo challenge brought
about by policies and privacy concerns, implicitly harnessing more data for
deep model training. However, traditional centralized FL models grapple with
diverse multi-center data, especially in the face of significant data
heterogeneity, notably in medical contexts. In the realm of medical image
segmentation, the growing imperative to curtail annotation costs has amplified
the importance of weakly-supervised techniques which utilize sparse annotations
such as points, scribbles, etc. A pragmatic FL paradigm shall accommodate
diverse annotation formats across different sites, which research topic remains
under-investigated. In such context, we propose a novel personalized FL
framework with learnable prompt and aggregation (FedLPPA) to uniformly leverage
heterogeneous weak supervision for medical image segmentation. In FedLPPA, a
learnable universal knowledge prompt is maintained, complemented by multiple
learnable personalized data distribution prompts and prompts representing the
supervision sparsity. Integrated with sample features through a dual-attention
mechanism, those prompts empower each local task decoder to adeptly adjust to
both the local distribution and the supervision form. Concurrently, a
dual-decoder strategy, predicated on prompt similarity, is introduced for
enhancing the generation of pseudo-labels in weakly-supervised learning,
alleviating overfitting and noise accumulation inherent to local data, while an
adaptable aggregation method is employed to customize the task decoder on a
parameter-wise basis. Extensive experiments on three distinct medical image
segmentation tasks involving different modalities underscore the superiority of
FedLPPA, with its efficacy closely parallels that of fully supervised
centralized training. Our code and data will be available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bit Rate Matching Algorithm Optimization in JPEG-AI Verification Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17487v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17487v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panqi Jia, A. Burakhan Koyuncu, Jue Mao, Ze Cui, Yi Ma, Tiansheng Guo, Timofey Solovyev, Alexander Karabutov, Yin Zhao, Jing Wang, Elena Alshina, Andre Kaup
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The research on neural network (NN) based image compression has shown
superior performance compared to classical compression frameworks. Unlike the
hand-engineered transforms in the classical frameworks, NN-based models learn
the non-linear transforms providing more compact bit representations, and
achieve faster coding speed on parallel devices over their classical
counterparts. Those properties evoked the attention of both scientific and
industrial communities, resulting in the standardization activity JPEG-AI. The
verification model for the standardization process of JPEG-AI is already in
development and has surpassed the advanced VVC intra codec. To generate
reconstructed images with the desired bits per pixel and assess the BD-rate
performance of both the JPEG-AI verification model and VVC intra, bit rate
matching is employed. However, the current state of the JPEG-AI verification
model experiences significant slowdowns during bit rate matching, resulting in
suboptimal performance due to an unsuitable model. The proposed methodology
offers a gradual algorithmic optimization for matching bit rates, resulting in
a fourfold acceleration and over 1% improvement in BD-rate at the base
operation point. At the high operation point, the acceleration increases up to
sixfold.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at (IEEE) PCS 2024; 6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MGE: A Training-Free and Efficient Model Generation and Enhancement
  Scheme 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan Wang, Zeshan Pang, Yuliang Lu, Xuehu Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To provide a foundation for the research of deep learning models, the
construction of model pool is an essential step. This paper proposes a
Training-Free and Efficient Model Generation and Enhancement Scheme (MGE). This
scheme primarily considers two aspects during the model generation process: the
distribution of model parameters and model performance. Experiments result
shows that generated models are comparable to models obtained through normal
training, and even superior in some cases. Moreover, the time consumed in
generating models accounts for only 1\% of the time required for normal model
training. More importantly, with the enhancement of Evolution-MGE, generated
models exhibits competitive generalization ability in few-shot tasks. And the
behavioral dissimilarity of generated models has the potential of adversarial
defense.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with
  Audio2Video Diffusion Model under Weak Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17485v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17485v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linrui Tian, Qi Wang, Bang Zhang, Liefeng Bo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we tackle the challenge of enhancing the realism and
expressiveness in talking head video generation by focusing on the dynamic and
nuanced relationship between audio cues and facial movements. We identify the
limitations of traditional techniques that often fail to capture the full
spectrum of human expressions and the uniqueness of individual facial styles.
To address these issues, we propose EMO, a novel framework that utilizes a
direct audio-to-video synthesis approach, bypassing the need for intermediate
3D models or facial landmarks. Our method ensures seamless frame transitions
and consistent identity preservation throughout the video, resulting in highly
expressive and lifelike animations. Experimental results demonsrate that EMO is
able to produce not only convincing speaking videos but also singing videos in
various styles, significantly outperforming existing state-of-the-art
methodologies in terms of expressiveness and realism.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AlignMiF: Geometry-Aligned Multimodal Implicit Field for LiDAR-Camera
  Joint Synthesis <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Tang, Guangrun Wang, Yixing Lao, Peng Chen, Jie Liu, Liang Lin, Kaicheng Yu, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural implicit fields have been a de facto standard in novel view synthesis.
Recently, there exist some methods exploring fusing multiple modalities within
a single field, aiming to share implicit features from different modalities to
enhance reconstruction performance. However, these modalities often exhibit
misaligned behaviors: optimizing for one modality, such as LiDAR, can adversely
affect another, like camera performance, and vice versa. In this work, we
conduct comprehensive analyses on the multimodal implicit field of LiDAR-camera
joint synthesis, revealing the underlying issue lies in the misalignment of
different sensors. Furthermore, we introduce AlignMiF, a geometrically aligned
multimodal implicit field with two proposed modules: Geometry-Aware Alignment
(GAA) and Shared Geometry Initialization (SGI). These modules effectively align
the coarse geometry across different modalities, significantly enhancing the
fusion process between LiDAR and camera data. Through extensive experiments
across various datasets and scenes, we demonstrate the effectiveness of our
approach in facilitating better interaction between LiDAR and camera modalities
within a unified neural field. Specifically, our proposed AlignMiF, achieves
remarkable improvement over recent implicit fusion methods (+2.01 and +3.11
image PSNR on the KITTI-360 and Waymo datasets) and consistently surpasses
single modality performance (13.8% and 14.2% reduction in LiDAR Chamfer
Distance on the respective datasets).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Classification of Phonetic Segments in Child Speech Using Raw
  Ultrasound Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saja Al Ani, Joanne Cleland, Ahmed Zoha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech sound disorder (SSD) is defined as a persistent impairment in speech
sound production leading to reduced speech intelligibility and hindered verbal
communication. Early recognition and intervention of children with SSD and
timely referral to speech and language therapists (SLTs) for treatment are
crucial. Automated detection of speech impairment is regarded as an efficient
method for examining and screening large populations. This study focuses on
advancing the automatic diagnosis of SSD in early childhood by proposing a
technical solution that integrates ultrasound tongue imaging (UTI) with
deep-learning models. The introduced FusionNet model combines UTI data with the
extracted texture features to classify UTI. The overarching aim is to elevate
the accuracy and efficiency of UTI analysis, particularly for classifying
speech sounds associated with SSD. This study compared the FusionNet approach
with standard deep-learning methodologies, highlighting the excellent
improvement results of the FusionNet model in UTI classification and the
potential of multi-learning in improving UTI classification in speech therapy
clinics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bit Distribution Study and Implementation of Spatial Quality Map in the
  JPEG-AI Standardization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17470v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17470v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panqi Jia, Jue Mao, Esin Koyuncu, A. Burakhan Koyuncu, Timofey Solovyev, Alexander Karabutov, Yin Zhao, Elena Alshina, Andre Kaup
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Currently, there is a high demand for neural network-based image compression
codecs. These codecs employ non-linear transforms to create compact bit
representations and facilitate faster coding speeds on devices compared to the
hand-crafted transforms used in classical frameworks. The scientific and
industrial communities are highly interested in these properties, leading to
the standardization effort of JPEG-AI. The JPEG-AI verification model has been
released and is currently under development for standardization. Utilizing
neural networks, it can outperform the classic codec VVC intra by over 10%
BD-rate operating at base operation point. Researchers attribute this success
to the flexible bit distribution in the spatial domain, in contrast to VVC
intra's anchor that is generated with a constant quality point. However, our
study reveals that VVC intra displays a more adaptable bit distribution
structure through the implementation of various block sizes. As a result of our
observations, we have proposed a spatial bit allocation method to optimize the
JPEG-AI verification model's bit distribution and enhance the visual quality.
Furthermore, by applying the VVC bit distribution strategy, the objective
performance of JPEG-AI verification mode can be further improved, resulting in
a maximum gain of 0.45 dB in PSNR-Y.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model X-ray:Detect Backdoored Models via Decision Boundary 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17465v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17465v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanghao Su, Jie Zhang, Ting Xu, Tianwei Zhang, Weiming Zhang, Nenghai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) have revolutionized various industries, leading
to the rise of Machine Learning as a Service (MLaaS). In this paradigm,
well-trained models are typically deployed through APIs. However, DNNs are
susceptible to backdoor attacks, which pose significant risks to their
applications. This vulnerability necessitates a method for users to ascertain
whether an API is compromised before usage. Although many backdoor detection
methods have been developed, they often operate under the assumption that the
defender has access to specific information such as details of the attack, soft
predictions from the model API, and even the knowledge of the model parameters,
limiting their practicality in MLaaS scenarios. To address it, in this paper,
we begin by presenting an intriguing observation: the decision boundary of the
backdoored model exhibits a greater degree of closeness than that of the clean
model. Simultaneously, if only one single label is infected, a larger portion
of the regions will be dominated by the attacked label. Building upon this
observation, we propose Model X-ray, a novel backdoor detection approach for
MLaaS through the analysis of decision boundaries. Model X-ray can not only
identify whether the target API is infected by backdoor attacks but also
determine the target attacked label under the all-to-one attack strategy.
Importantly, it accomplishes this solely by the hard prediction of clean
inputs, regardless of any assumptions about attacks and prior knowledge of the
training details of the model. Extensive experiments demonstrated that Model
X-ray can be effective for MLaaS across diverse backdoor attacks, datasets, and
architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative 3D Part Assembly via Part-Whole-Hierarchy Message Passing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bi'an Du, Xiang Gao, Wei Hu, Renjie Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative 3D part assembly involves understanding part relationships and
predicting their 6-DoF poses for assembling a realistic 3D shape. Prior work
often focus on the geometry of individual parts, neglecting part-whole
hierarchies of objects. Leveraging two key observations: 1) super-part poses
provide strong hints about part poses, and 2) predicting super-part poses is
easier due to fewer superparts, we propose a part-whole-hierarchy message
passing network for efficient 3D part assembly. We first introduce super-parts
by grouping geometrically similar parts without any semantic labels. Then we
employ a part-whole hierarchical encoder, wherein a super-part encoder predicts
latent super-part poses based on input parts. Subsequently, we transform the
point cloud using the latent poses, feeding it to the part encoder for
aggregating super-part information and reasoning about part relationships to
predict all part poses. In training, only ground-truth part poses are required.
During inference, the predicted latent poses of super-parts enhance
interpretability. Experimental results on the PartNet dataset show that our
method achieves state-of-the-art performance in part and connectivity accuracy
and enables an interpretable hierarchical part assembly.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Segment anything model for head and neck tumor segmentation with CT, PET
  and MRI multi-modality images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17454v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17454v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jintao Ren, Mathis Rasmussen, Jasper Nijkamp, Jesper Grau Eriksen, Stine Korreman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning presents novel opportunities for the auto-segmentation of gross
tumor volume (GTV) in head and neck cancer (HNC), yet fully automatic methods
usually necessitate significant manual refinement. This study investigates the
Segment Anything Model (SAM), recognized for requiring minimal human prompting
and its zero-shot generalization ability across natural images. We specifically
examine MedSAM, a version of SAM fine-tuned with large-scale public medical
images. Despite its progress, the integration of multi-modality images (CT,
PET, MRI) for effective GTV delineation remains a challenge. Focusing on SAM's
application in HNC GTV segmentation, we assess its performance in both
zero-shot and fine-tuned scenarios using single (CT-only) and fused
multi-modality images. Our study demonstrates that fine-tuning SAM
significantly enhances its segmentation accuracy, building upon the already
effective zero-shot results achieved with bounding box prompts. These findings
open a promising avenue for semi-automatic HNC GTV segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ V2C-Long: Longitudinal Cortex Reconstruction with Spatiotemporal
  Correspondence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17438v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17438v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Bongratz, Jan Fecht, Anne-Marie Rickmann, Christian Wachinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing the cortex from longitudinal MRI is indispensable for
analyzing morphological changes in the human brain. Despite the recent
disruption of cortical surface reconstruction with deep learning, challenges
arising from longitudinal data are still persistent. Especially the lack of
strong spatiotemporal point correspondence hinders downstream analyses due to
the introduced noise. To address this issue, we present V2C-Long, the first
dedicated deep learning-based cortex reconstruction method for longitudinal
MRI. In contrast to existing methods, V2C-Long surfaces are directly comparable
in a cross-sectional and longitudinal manner. We establish strong inherent
spatiotemporal correspondences via a novel composition of two deep mesh
deformation networks and fast aggregation of feature-enhanced within-subject
templates. The results on internal and external test data demonstrate that
V2C-Long yields cortical surfaces with improved accuracy and consistency
compared to previous methods. Finally, this improvement manifests in higher
sensitivity to regional cortical atrophy in Alzheimer's disease.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Enhanced Queries of Point Sets for Vectorized Map
  Construction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17430v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17430v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Liu, Xiaoyu Zhang, Guangwei Liu, Ji Zhao, Ningyi Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In autonomous driving, the high-definition (HD) map plays a crucial role in
localization and planning. Recently, several methods have facilitated
end-to-end online map construction in DETR-like frameworks. However, little
attention has been paid to the potential capabilities of exploring the query
mechanism. This paper introduces MapQR, an end-to-end method with an emphasis
on enhancing query capabilities for constructing online vectorized maps.
Although the map construction is essentially a point set prediction task, MapQR
utilizes instance queries rather than point queries. These instance queries are
scattered for the prediction of point sets and subsequently gathered for the
final matching. This query design, called the scatter-and-gather query, shares
content information in the same map element and avoids possible inconsistency
of content information in point queries. We further exploit prior information
to enhance an instance query by adding positional information embedded from
their reference points. Together with a simple and effective improvement of a
BEV encoder, the proposed MapQR achieves the best mean average precision (mAP)
and maintains good efficiency on both nuScenes and Argoverse 2. In addition,
integrating our query design into other models can boost their performance
significantly. The code will be available at https://github.com/HXMap/MapQR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VastGaussian: Vast 3D Gaussians for Large Scene Reconstruction <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17427v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17427v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Lin, Zhihao Li, Xiao Tang, Jianzhuang Liu, Shiyong Liu, Jiayue Liu, Yangdi Lu, Xiaofei Wu, Songcen Xu, Youliang Yan, Wenming Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing NeRF-based methods for large scene reconstruction often have
limitations in visual quality and rendering speed. While the recent 3D Gaussian
Splatting works well on small-scale and object-centric scenes, scaling it up to
large scenes poses challenges due to limited video memory, long optimization
time, and noticeable appearance variations. To address these challenges, we
present VastGaussian, the first method for high-quality reconstruction and
real-time rendering on large scenes based on 3D Gaussian Splatting. We propose
a progressive partitioning strategy to divide a large scene into multiple
cells, where the training cameras and point cloud are properly distributed with
an airspace-aware visibility criterion. These cells are merged into a complete
scene after parallel optimization. We also introduce decoupled appearance
modeling into the optimization process to reduce appearance variations in the
rendered images. Our approach outperforms existing NeRF-based methods and
achieves state-of-the-art results on multiple large scene datasets, enabling
fast optimization and high-fidelity real-time rendering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024. Project website:
  https://vastgaussian.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViTaL: An Advanced Framework for Automated Plant Disease Identification
  in Leaf Images Using Vision <span class="highlight-title">Transformer</span>s and Linear Projection For Feature
  Reduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17424v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17424v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhishek Sebastian, Annis Fathima A, Pragna R, Madhan Kumar S, Yaswanth Kannan G, Vinay Murali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our paper introduces a robust framework for the automated identification of
diseases in plant leaf images. The framework incorporates several key stages to
enhance disease recognition accuracy. In the pre-processing phase, a thumbnail
resizing technique is employed to resize images, minimizing the loss of
critical image details while ensuring computational efficiency. Normalization
procedures are applied to standardize image data before feature extraction.
Feature extraction is facilitated through a novel framework built upon Vision
Transformers, a state-of-the-art approach in image analysis. Additionally,
alternative versions of the framework with an added layer of linear projection
and blockwise linear projections are explored. This comparative analysis allows
for the evaluation of the impact of linear projection on feature extraction and
overall model performance. To assess the effectiveness of the proposed
framework, various Convolutional Neural Network (CNN) architectures are
utilized, enabling a com- prehensive evaluation of linear projection's
influence on key evaluation metrics. The findings demonstrate the efficacy of
the proposed framework, with the top- performing model achieving a Hamming loss
of 0.054. Furthermore, we propose a novel hardware design specifically tailored
for scanning diseased leaves in an omnidirectional fashion. The hardware
implementation utilizes a Raspberry Pi Compute Module to address low-memory
configurations, ensuring practicality and affordability. This innovative
hardware solution enhances the overall feasibility and accessibility of the
proposed automated disease identification system. This research contributes to
the field of agriculture by offering valuable insights and tools for the early
detection and management of plant diseases, potentially leading to improved
crop yields and enhanced food security.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PANDAS: Prototype-based Novel Class Discovery and Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tyler L. Hayes, César R. de Souza, Namil Kim, Jiwon Kim, Riccardo Volpi, Diane Larlus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detectors are typically trained once and for all on a fixed set of
classes. However, this closed-world assumption is unrealistic in practice, as
new classes will inevitably emerge after the detector is deployed in the wild.
In this work, we look at ways to extend a detector trained for a set of base
classes so it can i) spot the presence of novel classes, and ii) automatically
enrich its repertoire to be able to detect those newly discovered classes
together with the base ones. We propose PANDAS, a method for novel class
discovery and detection. It discovers clusters representing novel classes from
unlabeled data, and represents old and new classes with prototypes. During
inference, a distance-based classifier uses these prototypes to assign a label
to each detected object instance. The simplicity of our method makes it widely
applicable. We experimentally demonstrate the effectiveness of PANDAS on the
VOC 2012 and COCO-to-LVIS benchmarks. It performs favorably against the state
of the art for this task while being computationally more affordable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CARZero: Cross-Attention Alignment for Radiology Zero-Shot
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Lai, Qingsong Yao, Zihang Jiang, Rongsheng Wang, Zhiyang He, Xiaodong Tao, S. Kevin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancement of Zero-Shot Learning in the medical domain has been driven
forward by using pre-trained models on large-scale image-text pairs, focusing
on image-text alignment. However, existing methods primarily rely on cosine
similarity for alignment, which may not fully capture the complex relationship
between medical images and reports. To address this gap, we introduce a novel
approach called Cross-Attention Alignment for Radiology Zero-Shot
Classification (CARZero). Our approach innovatively leverages cross-attention
mechanisms to process image and report features, creating a Similarity
Representation that more accurately reflects the intricate relationships in
medical semantics. This representation is then linearly projected to form an
image-text similarity matrix for cross-modality alignment. Additionally,
recognizing the pivotal role of prompt selection in zero-shot learning, CARZero
incorporates a Large Language Model-based prompt alignment strategy. This
strategy standardizes diverse diagnostic expressions into a unified format for
both training and inference phases, overcoming the challenges of manual prompt
design. Our approach is simple yet effective, demonstrating state-of-the-art
performance in zero-shot classification on five official chest radiograph
diagnostic test sets, including remarkable results on datasets with long-tail
distributions of rare diseases. This achievement is attributed to our new
image-text alignment strategy, which effectively addresses the complex
relationship between medical images and reports.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Video Compression with Feature Modulation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17414v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17414v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Li, Bin Li, Yan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emerging conditional coding-based neural video codec (NVC) shows
superiority over commonly-used residual coding-based codec and the latest NVC
already claims to outperform the best traditional codec. However, there still
exist critical problems blocking the practicality of NVC. In this paper, we
propose a powerful conditional coding-based NVC that solves two critical
problems via feature modulation. The first is how to support a wide quality
range in a single model. Previous NVC with this capability only supports about
3.8 dB PSNR range on average. To tackle this limitation, we modulate the latent
feature of the current frame via the learnable quantization scaler. During the
training, we specially design the uniform quantization parameter sampling
mechanism to improve the harmonization of encoding and quantization. This
results in a better learning of the quantization scaler and helps our NVC
support about 11.4 dB PSNR range. The second is how to make NVC still work
under a long prediction chain. We expose that the previous SOTA NVC has an
obvious quality degradation problem when using a large intra-period setting. To
this end, we propose modulating the temporal feature with a periodically
refreshing mechanism to boost the quality. %Besides solving the above two
problems, we also design a single model that can support both RGB and YUV
colorspaces. Notably, under single intra-frame setting, our codec can achieve
29.7\% bitrate saving over previous SOTA NVC with 16\% MACs reduction. Our
codec serves as a notable landmark in the journey of NVC evolution. The codes
are at https://github.com/microsoft/DCVC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024. Codes are at https://github.com/microsoft/DCVC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffuseKronA: A Parameter Efficient Fine-tuning Method for Personalized
  Diffusion Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shyam Marjit, Harshit Singh, Nityanand Mathur, Sayak Paul, Chia-Mu Yu, Pin-Yu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of subject-driven text-to-image (T2I) generative models, recent
developments like DreamBooth and BLIP-Diffusion have led to impressive results
yet encounter limitations due to their intensive fine-tuning demands and
substantial parameter requirements. While the low-rank adaptation (LoRA) module
within DreamBooth offers a reduction in trainable parameters, it introduces a
pronounced sensitivity to hyperparameters, leading to a compromise between
parameter efficiency and the quality of T2I personalized image synthesis.
Addressing these constraints, we introduce \textbf{\textit{DiffuseKronA}}, a
novel Kronecker product-based adaptation module that not only significantly
reduces the parameter count by 35\% and 99.947\% compared to LoRA-DreamBooth
and the original DreamBooth, respectively, but also enhances the quality of
image synthesis. Crucially, \textit{DiffuseKronA} mitigates the issue of
hyperparameter sensitivity, delivering consistent high-quality generations
across a wide range of hyperparameters, thereby diminishing the necessity for
extensive fine-tuning. Furthermore, a more controllable decomposition makes
\textit{DiffuseKronA} more interpretable and even can achieve up to a 50\%
reduction with results comparable to LoRA-Dreambooth. Evaluated against diverse
and complex input images and text prompts, \textit{DiffuseKronA} consistently
outperforms existing models, producing diverse images of higher quality with
improved fidelity and a more accurate color distribution of objects, all the
while upholding exceptional parameter efficiency, thus presenting a substantial
advancement in the field of T2I generative modeling. Our project page,
consisting of links to the code, and pre-trained checkpoints, is available at
\href{https://diffusekrona.github.io/}{https://diffusekrona.github.io/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page:
  \href{https://diffusekrona.github.io/}{https://diffusekrona.github.io/}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A novel image space formalism of Fourier domain interpolation neural
  networks for noise propagation analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17410v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17410v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Dawood, Felix Breuer, Istvan Homolya, Jannik Stebani, Maximilian Gram, Peter M. Jakob, Moritz Zaiss, Martin Blaimer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: To develop an image space formalism of multi-layer convolutional
neural networks (CNNs) for Fourier domain interpolation in MRI reconstructions
and analytically estimate noise propagation during CNN inference. Theory and
Methods: Nonlinear activations in the Fourier domain (also known as k-space)
using complex-valued Rectifier Linear Units are expressed as elementwise
multiplication with activation masks. This operation is transformed into a
convolution in the image space. After network training in k-space, this
approach provides an algebraic expression for the derivative of the
reconstructed image with respect to the aliased coil images, which serve as the
input tensors to the network in the image space. This allows the variance in
the network inference to be estimated analytically and to be used to describe
noise characteristics. Monte-Carlo simulations and numerical approaches based
on auto-differentiation were used for validation. The framework was tested on
retrospectively undersampled invivo brain images. Results: Inferences conducted
in the image domain are quasi-identical to inferences in the k-space,
underlined by corresponding quantitative metrics. Noise variance maps obtained
from the analytical expression correspond with those obtained via Monte-Carlo
simulations, as well as via an auto-differentiation approach. The noise
resilience is well characterized, as in the case of classical Parallel Imaging.
Komolgorov-Smirnov tests demonstrate Gaussian distributions of voxel magnitudes
in variance maps obtained via Monte-Carlo simulations. Conclusion: The
quasi-equivalent image space formalism for neural networks for k-space
interpolation enables fast and accurate description of the noise
characteristics during CNN inference, analogous to geometry-factor maps in
traditional parallel imaging methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LSPT: Long-term Spatial <span class="highlight-title">Prompt</span> Tuning for Visual Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shentong Mo, Yansen Wang, Xufang Luo, Dongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Prompt Tuning (VPT) techniques have gained prominence for their
capacity to adapt pre-trained Vision Transformers (ViTs) to downstream visual
tasks using specialized learnable tokens termed as prompts. Contemporary VPT
methodologies, especially when employed with self-supervised vision
transformers, often default to the introduction of new learnable prompts or
gated prompt tokens predominantly sourced from the model's previous block. A
pivotal oversight in such approaches is their failure to harness the potential
of long-range previous blocks as sources of prompts within each self-supervised
ViT. To bridge this crucial gap, we introduce Long-term Spatial Prompt Tuning
(LSPT) - a revolutionary approach to visual representation learning. Drawing
inspiration from the intricacies of the human brain, LSPT ingeniously
incorporates long-term gated prompts. This feature serves as temporal coding,
curbing the risk of forgetting parameters acquired from earlier blocks. Further
enhancing its prowess, LSPT brings into play patch tokens, serving as spatial
coding. This is strategically designed to perpetually amass class-conscious
features, thereby fortifying the model's prowess in distinguishing and
identifying visual categories. To validate the efficacy of our proposed method,
we engaged in rigorous experimentation across 5 FGVC and 19 VTAB-1K benchmarks.
Our empirical findings underscore the superiority of LSPT, showcasing its
ability to set new benchmarks in visual prompt tuning performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sora Generates Videos with Stunning Geometrical Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17403v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17403v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanyi Li, Daquan Zhou, Chenxu Zhang, Shaodong Wei, Qibin Hou, Ming-Ming Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recently developed Sora model [1] has exhibited remarkable capabilities
in video generation, sparking intense discussions regarding its ability to
simulate real-world phenomena. Despite its growing popularity, there is a lack
of established metrics to evaluate its fidelity to real-world physics
quantitatively. In this paper, we introduce a new benchmark that assesses the
quality of the generated videos based on their adherence to real-world physics
principles. We employ a method that transforms the generated videos into 3D
models, leveraging the premise that the accuracy of 3D reconstruction is
heavily contingent on the video quality. From the perspective of 3D
reconstruction, we use the fidelity of the geometric constraints satisfied by
the constructed 3D models as a proxy to gauge the extent to which the generated
videos conform to real-world physics rules. Project page:
https://sora-geometrical-consistency.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerating Diffusion Sampling with Optimized Time Steps <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuchen Xue, Zhaoqiang Liu, Fei Chen, Shifeng Zhang, Tianyang Hu, Enze Xie, Zhenguo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion probabilistic models (DPMs) have shown remarkable performance in
high-resolution image synthesis, but their sampling efficiency is still to be
desired due to the typically large number of sampling steps. Recent
advancements in high-order numerical ODE solvers for DPMs have enabled the
generation of high-quality images with much fewer sampling steps. While this is
a significant development, most sampling methods still employ uniform time
steps, which is not optimal when using a small number of steps. To address this
issue, we propose a general framework for designing an optimization problem
that seeks more appropriate time steps for a specific numerical ODE solver for
DPMs. This optimization problem aims to minimize the distance between the
ground-truth solution to the ODE and an approximate solution corresponding to
the numerical solver. It can be efficiently solved using the constrained trust
region method, taking less than $15$ seconds. Our extensive experiments on both
unconditional and conditional sampling using pixel- and latent-space DPMs
demonstrate that, when combined with the state-of-the-art sampling method
UniPC, our optimized time steps significantly improve image generation
performance in terms of FID scores for datasets such as CIFAR-10 and ImageNet,
compared to using uniform time steps.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024. Under camera-ready revision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Coupled Laplacian Eigenmaps for Locally-Aware 3D Rigid Point Cloud
  Matching <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17372v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17372v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Bastico, Etienne Decencière, Laurent Corté, Yannick Tillier, David Ryckelynck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point cloud matching, a crucial technique in computer vision, medical and
robotics fields, is primarily concerned with finding correspondences between
pairs of point clouds or voxels. In some practical scenarios, emphasizing local
differences is crucial for accurately identifying a correct match, thereby
enhancing the overall robustness and reliability of the matching process.
Commonly used shape descriptors have several limitations and often fail to
provide meaningful local insights on the paired geometries. In this work, we
propose a new technique, based on graph Laplacian eigenmaps, to match point
clouds by taking into account fine local structures. To deal with the order and
sign ambiguity of Laplacian eigenmaps, we introduce a new operator, called
Coupled Laplacian, that allows to easily generate aligned eigenspaces for
multiple rigidly-registered geometries. We show that the similarity between
those aligned high-dimensional spaces provides a locally meaningful score to
match shapes. We initially evaluate the performance of the proposed technique
in a point-wise manner, specifically focusing on the task of object anomaly
localization using the MVTec 3D-AD dataset. Additionally, we define a new
medical task, called automatic Bone Side Estimation (BSE), which we address
through a global similarity score derived from coupled eigenspaces. In order to
test it, we propose a benchmark collecting bone surface structures from various
public datasets. Our matching technique, based on Coupled Laplacian,
outperforms other methods by reaching an impressive accuracy on both tasks. The
code to reproduce our experiments is publicly available at
https://github.com/matteo-bastico/CoupledLaplacian and in the Supplementary
Code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted at Computer Vision and Patter
  Recognition (CVPR) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Efficient MLP-based Point-guided Segmentation Network for Ore Images
  with Ambiguous Boundary 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guodong Sun, Yuting Peng, Le Cheng, Mengya Xu, An Wang, Bo Wu, Hongliang Ren, Yang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The precise segmentation of ore images is critical to the successful
execution of the beneficiation process. Due to the homogeneous appearance of
the ores, which leads to low contrast and unclear boundaries, accurate
segmentation becomes challenging, and recognition becomes problematic. This
paper proposes a lightweight framework based on Multi-Layer Perceptron (MLP),
which focuses on solving the problem of edge burring. Specifically, we
introduce a lightweight backbone better suited for efficiently extracting
low-level features. Besides, we design a feature pyramid network consisting of
two MLP structures that balance local and global information thus enhancing
detection accuracy. Furthermore, we propose a novel loss function that guides
the prediction points to match the instance edge points to achieve clear object
boundaries. We have conducted extensive experiments to validate the efficacy of
our proposed method. Our approach achieves a remarkable processing speed of
over 27 frames per second (FPS) with a model size of only 73 MB. Moreover, our
method delivers a consistently high level of accuracy, with impressive
performance scores of 60.4 and 48.9 in~$AP_{50}^{box}$ and~$AP_{50}^{mask}$
respectively, as compared to the currently available state-of-the-art
techniques, when tested on the ore image dataset. The source code will be
released at \url{https://github.com/MVME-HBUT/ORENEXT}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zicheng Zhang, Ruobing Zheng, Ziwen Liu, Congying Han, Tianqi Li, Meng Wang, Tiande Guo, Jingdong Chen, Bonan Li, Ming Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works in implicit representations, such as Neural Radiance Fields
(NeRF), have advanced the generation of realistic and animatable head avatars
from video sequences. These implicit methods are still confronted by visual
artifacts and jitters, since the lack of explicit geometric constraints poses a
fundamental challenge in accurately modeling complex facial deformations. In
this paper, we introduce Dynamic Tetrahedra (DynTet), a novel hybrid
representation that encodes explicit dynamic meshes by neural networks to
ensure geometric consistency across various motions and viewpoints. DynTet is
parameterized by the coordinate-based networks which learn signed distance,
deformation, and material texture, anchoring the training data into a
predefined tetrahedra grid. Leveraging Marching Tetrahedra, DynTet efficiently
decodes textured meshes with a consistent topology, enabling fast rendering
through a differentiable rasterizer and supervision via a pixel loss. To
enhance training efficiency, we incorporate classical 3D Morphable Models to
facilitate geometry learning and define a canonical space for simplifying
texture learning. These advantages are readily achievable owing to the
effective geometric representation employed in DynTet. Compared with prior
works, DynTet demonstrates significant improvements in fidelity, lip
synchronization, and real-time performance according to various metrics. Beyond
producing stable and visually appealing synthesis videos, our method also
outputs the dynamic meshes which is promising to enable many emerging
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAPT: Category-level Articulation Estimation from a Single Point Cloud
  Using <span class="highlight-title">Transformer</span> <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lian Fu, Ryoichi Ishikawa, Yoshihiro Sato, Takeshi Oishi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to estimate joint parameters is essential for various
applications in robotics and computer vision. In this paper, we propose CAPT:
category-level articulation estimation from a point cloud using Transformer.
CAPT uses an end-to-end transformer-based architecture for joint parameter and
state estimation of articulated objects from a single point cloud. The proposed
CAPT methods accurately estimate joint parameters and states for various
articulated objects with high precision and robustness. The paper also
introduces a motion loss approach, which improves articulation estimation
performance by emphasizing the dynamic features of articulated objects.
Additionally, the paper presents a double voting strategy to provide the
framework with coarse-to-fine parameter estimation. Experimental results on
several category datasets demonstrate that our methods outperform existing
alternatives for articulation estimation. Our research provides a promising
solution for applying Transformer-based architectures in articulated object
analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ICP-Flow: LiDAR Scene Flow Estimation with ICP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yancong Lin, Holger Caesar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene flow characterizes the 3D motion between two LiDAR scans captured by an
autonomous vehicle at nearby timesteps. Prevalent methods consider scene flow
as point-wise unconstrained flow vectors that can be learned by either
large-scale training beforehand or time-consuming optimization at inference.
However, these methods do not take into account that objects in autonomous
driving often move rigidly. We incorporate this rigid-motion assumption into
our design, where the goal is to associate objects over scans and then estimate
the locally rigid transformations. We propose ICP-Flow, a learning-free flow
estimator. The core of our design is the conventional Iterative Closest Point
(ICP) algorithm, which aligns the objects over time and outputs the
corresponding rigid transformations. Crucially, to aid ICP, we propose a
histogram-based initialization that discovers the most likely translation, thus
providing a good starting point for ICP. The complete scene flow is then
recovered from the rigid transformations. We outperform state-of-the-art
baselines, including supervised models, on the Waymo dataset and perform
competitively on Argoverse-v2 and nuScenes. Further, we train a feedforward
neural network, supervised by the pseudo labels from our model, and achieve top
performance among all models capable of real-time inference. We validate the
advantage of our model on scene flow estimation with longer temporal gaps, up
to 0.5 seconds where other models fail to deliver meaningful results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The IEEE/CVF Conference on Computer Vision and Pattern Recognition
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SocialCVAE: Predicting Pedestrian Trajectory via Interaction Conditioned
  Latents <span class="chip">AAAI'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Xiang, Haoteng Yin, He Wang, Xiaogang Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pedestrian trajectory prediction is the key technology in many applications
for providing insights into human behavior and anticipating human future
motions. Most existing empirical models are explicitly formulated by observed
human behaviors using explicable mathematical terms with a deterministic
nature, while recent work has focused on developing hybrid models combined with
learning-based techniques for powerful expressiveness while maintaining
explainability. However, the deterministic nature of the learned steering
behaviors from the empirical models limits the models' practical performance.
To address this issue, this work proposes the social conditional variational
autoencoder (SocialCVAE) for predicting pedestrian trajectories, which employs
a CVAE to explore behavioral uncertainty in human motion decisions. SocialCVAE
learns socially reasonable motion randomness by utilizing a socially
explainable interaction energy map as the CVAE's condition, which illustrates
the future occupancy of each pedestrian's local neighborhood area. The energy
map is generated using an energy-based interaction model, which anticipates the
energy cost (i.e., repulsion intensity) of pedestrians' interactions with
neighbors. Experimental results on two public benchmarks including 25 scenes
demonstrate that SocialCVAE significantly improves prediction accuracy compared
with the state-of-the-art methods, with up to 16.85% improvement in Average
Displacement Error (ADE) and 69.18% improvement in Final Displacement Error
(FDE).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SDDGR: Stable Diffusion-based Deep Generative Replay for Class
  Incremental Object Detection <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17323v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17323v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junsu Kim, Hoseong Cho, Jihyeon Kim, Yihalem Yimolal Tiruneh, Seungryul Baek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of class incremental learning (CIL), genera- tive replay has
become increasingly prominent as a method to mitigate the catastrophic
forgetting, alongside the con- tinuous improvements in generative models.
However, its application in class incremental object detection (CIOD) has been
significantly limited, primarily due to the com- plexities of scenes involving
multiple labels. In this paper, we propose a novel approach called stable
diffusion deep generative replay (SDDGR) for CIOD. Our method utilizes a
diffusion-based generative model with pre-trained text- to-diffusion networks
to generate realistic and diverse syn- thetic images. SDDGR incorporates an
iterative refinement strategy to produce high-quality images encompassing old
classes. Additionally, we adopt an L2 knowledge distilla- tion technique to
improve the retention of prior knowledge in synthetic images. Furthermore, our
approach includes pseudo-labeling for old objects within new task images, pre-
venting misclassification as background elements. Exten- sive experiments on
the COCO 2017 dataset demonstrate that SDDGR significantly outperforms existing
algorithms, achieving a new state-of-the-art in various CIOD scenarios. The
source code will be made available to the public.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024. We will post a camera-ready version later</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Vanilla Multi-Task Framework for Dense Visual Prediction Solution to
  1st VCL Challenge -- Multi-Task Robustness Track 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17319v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17319v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehui Chen, Qiuchen Wang, Zhenyu Li, Jiaming Liu, Shanghang Zhang, Feng Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this report, we present our solution to the multi-task robustness track of
the 1st Visual Continual Learning (VCL) Challenge at ICCV 2023 Workshop. We
propose a vanilla framework named UniNet that seamlessly combines various
visual perception algorithms into a multi-task model. Specifically, we choose
DETR3D, Mask2Former, and BinsFormer for 3D object detection, instance
segmentation, and depth estimation tasks, respectively. The final submission is
a single model with InternImage-L backbone, and achieves a 49.6 overall score
(29.5 Det mAP, 80.3 mTPS, 46.4 Seg mAP, and 7.93 silog) on SHIFT validation
set. Besides, we provide some interesting observations in our experiments which
may facilitate the development of multi-task learning in dense visual
prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Supervised Local Learning with Augmented Auxiliary Networks <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17318v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17318v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxiang Ma, Jibin Wu, Chenyang Si, Kay Chen Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks are typically trained using global error signals that
backpropagate (BP) end-to-end, which is not only biologically implausible but
also suffers from the update locking problem and requires huge memory
consumption. Local learning, which updates each layer independently with a
gradient-isolated auxiliary network, offers a promising alternative to address
the above problems. However, existing local learning methods are confronted
with a large accuracy gap with the BP counterpart, particularly for large-scale
networks. This is due to the weak coupling between local layers and their
subsequent network layers, as there is no gradient communication across layers.
To tackle this issue, we put forward an augmented local learning method, dubbed
AugLocal. AugLocal constructs each hidden layer's auxiliary network by
uniformly selecting a small subset of layers from its subsequent network layers
to enhance their synergy. We also propose to linearly reduce the depth of
auxiliary networks as the hidden layer goes deeper, ensuring sufficient network
capacity while reducing the computational cost of auxiliary networks. Our
extensive experiments on four image classification datasets (i.e., CIFAR-10,
SVHN, STL-10, and ImageNet) demonstrate that AugLocal can effectively scale up
to tens of local layers with a comparable accuracy to BP-trained networks while
reducing GPU memory usage by around 40%. The proposed AugLocal method,
therefore, opens up a myriad of opportunities for training high-performance
deep neural networks on resource-constrained platforms.Code is available at
https://github.com/ChenxiangMA/AugLocal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How we won BraTS 2023 Adult Glioma challenge? Just faking it! Enhanced
  Synthetic Data Augmentation and Model Ensemble for brain tumour segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        André Ferreira, Naida Solak, Jianning Li, Philipp Dammann, Jens Kleesiek, Victor Alves, Jan Egger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning is the state-of-the-art technology for segmenting brain
tumours. However, this requires a lot of high-quality data, which is difficult
to obtain, especially in the medical field. Therefore, our solutions address
this problem by using unconventional mechanisms for data augmentation.
Generative adversarial networks and registration are used to massively increase
the amount of available samples for training three different deep learning
models for brain tumour segmentation, the first task of the BraTS2023
challenge. The first model is the standard nnU-Net, the second is the Swin
UNETR and the third is the winning solution of the BraTS 2021 Challenge. The
entire pipeline is built on the nnU-Net implementation, except for the
generation of the synthetic data. The use of convolutional algorithms and
transformers is able to fill each other's knowledge gaps. Using the new metric,
our best solution achieves the dice results 0.9005, 0.8673, 0.8509 and HD95
14.940, 14.467, 17.699 (whole tumour, tumour core and enhancing tumour) in the
validation set.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Robust and Efficient Cloud-Edge Elastic Model Adaptation via
  Selective Entropy Distillation <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaofo Chen, Shuaicheng Niu, Shoukai Xu, Hengjie Song, Yaowei Wang, Mingkui Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The conventional deep learning paradigm often involves training a deep model
on a server and then deploying the model or its distilled ones to
resource-limited edge devices. Usually, the models shall remain fixed once
deployed (at least for some period) due to the potential high cost of model
adaptation for both the server and edge sides. However, in many real-world
scenarios, the test environments may change dynamically (known as distribution
shifts), which often results in degraded performance. Thus, one has to adapt
the edge models promptly to attain promising performance. Moreover, with the
increasing data collected at the edge, this paradigm also fails to further
adapt the cloud model for better performance. To address these, we encounter
two primary challenges: 1) the edge model has limited computation power and may
only support forward propagation; 2) the data transmission budget between cloud
and edge devices is limited in latency-sensitive scenarios. In this paper, we
establish a Cloud-Edge Elastic Model Adaptation (CEMA) paradigm in which the
edge models only need to perform forward propagation and the edge models can be
adapted online. In our CEMA, to reduce the communication burden, we devise two
criteria to exclude unnecessary samples from uploading to the cloud, i.e.,
dynamic unreliable and low-informative sample exclusion. Based on the uploaded
samples, we update and distribute the affine parameters of normalization layers
by distilling from the stronger foundation model to the edge model with a
sample replay strategy. Extensive experimental results on ImageNet-C and
ImageNet-R verify the effectiveness of our CEMA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Method of Tracking and Analysis of Fluorescent-Labeled Cells Using
  Automatic Thresholding and Labeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mizuki Fukasawa, Tomokazu Fukuda, Takuya Akashi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-throughput screening using cell images is an efficient method for
screening new candidates for pharmaceutical drugs. To complete the screening
process, it is essential to have an efficient process for analyzing cell
images. This paper presents a new method for efficiently tracking cells and
quantitatively detecting the signal ratio between cytoplasm and nuclei.
Existing methods include those that use image processing techniques and those
that utilize artificial intelligence (AI). However, these methods do not
consider the correspondence of cells between images, or require a significant
amount of new learning data to train AI. Therefore, our method uses automatic
thresholding and labeling algorithms to compare the position of each cell
between images, and continuously measure and analyze the signal ratio of cells.
This paper describes the algorithm of our method. Using the method, we
experimented to investigate the effect of the number of opening and closing
operations during the binarization process on the tracking of the cells.
Through the experiment, we determined the appropriate number of opening and
closing processes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Denoising Diffusion Models for Inpainting of Healthy Brain Tissue <span class="chip">MICCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alicia Durrer, Philippe C. Cattin, Julia Wolleb
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper is a contribution to the "BraTS 2023 Local Synthesis of Healthy
Brain Tissue via Inpainting Challenge". The task of this challenge is to
transform tumor tissue into healthy tissue in brain magnetic resonance (MR)
images. This idea originates from the problem that MR images can be evaluated
using automatic processing tools, however, many of these tools are optimized
for the analysis of healthy tissue. By solving the given inpainting task, we
enable the automatic analysis of images featuring lesions, and further
downstream tasks. Our approach builds on denoising diffusion probabilistic
models. We use a 2D model that is trained using slices in which healthy tissue
was cropped out and is learned to be inpainted again. This allows us to use the
ground truth healthy tissue during training. In the sampling stage, we replace
the slices containing diseased tissue in the original 3D volume with the slices
containing the healthy tissue inpainting. With our approach, we achieve
comparable results to the competing methods. On the validation set our model
achieves a mean SSIM of 0.7804, a PSNR of 20.3525 and a MSE of 0.0113. In
future we plan to extend our 2D model to a 3D model, allowing to inpaint the
region of interest as a whole without losing context information of neighboring
slices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures, MICCAI challenge submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ArcSin: Adaptive ranged cosine Similarity injected noise for
  Language-Driven Visual Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Liu, Xiaomin Yu, Gongyu Zhang, Christos Bergeles, Prokar Dasgupta, Alejandro Granados, Sebastien Ourselin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we address the challenging task of bridging the modality gap
between learning from language and inference for visual tasks, including Visual
Question Answering (VQA), Image Captioning (IC) and Visual Entailment (VE). We
train models for these tasks in a zero-shot cross-modal transfer setting, a
domain where the previous state-of-the-art method relied on the fixed scale
noise injection, often compromising the semantic content of the original
modality embedding. To combat it, we propose a novel method called Adaptive
ranged cosine Similarity injected noise (ArcSin). First, we introduce an
innovative adaptive noise scale that effectively generates the textual elements
with more variability while preserving the original text feature's integrity.
Second, a similarity pool strategy is employed, expanding the domain
generalization potential by broadening the overall noise scale. This dual
strategy effectively widens the scope of the original domain while safeguarding
content integrity. Our empirical results demonstrate that these models closely
rival those trained on images in terms of performance. Specifically, our method
exhibits substantial improvements over the previous state-of-the-art, achieving
gains of 1.9 and 1.1 CIDEr points in S-Cap and M-Cap, respectively.
Additionally, we observe increases of 1.5 percentage points (pp), 1.4 pp, and
1.4 pp in accuracy for VQA, VQA-E, and VE, respectively, pushing the boundaries
of what is achievable within the constraints of image-trained model benchmarks.
The code will be released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Exposure Correction in Dynamic Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Liu, Bo Wang, Chuanming Wang, Huiyuan Fu, Huadong Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Capturing videos with wrong exposure usually produces unsatisfactory visual
effects. While image exposure correction is a popular topic, the video
counterpart is less explored in the literature. Directly applying prior
image-based methods to input videos often results in temporal incoherence with
low visual quality. Existing research in this area is also limited by the lack
of high-quality benchmark datasets. To address these issues, we construct the
first real-world paired video dataset, including both underexposure and
overexposure dynamic scenes. To achieve spatial alignment, we utilize two DSLR
cameras and a beam splitter to simultaneously capture improper and normal
exposure videos. In addition, we propose a Video Exposure Correction Network
(VECNet) based on Retinex theory, which incorporates a two-stream illumination
learning mechanism to enhance the overexposure and underexposure factors,
respectively. The estimated multi-frame reflectance and dual-path illumination
components are fused at both feature and image levels, leading to visually
appealing results. Experimental results demonstrate that the proposed method
outperforms existing image exposure correction and underexposed video
enhancement methods. The code and dataset will be available soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DivAvatar: Diverse 3D Avatar Generation with a Single <span class="highlight-title">Prompt</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijing Tao, Biwen Lei, Kunhao Liu, Shijian Lu, Miaomiao Cui, Xuansong Xie, Chunyan Miao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-Avatar generation has recently made significant strides due to
advancements in diffusion models. However, most existing work remains
constrained by limited diversity, producing avatars with subtle differences in
appearance for a given text prompt. We design DivAvatar, a novel framework that
generates diverse avatars, empowering 3D creatives with a multitude of distinct
and richly varied 3D avatars from a single text prompt. Different from most
existing work that exploits scene-specific 3D representations such as NeRF,
DivAvatar finetunes a 3D generative model (i.e., EVA3D), allowing diverse
avatar generation from simply noise sampling in inference time. DivAvatar has
two key designs that help achieve generation diversity and visual quality. The
first is a noise sampling technique during training phase which is critical in
generating diverse appearances. The second is a semantic-aware zoom mechanism
and a novel depth loss, the former producing appearances of high textual
fidelity by separate fine-tuning of specific body parts and the latter
improving geometry quality greatly by smoothing the generated mesh in the
features space. Extensive experiments show that DivAvatar is highly versatile
in generating avatars of diverse appearances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Interpretable Evaluation of Entropy-based Novelty of Generative
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17287v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17287v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingwei Zhang, Cheuk Ting Li, Farzan Farnia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The massive developments of generative model frameworks and architectures
require principled methods for the evaluation of a model's novelty compared to
a reference dataset or baseline generative models. While the recent literature
has extensively studied the evaluation of the quality, diversity, and
generalizability of generative models, the assessment of a model's novelty
compared to a baseline model has not been adequately studied in the machine
learning community. In this work, we focus on the novelty assessment under
multi-modal generative models and attempt to answer the following question:
Given the samples of a generative model $\mathcal{G}$ and a reference dataset
$\mathcal{S}$, how can we discover and count the modes expressed by
$\mathcal{G}$ more frequently than in $\mathcal{S}$. We introduce a spectral
approach to the described task and propose the Kernel-based Entropic Novelty
(KEN) score to quantify the mode-based novelty of distribution $P_\mathcal{G}$
with respect to distribution $P_\mathcal{S}$. We analytically interpret the
behavior of the KEN score under mixture distributions with sub-Gaussian
components. Next, we develop a method based on Cholesky decomposition to
compute the KEN score from observed samples. We support the KEN-based
quantification of novelty by presenting several numerical results on synthetic
and real image distributions. Our numerical results indicate the success of the
proposed approach in detecting the novel modes and the comparison of
state-of-the-art generative models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Hyperspectral Images via Diffusion Model and Group-Autoencoder
  Super-resolution Network <span class="chip">AAAI2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyang Wang, Dongyang Li, Mingyang Zhang, Hao Luo, Maoguo Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing hyperspectral image (HSI) super-resolution (SR) methods struggle to
effectively capture the complex spectral-spatial relationships and low-level
details, while diffusion models represent a promising generative model known
for their exceptional performance in modeling complex relations and learning
high and low-level visual features. The direct application of diffusion models
to HSI SR is hampered by challenges such as difficulties in model convergence
and protracted inference time. In this work, we introduce a novel
Group-Autoencoder (GAE) framework that synergistically combines with the
diffusion model to construct a highly effective HSI SR model (DMGASR). Our
proposed GAE framework encodes high-dimensional HSI data into low-dimensional
latent space where the diffusion model works, thereby alleviating the
difficulty of training the diffusion model while maintaining band correlation
and considerably reducing inference time. Experimental results on both natural
and remote sensing hyperspectral datasets demonstrate that the proposed method
is superior to other state-of-the-art methods both visually and metrically.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One-Shot Structure-Aware Stylized Image Synthesis <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17275v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17275v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hansam Cho, Jonghyun Lee, Seunggyu Chang, Yonghyun Jeong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While GAN-based models have been successful in image stylization tasks, they
often struggle with structure preservation while stylizing a wide range of
input images. Recently, diffusion models have been adopted for image
stylization but still lack the capability to maintain the original quality of
input images. Building on this, we propose OSASIS: a novel one-shot stylization
method that is robust in structure preservation. We show that OSASIS is able to
effectively disentangle the semantics from the structure of an image, allowing
it to control the level of content and style implemented to a given input. We
apply OSASIS to various experimental settings, including stylization with
out-of-domain reference images and stylization with text-driven manipulation.
Results show that OSASIS outperforms other stylization methods, especially for
input images that were rarely encountered during training, providing a
promising solution to stylization via diffusion models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explicit Interaction for Fusion-Based Place Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17264v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17264v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyi Xu, Junyi Ma, Qi Wu, Zijie Zhou, Yue Wang, Xieyuanli Chen, Ling Pei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fusion-based place recognition is an emerging technique jointly utilizing
multi-modal perception data, to recognize previously visited places in
GPS-denied scenarios for robots and autonomous vehicles. Recent fusion-based
place recognition methods combine multi-modal features in implicit manners.
While achieving remarkable results, they do not explicitly consider what the
individual modality affords in the fusion system. Therefore, the benefit of
multi-modal feature fusion may not be fully explored. In this paper, we propose
a novel fusion-based network, dubbed EINet, to achieve explicit interaction of
the two modalities. EINet uses LiDAR ranges to supervise more robust vision
features for long time spans, and simultaneously uses camera RGB data to
improve the discrimination of LiDAR point clouds. In addition, we develop a new
benchmark for the place recognition task based on the nuScenes dataset. To
establish this benchmark for future research with comprehensive comparisons, we
introduce both supervised and self-supervised training schemes alongside
evaluation protocols. We conduct extensive experiments on the proposed
benchmark, and the experimental results show that our EINet exhibits better
recognition performance as well as solid generalization ability compared to the
state-of-the-art fusion-based place recognition approaches. Our open-source
code and benchmark are released at: https://github.com/BIT-XJY/EINet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context-based and Diversity-driven Specificity in Compositional
  Zero-Shot Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17251v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17251v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun Li, Zhe Liu, Hang Chen, Lina Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compositional Zero-Shot Learning (CZSL) aims to recognize unseen
attribute-object pairs based on a limited set of observed examples. Current
CZSL methodologies, despite their advancements, tend to neglect the distinct
specificity levels present in attributes. For instance, given images of sliced
strawberries, they may fail to prioritize `Sliced-Strawberry' over a generic
`Red-Strawberry', despite the former being more informative. They also suffer
from ballooning search space when shifting from Close-World (CW) to Open-World
(OW) CZSL. To address the issues, we introduce the Context-based and
Diversity-driven Specificity learning framework for CZSL (CDS-CZSL). Our
framework evaluates the specificity of attributes by considering the diversity
of objects they apply to and their related context. This novel approach allows
for more accurate predictions by emphasizing specific attribute-object pairs
and improves composition filtering in OW-CZSL. We conduct experiments in both
CW and OW scenarios, and our model achieves state-of-the-art results across
three datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning-Based Speech and Vision Synthesis to Improve Phishing
  Attack Detection through a Multi-layer Adaptive Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17249v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17249v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tosin Ige, Christopher Kiekintveld, Aritran Piplai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ever-evolving ways attacker continues to im prove their phishing
techniques to bypass existing state-of-the-art phishing detection methods pose
a mountain of challenges to researchers in both industry and academia research
due to the inability of current approaches to detect complex phishing attack.
Thus, current anti-phishing methods remain vulnerable to complex phishing
because of the increasingly sophistication tactics adopted by attacker coupled
with the rate at which new tactics are being developed to evade detection. In
this research, we proposed an adaptable framework that combines Deep learning
and Randon Forest to read images, synthesize speech from deep-fake videos, and
natural language processing at various predictions layered to significantly
increase the performance of machine learning models for phishing attack
detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SDR-Former: A Siamese Dual-Resolution <span class="highlight-title">Transformer</span> for Liver Lesion
  Classification Using 3D Multi-Phase Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng Lou, Hanning Ying, Xiaoqing Liu, Hong-Yu Zhou, Yuqing Zhang, Yizhou Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated classification of liver lesions in multi-phase CT and MR scans is
of clinical significance but challenging. This study proposes a novel Siamese
Dual-Resolution Transformer (SDR-Former) framework, specifically designed for
liver lesion classification in 3D multi-phase CT and MR imaging with varying
phase counts. The proposed SDR-Former utilizes a streamlined Siamese Neural
Network (SNN) to process multi-phase imaging inputs, possessing robust feature
representations while maintaining computational efficiency. The weight-sharing
feature of the SNN is further enriched by a hybrid Dual-Resolution Transformer
(DR-Former), comprising a 3D Convolutional Neural Network (CNN) and a tailored
3D Transformer for processing high- and low-resolution images, respectively.
This hybrid sub-architecture excels in capturing detailed local features and
understanding global contextual information, thereby, boosting the SNN's
feature extraction capabilities. Additionally, a novel Adaptive Phase Selection
Module (APSM) is introduced, promoting phase-specific intercommunication and
dynamically adjusting each phase's influence on the diagnostic outcome. The
proposed SDR-Former framework has been validated through comprehensive
experiments on two clinical datasets: a three-phase CT dataset and an
eight-phase MR dataset. The experimental results affirm the efficacy of the
proposed framework. To support the scientific community, we are releasing our
extensive multi-phase MR dataset for liver lesion analysis to the public. This
pioneering dataset, being the first publicly available multi-phase MR dataset
in this field, also underpins the MICCAI LLD-MMRI Challenge. The dataset is
accessible at:https://bit.ly/3IyYlgN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Playground v2.5: Three Insights towards Enhancing Aesthetic Quality in
  Text-to-Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, Suhail Doshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we share three insights for achieving state-of-the-art
aesthetic quality in text-to-image generative models. We focus on three
critical aspects for model improvement: enhancing color and contrast, improving
generation across multiple aspect ratios, and improving human-centric fine
details. First, we delve into the significance of the noise schedule in
training a diffusion model, demonstrating its profound impact on realism and
visual fidelity. Second, we address the challenge of accommodating various
aspect ratios in image generation, emphasizing the importance of preparing a
balanced bucketed dataset. Lastly, we investigate the crucial role of aligning
model outputs with human preferences, ensuring that generated images resonate
with human perceptual expectations. Through extensive analysis and experiments,
Playground v2.5 demonstrates state-of-the-art performance in terms of aesthetic
quality under various conditions and aspect ratios, outperforming both
widely-used open-source models like SDXL and Playground v2, and closed-source
commercial systems such as DALLE 3 and Midjourney v5.2. Our model is
open-source, and we hope the development of Playground v2.5 provides valuable
guidelines for researchers aiming to elevate the aesthetic quality of
diffusion-based image generation models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Model weights:
  https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image-Text Matching with Multi-View Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17237v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17237v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Cheng, Wanqing Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing two-stream models for image-text matching show good performance
while ensuring retrieval speed and have received extensive attention from
industry and academia. These methods use a single representation to encode
image and text separately and get a matching score with cosine similarity or
the inner product of vectors. However, the performance of the two-stream model
is often sub-optimal. On the one hand, a single representation is challenging
to cover complex content comprehensively. On the other hand, in this framework
of lack of interaction, it is challenging to match multiple meanings which
leads to information being ignored. To address the problems mentioned above and
facilitate the performance of the two-stream model, we propose a multi-view
attention approach for two-stream image-text matching MVAM
(\textbf{M}ulti-\textbf{V}iew \textbf{A}ttention \textbf{M}odel). It first
learns multiple image and text representations by diverse attention heads with
different view codes. And then concatenate these representations into one for
matching. A diversity objective is also used to promote diversity between
attention heads. With this method, models are able to encode images and text
from different views and attend to more key points. So we can get
representations that contain more information. When doing retrieval tasks, the
matching scores between images and texts can be calculated from different
aspects, leading to better matching performance. Experiment results on MSCOCO
and Flickr30K show that our proposed model brings improvements over existing
models. Further case studies show that different attention heads can focus on
different contents and finally obtain a more comprehensive representation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Preserving Fairness Generalization in Deepfake Detection <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17229v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17229v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Lin, Xinan He, Yan Ju, Xin Wang, Feng Ding, Shu Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although effective deepfake detection models have been developed in recent
years, recent studies have revealed that these models can result in unfair
performance disparities among demographic groups, such as race and gender. This
can lead to particular groups facing unfair targeting or exclusion from
detection, potentially allowing misclassified deepfakes to manipulate public
opinion and undermine trust in the model. The existing method for addressing
this problem is providing a fair loss function. It shows good fairness
performance for intra-domain evaluation but does not maintain fairness for
cross-domain testing. This highlights the significance of fairness
generalization in the fight against deepfakes. In this work, we propose the
first method to address the fairness generalization problem in deepfake
detection by simultaneously considering features, loss, and optimization
aspects. Our method employs disentanglement learning to extract demographic and
domain-agnostic forgery features, fusing them to encourage fair learning across
a flattened loss landscape. Extensive experiments on prominent deepfake
datasets demonstrate our method's effectiveness, surpassing state-of-the-art
approaches in preserving fairness during cross-domain deepfake detection. The
code is available at https://github.com/Purdue-M2/Fairness-Generalization
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by The IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feature Re-Embedding: Towards Foundation Model-Level Performance in
  Computational Pathology <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17228v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17228v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Tang, Fengtao Zhou, Sheng Huang, Xiang Zhu, Yi Zhang, Bo Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple instance learning (MIL) is the most widely used framework in
computational pathology, encompassing sub-typing, diagnosis, prognosis, and
more. However, the existing MIL paradigm typically requires an offline instance
feature extractor, such as a pre-trained ResNet or a foundation model. This
approach lacks the capability for feature fine-tuning within the specific
downstream tasks, limiting its adaptability and performance. To address this
issue, we propose a Re-embedded Regional Transformer (R$^2$T) for re-embedding
the instance features online, which captures fine-grained local features and
establishes connections across different regions. Unlike existing works that
focus on pre-training powerful feature extractor or designing sophisticated
instance aggregator, R$^2$T is tailored to re-embed instance features online.
It serves as a portable module that can seamlessly integrate into mainstream
MIL models. Extensive experimental results on common computational pathology
tasks validate that: 1) feature re-embedding improves the performance of MIL
models based on ResNet-50 features to the level of foundation model features,
and further enhances the performance of foundation model features; 2) the
R$^2$T can introduce more significant performance improvements to various MIL
models; 3) R$^2$T-MIL, as an R$^2$T-enhanced AB-MIL, outperforms other latest
methods by a large margin. The code is available
at:~\href{https://github.com/DearCaat/RRT-MIL}{https://github.com/DearCaat/RRT-MIL}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CharacterGen: Efficient 3D Character Generation from Single Images with
  Multi-View Pose Canonicalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao-Yang Peng, Jia-Peng Zhang, Meng-Hao Guo, Yan-Pei Cao, Shi-Min Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of digital content creation, generating high-quality 3D
characters from single images is challenging, especially given the complexities
of various body poses and the issues of self-occlusion and pose ambiguity. In
this paper, we present CharacterGen, a framework developed to efficiently
generate 3D characters. CharacterGen introduces a streamlined generation
pipeline along with an image-conditioned multi-view diffusion model. This model
effectively calibrates input poses to a canonical form while retaining key
attributes of the input image, thereby addressing the challenges posed by
diverse poses. A transformer-based, generalizable sparse-view reconstruction
model is the other core component of our approach, facilitating the creation of
detailed 3D models from multi-view images. We also adopt a
texture-back-projection strategy to produce high-quality texture maps.
Additionally, we have curated a dataset of anime characters, rendered in
multiple poses and views, to train and evaluate our model. Our approach has
been thoroughly evaluated through quantitative and qualitative experiments,
showing its proficiency in generating 3D characters with high-quality shapes
and textures, ready for downstream applications such as rigging and animation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VCD: Knowledge Base Guided Visual Commonsense Discovery in Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17213v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17213v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangqing Shen, Yurun Song, Siwei Wu, Rui Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual commonsense contains knowledge about object properties, relationships,
and behaviors in visual data. Discovering visual commonsense can provide a more
comprehensive and richer understanding of images, and enhance the reasoning and
decision-making capabilities of computer vision systems. However, the visual
commonsense defined in existing visual commonsense discovery studies is
coarse-grained and incomplete. In this work, we draw inspiration from a
commonsense knowledge base ConceptNet in natural language processing, and
systematically define the types of visual commonsense. Based on this, we
introduce a new task, Visual Commonsense Discovery (VCD), aiming to extract
fine-grained commonsense of different types contained within different objects
in the image. We accordingly construct a dataset (VCDD) from Visual Genome and
ConceptNet for VCD, featuring over 100,000 images and 14 million
object-commonsense pairs. We furthermore propose a generative model (VCDM) that
integrates a vision-language model with instruction tuning to tackle VCD.
Automatic and human evaluations demonstrate VCDM's proficiency in VCD,
particularly outperforming GPT-4V in implicit commonsense discovery. The value
of VCD is further demonstrated by its application to two downstream tasks,
including visual commonsense evaluation and visual question answering. The data
and code will be made available on GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Purified and Unified Steganographic Network <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17210v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17210v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guobiao Li, Sheng Li, Zicong Luo, Zhenxing Qian, Xinpeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Steganography is the art of hiding secret data into the cover media for
covert communication. In recent years, more and more deep neural network
(DNN)-based steganographic schemes are proposed to train steganographic
networks for secret embedding and recovery, which are shown to be promising.
Compared with the handcrafted steganographic tools, steganographic networks
tend to be large in size. It raises concerns on how to imperceptibly and
effectively transmit these networks to the sender and receiver to facilitate
the covert communication. To address this issue, we propose in this paper a
Purified and Unified Steganographic Network (PUSNet). It performs an ordinary
machine learning task in a purified network, which could be triggered into
steganographic networks for secret embedding or recovery using different keys.
We formulate the construction of the PUSNet into a sparse weight filling
problem to flexibly switch between the purified and steganographic networks. We
further instantiate our PUSNet as an image denoising network with two
steganographic networks concealed for secret image embedding and recovery.
Comprehensive experiments demonstrate that our PUSNet achieves good performance
on secret image embedding, secret image recovery, and image denoising in a
single architecture. It is also shown to be capable of imperceptibly carrying
the steganographic networks in a purified network. Code is available at
\url{https://github.com/albblgb/PUSNet}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures, Accepted at CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deployment Prior Injection for Run-time Calibratable Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17207v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17207v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mo Zhou, Yiding Yang, Haoxiang Li, Vishal M. Patel, Gang Hua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With a strong alignment between the training and test distributions, object
relation as a context prior facilitates object detection. Yet, it turns into a
harmful but inevitable training set bias upon test distributions that shift
differently across space and time. Nevertheless, the existing detectors cannot
incorporate deployment context prior during the test phase without parameter
update. Such kind of capability requires the model to explicitly learn
disentangled representations with respect to context prior. To achieve this, we
introduce an additional graph input to the detector, where the graph represents
the deployment context prior, and its edge values represent object relations.
Then, the detector behavior is trained to bound to the graph with a modified
training objective. As a result, during the test phase, any suitable deployment
context prior can be injected into the detector via graph edits, hence
calibrating, or "re-biasing" the detector towards the given prior at run-time
without parameter update. Even if the deployment prior is unknown, the detector
can self-calibrate using deployment prior approximated using its own
predictions. Comprehensive experimental results on the COCO dataset, as well as
cross-dataset testing on the Objects365 dataset, demonstrate the effectiveness
of the run-time calibratable detector.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Generative Model Evaluation: A Novel Algorithm for Realistic
  Image Synthesis and Comparison in OCR System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17204v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17204v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Majid Memari, Khaled R. Ahmed, Shahram Rahimi, Noorbakhsh Amiri Golilarz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research addresses a critical challenge in the field of generative
models, particularly in the generation and evaluation of synthetic images.
Given the inherent complexity of generative models and the absence of a
standardized procedure for their comparison, our study introduces a pioneering
algorithm to objectively assess the realism of synthetic images. This approach
significantly enhances the evaluation methodology by refining the Fr\'echet
Inception Distance (FID) score, allowing for a more precise and subjective
assessment of image quality. Our algorithm is particularly tailored to address
the challenges in generating and evaluating realistic images of Arabic
handwritten digits, a task that has traditionally been near-impossible due to
the subjective nature of realism in image generation. By providing a systematic
and objective framework, our method not only enables the comparison of
different generative models but also paves the way for improvements in their
design and output. This breakthrough in evaluation and comparison is crucial
for advancing the field of OCR, especially for scripts that present unique
complexities, and sets a new standard in the generation and assessment of
high-quality synthetic images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>My manuscript entitled "Advancing Generative Model Evaluation: A
  Novel Algorithm for Realistic Image Synthesis and Comparison in OCR Systems"
  has been submitted on 29-Jan-2024 to IEEE Access and is presently being given
  full consideration for publication in IEEE Access</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Quality of Compressed Images by Mitigating Enhancement Bias
  Towards Compression Domain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17200v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17200v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qunliang Xing, Mai Xu, Shengxi Li, Xin Deng, Meisong Zheng, Huaida Liu, Ying Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing quality enhancement methods for compressed images focus on aligning
the enhancement domain with the raw domain to yield realistic images. However,
these methods exhibit a pervasive enhancement bias towards the compression
domain, inadvertently regarding it as more realistic than the raw domain. This
bias makes enhanced images closely resemble their compressed counterparts, thus
degrading their perceptual quality. In this paper, we propose a simple yet
effective method to mitigate this bias and enhance the quality of compressed
images. Our method employs a conditional discriminator with the compressed
image as a key condition, and then incorporates a domain-divergence
regularization to actively distance the enhancement domain from the compression
domain. Through this dual strategy, our method enables the discrimination
against the compression domain, and brings the enhancement domain closer to the
raw domain. Comprehensive quality evaluations confirm the superiority of our
method over other state-of-the-art methods without incurring inference
overheads.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentiable Biomechanics Unlocks Opportunities for Markerless Motion
  Capture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        R. James Cotton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent developments have created differentiable physics simulators designed
for machine learning pipelines that can be accelerated on a GPU. While these
can simulate biomechanical models, these opportunities have not been exploited
for biomechanics research or markerless motion capture. We show that these
simulators can be used to fit inverse kinematics to markerless motion capture
data, including scaling the model to fit the anthropomorphic measurements of an
individual. This is performed end-to-end with an implicit representation of the
movement trajectory, which is propagated through the forward kinematic model to
minimize the error from the 3D markers reprojected into the images. The
differential optimizer yields other opportunities, such as adding bundle
adjustment during trajectory optimization to refine the extrinsic camera
parameters or meta-optimization to improve the base model jointly over
trajectories from multiple participants. This approach improves the
reprojection error from markerless motion capture over prior methods and
produces accurate spatial step parameters compared to an instrumented walkway
for control and clinical populations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PE-MVCNet: Multi-view and Cross-modal Fusion Network for Pulmonary
  Embolism Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoxin Guo, Zhipeng Wang, Ruiquan Ge, Jianxun Yu, Feiwei Qin, Yuan Tian, Yuqing Peng, Yonghong Li, Changmiao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The early detection of a pulmonary embolism (PE) is critical for enhancing
patient survival rates. Both image-based and non-image-based features are of
utmost importance in medical classification tasks. In a clinical setting,
physicians tend to rely on the contextual information provided by Electronic
Medical Records (EMR) to interpret medical imaging. However, very few models
effectively integrate clinical information with imaging data. To address this
shortcoming, we suggest a multimodal fusion methodology, termed PE-MVCNet,
which capitalizes on Computed Tomography Pulmonary Angiography imaging and EMR
data. This method comprises the Image-only module with an integrated multi-view
block, the EMR-only module, and the Cross-modal Attention Fusion (CMAF) module.
These modules cooperate to extract comprehensive features that subsequently
generate predictions for PE. We conducted experiments using the publicly
accessible Stanford University Medical Center dataset, achieving an AUROC of
94.1%, an accuracy rate of 90.2%, and an F1 score of 90.6%. Our proposed model
outperforms existing methodologies, corroborating that our multimodal fusion
model excels compared to models that use a single data modality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sora: A <span class="highlight-title">Review</span> on Background, Technology, Limitations, and Opportunities
  of Large Vision Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, Lifang He, Lichao Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sora is a text-to-video generative AI model, released by OpenAI in February
2024. The model is trained to generate videos of realistic or imaginative
scenes from text instructions and show potential in simulating the physical
world. Based on public technical reports and reverse engineering, this paper
presents a comprehensive review of the model's background, related
technologies, applications, remaining challenges, and future directions of
text-to-video AI models. We first trace Sora's development and investigate the
underlying technologies used to build this "world simulator". Then, we describe
in detail the applications and potential impact of Sora in multiple industries
ranging from film-making and education to marketing. We discuss the main
challenges and limitations that need to be addressed to widely deploy Sora,
such as ensuring safe and unbiased video generation. Lastly, we discuss the
future development of Sora and video generation models in general, and how
advancements in the field could enable new ways of human-AI interaction,
boosting productivity and creativity of video generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 18 figures; Our GitHub Homepage:
  https://github.com/lichao-sun/SoraReview</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lane2Seq: Towards Unified Lane Detection via Sequence Generation <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17172v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17172v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunyang Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a novel sequence generation-based framework for
lane detection, called Lane2Seq. It unifies various lane detection formats by
casting lane detection as a sequence generation task. This is different from
previous lane detection methods, which depend on well-designed task-specific
head networks and corresponding loss functions. Lane2Seq only adopts a plain
transformer-based encoder-decoder architecture with a simple cross-entropy
loss. Additionally, we propose a new multi-format model tuning based on
reinforcement learning to incorporate the task-specific knowledge into
Lane2Seq. Experimental results demonstrate that such a simple sequence
generation paradigm not only unifies lane detection but also achieves
competitive performance on benchmarks. For example, Lane2Seq gets 97.95\% and
97.42\% F1 score on Tusimple and LLAMAS datasets, establishing a new
state-of-the-art result for two benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2024 acceptance</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LiveHPS: LiDAR-based Scene-level Human Pose and Shape Estimation in Free
  Environment <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17171v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17171v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Ren, Xiao Han, Chengfeng Zhao, Jingya Wang, Lan Xu, Jingyi Yu, Yuexin Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For human-centric large-scale scenes, fine-grained modeling for 3D human
global pose and shape is significant for scene understanding and can benefit
many real-world applications. In this paper, we present LiveHPS, a novel
single-LiDAR-based approach for scene-level human pose and shape estimation
without any limitation of light conditions and wearable devices. In particular,
we design a distillation mechanism to mitigate the distribution-varying effect
of LiDAR point clouds and exploit the temporal-spatial geometric and dynamic
information existing in consecutive frames to solve the occlusion and noise
disturbance. LiveHPS, with its efficient configuration and high-quality output,
is well-suited for real-world applications. Moreover, we propose a huge human
motion dataset, named FreeMotion, which is collected in various scenarios with
diverse human poses, shapes and translations. It consists of multi-modal and
multi-view acquisition data from calibrated and synchronized LiDARs, cameras,
and IMUs. Extensive experiments on our new dataset and other public datasets
demonstrate the SOTA performance and robustness of our approach. We will
release our code and dataset soon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Umbra: A Generative Approach for Sunlight Access Computation in
  Urban Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17169v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17169v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazi Shahrukh Omar, Gustavo Moreira, Daniel Hodczak, Maryam Hosseini, Nicola Colaninno, Marcos Lage, Fabio Miranda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sunlight and shadow play critical roles in how urban spaces are utilized,
thrive, and grow. While access to sunlight is essential to the success of urban
environments, shadows can provide shaded places to stay during the hot seasons,
mitigate heat island effect, and increase pedestrian comfort levels. Properly
quantifying sunlight access and shadows in large urban environments is key in
tackling some of the important challenges facing cities today. In this paper,
we propose Deep Umbra, a novel computational framework that enables the
quantification of sunlight access and shadows at a global scale. Our framework
is based on a conditional generative adversarial network that considers the
physical form of cities to compute high-resolution spatial information of
accumulated sunlight access for the different seasons of the year. We use data
from seven different cities to train our model, and show, through an extensive
set of experiments, its low overall RMSE (below 0.1) as well as its
extensibility to cities that were not part of the training set. Additionally,
we contribute a set of case studies and a comprehensive dataset with sunlight
access information for more than 100 cities across six continents of the world.
Deep Umbra is available at https://urbantk.org/shadows.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE Transactions on Big Data. Deep Umbra is available at
  https://urbantk.org/shadows</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-shot adaptation for morphology-independent cell instance
  segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17165v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17165v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ram J. Zaveri, Voke Brume, Gianfranco Doretto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Microscopy data collections are becoming larger and more frequent. Accurate
and precise quantitative analysis tools like cell instance segmentation are
necessary to benefit from them. This is challenging due to the variability in
the data, which requires retraining the segmentation model to maintain high
accuracy on new collections. This is needed especially for segmenting cells
with elongated and non-convex morphology like bacteria. We propose to reduce
the amount of annotation and computing power needed for retraining the model by
introducing a few-shot domain adaptation approach that requires annotating only
one to five cells of the new data to process and that quickly adapts the model
to maintain high accuracy. Our results show a significant boost in accuracy
after adaptation to very challenging bacteria datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ISBI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NocPlace: Nocturnal Visual Place Recognition Using Generative and
  Inherited Knowledge Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17159v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17159v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingxi Liu, Yiqun Wang, Huaqi Tao, Tingjun Huang, Fulin Tang, Yihong Wu, Jinqiang Cui, Hong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Place Recognition (VPR) is crucial in computer vision, aiming to
retrieve database images similar to a query image from an extensive collection
of known images. However, like many vision-related tasks, learning-based VPR
often experiences a decline in performance during nighttime due to the scarcity
of nighttime images. Specifically, VPR needs to address the cross-domain
problem of night-to-day rather than just the issue of a single nighttime
domain. In response to these issues, we present NocPlace, which leverages a
generated large-scale, multi-view, nighttime VPR dataset to embed resilience
against dazzling lights and extreme darkness in the learned global descriptor.
Firstly, we establish a day-night urban scene dataset called NightCities,
capturing diverse nighttime scenarios and lighting variations across 60 cities
globally. Following this, an unpaired image-to-image translation network is
trained on this dataset. Using this trained translation network, we process an
existing VPR dataset, thereby obtaining its nighttime version. The NocPlace is
then fine-tuned using night-style images, the original labels, and descriptors
inherited from the Daytime VPR model. Comprehensive experiments on various
nighttime VPR test sets reveal that NocPlace considerably surpasses previous
state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages,9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Video as the New Language for Real-World Decision Making 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17139v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17139v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sherry Yang, Jacob Walker, Jack Parker-Holder, Yilun Du, Jake Bruce, Andre Barreto, Pieter Abbeel, Dale Schuurmans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Both text and video data are abundant on the internet and support large-scale
self-supervised learning through next token or frame prediction. However, they
have not been equally leveraged: language models have had significant
real-world impact, whereas video generation has remained largely limited to
media entertainment. Yet video data captures important information about the
physical world that is difficult to express in language. To address this gap,
we discuss an under-appreciated opportunity to extend video generation to solve
tasks in the real world. We observe how, akin to language, video can serve as a
unified interface that can absorb internet knowledge and represent diverse
tasks. Moreover, we demonstrate how, like language models, video generation can
serve as planners, agents, compute engines, and environment simulators through
techniques such as in-context learning, planning and reinforcement learning. We
identify major impact opportunities in domains such as robotics, self-driving,
and science, supported by recent work that demonstrates how such advanced
capabilities in video generation are plausibly within reach. Lastly, we
identify key challenges in video generation that mitigate progress. Addressing
these challenges will enable video generation models to demonstrate unique
value alongside language models in a wider array of AI applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficiently Leveraging Linguistic Priors for Scene Text Spotting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17134v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17134v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nguyen Nguyen, Yapeng Tian, Chenliang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incorporating linguistic knowledge can improve scene text recognition, but it
is questionable whether the same holds for scene text spotting, which typically
involves text detection and recognition. This paper proposes a method that
leverages linguistic knowledge from a large text corpus to replace the
traditional one-hot encoding used in auto-regressive scene text spotting and
recognition models. This allows the model to capture the relationship between
characters in the same word. Additionally, we introduce a technique to generate
text distributions that align well with scene text datasets, removing the need
for in-domain fine-tuning. As a result, the newly created text distributions
are more informative than pure one-hot encoding, leading to improved spotting
and recognition performance. Our method is simple and efficient, and it can
easily be integrated into existing auto-regressive-based approaches.
Experimental results show that our method not only improves recognition
accuracy but also enables more accurate localization of words. It significantly
improves both state-of-the-art scene text spotting and recognition pipelines,
achieving state-of-the-art results on several benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Stochastic positional embeddings improve masked image modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.00566v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.00566v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Bar, Florian Bordes, Assaf Shocher, Mahmoud Assran, Pascal Vincent, Nicolas Ballas, Trevor Darrell, Amir Globerson, <span class="highlight-author">Yann LeCun</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Masked Image Modeling (MIM) is a promising self-supervised learning approach
that enables learning from unlabeled images. Despite its recent success,
learning good representations through MIM remains challenging because it
requires predicting the right semantic content in accurate locations. For
example, given an incomplete picture of a dog, we can guess that there is a
tail, but we cannot determine its exact location. In this work, we propose to
incorporate location uncertainty into MIM by using stochastic positional
embeddings (StoP). Specifically, we condition the model on stochastic masked
token positions drawn from a Gaussian distribution. StoP reduces overfitting to
location features and guides the model toward learning features that are more
robust to location uncertainties. Quantitatively, StoP improves downstream MIM
performance on a variety of downstream tasks, including $+1.7\%$ on ImageNet
linear probing using ViT-B, and $+2.5\%$ for ViT-H using $1\%$ of the data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and models available in https://github.com/amirbar/StoP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FairSeg: A Large-Scale Medical Image Segmentation <span class="highlight-title">Dataset</span> for Fairness
  Learning Using Segment Anything Model with Fair Error-Bound Scaling <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.02189v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.02189v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Tian, Min Shi, Yan Luo, Ava Kouhana, Tobias Elze, Mengyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fairness in artificial intelligence models has gained significantly more
attention in recent years, especially in the area of medicine, as fairness in
medical models is critical to people's well-being and lives. High-quality
medical fairness datasets are needed to promote fairness learning research.
Existing medical fairness datasets are all for classification tasks, and no
fairness datasets are available for medical segmentation, while medical
segmentation is an equally important clinical task as classifications, which
can provide detailed spatial information on organ abnormalities ready to be
assessed by clinicians. In this paper, we propose the first fairness dataset
for medical segmentation named Harvard-FairSeg with 10,000 subject samples. In
addition, we propose a fair error-bound scaling approach to reweight the loss
function with the upper error-bound in each identity group, using the segment
anything model (SAM). We anticipate that the segmentation performance equity
can be improved by explicitly tackling the hard cases with high training errors
in each identity group. To facilitate fair comparisons, we utilize a novel
equity-scaled segmentation performance metric to compare segmentation metrics
in the context of fairness, such as the equity-scaled Dice coefficient. Through
comprehensive experiments, we demonstrate that our fair error-bound scaling
approach either has superior or comparable fairness performance to the
state-of-the-art fairness learning models. The dataset and code are publicly
accessible via https://ophai.hms.harvard.edu/harvard-fairseg10k.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024; Codes available at
  https://github.com/Harvard-Ophthalmology-AI-Lab/FairSeg</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Weighted Monte Carlo augmented spherical Fourier-Bessel convolutional
  layers for 3D abdominal organ segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16825v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16825v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenzhao Zhao, Steffen Albert, Barbara D. Wichtmann, Angelika Maurer, Ulrike Attenberger, Frank G. Zöllner, Jürgen Hesser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Filter-decomposition-based group equivariant convolutional neural networks
show promising stability and data efficiency for 3D image feature extraction.
However, the existing filter-decomposition-based 3D group equivariant neural
networks rely on parameter-sharing designs and are mostly limited to rotation
transform groups, where the chosen spherical harmonic filter bases consider
only angular orthogonality. These limitations hamper its application to deep
neural network architectures for medical image segmentation. To address these
issues, this paper describes a non-parameter-sharing affine group equivariant
neural network for 3D medical image segmentation based on an adaptive
aggregation of Monte Carlo augmented spherical Fourier Bessel filter bases. The
efficiency and flexibility of the adopted non-parameter strategy enable for the
first time an efficient implementation of 3D affine group equivariant
convolutional neural networks for volumetric data. The introduced spherical
Bessel Fourier filter basis combines both angular and radial orthogonality for
better feature extraction. The 3D image segmentation experiments on two
abdominal image sets, BTCV and the NIH Pancreas datasets, show that the
proposed methods excel the state-of-the-art 3D neural networks with high
training stability and data efficiency. The code will be available at
https://github.com/ZhaoWenzhao/WVMS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributed Deep Joint Source-Channel Coding with Decoder-Only Side
  Information <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04311v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04311v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Selim F. Yilmaz, Ezgi Ozyilkan, Deniz Gunduz, Elza Erkip
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider low-latency image transmission over a noisy wireless channel when
correlated side information is present only at the receiver side (the Wyner-Ziv
scenario). In particular, we are interested in developing practical schemes
using a data-driven joint source-channel coding (JSCC) approach, which has been
previously shown to outperform conventional separation-based approaches in the
practical finite blocklength regimes, and to provide graceful degradation with
channel quality. We propose a novel neural network architecture that
incorporates the decoder-only side information at multiple stages at the
receiver side. Our results demonstrate that the proposed method succeeds in
integrating the side information, yielding improved performance at all channel
conditions in terms of the various quality measures considered here, especially
at low channel signal-to-noise ratios (SNRs) and small bandwidth ratios (BRs).
We have made the source code of the proposed method public to enable further
research, and the reproducibility of the results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in IEEE International Conference on Machine Learning for
  Communication and Networking (ICMLCN) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OCAtari: Object-Centric Atari 2600 Reinforcement Learning Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.08649v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.08649v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quentin Delfosse, Jannis Blüml, Bjarne Gregori, Sebastian Sztwiertnia, Kristian Kersting
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cognitive science and psychology suggest that object-centric representations
of complex scenes are a promising step towards enabling efficient abstract
reasoning from low-level perceptual features. Yet, most deep reinforcement
learning approaches only rely on pixel-based representations that do not
capture the compositional properties of natural scenes. For this, we need
environments and datasets that allow us to work and evaluate object-centric
approaches. In our work, we extend the Atari Learning Environments, the
most-used evaluation framework for deep RL approaches, by introducing OCAtari,
that performs resource-efficient extractions of the object-centric states for
these games. Our framework allows for object discovery, object representation
learning, as well as object-centric RL. We evaluate OCAtari's detection
capabilities and resource efficiency. Our source code is available at
github.com/k4ntz/OC_Atari.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 8 main paper pages, 36 appendix pages. In main paper: 4
  figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detecting Heart Disease from Multi-View Ultrasound Images via Supervised
  Attention Multiple Instance Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.00003v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.00003v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Huang, Benjamin S. Wessler, Michael C. Hughes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aortic stenosis (AS) is a degenerative valve condition that causes
substantial morbidity and mortality. This condition is under-diagnosed and
under-treated. In clinical practice, AS is diagnosed with expert review of
transthoracic echocardiography, which produces dozens of ultrasound images of
the heart. Only some of these views show the aortic valve. To automate
screening for AS, deep networks must learn to mimic a human expert's ability to
identify views of the aortic valve then aggregate across these relevant images
to produce a study-level diagnosis. We find previous approaches to AS detection
yield insufficient accuracy due to relying on inflexible averages across
images. We further find that off-the-shelf attention-based multiple instance
learning (MIL) performs poorly. We contribute a new end-to-end MIL approach
with two key methodological innovations. First, a supervised attention
technique guides the learned attention mechanism to favor relevant views.
Second, a novel self-supervised pretraining strategy applies contrastive
learning on the representation of the whole study instead of individual images
as commonly done in prior literature. Experiments on an open-access dataset and
an external validation set show that our approach yields higher accuracy while
reducing model size.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Echocardiogram; multiple-instance learning; self-supervised learning;
  semi-supervised learning; medical imaging</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NaVid: Video-based VLM Plans the Next Step for Vision-and-Language
  Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15852v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15852v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiazhao Zhang, Kunyu Wang, Rongtao Xu, Gengze Zhou, Yicong Hong, Xiaomeng Fang, Qi Wu, Zhizheng Zhang, Wang He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-and-Language Navigation (VLN) stands as a key research problem of
Embodied AI, aiming at enabling agents to navigate in unseen environments
following linguistic instructions. In this field, generalization is a
long-standing challenge, either to out-of-distribution scenes or from Sim to
Real. In this paper, we propose NaVid, a video-based large vision language
model (VLM), to mitigate such a generalization gap. NaVid makes the first
endeavour to showcase the capability of VLMs to achieve state-of-the-art level
navigation performance without any maps, odometer and depth inputs. Following
human instruction, NaVid only requires an on-the-fly video stream from a
monocular RGB camera equipped on the robot to output the next-step action. Our
formulation mimics how humans navigate and naturally gets rid of the problems
introduced by odometer noises, and the Sim2Real gaps from map or depth inputs.
Moreover, our video-based approach can effectively encode the historical
observations of robots as spatio-temporal contexts for decision-making and
instruction following. We train NaVid with 550k navigation samples collected
from VLN-CE trajectories, including action-planning and instruction-reasoning
samples, along with 665k large-scale web data. Extensive experiments show that
NaVid achieves SOTA performance in simulation environments and the real world,
demonstrating superior cross-dataset and Sim2Real transfer. We thus believe our
proposed VLM approach plans the next step for not only the navigation agents
but also this research field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Consistency-guided <span class="highlight-title">Prompt</span> Learning for Vision-Language Models <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01195v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01195v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuvendu Roy, Ali Etemad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Consistency-guided Prompt learning (CoPrompt), a new fine-tuning
method for vision-language models. Our approach improves the generalization of
large foundation models when fine-tuned on downstream tasks in a few-shot
setting. The basic idea of CoPrompt is to enforce a consistency constraint in
the prediction of the trainable and pre-trained models to prevent overfitting
on the downstream task. Additionally, we introduce the following two components
into our consistency constraint to further boost the performance: enforcing
consistency on two perturbed inputs and combining two dominant paradigms of
tuning, prompting and adapter. Enforcing consistency on perturbed input serves
to further regularize the consistency constraint, thereby improving
generalization. Moreover, the integration of adapters and prompts not only
enhances performance on downstream tasks but also offers increased tuning
flexibility in both input and output spaces. This facilitates more effective
adaptation to downstream tasks in a few-shot learning setting. Experiments show
that CoPrompt outperforms existing methods on a range of evaluation suites,
including base-to-novel generalization, domain generalization, and
cross-dataset evaluation. On generalization, CoPrompt improves the
state-of-the-art on zero-shot tasks and the overall harmonic mean over 11
datasets. Detailed ablation studies show the effectiveness of each of the
components in CoPrompt. We make our code available at
https://github.com/ShuvenduRoy/CoPrompt.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Overcoming Dimensional Collapse in <span class="highlight-title">Self-supervised</span> Contrastive Learning
  for Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14611v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14611v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jamshid Hassanpour, Vinkle Srivastav, Didier Mutter, Nicolas Padoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) approaches have achieved great success when
the amount of labeled data is limited. Within SSL, models learn robust feature
representations by solving pretext tasks. One such pretext task is contrastive
learning, which involves forming pairs of similar and dissimilar input samples,
guiding the model to distinguish between them. In this work, we investigate the
application of contrastive learning to the domain of medical image analysis.
Our findings reveal that MoCo v2, a state-of-the-art contrastive learning
method, encounters dimensional collapse when applied to medical images. This is
attributed to the high degree of inter-image similarity shared between the
medical images. To address this, we propose two key contributions: local
feature learning and feature decorrelation. Local feature learning improves the
ability of the model to focus on the local regions of the image, while feature
decorrelation removes the linear dependence among the features. Our
experimental findings demonstrate that our contributions significantly enhance
the model's performance in the downstream task of medical segmentation, both in
the linear evaluation and full fine-tuning settings. This work illustrates the
importance of effectively adapting SSL techniques to the characteristics of
medical imaging tasks. The source code will be made publicly available at:
https://github.com/CAMMA-public/med-moco
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at at ISBI-2024 (https://biomedicalimaging.org/2024/). 4
  pages, 2 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UVDoc: Neural Grid-based Document Unwarping <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.02887v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.02887v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Floor Verhoeven, Tanguy Magne, Olga Sorkine-Hornung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Restoring the original, flat appearance of a printed document from casual
photographs of bent and wrinkled pages is a common everyday problem. In this
paper we propose a novel method for grid-based single-image document unwarping.
Our method performs geometric distortion correction via a fully convolutional
deep neural network that learns to predict the 3D grid mesh of the document and
the corresponding 2D unwarping grid in a dual-task fashion, implicitly encoding
the coupling between the shape of a 3D piece of paper and its 2D image. In
order to allow unwarping models to train on data that is more realistic in
appearance than the commonly used synthetic Doc3D dataset, we create and
publish our own dataset, called UVDoc, which combines pseudo-photorealistic
document images with physically accurate 3D shape and unwarping function
annotations. Our dataset is labeled with all the information necessary to train
our unwarping network, without having to engineer separate loss functions that
can deal with the lack of ground-truth typically found in document in the wild
datasets. We perform an in-depth evaluation that demonstrates that with the
inclusion of our novel pseudo-photorealistic dataset, our relatively small
network architecture achieves state-of-the-art results on the DocUNet
benchmark. We show that the pseudo-photorealistic nature of our UVDoc dataset
allows for new and better evaluation methods, such as lighting-corrected
MS-SSIM. We provide a novel benchmark dataset that facilitates such
evaluations, and propose a metric that quantifies line straightness after
unwarping. Our code, results and UVDoc dataset are available at
https://github.com/tanguymagne/UVDoc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, published in SIGGRAPH Asia 2023 Conference Papers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NECO: NEural Collapse Based Out-of-distribution detection <span class="chip">ICLR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06823v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06823v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mouïn Ben Ammar, Nacim Belkhir, Sebastian Popescu, Antoine Manzanera, Gianni Franchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting out-of-distribution (OOD) data is a critical challenge in machine
learning due to model overconfidence, often without awareness of their
epistemological limits. We hypothesize that ``neural collapse'', a phenomenon
affecting in-distribution data for models trained beyond loss convergence, also
influences OOD data. To benefit from this interplay, we introduce NECO, a novel
post-hoc method for OOD detection, which leverages the geometric properties of
``neural collapse'' and of principal component spaces to identify OOD data. Our
extensive experiments demonstrate that NECO achieves state-of-the-art results
on both small and large-scale OOD detection tasks while exhibiting strong
generalization capabilities across different network architectures.
Furthermore, we provide a theoretical explanation for the effectiveness of our
method in OOD detection. Code is available at https://gitlab.com/drti/neco
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DrivingGaussian: Composite Gaussian Splatting for Surrounding Dynamic
  Autonomous Driving Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07920v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07920v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Zhou, Zhiwei Lin, Xiaojun Shan, Yongtao Wang, Deqing Sun, Ming-Hsuan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present DrivingGaussian, an efficient and effective framework for
surrounding dynamic autonomous driving scenes. For complex scenes with moving
objects, we first sequentially and progressively model the static background of
the entire scene with incremental static 3D Gaussians. We then leverage a
composite dynamic Gaussian graph to handle multiple moving objects,
individually reconstructing each object and restoring their accurate positions
and occlusion relationships within the scene. We further use a LiDAR prior for
Gaussian Splatting to reconstruct scenes with greater details and maintain
panoramic consistency. DrivingGaussian outperforms existing methods in driving
scene reconstruction and enables photorealistic surround-view synthesis with
high-fidelity and multi-camera consistency. Our project page is at:
https://github.com/VDIGPKU/DrivingGaussian.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Implicit Visual Bias Mitigation by Posterior Estimate Sharpening of a
  Bayesian Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.16564v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.16564v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rebecca S Stone, Nishant Ravikumar, Andrew J Bulpitt, David C Hogg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The fairness of a deep neural network is strongly affected by dataset bias
and spurious correlations, both of which are usually present in modern
feature-rich and complex visual datasets. Due to the difficulty and variability
of the task, no single de-biasing method has been universally successful. In
particular, implicit methods not requiring explicit knowledge of bias variables
are especially relevant for real-world applications. We propose a novel
implicit mitigation method using a Bayesian neural network, allowing us to
leverage the relationship between epistemic uncertainties and the presence of
bias or spurious correlations in a sample. Our proposed posterior estimate
sharpening procedure encourages the network to focus on core features that do
not contribute to high uncertainties. Experimental results on three benchmark
datasets demonstrate that Bayesian networks with sharpened posterior estimates
perform comparably to prior existing methods and show potential worthy of
further exploration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We are revising this paper with significant changes</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ISCUTE: Instance Segmentation of Cables Using Text Embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11996v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11996v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shir Kozlovsky, Omkar Joglekar, Dotan Di Castro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of robotics and automation, conventional object recognition and
instance segmentation methods face a formidable challenge when it comes to
perceiving Deformable Linear Objects (DLOs) like wires, cables, and flexible
tubes. This challenge arises primarily from the lack of distinct attributes
such as shape, color, and texture, which calls for tailored solutions to
achieve precise identification. In this work, we propose a foundation
model-based DLO instance segmentation technique that is text-promptable and
user-friendly. Specifically, our approach combines the text-conditioned
semantic segmentation capabilities of CLIPSeg model with the zero-shot
generalization capabilities of Segment Anything Model (SAM). We show that our
method exceeds SOTA performance on DLO instance segmentation, achieving a mIoU
of $91.21\%$. We also introduce a rich and diverse DLO-specific dataset for
instance segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual Abductive Reasoning Meets Driving Hazard Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04671v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04671v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Korawat Charoenpitaks, Van-Quang Nguyen, Masanori Suganuma, Masahiro Takahashi, Ryoma Niihara, Takayuki Okatani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the problem of predicting hazards that drivers may
encounter while driving a car. We formulate it as a task of anticipating
impending accidents using a single input image captured by car dashcams. Unlike
existing approaches to driving hazard prediction that rely on computational
simulations or anomaly detection from videos, this study focuses on high-level
inference from static images. The problem needs predicting and reasoning about
future events based on uncertain observations, which falls under visual
abductive reasoning. To enable research in this understudied area, a new
dataset named the DHPR (Driving Hazard Prediction and Reasoning) dataset is
created. The dataset consists of 15K dashcam images of street scenes, and each
image is associated with a tuple containing car speed, a hypothesized hazard
description, and visual entities present in the scene. These are annotated by
human annotators, who identify risky scenes and provide descriptions of
potential accidents that could occur a few seconds later. We present several
baseline methods and evaluate their performance on our dataset, identifying
remaining issues and discussing future directions. This study contributes to
the field by introducing a novel problem formulation and dataset, enabling
researchers to explore the potential of multi-modal AI for driving hazard
prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main Paper: 10 pages, Supplementary Materials: 28 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Contextformer: Spatio-Channel Window Attention for Fast
  Context Modeling in Learned Image Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.14287v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.14287v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        A. Burakhan Koyuncu, Panqi Jia, Atanas Boev, Elena Alshina, Eckehard Steinbach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entropy estimation is essential for the performance of learned image
compression. It has been demonstrated that a transformer-based entropy model is
of critical importance for achieving a high compression ratio, however, at the
expense of a significant computational effort. In this work, we introduce the
Efficient Contextformer (eContextformer) - a computationally efficient
transformer-based autoregressive context model for learned image compression.
The eContextformer efficiently fuses the patch-wise, checkered, and
channel-wise grouping techniques for parallel context modeling, and introduces
a shifted window spatio-channel attention mechanism. We explore better training
strategies and architectural designs and introduce additional complexity
optimizations. During decoding, the proposed optimization techniques
dynamically scale the attention span and cache the previous attention
computations, drastically reducing the model and runtime complexity. Compared
to the non-parallel approach, our proposal has ~145x lower model complexity and
~210x faster decoding speed, and achieves higher average bit savings on Kodak,
CLIC2020, and Tecnick datasets. Additionally, the low complexity of our context
model enables online rate-distortion algorithms, which further improve the
compression performance. We achieve up to 17% bitrate savings over the intra
coding of Versatile Video Coding (VVC) Test Model (VTM) 16.2 and surpass
various learning-based compression models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for IEEE TCSVT (14 pages, 10 figures, 9 tables)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Pretrain</span>ed Visual Uncertainties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16569v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16569v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Kirchhof, Mark Collier, Seong Joon Oh, Enkelejda Kasneci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate uncertainty estimation is vital to trustworthy machine learning, yet
uncertainties typically have to be learned for each task anew. This work
introduces the first pretrained uncertainty modules for vision models. Similar
to standard pretraining this enables the zero-shot transfer of uncertainties
learned on a large pretraining dataset to specialized downstream datasets. We
enable our large-scale pretraining on ImageNet-21k by solving a gradient
conflict in previous uncertainty modules and accelerating the training by up to
180x. We find that the pretrained uncertainties generalize to unseen datasets.
In scrutinizing the learned uncertainties, we find that they capture aleatoric
uncertainty, disentangled from epistemic components. We demonstrate that this
enables safe retrieval and uncertainty-aware dataset visualization. To
encourage applications to further problems and domains, we release all
pretrained checkpoints and code under https://github.com/mkirchhof/url .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PonderV2: Pave the Way for 3D Foundation Model with A Universal
  <span class="highlight-title">Pre-train</span>ing Paradigm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08586v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08586v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyi Zhu, Honghui Yang, Xiaoyang Wu, Di Huang, Sha Zhang, Xianglong He, Hengshuang Zhao, Chunhua Shen, Yu Qiao, Tong He, Wanli Ouyang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In contrast to numerous NLP and 2D vision foundational models, learning a 3D
foundational model poses considerably greater challenges. This is primarily due
to the inherent data variability and diversity of downstream tasks. In this
paper, we introduce a novel universal 3D pre-training framework designed to
facilitate the acquisition of efficient 3D representation, thereby establishing
a pathway to 3D foundational models. Considering that informative 3D features
should encode rich geometry and appearance cues that can be utilized to render
realistic images, we propose to learn 3D representations by differentiable
neural rendering. We train a 3D backbone with a devised volumetric neural
renderer by comparing the rendered with the real images. Notably, our approach
seamlessly integrates the learned 3D encoder into various downstream tasks.
These tasks encompass not only high-level challenges such as 3D detection and
segmentation but also low-level objectives like 3D reconstruction and image
synthesis, spanning both indoor and outdoor scenarios. Besides, we also
illustrate the capability of pre-training a 2D backbone using the proposed
methodology, surpassing conventional pre-training methods by a large margin.
For the first time, PonderV2 achieves state-of-the-art performance on 11 indoor
and outdoor benchmarks, implying its effectiveness. Code and models are
available at https://github.com/OpenGVLab/PonderV2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2301.00157</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AsymFormer: Asymmetrical Cross-Modal Representation Learning for Mobile
  Platform Real-Time RGB-D Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14065v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14065v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siqi Du, Weixi Wang, Renzhong Guo, Shengjun Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of robotic intelligence, achieving efficient and precise RGB-D
semantic segmentation is a key cornerstone. State-of-the-art multimodal
semantic segmentation methods, primarily rooted in symmetrical skeleton
networks, find it challenging to harmonize computational efficiency and
precision. In this work, we propose AsymFormer, a novel network for real-time
RGB-D semantic segmentation, which targets the minimization of superfluous
parameters by optimizing the distribution of computational resources and
introduces an asymmetrical backbone to allow for the effective fusion of
multimodal features. Furthermore, we explore techniques to bolster network
accuracy by redefining feature selection and extracting multi-modal
self-similarity features without a substantial increase in the parameter count,
thereby ensuring real-time execution on robotic platforms. Additionally, a
Local Attention-Guided Feature Selection (LAFS) module is used to selectively
fuse features from different modalities by leveraging their dependencies.
Subsequently, a Cross-Modal Attention-Guided Feature Correlation Embedding
(CMA) module is introduced to further extract cross-modal representations. This
method is evaluated on NYUv2 and SUNRGBD datasets, with AsymFormer
demonstrating competitive results with 54.1% mIoU on NYUv2 and 49.1% mIoU on
SUNRGBD. Notably, AsymFormer achieves an inference speed of 65 FPS and after
implementing mixed precision quantization, it attains an impressive inference
speed of 79 FPS on RTX3090. This significantly outperforms existing multi-modal
methods, thereby demonstrating that AsymFormer can strike a balance between
high accuracy and efficiency for RGB-D semantic segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pay Attention to the Atlas: Atlas-Guided Test-Time Adaptation Method for
  Robust 3D Medical Image Segmentation <span class="chip">MICCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.00676v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.00676v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingjie Guo, Weitong Zhang, Matthew Sinclair, Daniel Rueckert, Chen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks (CNNs) often suffer from poor performance when
tested on target data that differs from the training (source) data
distribution, particularly in medical imaging applications where variations in
imaging protocols across different clinical sites and scanners lead to
different imaging appearances. However, re-accessing source training data for
unsupervised domain adaptation or labeling additional test data for model
fine-tuning can be difficult due to privacy issues and high labeling costs,
respectively. To solve this problem, we propose a novel atlas-guided test-time
adaptation (TTA) method for robust 3D medical image segmentation, called
AdaAtlas. AdaAtlas only takes one single unlabeled test sample as input and
adapts the segmentation network by minimizing an atlas-based loss.
Specifically, the network is adapted so that its prediction after registration
is aligned with the learned atlas in the atlas space, which helps to reduce
anatomical segmentation errors at test time. In addition, different from most
existing TTA methods which restrict the adaptation to batch normalization
blocks in the segmentation network only, we further exploit the use of channel
and spatial attention blocks for improved adaptability at test time. Extensive
experiments on multiple datasets from different sites show that AdaAtlas with
attention blocks adapted (AdaAtlas-Attention) achieves superior performance
improvements, greatly outperforming other competitive TTA methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by MICCAI BTSD-1001AI workshop. (Oral
  presentation).https://btsdmiccai.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HRTF upsampling with a generative adversarial network using a gnomonic
  equiangular projection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05812v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05812v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aidan O. T. Hogg, Mads Jenkins, He Liu, Isaac Squires, Samuel J. Cooper, Lorenzo Picinali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An individualised head-related transfer function (HRTF) is very important for
creating realistic virtual reality (VR) and augmented reality (AR)
environments. However, acoustically measuring high-quality HRTFs requires
expensive equipment and an acoustic lab setting. To overcome these limitations
and to make this measurement more efficient HRTF upsampling has been exploited
in the past where a high-resolution HRTF is created from a low-resolution one.
This paper demonstrates how generative adversarial networks (GANs) can be
applied to HRTF upsampling. We propose a novel approach that transforms the
HRTF data for direct use with a convolutional super-resolution generative
adversarial network (SRGAN). This new approach is benchmarked against three
baselines: barycentric upsampling, spherical harmonic (SH) upsampling and an
HRTF selection approach. Experimental results show that the proposed method
outperforms all three baselines in terms of log-spectral distortion (LSD) and
localisation performance using perceptual models when the input HRTF is sparse
(less than 20 measured positions).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 9 figures, Preprint (Accepted to IEEE/ACM Transactions on
  Audio, Speech, and Language Processing on the 15 Feb 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.12499v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.12499v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuelong Dai, Kaisheng Liang, Bin Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unrestricted adversarial attacks present a serious threat to deep learning
models and adversarial defense techniques. They pose severe security problems
for deep learning applications because they can effectively bypass defense
mechanisms. However, previous attack methods often utilize Generative
Adversarial Networks (GANs), which are not theoretically provable and thus
generate unrealistic examples by incorporating adversarial objectives,
especially for large-scale datasets like ImageNet. In this paper, we propose a
new method, called AdvDiff, to generate unrestricted adversarial examples with
diffusion models. We design two novel adversarial guidance techniques to
conduct adversarial sampling in the reverse generation process of diffusion
models. These two techniques are effective and stable to generate high-quality,
realistic adversarial examples by integrating gradients of the target
classifier interpretably. Experimental results on MNIST and ImageNet datasets
demonstrate that AdvDiff is effective to generate unrestricted adversarial
examples, which outperforms GAN-based methods in terms of attack performance
and generation quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PolypNextLSTM: A lightweight and fast polyp video segmentation network
  using ConvNext and ConvLSTM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11585v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11585v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debayan Bhattacharya, Konrad Reuter, Finn Behrendnt, Lennart Maack, Sarah Grube, Alexander Schlaefer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Commonly employed in polyp segmentation, single image UNet architectures lack
the temporal insight clinicians gain from video data in diagnosing polyps. To
mirror clinical practices more faithfully, our proposed solution,
PolypNextLSTM, leverages video-based deep learning, harnessing temporal
information for superior segmentation performance with the least parameter
overhead, making it possibly suitable for edge devices. PolypNextLSTM employs a
UNet-like structure with ConvNext-Tiny as its backbone, strategically omitting
the last two layers to reduce parameter overhead. Our temporal fusion module, a
Convolutional Long Short Term Memory (ConvLSTM), effectively exploits temporal
features. Our primary novelty lies in PolypNextLSTM, which stands out as the
leanest in parameters and the fastest model, surpassing the performance of five
state-of-the-art image and video-based deep learning models. The evaluation of
the SUN-SEG dataset spans easy-to-detect and hard-to-detect polyp scenarios,
along with videos containing challenging artefacts like fast motion and
occlusion. Comparison against 5 image-based and 5 video-based models
demonstrates PolypNextLSTM's superiority, achieving a Dice score of 0.7898 on
the hard-to-detect polyp test set, surpassing image-based PraNet (0.7519) and
video-based PNSPlusNet (0.7486). Notably, our model excels in videos featuring
complex artefacts such as ghosting and occlusion. PolypNextLSTM, integrating
pruned ConvNext-Tiny with ConvLSTM for temporal fusion, not only exhibits
superior segmentation performance but also maintains the highest frames per
speed among evaluated models. Access code here
https://github.com/mtec-tuhh/PolypNextLSTM
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Perturbation for Adversarial Attack 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.13841v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.13841v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Yuan, Jie Zhang, Zhaoyan Jiang, Liangliang Li, Shiguang Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the security of deep learning models achieves more and more
attentions with the rapid development of neural networks, which are vulnerable
to adversarial examples. Almost all existing gradient-based attack methods use
the sign function in the generation to meet the requirement of perturbation
budget on $L_\infty$ norm. However, we find that the sign function may be
improper for generating adversarial examples since it modifies the exact
gradient direction. Instead of using the sign function, we propose to directly
utilize the exact gradient direction with a scaling factor for generating
adversarial perturbations, which improves the attack success rates of
adversarial examples even with fewer perturbations. At the same time, we also
theoretically prove that this method can achieve better black-box
transferability. Moreover, considering that the best scaling factor varies
across different images, we propose an adaptive scaling factor generator to
seek an appropriate scaling factor for each image, which avoids the
computational cost for manually searching the scaling factor. Our method can be
integrated with almost all existing gradient-based attack methods to further
improve their attack success rates. Extensive experiments on the CIFAR10 and
ImageNet datasets show that our method exhibits higher transferability and
outperforms the state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI). 18 pages, 7 figures, 14 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EFLNet: Enhancing Feature Learning for Infrared Small Target Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.14723v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.14723v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Yang, Xinyu Zhang, Jian Zhang, Jun Luo, Mingliang Zhou, Yangjun Pi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Single-frame infrared small target detection is considered to be a
challenging task, due to the extreme imbalance between target and background,
bounding box regression is extremely sensitive to infrared small target, and
target information is easy to lose in the high-level semantic layer. In this
article, we propose an enhancing feature learning network (EFLNet) to address
these problems. First, we notice that there is an extremely imbalance between
the target and the background in the infrared image, which makes the model pay
more attention to the background features rather than target features. To
address this problem, we propose a new adaptive threshold focal loss (ATFL)
function that decouples the target and the background, and utilizes the
adaptive mechanism to adjust the loss weight to force the model to allocate
more attention to target features. Second, we introduce the normalized Gaussian
Wasserstein distance (NWD) to alleviate the difficulty of convergence caused by
the extreme sensitivity of the bounding box regression to infrared small
target. Finally, we incorporate a dynamic head mechanism into the network to
enable adaptive learning of the relative importance of each semantic layer.
Experimental results demonstrate our method can achieve better performance in
the detection performance of infrared small target compared to the
state-of-the-art (SOTA) deep-learning-based methods. The source codes and
bounding box annotated datasets are available at
https://github.com/YangBo0411/infrared-small-target.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Removal and Selection: Improving RGB-Infrared Object Detection via
  Coarse-to-Fine Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.10731v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.10731v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Zhao, Maoxun Yuan, Xingxing Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detection in visible (RGB) and infrared (IR) images has been widely
applied in recent years. Leveraging the complementary characteristics of RGB
and IR images, the object detector provides reliable and robust object
localization from day to night. Existing fusion strategies directly inject RGB
and IR images into convolution neural networks, leading to inferior detection
performance. Since the RGB and IR features have modality-specific noise, these
strategies will worsen the fused features along with the propagation. Inspired
by the mechanism of human brain processing multimodal information, this work
introduces a new coarse-to-fine perspective to purify and fuse two modality
features. Specifically, following this perspective, we design a Redundant
Spectrum Removal module to coarsely remove interfering information within each
modality and a Dynamic Feature Selection module to finely select the desired
features for feature fusion. To verify the effectiveness of the coarse-to-fine
fusion strategy, we construct a new object detector called Removal and
Selection Detector (RSDet). Extensive experiments on three RGB-IR object
detection datasets verify the superior performance of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9pages, 7figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Good is Chat<span class="highlight-title">GPT</span> at Face Biometrics? A First Look into Recognition,
  Soft Biometrics, and Explainability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.13641v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.13641v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivan DeAndres-Tame, Ruben Tolosana, Ruben Vera-Rodriguez, Aythami Morales, Julian Fierrez, Javier Ortega-Garcia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) such as GPT developed by OpenAI, have already
shown astonishing results, introducing quick changes in our society. This has
been intensified by the release of ChatGPT which allows anyone to interact in a
simple conversational way with LLMs, without any experience in the field
needed. As a result, ChatGPT has been rapidly applied to many different tasks
such as code- and song-writer, education, virtual assistants, etc., showing
impressive results for tasks for which it was not trained (zero-shot learning).
  The present study aims to explore the ability of ChatGPT, based on the recent
GPT-4 multimodal LLM, for the task of face biometrics. In particular, we
analyze the ability of ChatGPT to perform tasks such as face verification,
soft-biometrics estimation, and explainability of the results. ChatGPT could be
very valuable to further increase the explainability and transparency of
automatic decisions in human scenarios. Experiments are carried out in order to
evaluate the performance and robustness of ChatGPT, using popular public
benchmarks and comparing the results with state-of-the-art methods in the
field. The results achieved in this study show the potential of LLMs such as
ChatGPT for face biometrics, especially to enhance explainability. For
reproducibility reasons, we release all the code in GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Snapture -- A Novel Neural Architecture for Combined Static and Dynamic
  Hand Gesture Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.15862v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.15862v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hassan Ali, Doreen Jirak, Stefan Wermter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As robots are expected to get more involved in people's everyday lives,
frameworks that enable intuitive user interfaces are in demand. Hand gesture
recognition systems provide a natural way of communication and, thus, are an
integral part of seamless Human-Robot Interaction (HRI). Recent years have
witnessed an immense evolution of computational models powered by deep
learning. However, state-of-the-art models fall short in expanding across
different gesture domains, such as emblems and co-speech. In this paper, we
propose a novel hybrid hand gesture recognition system. Our architecture
enables learning both static and dynamic gestures: by capturing a so-called
"snapshot" of the gesture performance at its peak, we integrate the hand pose
along with the dynamic movement. Moreover, we present a method for analyzing
the motion profile of a gesture to uncover its dynamic characteristics and
which allows regulating a static channel based on the amount of motion. Our
evaluation demonstrates the superiority of our approach on two gesture
benchmarks compared to a CNNLSTM baseline. We also provide an analysis on a
gesture class basis that unveils the potential of our Snapture architecture for
performance improvements. Thanks to its modular implementation, our framework
allows the integration of other multimodal data like facial expressions and
head tracking, which are important cues in HRI scenarios, into one
architecture. Thus, our work contributes both to gesture recognition research
and machine learning applications for non-verbal communication with robots.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Cognitive Computation(Accepted:30/06/2023,
  Published:17/07/2023),20 pages,20 figures,4 tables;Please find the published
  version/info to cite:
  https://doi.org/10.1007/s12559-023-10174-z;Repositories:
  https://zenodo.org/doi/10.5281/zenodo.10679196,
  https://zenodo.org/doi/10.5281/zenodo.10693816;This work was co-funded by
  Horizon Europe project TERAIS under Grant agreement number 101079338</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SED: A Simple Encoder-Decoder for Open-Vocabulary Semantic Segmentation <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.15537v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.15537v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Xie, Jiale Cao, Jin Xie, Fahad Shahbaz Khan, Yanwei Pang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-vocabulary semantic segmentation strives to distinguish pixels into
different semantic groups from an open set of categories. Most existing methods
explore utilizing pre-trained vision-language models, in which the key is to
adopt the image-level model for pixel-level segmentation task. In this paper,
we propose a simple encoder-decoder, named SED, for open-vocabulary semantic
segmentation, which comprises a hierarchical encoder-based cost map generation
and a gradual fusion decoder with category early rejection. The hierarchical
encoder-based cost map generation employs hierarchical backbone, instead of
plain transformer, to predict pixel-level image-text cost map. Compared to
plain transformer, hierarchical backbone better captures local spatial
information and has linear computational complexity with respect to input size.
Our gradual fusion decoder employs a top-down structure to combine cost map and
the feature maps of different backbone levels for segmentation. To accelerate
inference speed, we introduce a category early rejection scheme in the decoder
that rejects many no-existing categories at the early layer of decoder,
resulting in at most 4.7 times acceleration without accuracy degradation.
Experiments are performed on multiple open-vocabulary semantic segmentation
datasets, which demonstrates the efficacy of our SED method. When using
ConvNeXt-B, our SED method achieves mIoU score of 31.6\% on ADE20K with 150
categories at 82 millisecond ($ms$) per image on a single A6000. We will
release it at \url{https://github.com/xb534/SED.git}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Point <span class="highlight-title">Transformer</span> with Federated Learning for Predicting Breast Cancer
  HER2 Status from Hematoxylin and Eosin-Stained Whole Slide Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06454v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06454v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bao Li, Zhenyu Liu, Lizhi Shao, Bensheng Qiu, Hong Bu, Jie Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Directly predicting human epidermal growth factor receptor 2 (HER2) status
from widely available hematoxylin and eosin (HE)-stained whole slide images
(WSIs) can reduce technical costs and expedite treatment selection. Accurately
predicting HER2 requires large collections of multi-site WSIs. Federated
learning enables collaborative training of these WSIs without gigabyte-size
WSIs transportation and data privacy concerns. However, federated learning
encounters challenges in addressing label imbalance in multi-site WSIs from the
real world. Moreover, existing WSI classification methods cannot simultaneously
exploit local context information and long-range dependencies in the site-end
feature representation of federated learning. To address these issues, we
present a point transformer with federated learning for multi-site HER2 status
prediction from HE-stained WSIs. Our approach incorporates two novel designs.
We propose a dynamic label distribution strategy and an auxiliary classifier,
which helps to establish a well-initialized model and mitigate label
distribution variations across sites. Additionally, we propose a farthest
cosine sampling based on cosine distance. It can sample the most distinctive
features and capture the long-range dependencies. Extensive experiments and
analysis show that our method achieves state-of-the-art performance at four
sites with a total of 2687 WSIs. Furthermore, we demonstrate that our model can
generalize to two unseen sites with 229 WSIs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Impact of Loss Functions and Scene Representations for 3D/2D
  Registration on Single-view Fluoroscopic X-ray Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.00214v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.00214v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaochao Zhou, Syed Hasib Akhter Faruqui, Abhinav Patel, Ramez N. Abdalla, Michael C. Hurley, Ali Shaibani, Matthew B. Potts, Babak S. Jahromi, Sameer A. Ansari, Donald R. Cantrell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many tasks performed in image-guided procedures can be cast as pose
estimation problems, where specific projections are chosen to reach a target in
3D space. In this study, we first develop a differentiable projection
(DiffProj) rendering framework for the efficient computation of Digitally
Reconstructed Radiographs (DRRs) with automatic differentiability from either
Cone-Beam Computerized Tomography (CBCT) or neural scene representations,
including two newly proposed methods, Neural Tuned Tomography (NeTT) and masked
Neural Radiance Fields (mNeRF). We then perform pose estimation by iterative
gradient descent using various candidate loss functions, that quantify the
image discrepancy of the synthesized DRR with respect to the ground-truth
fluoroscopic X-ray image. Compared to alternative loss functions, the Mutual
Information loss function can significantly improve pose estimation accuracy,
as it can effectively prevent entrapment in local optima. Using the Mutual
Information loss, a comprehensive evaluation of pose estimation performed on a
tomographic X-ray dataset of 50 patients$'$ skulls shows that utilizing either
discretized (CBCT) or neural (NeTT/mNeRF) scene representations in DiffProj
leads to comparable performance in DRR appearance and pose estimation (3D angle
errors: mean $\leq$ 3.2{\deg} and 90% quantile $\leq$ 3.4{\deg}), despite the
latter often incurring considerable training expenses and time. These findings
could be instrumental for selecting appropriate approaches to improve the
efficiency and effectiveness of fluoroscopic X-ray pose estimation in
widespread image-guided interventions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Private, fair and accurate: Training large-scale, privacy-preserving AI
  models in medical imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01622v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01622v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soroosh Tayebi Arasteh, Alexander Ziller, Christiane Kuhl, Marcus Makowski, Sven Nebelung, Rickmer Braren, Daniel Rueckert, Daniel Truhn, Georgios Kaissis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI) models are increasingly used in the medical
domain. However, as medical data is highly sensitive, special precautions to
ensure its protection are required. The gold standard for privacy preservation
is the introduction of differential privacy (DP) to model training. Prior work
indicates that DP has negative implications on model accuracy and fairness,
which are unacceptable in medicine and represent a main barrier to the
widespread use of privacy-preserving techniques. In this work, we evaluated the
effect of privacy-preserving training of AI models regarding accuracy and
fairness compared to non-private training. For this, we used two datasets: (1)
A large dataset (N=193,311) of high quality clinical chest radiographs, and (2)
a dataset (N=1,625) of 3D abdominal computed tomography (CT) images, with the
task of classifying the presence of pancreatic ductal adenocarcinoma (PDAC).
Both were retrospectively collected and manually labeled by experienced
radiologists. We then compared non-private deep convolutional neural networks
(CNNs) and privacy-preserving (DP) models with respect to privacy-utility
trade-offs measured as area under the receiver-operator-characteristic curve
(AUROC), and privacy-fairness trade-offs, measured as Pearson's r or
Statistical Parity Difference. We found that, while the privacy-preserving
trainings yielded lower accuracy, they did largely not amplify discrimination
against age, sex or co-morbidity. Our study shows that -- under the challenging
realistic circumstances of a real-life clinical dataset -- the
privacy-preserving training of diagnostic deep learning models is possible with
excellent diagnostic accuracy and fairness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Communications Medicine. Nature Portfolio</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comparing the Robustness of Modern No-Reference Image- and Video-Quality
  Metrics to Adversarial Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06958v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06958v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anastasia Antsiferova, Khaled Abud, Aleksandr Gushchin, Ekaterina Shumitskaya, Sergey Lavrushkin, Dmitriy Vatolin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, neural-network-based image- and video-quality metrics perform
better than traditional methods. However, they also became more vulnerable to
adversarial attacks that increase metrics' scores without improving visual
quality. The existing benchmarks of quality metrics compare their performance
in terms of correlation with subjective quality and calculation time.
Nonetheless, the adversarial robustness of image-quality metrics is also an
area worth researching. This paper analyses modern metrics' robustness to
different adversarial attacks. We adapted adversarial attacks from computer
vision tasks and compared attacks' efficiency against 15 no-reference image-
and video-quality metrics. Some metrics showed high resistance to adversarial
attacks, which makes their usage in benchmarks safer than vulnerable metrics.
The benchmark accepts submissions of new metrics for researchers who want to
make their metrics more robust to attacks or to find such metrics for their
needs. The latest results can be found online:
https://videoprocessing.ai/benchmarks/metrics-robustness.html.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conditional Unscented Autoencoders for Trajectory Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.19944v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.19944v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Faris Janjoš, Marcel Hallgarten, Anthony Knittel, Maxim Dolgov, Andreas Zell, J. Marius Zöllner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The CVAE is one of the most widely-used models in trajectory prediction for
AD. It captures the interplay between a driving context and its ground-truth
future into a probabilistic latent space and uses it to produce predictions. In
this paper, we challenge key components of the CVAE. We leverage recent
advances in the space of the VAE, the foundation of the CVAE, which show that a
simple change in the sampling procedure can greatly benefit performance. We
find that unscented sampling, which draws samples from any learned distribution
in a deterministic manner, can naturally be better suited to trajectory
prediction than potentially dangerous random sampling. We go further and offer
additional improvements including a more structured Gaussian mixture latent
space, as well as a novel, potentially more expressive way to do inference with
CVAEs. We show wide applicability of our models by evaluating them on the
INTERACTION prediction dataset, outperforming the state of the art, as well as
at the task of image modeling on the CelebA dataset, outperforming the baseline
vanilla CVAE. Code is available at
https://github.com/boschresearch/cuae-prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MIGC: Multi-Instance Generation Controller for Text-to-Image Synthesis <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.05408v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.05408v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dewei Zhou, You Li, Fan Ma, Xiaoting Zhang, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a Multi-Instance Generation (MIG) task, simultaneously generating
multiple instances with diverse controls in one image. Given a set of
predefined coordinates and their corresponding descriptions, the task is to
ensure that generated instances are accurately at the designated locations and
that all instances' attributes adhere to their corresponding description. This
broadens the scope of current research on Single-instance generation, elevating
it to a more versatile and practical dimension. Inspired by the idea of divide
and conquer, we introduce an innovative approach named Multi-Instance
Generation Controller (MIGC) to address the challenges of the MIG task.
Initially, we break down the MIG task into several subtasks, each involving the
shading of a single instance. To ensure precise shading for each instance, we
introduce an instance enhancement attention mechanism. Lastly, we aggregate all
the shaded instances to provide the necessary information for accurately
generating multiple instances in stable diffusion (SD). To evaluate how well
generation models perform on the MIG task, we provide a COCO-MIG benchmark
along with an evaluation pipeline. Extensive experiments were conducted on the
proposed COCO-MIG benchmark, as well as on various commonly used benchmarks.
The evaluation results illustrate the exceptional control capabilities of our
model in terms of quantity, position, attribute, and interaction. Code and
demos will be released at https://migcproject.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ActiveDC: Distribution Calibration for Active Finetuning <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07634v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07634v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenshuai Xu, Zhenghui Hu, Yu Lu, Jinzhou Meng, Qingjie Liu, Yunhong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pretraining-finetuning paradigm has gained popularity in various computer
vision tasks. In this paradigm, the emergence of active finetuning arises due
to the abundance of large-scale data and costly annotation requirements. Active
finetuning involves selecting a subset of data from an unlabeled pool for
annotation, facilitating subsequent finetuning. However, the use of a limited
number of training samples can lead to a biased distribution, potentially
resulting in model overfitting. In this paper, we propose a new method called
ActiveDC for the active finetuning tasks. Firstly, we select samples for
annotation by optimizing the distribution similarity between the subset to be
selected and the entire unlabeled pool in continuous space. Secondly, we
calibrate the distribution of the selected samples by exploiting implicit
category information in the unlabeled pool. The feature visualization provides
an intuitive sense of the effectiveness of our approach to distribution
calibration. We conducted extensive experiments on three image classification
datasets with different sampling ratios. The results indicate that ActiveDC
consistently outperforms the baseline performance in all image classification
tasks. The improvement is particularly significant when the sampling ratio is
low, with performance gains of up to 10%. Our code will be released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024 Accept</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoLo-CAM: Class Activation Mapping for Object Co-Localization in
  Weakly-Labeled Unconstrained Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09044v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09044v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soufiane Belharbi, Shakeeb Murtaza, Marco Pedersoli, Ismail Ben Ayed, Luke McCaffrey, Eric Granger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging spatiotemporal information in videos is critical for weakly
supervised video object localization (WSVOL) tasks. However, state-of-the-art
methods only rely on visual and motion cues, while discarding discriminative
information, making them susceptible to inaccurate localizations. Recently,
discriminative models have been explored for WSVOL tasks using a temporal class
activation mapping (CAM) method. Although their results are promising, objects
are assumed to have limited movement from frame to frame, leading to
degradation in performance for relatively long-term dependencies. This paper
proposes a novel CAM method for WSVOL that exploits spatiotemporal information
in activation maps during training without constraining an object's position.
Its training relies on Co-Localization, hence, the name CoLo-CAM. Given a
sequence of frames, localization is jointly learned based on color cues
extracted across the corresponding maps, by assuming that an object has similar
color in consecutive frames. CAM activations are constrained to respond
similarly over pixels with similar colors, achieving co-localization. This
improves localization performance because the joint learning creates direct
communication among pixels across all image locations and over all frames,
allowing for transfer, aggregation, and correction of localizations.
Co-localization is integrated into training by minimizing the color term of a
conditional random field (CRF) loss over a sequence of frames/CAMs. Extensive
experiments on two challenging YouTube-Objects datasets of unconstrained videos
show the merits of our CoLo-CAM method, and its robustness to long-term
dependencies, leading to new state-of-the-art performance for WSVOL task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Foundation Model for General Moving Object Segmentation in Medical
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.17264v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.17264v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongnuo Yan, Tong Han, Yuhao Huang, Lian Liu, Han Zhou, Jiongquan Chen, Wenlong Shi, Yan Cao, Xin Yang, Dong Ni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image segmentation aims to delineate the anatomical or pathological
structures of interest, playing a crucial role in clinical diagnosis. A
substantial amount of high-quality annotated data is crucial for constructing
high-precision deep segmentation models. However, medical annotation is highly
cumbersome and time-consuming, especially for medical videos or 3D volumes, due
to the huge labeling space and poor inter-frame consistency. Recently, a
fundamental task named Moving Object Segmentation (MOS) has made significant
advancements in natural images. Its objective is to delineate moving objects
from the background within image sequences, requiring only minimal annotations.
In this paper, we propose the first foundation model, named iMOS, for MOS in
medical images. Extensive experiments on a large multi-modal medical dataset
validate the effectiveness of the proposed iMOS. Specifically, with the
annotation of only a small number of images in the sequence, iMOS can achieve
satisfactory tracking and segmentation performance of moving objects throughout
the entire sequence in bi-directions. We hope that the proposed iMOS can help
accelerate the annotation speed of experts, and boost the development of
medical foundation models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 7 figures, 3 tables. This paper has been accepted by ISBI
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Saliency-Aware Distillation for Few-shot Fine-grained Visual
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.07180v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.07180v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiqi Liu, C. L. Philip Chen, Xinrong Gong, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recognizing novel sub-categories with scarce samples is an essential and
challenging research topic in computer vision. Existing literature addresses
this challenge by employing local-based representation approaches, which may
not sufficiently facilitate meaningful object-specific semantic understanding,
leading to a reliance on apparent background correlations. Moreover, they
primarily rely on high-dimensional local descriptors to construct complex
embedding space, potentially limiting the generalization. To address the above
challenges, this article proposes a novel model, Robust Saliency-aware
Distillation (RSaD), for few-shot fine-grained visual recognition. RSaD
introduces additional saliency-aware supervision via saliency detection to
guide the model toward focusing on the intrinsic discriminative regions.
Specifically, RSaD utilizes the saliency detection model to emphasize the
critical regions of each sub-category, providing additional object-specific
information for fine-grained prediction. RSaD transfers such information with
two symmetric branches in a mutual learning paradigm. Furthermore, RSaD
exploits inter-regional relationships to enhance the informativeness of the
representation and subsequently summarize the highlighted details into
contextual embeddings to facilitate the effective transfer, enabling quick
generalization to novel sub-categories. The proposed approach is empirically
evaluated on three widely used benchmarks, demonstrating its superior
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Multimedia</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Neural Path Guiding with Normalized Anisotropic Spherical
  Gaussians 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08064v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08064v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Huang, Akito Iizuka, Hajime Tanaka, Taku Komura, Yoshifumi Kitamura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The variance reduction speed of physically-based rendering is heavily
affected by the adopted importance sampling technique. In this paper we propose
a novel online framework to learn the spatial-varying density model with a
single small neural network using stochastic ray samples. To achieve this task,
we propose a novel closed-form density model called the normalized anisotropic
spherical gaussian mixture, that can express complex irradiance fields with a
small number of parameters. Our framework learns the distribution in a
progressive manner and does not need any warm-up phases. Due to the compact and
expressive representation of our density model, our framework can be
implemented entirely on the GPU, allowing it produce high quality images with
limited computational resources.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stochastic Conditional Diffusion Models for Semantic Image Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16506v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16506v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juyeon Ko, Inho Kong, Dogyun Park, Hyunwoo J. Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic image synthesis (SIS) is a task to generate realistic images
corresponding to semantic maps (labels). It can be applied to diverse
real-world practices such as photo editing or content creation. However, in
real-world applications, SIS often encounters noisy user inputs. To address
this, we propose Stochastic Conditional Diffusion Model (SCDM), which is a
robust conditional diffusion model that features novel forward and generation
processes tailored for SIS with noisy labels. It enhances robustness by
stochastically perturbing the semantic label maps through Label Diffusion,
which diffuses the labels with discrete diffusion. Through the diffusion of
labels, the noisy and clean semantic maps become similar as the timestep
increases, eventually becoming identical at $t=T$. This facilitates the
generation of an image close to a clean image, enabling robust generation.
Furthermore, we propose a class-wise noise schedule to differentially diffuse
the labels depending on the class. We demonstrate that the proposed method
generates high-quality samples through extensive experiments and analyses on
benchmark datasets, including a novel experimental setup simulating human
errors during real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 21 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HarmBench: A Standardized Evaluation Framework for Automated Red Teaming
  and Robust Refusal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04249v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04249v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo Li, David Forsyth, Dan Hendrycks
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated red teaming holds substantial promise for uncovering and mitigating
the risks associated with the malicious use of large language models (LLMs),
yet the field lacks a standardized evaluation framework to rigorously assess
new methods. To address this issue, we introduce HarmBench, a standardized
evaluation framework for automated red teaming. We identify several desirable
properties previously unaccounted for in red teaming evaluations and
systematically design HarmBench to meet these criteria. Using HarmBench, we
conduct a large-scale comparison of 18 red teaming methods and 33 target LLMs
and defenses, yielding novel insights. We also introduce a highly efficient
adversarial training method that greatly enhances LLM robustness across a wide
range of attacks, demonstrating how HarmBench enables codevelopment of attacks
and defenses. We open source HarmBench at
https://github.com/centerforaisafety/HarmBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: https://www.harmbench.org</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Masking Augmentation for Supervised Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.11339v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.11339v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Byeongho Heo, Taekyung Kim, Sangdoo Yun, Dongyoon Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training using random masking has emerged as a novel trend in training
techniques. However, supervised learning faces a challenge in adopting masking
augmentations, primarily due to unstable training. In this paper, we propose a
novel way to involve masking augmentations dubbed Masked Sub-model (MaskSub).
MaskSub consists of the main-model and sub-model; while the former enjoys
conventional training recipes, the latter leverages the benefit of strong
masking augmentations in training. MaskSub addresses the challenge by
mitigating adverse effects through a relaxed loss function similar to a
self-distillation loss. Our analysis shows that MaskSub improves performance,
with the training loss converging even faster than regular training, which
suggests our method facilitates training. We further validate MaskSub across
diverse training recipes and models, including DeiT-III, MAE fine-tuning, CLIP
fine-tuning, ResNet, and Swin Transformer. Our results show that MaskSub
consistently provides significant performance gains across all the cases.
MaskSub provides a practical and effective solution for introducing additional
regularization under various training recipes. Code available at
https://github.com/naver-ai/augsub
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AttriHuman-3D: Editable 3D Human Avatar Generation with Attribute
  Decomposition and Indexing <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02209v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02209v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Yang, Tianyi Chen, Xiaosheng He, Zhongang Cai, Lei Yang, Si Wu, Guosheng Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Editable 3D-aware generation, which supports user-interacted editing, has
witnessed rapid development recently. However, existing editable 3D GANs either
fail to achieve high-accuracy local editing or suffer from huge computational
costs. We propose AttriHuman-3D, an editable 3D human generation model, which
address the aforementioned problems with attribute decomposition and indexing.
The core idea of the proposed model is to generate all attributes (e.g. human
body, hair, clothes and so on) in an overall attribute space with six feature
planes, which are then decomposed and manipulated with different attribute
indexes. To precisely extract features of different attributes from the
generated feature planes, we propose a novel attribute indexing method as well
as an orthogonal projection regularization to enhance the disentanglement. We
also introduce a hyper-latent training strategy and an attribute-specific
sampling strategy to avoid style entanglement and misleading punishment from
the discriminator. Our method allows users to interactively edit selected
attributes in the generated 3D human avatars while keeping others fixed. Both
qualitative and quantitative experiments demonstrate that our model provides a
strong disentanglement between different attributes, allows fine-grained image
editing and generates high-quality 3D human avatars.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DisenBooth: Identity-Preserving Disentangled Tuning for Subject-Driven
  Text-to-Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03374v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03374v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong Chen, Yipeng Zhang, Simin Wu, Xin Wang, Xuguang Duan, Yuwei Zhou, Wenwu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Subject-driven text-to-image generation aims to generate customized images of
the given subject based on the text descriptions, which has drawn increasing
attention. Existing methods mainly resort to finetuning a pretrained generative
model, where the identity-relevant information (e.g., the boy) and the
identity-irrelevant information (e.g., the background or the pose of the boy)
are entangled in the latent embedding space. However, the highly entangled
latent embedding may lead to the failure of subject-driven text-to-image
generation as follows: (i) the identity-irrelevant information hidden in the
entangled embedding may dominate the generation process, resulting in the
generated images heavily dependent on the irrelevant information while ignoring
the given text descriptions; (ii) the identity-relevant information carried in
the entangled embedding can not be appropriately preserved, resulting in
identity change of the subject in the generated images. To tackle the problems,
we propose DisenBooth, an identity-preserving disentangled tuning framework for
subject-driven text-to-image generation. Specifically, DisenBooth finetunes the
pretrained diffusion model in the denoising process. Different from previous
works that utilize an entangled embedding to denoise each image, DisenBooth
instead utilizes disentangled embeddings to respectively preserve the subject
identity and capture the identity-irrelevant information. We further design the
novel weak denoising and contrastive embedding auxiliary tuning objectives to
achieve the disentanglement. Extensive experiments show that our proposed
DisenBooth framework outperforms baseline models for subject-driven
text-to-image generation with the identity-preserved embedding. Additionally,
by combining the identity-preserved embedding and identity-irrelevant
embedding, DisenBooth demonstrates more generation flexibility and
controllability
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IRFundusSet: An Integrated Retinal Fundus <span class="highlight-title">Dataset</span> with a Harmonized
  Healthy Label 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11488v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11488v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        P. Bilha Githinji, Keming Zhao, Jiantao Wang, Peiwu Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ocular conditions are a global concern and computational tools utilizing
retinal fundus color photographs can aid in routine screening and management.
Obtaining comprehensive and sufficiently sized datasets, however, is
non-trivial for the intricate retinal fundus, which exhibits heterogeneities
within pathologies, in addition to variations from demographics and
acquisition. Moreover, retinal fundus datasets in the public space suffer
fragmentation in the organization of data and definition of a healthy
observation. We present Integrated Retinal Fundus Set (IRFundusSet), a dataset
that consolidates, harmonizes and curates several public datasets, facilitating
their consumption as a unified whole and with a consistent is_normal label.
IRFundusSet comprises a Python package that automates harmonization and avails
a dataset object in line with the PyTorch approach. Moreover, images are
physically reviewed and a new is_normal label is annotated for a consistent
definition of a healthy observation. Ten public datasets are initially
considered with a total of 46064 images, of which 25406 are curated for a new
is_normal label and 3515 are deemed healthy across the sources.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rotated Multi-Scale Interaction Network for Referring Remote Sensing
  Image Segmentation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12470v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12470v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sihan Liu, Yiwei Ma, Xiaoqing Zhang, Haowei Wang, Jiayi Ji, Xiaoshuai Sun, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Referring Remote Sensing Image Segmentation (RRSIS) is a new challenge that
combines computer vision and natural language processing, delineating specific
regions in aerial images as described by textual queries. Traditional Referring
Image Segmentation (RIS) approaches have been impeded by the complex spatial
scales and orientations found in aerial imagery, leading to suboptimal
segmentation results. To address these challenges, we introduce the Rotated
Multi-Scale Interaction Network (RMSIN), an innovative approach designed for
the unique demands of RRSIS. RMSIN incorporates an Intra-scale Interaction
Module (IIM) to effectively address the fine-grained detail required at
multiple scales and a Cross-scale Interaction Module (CIM) for integrating
these details coherently across the network. Furthermore, RMSIN employs an
Adaptive Rotated Convolution (ARC) to account for the diverse orientations of
objects, a novel contribution that significantly enhances segmentation
accuracy. To assess the efficacy of RMSIN, we have curated an expansive dataset
comprising 17,402 image-caption-mask triplets, which is unparalleled in terms
of scale and variety. This dataset not only presents the model with a wide
range of spatial and rotational scenarios but also establishes a stringent
benchmark for the RRSIS task, ensuring a rigorous evaluation of performance.
Our experimental evaluations demonstrate the exceptional performance of RMSIN,
surpassing existing state-of-the-art models by a significant margin. All
datasets and code are made available at https://github.com/Lsan2401/RMSIN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TriAug: Out-of-Distribution Detection for Imbalanced Breast Lesion in
  Ultrasound 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07452v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07452v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinyu Ye, Shijing Chen, Dong Ni, Ruobing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Different diseases, such as histological subtypes of breast lesions, have
severely varying incidence rates. Even trained with substantial amount of
in-distribution (ID) data, models often encounter out-of-distribution (OOD)
samples belonging to unseen classes in clinical reality. To address this, we
propose a novel framework built upon a long-tailed OOD detection task for
breast ultrasound images. It is equipped with a triplet state augmentation
(TriAug) which improves ID classification accuracy while maintaining a
promising OOD detection performance. Meanwhile, we designed a balanced sphere
loss to handle the class imbalanced problem. Experimental results show that the
model outperforms state-of-art OOD approaches both in ID classification
(F1-score=42.12%) and OOD detection (AUROC=78.06%).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DifFlow3D: Toward Robust Uncertainty-Aware Scene Flow Estimation with
  Iterative Diffusion-Based Refinement <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17456v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17456v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiuming Liu, Guangming Wang, Weicai Ye, Chaokang Jiang, Jinru Han, Zhe Liu, Guofeng Zhang, Dalong Du, Hesheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene flow estimation, which aims to predict per-point 3D displacements of
dynamic scenes, is a fundamental task in the computer vision field. However,
previous works commonly suffer from unreliable correlation caused by locally
constrained searching ranges, and struggle with accumulated inaccuracy arising
from the coarse-to-fine structure. To alleviate these problems, we propose a
novel uncertainty-aware scene flow estimation network (DifFlow3D) with the
diffusion probabilistic model. Iterative diffusion-based refinement is designed
to enhance the correlation robustness and resilience to challenging cases,
e.g., dynamics, noisy inputs, repetitive patterns, etc. To restrain the
generation diversity, three key flow-related features are leveraged as
conditions in our diffusion model. Furthermore, we also develop an uncertainty
estimation module within diffusion to evaluate the reliability of estimated
scene flow. Our DifFlow3D achieves state-of-the-art performance, with 6.7\% and
19.1\% EPE3D reduction respectively on FlyingThings3D and KITTI 2015 datasets.
Notably, our method achieves an unprecedented millimeter-level accuracy
(0.0089m in EPE3D) on the KITTI dataset. Additionally, our diffusion-based
refinement paradigm can be readily integrated as a plug-and-play module into
existing scene flow networks, significantly increasing their estimation
accuracy. Codes will be released on https://github.com/IRMVLab/DifFlow3D.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024. Codes will be released on
  https://github.com/IRMVLab/DifFlow3D</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FViT: A Focal Vision <span class="highlight-title">Transformer</span> with Gabor Filter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11303v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11303v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulong Shi, Mingwei Sun, Yongshuai Wang, Rui Wang, Hui Sun, Zengqiang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision transformers have achieved encouraging progress in various computer
vision tasks. A common belief is that this is attributed to the competence of
self-attention in modeling the global dependencies among feature tokens.
Unfortunately, self-attention still faces some challenges in dense prediction
tasks, such as the high computational complexity and absence of desirable
inductive bias. To address these issues, we revisit the potential benefits of
integrating vision transformer with Gabor filter, and propose a Learnable Gabor
Filter (LGF) by using convolution. As an alternative to self-attention, we
employ LGF to simulate the response of simple cells in the biological visual
system to input images, prompting models to focus on discriminative feature
representations of targets from various scales and orientations. Additionally,
we design a Bionic Focal Vision (BFV) block based on the LGF. This block draws
inspiration from neuroscience and introduces a Multi-Path Feed Forward Network
(MPFFN) to emulate the working way of biological visual cortex processing
information in parallel. Furthermore, we develop a unified and efficient
pyramid backbone network family called Focal Vision Transformers (FViTs) by
stacking BFV blocks. Experimental results show that FViTs exhibit highly
competitive performance in various vision tasks. Especially in terms of
computational efficiency and scalability, FViTs show significant advantages
compared with other counterparts. Code is available at
https://github.com/nkusyl/FViT
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InteractDiffusion: Interaction Control in Text-to-Image Diffusion Models <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05849v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05849v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiun Tian Hoe, Xudong Jiang, Chee Seng Chan, Yap-Peng Tan, Weipeng Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale text-to-image (T2I) diffusion models have showcased incredible
capabilities in generating coherent images based on textual descriptions,
enabling vast applications in content generation. While recent advancements
have introduced control over factors such as object localization, posture, and
image contours, a crucial gap remains in our ability to control the
interactions between objects in the generated content. Well-controlling
interactions in generated images could yield meaningful applications, such as
creating realistic scenes with interacting characters. In this work, we study
the problems of conditioning T2I diffusion models with Human-Object Interaction
(HOI) information, consisting of a triplet label (person, action, object) and
corresponding bounding boxes. We propose a pluggable interaction control model,
called InteractDiffusion that extends existing pre-trained T2I diffusion models
to enable them being better conditioned on interactions. Specifically, we
tokenize the HOI information and learn their relationships via interaction
embeddings. A conditioning self-attention layer is trained to map HOI tokens to
visual tokens, thereby conditioning the visual tokens better in existing T2I
diffusion models. Our model attains the ability to control the interaction and
location on existing T2I diffusion models, which outperforms existing baselines
by a large margin in HOI detection score, as well as fidelity in FID and KID.
Project page: https://jiuntian.github.io/interactdiffusion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: https://jiuntian.github.io/interactdiffusion. Accepted at
  CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">11</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Learned Sparse Retrieval with Probabilistic Expansion Control <span class="chip">ECIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17535v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17535v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thong Nguyen, Mariya Hendriksen, Andrew Yates, Maarten de Rijke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learned sparse retrieval (LSR) is a family of neural methods that encode
queries and documents into sparse lexical vectors that can be indexed and
retrieved efficiently with an inverted index. We explore the application of LSR
to the multi-modal domain, with a focus on text-image retrieval. While LSR has
seen success in text retrieval, its application in multimodal retrieval remains
underexplored. Current approaches like LexLIP and STAIR require complex
multi-step training on massive datasets. Our proposed approach efficiently
transforms dense vectors from a frozen dense model into sparse lexical vectors.
We address issues of high dimension co-activation and semantic deviation
through a new training algorithm, using Bernoulli random variables to control
query expansion. Experiments with two dense models (BLIP, ALBEF) and two
datasets (MSCOCO, Flickr30k) show that our proposed algorithm effectively
reduces co-activation and semantic deviation. Our best-performing sparsified
model outperforms state-of-the-art text-image LSR models with a shorter
training time and lower GPU memory requirements. Our approach offers an
effective solution for training LSR retrieval models in multimodal settings.
Our code and model checkpoints are available at
github.com/thongnt99/lsr-multimodal
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, accepted as a full paper at ECIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BASES: Large-scale Web Search User Simulation with Large Language Model
  based Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17505v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17505v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiyang Ren, Peng Qiu, Yingqi Qu, Jing Liu, Wayne Xin Zhao, Hua Wu, Ji-Rong Wen, Haifeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the excellent capacities of large language models (LLMs), it becomes
feasible to develop LLM-based agents for reliable user simulation. Considering
the scarcity and limit (e.g., privacy issues) of real user data, in this paper,
we conduct large-scale user simulation for web search, to improve the analysis
and modeling of user search behavior. Specially, we propose BASES, a novel user
simulation framework with LLM-based agents, designed to facilitate
comprehensive simulations of web search user behaviors. Our simulation
framework can generate unique user profiles at scale, which subsequently leads
to diverse search behaviors. To demonstrate the effectiveness of BASES, we
conduct evaluation experiments based on two human benchmarks in both Chinese
and English, demonstrating that BASES can effectively simulate large-scale
human-like search behaviors. To further accommodate the research on web search,
we develop WARRIORS, a new large-scale dataset encompassing web search user
behaviors, including both Chinese and English versions, which can greatly
bolster research in the field of information retrieval. Our code and data will
be publicly released soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain
  Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Wang, Ruiyang Ren, Junyi Li, Wayne Xin Zhao, Jing Liu, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Considering the limited internal parametric knowledge, retrieval-augmented
generation (RAG) has been widely used to extend the knowledge scope of large
language models (LLMs). Despite the extensive efforts on RAG research, in
existing methods, LLMs cannot precisely assess the relevance of retrieved
documents, thus likely leading to misleading or even incorrect utilization of
external knowledge (i.e., retrieved documents). To address this issue, in this
paper, we propose REAR, a RElevance-Aware Retrieval-augmented approach for
open-domain question answering (QA). As the key motivation, we aim to enhance
the self-awareness of source relevance for LLMs, so as to adaptively utilize
external knowledge in RAG systems. Specially, we develop a new architecture for
LLM based RAG system, by incorporating a specially designed rank head that
precisely assesses the relevance of retrieved documents. Furthermore, we
propose an improved training method based on bi-granularity relevance fusion
and noise-resistant training. By combining the improvements in both
architecture and training, our proposed REAR can better utilize external
knowledge by effectively perceiving the relevance of retrieved documents.
Experiments on four open-domain QA tasks show that REAR significantly
outperforms previous a number of competitive RAG approaches. Our code and data
can be accessed at https://github.com/RUCAIBox/REAR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Natural Language Processing Methods for Symbolic Music Generation and
  Information Retrieval: a <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17467v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17467v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dinh-Viet-Toan Le, Louis Bigo, Mikaela Keller, Dorien Herremans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several adaptations of Transformers models have been developed in various
domains since its breakthrough in Natural Language Processing (NLP). This trend
has spread into the field of Music Information Retrieval (MIR), including
studies processing music data. However, the practice of leveraging NLP tools
for symbolic music data is not novel in MIR. Music has been frequently compared
to language, as they share several similarities, including sequential
representations of text and music. These analogies are also reflected through
similar tasks in MIR and NLP. This survey reviews NLP methods applied to
symbolic music generation and information retrieval studies following two axes.
We first propose an overview of representations of symbolic music adapted from
natural language sequential representations. Such representations are designed
by considering the specificities of symbolic music. These representations are
then processed by models. Such models, possibly originally developed for text
and adapted for symbolic music, are trained on various tasks. We describe these
models, in particular deep learning models, through different prisms,
highlighting music-specialized mechanisms. We finally present a discussion
surrounding the effective use of NLP tools for symbolic music data. This
includes technical issues regarding NLP methods and fundamental differences
between text and music, which may open several doors for further research into
more effectively adapting NLP tools to symbolic MIR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 5 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning Based Named Entity Recognition Models for Recipes <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mansi Goel, Ayush Agarwal, Shubham Agrawal, Janak Kapuriya, Akhil Vamshi Konam, Rishabh Gupta, Shrey Rastogi,  Niharika, Ganesh Bagler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Food touches our lives through various endeavors, including flavor,
nourishment, health, and sustainability. Recipes are cultural capsules
transmitted across generations via unstructured text. Automated protocols for
recognizing named entities, the building blocks of recipe text, are of immense
value for various applications ranging from information extraction to novel
recipe generation. Named entity recognition is a technique for extracting
information from unstructured or semi-structured data with known labels.
Starting with manually-annotated data of 6,611 ingredient phrases, we created
an augmented dataset of 26,445 phrases cumulatively. Simultaneously, we
systematically cleaned and analyzed ingredient phrases from RecipeDB, the
gold-standard recipe data repository, and annotated them using the Stanford
NER. Based on the analysis, we sampled a subset of 88,526 phrases using a
clustering-based approach while preserving the diversity to create the
machine-annotated dataset. A thorough investigation of NER approaches on these
three datasets involving statistical, fine-tuning of deep learning-based
language models and few-shot prompting on large language models (LLMs) provides
deep insights. We conclude that few-shot prompting on LLMs has abysmal
performance, whereas the fine-tuned spaCy-transformer emerges as the best model
with macro-F1 scores of 95.9%, 96.04%, and 95.71% for the manually-annotated,
augmented, and machine-annotated datasets, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 main figures and 2 in appendices, and 3 main tables;
  Accepted for publication in LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BiVRec: Bidirectional View-based Multimodal Sequential Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxi Hu, Jingtong Gao, Xiangyu Zhao, Yuehong Hu, Yuxuan Liang, Yiqi Wang, Ming He, Zitao Liu, Hongzhi Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of multimodal information into sequential recommender systems
has attracted significant attention in recent research. In the initial stages
of multimodal sequential recommendation models, the mainstream paradigm was
ID-dominant recommendations, wherein multimodal information was fused as side
information. However, due to their limitations in terms of transferability and
information intrusion, another paradigm emerged, wherein multimodal features
were employed directly for recommendation, enabling recommendation across
datasets. Nonetheless, it overlooked user ID information, resulting in low
information utilization and high training costs. To this end, we propose an
innovative framework, BivRec, that jointly trains the recommendation tasks in
both ID and multimodal views, leveraging their synergistic relationship to
enhance recommendation performance bidirectionally. To tackle the information
heterogeneity issue, we first construct structured user interest
representations and then learn the synergistic relationship between them.
Specifically, BivRec comprises three modules: Multi-scale Interest Embedding,
comprehensively modeling user interests by expanding user interaction sequences
with multi-scale patching; Intra-View Interest Decomposition, constructing
highly structured interest representations using carefully designed Gaussian
attention and Cluster attention; and Cross-View Interest Learning, learning the
synergistic relationship between the two recommendation views through
coarse-grained overall semantic similarity and fine-grained interest allocation
similarity BiVRec achieves state-of-the-art performance on five datasets and
showcases various practical advantages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiFashion: Towards Personalized Outfit Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyan Xu, Wenjie Wang, Fuli Feng, Yunshan Ma, Jizhi Zhang, Xiangnan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evolution of Outfit Recommendation (OR) in the realm of fashion has
progressed through two distinct phases: Pre-defined Outfit Recommendation and
Personalized Outfit Composition. Despite these advancements, both phases face
limitations imposed by existing fashion products, hindering their effectiveness
in meeting users' diverse fashion needs. The emergence of AI-generated content
has paved the way for OR to overcome these constraints, demonstrating the
potential for personalized outfit generation.
  In pursuit of this, we introduce an innovative task named Generative Outfit
Recommendation (GOR), with the goal of synthesizing a set of fashion images and
assembling them to form visually harmonious outfits customized to individual
users. The primary objectives of GOR revolve around achieving high fidelity,
compatibility, and personalization of the generated outfits. To accomplish
these, we propose DiFashion, a generative outfit recommender model that
harnesses exceptional diffusion models for the simultaneous generation of
multiple fashion images. To ensure the fulfillment of these objectives, three
types of conditions are designed to guide the parallel generation process and
Classifier-Free-Guidance are employed to enhance the alignment between
generated images and conditions. DiFashion is applied to both personalized
Fill-In-The-Blank and GOR tasks, and extensive experiments are conducted on the
iFashion and Polyvore-U datasets. The results of quantitative and
human-involved qualitative evaluations highlight the superiority of DiFashion
over competitive baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span>MM: Multi-Modal Knowledge Distillation for Recommendation with
  <span class="highlight-title">Prompt</span>-Tuning <span class="chip">WWW 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17188v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17188v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Wei, Jiabin Tang, Yangqin Jiang, Lianghao Xia, Chao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimedia online platforms (e.g., Amazon, TikTok) have greatly benefited
from the incorporation of multimedia (e.g., visual, textual, and acoustic)
content into their personal recommender systems. These modalities provide
intuitive semantics that facilitate modality-aware user preference modeling.
However, two key challenges in multi-modal recommenders remain unresolved: i)
The introduction of multi-modal encoders with a large number of additional
parameters causes overfitting, given high-dimensional multi-modal features
provided by extractors (e.g., ViT, BERT). ii) Side information inevitably
introduces inaccuracies and redundancies, which skew the modality-interaction
dependency from reflecting true user preference. To tackle these problems, we
propose to simplify and empower recommenders through Multi-modal Knowledge
Distillation (PromptMM) with the prompt-tuning that enables adaptive quality
distillation. Specifically, PromptMM conducts model compression through
distilling u-i edge relationship and multi-modal node content from cumbersome
teachers to relieve students from the additional feature reduction parameters.
To bridge the semantic gap between multi-modal context and collaborative
signals for empowering the overfitting teacher, soft prompt-tuning is
introduced to perform student task-adaptive. Additionally, to adjust the impact
of inaccuracies in multimedia data, a disentangled multi-modal list-wise
distillation is developed with modality-aware re-weighting mechanism.
Experiments on real-world data demonstrate PromptMM's superiority over existing
techniques. Ablation tests confirm the effectiveness of key components.
Additional tests show the efficiency and effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WWW 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Actions Speak Louder than Words: Trillion-Parameter Sequential
  Transducers for Generative Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Zhai, Lucy Liao, Xing Liu, Yueming Wang, Rui Li, Xuan Cao, Leon Gao, Zhaojie Gong, Fangda Gu, Michael He, Yinghai Lu, Yu Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale recommendation systems are characterized by their reliance on
high cardinality, heterogeneous features and the need to handle tens of
billions of user actions on a daily basis. Despite being trained on huge volume
of data with thousands of features, most Deep Learning Recommendation Models
(DLRMs) in industry fail to scale with compute.
  Inspired by success achieved by Transformers in language and vision domains,
we revisit fundamental design choices in recommendation systems. We reformulate
recommendation problems as sequential transduction tasks within a generative
modeling framework (``Generative Recommenders''), and propose a new
architecture, HSTU, designed for high cardinality, non-stationary streaming
recommendation data.
  HSTU outperforms baselines over synthetic and public datasets by up to 65.8\%
in NDCG, and is 5.3x to 15.2x faster than FlashAttention2-based Transformers on
8192 length sequences. HSTU-based Generative Recommenders, with 1.5 trillion
parameters, improve metrics in online A/B tests by 12.4\% and have been
deployed on multiple surfaces of a large internet platform with billions of
users. More importantly, the model quality of Generative Recommenders
empirically scales as a power-law of training compute across three orders of
magnitude, up to GPT-3/LLaMa-2 scale, which reduces carbon footprint needed for
future model developments, and further paves the way for the first foundational
models in recommendations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Full technical report to follow</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Side Information-Driven Session-based Recommendation: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaokun Zhang, Bo Xu, Chenliang Li, Yao Zhou, Liangyue Li, Hongfei Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The session-based recommendation (SBR) garners increasing attention due to
its ability to predict anonymous user intents within limited interactions.
Emerging efforts incorporate various kinds of side information into their
methods for enhancing task performance. In this survey, we thoroughly review
the side information-driven session-based recommendation from a data-centric
perspective. Our survey commences with an illustration of the motivation and
necessity behind this research topic. This is followed by a detailed
exploration of various benchmarks rich in side information, pivotal for
advancing research in this field. Moreover, we delve into how these diverse
types of side information enhance SBR, underscoring their characteristics and
utility. A systematic review of research progress is then presented, offering
an analysis of the most recent and representative developments within this
topic. Finally, we present the future prospects of this vibrant topic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is a survey on side information-driven session-based
  recommendation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Robust Cybersecurity Topic Classification Tool 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.02473v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.02473v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elijah Pelofske, Lorie M. Liebrock, Vincent Urias
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this research, we use user defined labels from three internet text sources
(Reddit, Stackexchange, Arxiv) to train 21 different machine learning models
for the topic classification task of detecting cybersecurity discussions in
natural text. We analyze the false positive and false negative rates of each of
the 21 model's in a cross validation experiment. Then we present a
Cybersecurity Topic Classification (CTC) tool, which takes the majority vote of
the 21 trained machine learning models as the decision mechanism for detecting
cybersecurity related text. We also show that the majority vote mechanism of
the CTC tool provides lower false negative and false positive rates on average
than any of the 21 individual models. We show that the CTC tool is scalable to
the hundreds of thousands of documents with a wall clock time on the order of
hours.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Improved formatting</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion Meets DAgger: Supercharging Eye-in-hand Imitation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17768v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17768v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Zhang, Matthew Chang, Pranav Kumar, Saurabh Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A common failure mode for policies trained with imitation is compounding
execution errors at test time. When the learned policy encounters states that
were not present in the expert demonstrations, the policy fails, leading to
degenerate behavior. The Dataset Aggregation, or DAgger approach to this
problem simply collects more data to cover these failure states. However, in
practice, this is often prohibitively expensive. In this work, we propose
Diffusion Meets DAgger (DMD), a method to reap the benefits of DAgger without
the cost for eye-in-hand imitation learning problems. Instead of collecting new
samples to cover out-of-distribution states, DMD uses recent advances in
diffusion models to create these samples with diffusion models. This leads to
robust performance from few demonstrations. In experiments conducted for
non-prehensile pushing on a Franka Research 3, we show that DMD can achieve a
success rate of 80% with as few as 8 expert demonstrations, where naive
behavior cloning reaches only 20%. DMD also outperform competing NeRF-based
augmentation schemes by 50%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>for project website with video, see
  https://sites.google.com/view/diffusion-meets-dagger</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Opening Cabinets and Drawers in the Real World using a Commodity Mobile
  Manipulator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17767v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17767v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arjun Gupta, Michelle Zhang, Rishik Sathua, Saurabh Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pulling open cabinets and drawers presents many difficult technical
challenges in perception (inferring articulation parameters for objects from
onboard sensors), planning (producing motion plans that conform to tight task
constraints), and control (making and maintaining contact while applying forces
on the environment). In this work, we build an end-to-end system that enables a
commodity mobile manipulator (Stretch RE2) to pull open cabinets and drawers in
diverse previously unseen real world environments. We conduct 4 days of real
world testing of this system spanning 31 different objects from across 13
different real world environments. Our system achieves a success rate of 61% on
opening novel cabinets and drawers in unseen environments zero-shot. An
analysis of the failure modes suggests that errors in perception are the most
significant challenge for our system. We will open source code and models for
others to replicate and build upon our system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project webpage:
  https://arjung128.github.io/opening-cabinets-and-drawers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17764v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17764v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research, such as BitNet, is paving the way for a new era of 1-bit
Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant,
namely BitNet b1.58, in which every single parameter (or weight) of the LLM is
ternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16)
Transformer LLM with the same model size and training tokens in terms of both
perplexity and end-task performance, while being significantly more
cost-effective in terms of latency, memory, throughput, and energy consumption.
More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for
training new generations of LLMs that are both high-performance and
cost-effective. Furthermore, it enables a new computation paradigm and opens
the door for designing specific hardware optimized for 1-bit LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Massive Activations in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingjie Sun, Xinlei Chen, J. Zico Kolter, Zhuang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We observe an empirical phenomenon in Large Language Models (LLMs) -- very
few activations exhibit significantly larger values than others (e.g., 100,000
times larger). We call them massive activations. First, we demonstrate the
widespread existence of massive activations across various LLMs and
characterize their locations. Second, we find their values largely stay
constant regardless of the input, and they function as indispensable bias terms
in LLMs. Third, these massive activations lead to the concentration of
attention probabilities to their corresponding tokens, and further, implicit
bias terms in the self-attention output. Last, we also study massive
activations in Vision Transformers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website at
  https://eric-mingjie.github.io/massive-activations/index.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Program Variational Quantum Circuits with Fast Weights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17760v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17760v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Yen-Chi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum Machine Learning (QML) has surfaced as a pioneering framework
addressing sequential control tasks and time-series modeling. It has
demonstrated empirical quantum advantages notably within domains such as
Reinforcement Learning (RL) and time-series prediction. A significant
advancement lies in Quantum Recurrent Neural Networks (QRNNs), specifically
tailored for memory-intensive tasks encompassing partially observable
environments and non-linear time-series prediction. Nevertheless, QRNN-based
models encounter challenges, notably prolonged training duration stemming from
the necessity to compute quantum gradients using backpropagation-through-time
(BPTT). This predicament exacerbates when executing the complete model on
quantum devices, primarily due to the substantial demand for circuit evaluation
arising from the parameter-shift rule. This paper introduces the Quantum Fast
Weight Programmers (QFWP) as a solution to the temporal or sequential learning
challenge. The QFWP leverages a classical neural network (referred to as the
'slow programmer') functioning as a quantum programmer to swiftly modify the
parameters of a variational quantum circuit (termed the 'fast programmer').
Instead of completely overwriting the fast programmer at each time-step, the
slow programmer generates parameter changes or updates for the quantum circuit
parameters. This approach enables the fast programmer to incorporate past
observations or information. Notably, the proposed QFWP model achieves learning
of temporal dependencies without necessitating the use of quantum recurrent
neural networks. Numerical simulations conducted in this study showcase the
efficacy of the proposed QFWP model in both time-series prediction and RL
tasks. The model exhibits performance levels either comparable to or surpassing
those achieved by QLSTM-based models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robustly Learning Single-Index Models via Alignment Sharpness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikos Zarifis, Puqian Wang, Ilias Diakonikolas, Jelena Diakonikolas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of learning Single-Index Models under the $L_2^2$ loss
in the agnostic model. We give an efficient learning algorithm, achieving a
constant factor approximation to the optimal loss, that succeeds under a range
of distributions (including log-concave distributions) and a broad class of
monotone and Lipschitz link functions. This is the first efficient constant
factor approximate agnostic learner, even for Gaussian data and for any
nontrivial class of link functions. Prior work for the case of unknown link
function either works in the realizable setting or does not attain constant
factor approximation. The main technical ingredient enabling our algorithm and
analysis is a novel notion of a local error bound in optimization that we term
alignment sharpness and that may be of broader interest.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Very Long-Term Conversational Memory of LLM Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, Yuwei Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing works on long-term open-domain dialogues focus on evaluating model
responses within contexts spanning no more than five chat sessions. Despite
advancements in long-context large language models (LLMs) and retrieval
augmented generation (RAG) techniques, their efficacy in very long-term
dialogues remains unexplored. To address this research gap, we introduce a
machine-human pipeline to generate high-quality, very long-term dialogues by
leveraging LLM-based agent architectures and grounding their dialogues on
personas and temporal event graphs. Moreover, we equip each agent with the
capability of sharing and reacting to images. The generated conversations are
verified and edited by human annotators for long-range consistency and
grounding to the event graphs. Using this pipeline, we collect LoCoMo, a
dataset of very long-term conversations, each encompassing 300 turns and 9K
tokens on avg., over up to 35 sessions. Based on LoCoMo, we present a
comprehensive evaluation benchmark to measure long-term memory in models,
encompassing question answering, event summarization, and multi-modal dialogue
generation tasks. Our experimental results indicate that LLMs exhibit
challenges in understanding lengthy conversations and comprehending long-range
temporal and causal dynamics within dialogues. Employing strategies like
long-context LLMs or RAG can offer improvements but these models still
substantially lag behind human performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages; Project page: https://snap-research.github.io/locomo/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling on-chip photonic neural processors using arbitrarily
  programmable wave propagation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17750v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17750v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tatsuhiro Onodera, Martin M. Stein, Benjamin A. Ash, Mandar M. Sohoni, Melissa Bosch, Ryotatsu Yanagimoto, Marc Jankowski, Timothy P. McKenna, Tianyu Wang, Gennady Shvets, Maxim R. Shcherbakov, Logan G. Wright, Peter L. McMahon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  On-chip photonic processors for neural networks have potential benefits in
both speed and energy efficiency but have not yet reached the scale at which
they can outperform electronic processors. The dominant paradigm for designing
on-chip photonics is to make networks of relatively bulky discrete components
connected by one-dimensional waveguides. A far more compact alternative is to
avoid explicitly defining any components and instead sculpt the continuous
substrate of the photonic processor to directly perform the computation using
waves freely propagating in two dimensions. We propose and demonstrate a device
whose refractive index as a function of space, $n(x,z)$, can be rapidly
reprogrammed, allowing arbitrary control over the wave propagation in the
device. Our device, a 2D-programmable waveguide, combines photoconductive gain
with the electro-optic effect to achieve massively parallel modulation of the
refractive index of a slab waveguide, with an index modulation depth of
$10^{-3}$ and approximately $10^4$ programmable degrees of freedom. We used a
prototype device with a functional area of $12\,\text{mm}^2$ to perform
neural-network inference with up to 49-dimensional input vectors in a single
pass, achieving 96% accuracy on vowel classification and 86% accuracy on $7
\times 7$-pixel MNIST handwritten-digit classification. This is a scale beyond
that of previous photonic chips relying on discrete components, illustrating
the benefit of the continuous-waves paradigm. In principle, with large enough
chip area, the reprogrammability of the device's refractive index distribution
enables the reconfigurable realization of any passive, linear photonic circuit
or device. This promises the development of more compact and versatile photonic
systems for a wide range of applications, including optical processing, smart
sensing, spectroscopy, and optical communications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When Your AI Deceives You: Challenges with Partial Observability of
  Human Evaluators in Reward Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17747v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17747v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leon Lang, Davis Foote, Stuart Russell, Anca Dragan, Erik Jenner, Scott Emmons
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Past analyses of reinforcement learning from human feedback (RLHF) assume
that the human fully observes the environment. What happens when human feedback
is based only on partial observations? We formally define two failure cases:
deception and overjustification. Modeling the human as Boltzmann-rational
w.r.t. a belief over trajectories, we prove conditions under which RLHF is
guaranteed to result in policies that deceptively inflate their performance,
overjustify their behavior to make an impression, or both. To help address
these issues, we mathematically characterize how partial observability of the
environment translates into (lack of) ambiguity in the learned return function.
In some cases, accounting for partial observability makes it theoretically
possible to recover the return function and thus the optimal policy, while in
other cases, there is irreducible ambiguity. We caution against blindly
applying RLHF in partially observable settings and propose research directions
to help tackle these challenges.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ reBandit: Random Effects based Online RL algorithm for Reducing Cannabis
  Use 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Susobhan Ghosh, Yongyi Guo, Pei-Yao Hung, Lara Coughlin, Erin Bonar, Inbal Nahum-Shani, Maureen Walton, Susan Murphy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The escalating prevalence of cannabis use, and associated cannabis-use
disorder (CUD), poses a significant public health challenge globally. With a
notably wide treatment gap, especially among emerging adults (EAs; ages 18-25),
addressing cannabis use and CUD remains a pivotal objective within the 2030
United Nations Agenda for Sustainable Development Goals (SDG). In this work, we
develop an online reinforcement learning (RL) algorithm called reBandit which
will be utilized in a mobile health study to deliver personalized mobile health
interventions aimed at reducing cannabis use among EAs. reBandit utilizes
random effects and informative Bayesian priors to learn quickly and efficiently
in noisy mobile health environments. Moreover, reBandit employs Empirical Bayes
and optimization techniques to autonomously update its hyper-parameters online.
To evaluate the performance of our algorithm, we construct a simulation testbed
using data from a prior study, and compare against commonly used algorithms in
mobile health studies. We show that reBandit performs equally well or better
than all the baseline algorithms, and the performance gap widens as population
heterogeneity increases in the simulation environment, proving its adeptness to
adapt to diverse population of study participants.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning-Based Algorithms for Graph Searching Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17736v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17736v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adela Frances DePavia, Erasmo Tani, Ali Vakilian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of graph searching with prediction recently
introduced by Banerjee et al. (2022). In this problem, an agent, starting at
some vertex $r$ has to traverse a (potentially unknown) graph $G$ to find a
hidden goal node $g$ while minimizing the total distance travelled. We study a
setting in which at any node $v$, the agent receives a noisy estimate of the
distance from $v$ to $g$. We design algorithms for this search task on unknown
graphs. We establish the first formal guarantees on unknown weighted graphs and
provide lower bounds showing that the algorithms we propose have optimal or
nearly-optimal dependence on the prediction error. Further, we perform
numerical experiments demonstrating that in addition to being robust to
adversarial error, our algorithms perform well in typical instances in which
the error is stochastic. Finally, we provide alternative simpler performance
bounds on the algorithms of Banerjee et al. (2022) for the case of searching on
a known graph, and establish new lower bounds for this setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Batched Nonparametric Contextual Bandits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17732v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17732v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rong Jiang, Cong Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study nonparametric contextual bandits under batch constraints, where the
expected reward for each action is modeled as a smooth function of covariates,
and the policy updates are made at the end of each batch of observations. We
establish a minimax regret lower bound for this setting and propose Batched
Successive Elimination with Dynamic Binning (BaSEDB) that achieves optimal
regret (up to logarithmic factors). In essence, BaSEDB dynamically splits the
covariate space into smaller bins, carefully aligning their widths with the
batch size. We also show the suboptimality of static binning under batch
constraints, highlighting the necessity of dynamic binning. Additionally, our
results suggest that a nearly constant number of policy updates can attain
optimal regret in the fully online setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Markovletics: Methods and A Novel Application for Learning
  Continuous-Time Markov Chain Mixtures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Spaeh, Charalampos E. Tsourakakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential data naturally arises from user engagement on digital platforms
like social media, music streaming services, and web navigation, encapsulating
evolving user preferences and behaviors through continuous information streams.
A notable unresolved query in stochastic processes is learning mixtures of
continuous-time Markov chains (CTMCs). While there is progress in learning
mixtures of discrete-time Markov chains with recovery guarantees
[GKV16,ST23,KTT2023], the continuous scenario uncovers unique unexplored
challenges. The intrigue in CTMC mixtures stems from their potential to model
intricate continuous-time stochastic processes prevalent in various fields
including social media, finance, and biology.
  In this study, we introduce a novel framework for exploring CTMCs,
emphasizing the influence of observed trails' length and mixture parameters on
problem regimes, which demands specific algorithms. Through thorough
experimentation, we examine the impact of discretizing continuous-time trails
on the learnability of the continuous-time mixture, given that these processes
are often observed via discrete, resource-demanding observations. Our
comparative analysis with leading methods explores sample complexity and the
trade-off between the number of trails and their lengths, offering crucial
insights for method selection in different problem instances. We apply our
algorithms on an extensive collection of Lastfm's user-generated trails
spanning three years, demonstrating the capability of our algorithms to
differentiate diverse user preferences. We pioneer the use of CTMC mixtures on
a basketball passing dataset to unveil intricate offensive tactics of NBA
teams. This underscores the pragmatic utility and versatility of our proposed
framework. All results presented in this study are replicable, and we provide
the implementations to facilitate reproducibility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Taming Nonconvex Stochastic Mirror Descent with General Bregman
  Divergence <span class="chip">AISTATS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17722v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17722v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilyas Fatkhullin, Niao He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper revisits the convergence of Stochastic Mirror Descent (SMD) in the
contemporary nonconvex optimization setting. Existing results for batch-free
nonconvex SMD restrict the choice of the distance generating function (DGF) to
be differentiable with Lipschitz continuous gradients, thereby excluding
important setups such as Shannon entropy. In this work, we present a new
convergence analysis of nonconvex SMD supporting general DGF, that overcomes
the above limitations and relies solely on the standard assumptions. Moreover,
our convergence is established with respect to the Bregman Forward-Backward
envelope, which is a stronger measure than the commonly used squared norm of
gradient mapping. We further extend our results to guarantee high probability
convergence under sub-Gaussian noise and global convergence under the
generalized Bregman Proximal Polyak-{\L}ojasiewicz condition. Additionally, we
illustrate the advantages of our improved SMD theory in various nonconvex
machine learning tasks by harnessing nonsmooth DGFs. Notably, in the context of
nonconvex differentially private (DP) learning, our theory yields a simple
algorithm with a (nearly) dimension-independent utility bound. For the problem
of training linear neural networks, we develop provably convergent stochastic
algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at AISTATS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The SMART approach to instance-optimal online learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17720v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17720v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddhartha Banerjee, Alankrita Bhatt, Christina Lee Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We devise an online learning algorithm -- titled Switching via Monotone
Adapted Regret Traces (SMART) -- that adapts to the data and achieves regret
that is instance optimal, i.e., simultaneously competitive on every input
sequence compared to the performance of the follow-the-leader (FTL) policy and
the worst case guarantee of any other input policy. We show that the regret of
the SMART policy on any input sequence is within a multiplicative factor
$e/(e-1) \approx 1.58$ of the smaller of: 1) the regret obtained by FTL on the
sequence, and 2) the upper bound on regret guaranteed by the given worst-case
policy. This implies a strictly stronger guarantee than typical
`best-of-both-worlds' bounds as the guarantee holds for every input sequence
regardless of how it is generated. SMART is simple to implement as it begins by
playing FTL and switches at most once during the time horizon to the worst-case
algorithm. Our approach and results follow from an operational reduction of
instance optimal online learning to competitive analysis for the ski-rental
problem. We complement our competitive ratio upper bounds with a fundamental
lower bound showing that over all input sequences, no algorithm can get better
than a $1.43$-fraction of the minimum regret achieved by FTL and the
minimax-optimal policy. We also present a modification of SMART that combines
FTL with a ``small-loss" algorithm to achieve instance optimality between the
regret of FTL and the small loss regret bound.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a Digital Twin Framework in Additive Manufacturing: Machine
  Learning and Bayesian Optimization for Time Series Process Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vispi Karkaria, Anthony Goeckner, Rujing Zha, Jie Chen, Jianjing Zhang, Qi Zhu, Jian Cao, Robert X. Gao, Wei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Laser-directed-energy deposition (DED) offers advantages in additive
manufacturing (AM) for creating intricate geometries and material grading. Yet,
challenges like material inconsistency and part variability remain, mainly due
to its layer-wise fabrication. A key issue is heat accumulation during DED,
which affects the material microstructure and properties. While closed-loop
control methods for heat management are common in DED research, few integrate
real-time monitoring, physics-based modeling, and control in a unified
framework. Our work presents a digital twin (DT) framework for real-time
predictive control of DED process parameters to meet specific design
objectives. We develop a surrogate model using Long Short-Term Memory
(LSTM)-based machine learning with Bayesian Inference to predict temperatures
in DED parts. This model predicts future temperature states in real time. We
also introduce Bayesian Optimization (BO) for Time Series Process Optimization
(BOTSPO), based on traditional BO but featuring a unique time series process
profile generator with reduced dimensions. BOTSPO dynamically optimizes
processes, identifying optimal laser power profiles to attain desired
mechanical properties. The established process trajectory guides online
optimizations, aiming to enhance performance. This paper outlines the digital
twin framework's components, promoting its integration into a comprehensive
system for AM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 Pages, 10 Figures, 1 Table, NAMRC Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Neural Network Binarization with Forward and Backward
  Proximal Quantizers <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17710v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17710v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwei Lu, Yaoliang Yu, Xinlin Li, Vahid Partovi Nia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In neural network binarization, BinaryConnect (BC) and its variants are
considered the standard. These methods apply the sign function in their forward
pass and their respective gradients are backpropagated to update the weights.
However, the derivative of the sign function is zero whenever defined, which
consequently freezes training. Therefore, implementations of BC (e.g., BNN)
usually replace the derivative of sign in the backward computation with
identity or other approximate gradient alternatives. Although such practice
works well empirically, it is largely a heuristic or ''training trick.'' We aim
at shedding some light on these training tricks from the optimization
perspective. Building from existing theory on ProxConnect (PC, a generalization
of BC), we (1) equip PC with different forward-backward quantizers and obtain
ProxConnect++ (PC++) that includes existing binarization techniques as special
cases; (2) derive a principled way to synthesize forward-backward quantizers
with automatic theoretical guarantees; (3) illustrate our theory by proposing
an enhanced binarization algorithm BNN++; (4) conduct image classification
experiments on CNNs and vision transformers, and empirically verify that BNN++
generally achieves competitive results on binarizing these models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Learning for Estimating Heterogeneous Treatment Effects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17705v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17705v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Disha Makhija, Joydeep Ghosh, Yejin Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning methods for estimating heterogeneous treatment effects (HTE)
facilitate large-scale personalized decision-making across various domains such
as healthcare, policy making, education, and more. Current machine learning
approaches for HTE require access to substantial amounts of data per treatment,
and the high costs associated with interventions makes centrally collecting so
much data for each intervention a formidable challenge. To overcome this
obstacle, in this work, we propose a novel framework for collaborative learning
of HTE estimators across institutions via Federated Learning. We show that even
under a diversity of interventions and subject populations across clients, one
can jointly learn a common feature representation, while concurrently and
privately learning the specific predictive functions for outcomes under
distinct interventions across institutions. Our framework and the associated
algorithm are based on this insight, and leverage tabular transformers to map
multiple input data to feature representations which are then used for outcome
prediction via multi-task learning. We also propose a novel way of federated
training of personalised transformers that can work with heterogeneous input
feature spaces. Experimental results on real-world clinical trial data
demonstrate the effectiveness of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transfer Learning Bayesian Optimization to Design Competitor DNA
  Molecules for Use in Diagnostic Assays 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17704v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17704v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruby Sedgwick, John P. Goertz, Molly M. Stevens, Ruth Misener, Mark van der Wilk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rise in engineered biomolecular devices, there is an increased need
for tailor-made biological sequences. Often, many similar biological sequences
need to be made for a specific application meaning numerous, sometimes
prohibitively expensive, lab experiments are necessary for their optimization.
This paper presents a transfer learning design of experiments workflow to make
this development feasible. By combining a transfer learning surrogate model
with Bayesian optimization, we show how the total number of experiments can be
reduced by sharing information between optimization tasks. We demonstrate the
reduction in the number of experiments using data from the development of DNA
competitors for use in an amplification-based diagnostic assay. We use
cross-validation to compare the predictive accuracy of different transfer
learning models, and then compare the performance of the models for both single
objective and penalized optimization tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time Low-latency Music Source Separation using Hybrid
  Spectrogram-TasNet <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Satvik Venkatesh, Arthur Benilov, Philip Coleman, Frederic Roskam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There have been significant advances in deep learning for music demixing in
recent years. However, there has been little attention given to how these
neural networks can be adapted for real-time low-latency applications, which
could be helpful for hearing aids, remixing audio streams and live shows. In
this paper, we investigate the various challenges involved in adapting current
demixing models in the literature for this use case. Subsequently, inspired by
the Hybrid Demucs architecture, we propose the Hybrid Spectrogram Time-domain
Audio Separation Network HS-TasNet, which utilises the advantages of spectral
and waveform domains. For a latency of 23 ms, the HS-TasNet obtains an overall
signal-to-distortion ratio (SDR) of 4.65 on the MusDB test set, and increases
to 5.55 with additional training data. These results demonstrate the potential
of efficient demixing for real-time low-latency music applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAVEL: Evaluating Interpretability Methods on Disentangling Language
  Model Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17700v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17700v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Huang, Zhengxuan Wu, Christopher Potts, Mor Geva, Atticus Geiger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Individual neurons participate in the representation of multiple high-level
concepts. To what extent can different interpretability methods successfully
disentangle these roles? To help address this question, we introduce RAVEL
(Resolving Attribute-Value Entanglements in Language Models), a dataset that
enables tightly controlled, quantitative comparisons between a variety of
existing interpretability methods. We use the resulting conceptual framework to
define the new method of Multi-task Distributed Alignment Search (MDAS), which
allows us to find distributed representations satisfying multiple causal
criteria. With Llama2-7B as the target language model, MDAS achieves
state-of-the-art results on RAVEL, demonstrating the importance of going beyond
neuron-level analyses to identify features distributed across activations. We
release our benchmark at https://github.com/explanare/ravel.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gradient-based Discrete Sampling with Automatic Cyclical Scheduling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Pynadath, Riddhiman Bhattacharya, Arun Hariharan, Ruqi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discrete distributions, particularly in high-dimensional deep models, are
often highly multimodal due to inherent discontinuities. While gradient-based
discrete sampling has proven effective, it is susceptible to becoming trapped
in local modes due to the gradient information. To tackle this challenge, we
propose an automatic cyclical scheduling, designed for efficient and accurate
sampling in multimodal discrete distributions. Our method contains three key
components: (1) a cyclical step size schedule where large steps discover new
modes and small steps exploit each mode; (2) a cyclical balancing schedule,
ensuring ``balanced" proposals for given step sizes and high efficiency of the
Markov chain; and (3) an automatic tuning scheme for adjusting the
hyperparameters in the cyclical schedules, allowing adaptability across diverse
datasets with minimal tuning. We prove the non-asymptotic convergence and
inference guarantee for our method in general discrete distributions. Extensive
experiments demonstrate the superiority of our method in sampling complex
multimodal discrete distributions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning reduced-order Quadratic-Linear models in Process Engineering
  using Operator Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17698v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17698v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ion Victor Gosea, Luisa Peterson, Pawan Goyal, Jens Bremer, Kai Sundmacher, Peter Benner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we address the challenge of efficiently modeling dynamical
systems in process engineering. We use reduced-order model learning,
specifically operator inference. This is a non-intrusive, data-driven method
for learning dynamical systems from time-domain data. The application in our
study is carbon dioxide methanation, an important reaction within the
Power-to-X framework, to demonstrate its potential. The numerical results show
the ability of the reduced-order models constructed with operator inference to
provide a reduced yet accurate surrogate solution. This represents an important
milestone towards the implementation of fast and reliable digital twin
architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Geometric Deep Learning for Computer-Aided Design: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Negar Heidari, Alexandros Iosifidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geometric Deep Learning techniques have become a transformative force in the
field of Computer-Aided Design (CAD), and have the potential to revolutionize
how designers and engineers approach and enhance the design process. By
harnessing the power of machine learning-based methods, CAD designers can
optimize their workflows, save time and effort while making better informed
decisions, and create designs that are both innovative and practical. The
ability to process the CAD designs represented by geometric data and to analyze
their encoded features enables the identification of similarities among diverse
CAD models, the proposition of alternative designs and enhancements, and even
the generation of novel design alternatives. This survey offers a comprehensive
overview of learning-based methods in computer-aided design across various
categories, including similarity analysis and retrieval, 2D and 3D CAD model
synthesis, and CAD generation from point clouds. Additionally, it provides a
complete list of benchmark datasets and their characteristics, along with
open-source codes that have propelled research in this domain. The final
discussion delves into the challenges prevalent in this field, followed by
potential future research directions in this rapidly evolving field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 14 figures, journal article</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Autonomous Vehicles: Evolution of Artificial Intelligence and Learning
  Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sneha Sudhir Shetiya, Divya Garikapati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of autonomous vehicles has heralded a transformative era in
transportation, reshaping the landscape of mobility through cutting-edge
technologies. Central to this evolu- tion is the integration of Artificial
Intelligence (AI) and learning algorithms, propelling vehicles into realms of
unprecedented autonomy. This paper provides a comprehensive exploration of the
evolutionary trajectory of AI within autonomous vehicles, tracing the journey
from foundational principles to the most recent advancements. Commencing with a
current landscape overview, the paper delves into the fundamental role of AI in
shaping the autonomous decision-making capabilities of vehicles. It elucidates
the steps involved in the AI-powered development life cycle in vehicles,
addressing ethical considerations and bias in AI-driven software development
for autonomous vehicles. The study presents statis- tical insights into the
usage and types of AI/learning algorithms over the years, showcasing the
evolving research landscape within the automotive industry. Furthermore, the
paper highlights the pivotal role of parameters in refining algorithms for both
trucks and cars, facilitating vehicles to adapt, learn, and improve performance
over time. It concludes by outlining different levels of autonomy, elucidating
the nuanced usage of AI and learning algorithms, and automating key tasks at
each level. Additionally, the document discusses the variation in software
package sizes across different autonomy levels
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QoS prediction in radio vehicular environments via prior user
  information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noor Ul Ain, Rodrigo Hernangómez, Alexandros Palaios, Martin Kasparick, Sławomir Stańczak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliable wireless communications play an important role in the automotive
industry as it helps to enhance current use cases and enable new ones such as
connected autonomous driving, platooning, cooperative maneuvering, teleoperated
driving, and smart navigation. These and other use cases often rely on specific
quality of service (QoS) levels for communication. Recently, the area of
predictive quality of service (QoS) has received a great deal of attention as a
key enabler to forecast communication quality well enough in advance. However,
predicting QoS in a reliable manner is a notoriously difficult task. In this
paper, we evaluate ML tree-ensemble methods to predict QoS in the range of
minutes with data collected from a cellular test network. We discuss radio
environment characteristics and we showcase how these can be used to improve ML
performance and further support the uptake of ML in commercial networks.
Specifically, we use the correlations of the measurements coming from the radio
environment by including information of prior vehicles to enhance the
prediction of the target vehicles. Moreover, we are extending prior art by
showing how longer prediction horizons can be supported.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Outlier-Detection for Reactive Machine Learned Potential Energy Surfaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17686v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17686v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luis Itza Vazquez-Salazar, Silvan Käser, Markus Meuwly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncertainty quantification (UQ) to detect samples with large expected errors
(outliers) is applied to reactive molecular potential energy surfaces (PESs).
Three methods - Ensembles, Deep Evidential Regression (DER), and Gaussian
Mixture Models (GMM) - were applied to the H-transfer reaction between ${\it
syn-}$Criegee and vinyl hydroxyperoxide. The results indicate that ensemble
models provide the best results for detecting outliers, followed by GMM. For
example, from a pool of 1000 structures with the largest uncertainty, the
detection quality for outliers is $\sim 90$ \% and $\sim 50$ \%, respectively,
if 25 or 1000 structures with large errors are sought. On the contrary, the
limitations of the statistical assumptions of DER greatly impacted its
prediction capabilities. Finally, a structure-based indicator was found to be
correlated with large average error, which may help to rapidly classify new
structures into those that provide an advantage for refining the neural
network.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Securing Reliability: A Brief <span class="highlight-title">Overview</span> on Enhancing In-Context Learning
  for Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17671v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17671v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunpeng Huang, Yaonan Gu, Jingwei Xu, Zhihong Zhu, Zhaorun Chen, Xiaoxing Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As foundation models (FMs) continue to shape the landscape of AI, the
in-context learning (ICL) paradigm thrives but also encounters issues such as
toxicity, hallucination, disparity, adversarial vulnerability, and
inconsistency. Ensuring the reliability and responsibility of FMs is crucial
for the sustainable development of the AI ecosystem. In this concise overview,
we investigate recent advancements in enhancing the reliability and
trustworthiness of FMs within ICL frameworks, focusing on four key
methodologies, each with its corresponding subgoals. We sincerely hope this
paper can provide valuable insights for researchers and practitioners
endeavoring to build safe and dependable FMs and foster a stable and consistent
ICL environment, thereby unlocking their vast potential.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Agent Deep Reinforcement Learning for Distributed Satellite
  Routing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17666v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17666v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Lozano-Cuadra, Beatriz Soret
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a Multi-Agent Deep Reinforcement Learning (MA-DRL)
approach for routing in Low Earth Orbit Satellite Constellations (LSatCs). Each
satellite is an independent decision-making agent with a partial knowledge of
the environment, and supported by feedback received from the nearby agents.
Building on our previous work that introduced a Q-routing solution, the
contribution of this paper is to extend it to a deep learning framework able to
quickly adapt to the network and traffic changes, and based on two phases: (1)
An offline exploration learning phase that relies on a global Deep Neural
Network (DNN) to learn the optimal paths at each possible position and
congestion level; (2) An online exploitation phase with local, on-board,
pre-trained DNNs. Results show that MA-DRL efficiently learns optimal routes
offline that are then loaded for an efficient distributed routing online.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TorchMD-Net 2.0: Fast Neural Network Potentials for Molecular
  Simulations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raul P. Pelaez, Guillem Simeon, Raimondas Galvelis, Antonio Mirarchi, Peter Eastman, Stefan Doerr, Philipp Thölke, Thomas E. Markland, Gianni De Fabritiis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving a balance between computational speed, prediction accuracy, and
universal applicability in molecular simulations has been a persistent
challenge. This paper presents substantial advancements in the TorchMD-Net
software, a pivotal step forward in the shift from conventional force fields to
neural network-based potentials. The evolution of TorchMD-Net into a more
comprehensive and versatile framework is highlighted, incorporating
cutting-edge architectures such as TensorNet. This transformation is achieved
through a modular design approach, encouraging customized applications within
the scientific community. The most notable enhancement is a significant
improvement in computational efficiency, achieving a very remarkable
acceleration in the computation of energy and forces for TensorNet models, with
performance gains ranging from 2-fold to 10-fold over previous iterations.
Other enhancements include highly optimized neighbor search algorithms that
support periodic boundary conditions and the smooth integration with existing
molecular dynamics frameworks. Additionally, the updated version introduces the
capability to integrate physical priors, further enriching its application
spectrum and utility in research. The software is available at
https://github.com/torchmd/torchmd-net.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Confidence-Aware Multi-Field Model Calibration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17655v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17655v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuang Zhao, Chuhan Wu, Qinglin Jia, Hong Zhu, Jia Yan, Libin Zong, Linxuan Zhang, Zhenhua Dong, Muyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately predicting the probabilities of user feedback, such as clicks and
conversions, is critical for ad ranking and bidding. However, there often exist
unwanted mismatches between predicted probabilities and true likelihoods due to
the shift of data distributions and intrinsic model biases. Calibration aims to
address this issue by post-processing model predictions, and field-aware
calibration can adjust model output on different feature field values to
satisfy fine-grained advertising demands. Unfortunately, the observed samples
corresponding to certain field values can be too limited to make confident
calibrations, which may yield bias amplification and online disturbance. In
this paper, we propose a confidence-aware multi-field calibration method, which
adaptively adjusts the calibration intensity based on the confidence levels
derived from sample statistics. It also utilizes multiple feature fields for
joint model calibration with awareness of their importance to mitigate the data
sparsity effect of a single field. Extensive offline and online experiments
show the superiority of our method in boosting advertising performance and
reducing prediction deviations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Variational Learning is Effective for Large Deep Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17641v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17641v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuesong Shen, Nico Daheim, Bai Cong, Peter Nickl, Gian Maria Marconi, Clement Bazan, Rio Yokota, Iryna Gurevych, Daniel Cremers, Mohammad Emtiyaz Khan, Thomas Möllenhoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We give extensive empirical evidence against the common belief that
variational learning is ineffective for large neural networks. We show that an
optimizer called Improved Variational Online Newton (IVON) consistently matches
or outperforms Adam for training large networks such as GPT-2 and ResNets from
scratch. IVON's computational costs are nearly identical to Adam but its
predictive uncertainty is better. We show several new use cases of IVON where
we improve fine-tuning and model merging in Large Language Models, accurately
predict generalization error, and faithfully estimate sensitivity to data. We
find overwhelming evidence in support of effectiveness of variational learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally. Code is available here:
  https://github.com/team-approx-bayes/ivon</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Supervised machine learning for microbiomics: bridging the gap between
  current and best practices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Natasha K. Dudek, Mariam Chakhvadze, Saba Kobakhidze, Omar Kantidze, Yuriy Gankin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) is set to accelerate innovations in clinical
microbiomics, such as in disease diagnostics and prognostics. This will require
high-quality, reproducible, interpretable workflows whose predictive
capabilities meet or exceed the high thresholds set for clinical tools by
regulatory agencies. Here, we capture a snapshot of current practices in the
application of supervised ML to microbiomics data, through an in-depth analysis
of 100 peer-reviewed journal articles published in 2021-2022. We apply a
data-driven approach to steer discussion of the merits of varied approaches to
experimental design, including key considerations such as how to mitigate the
effects of small dataset size while avoiding data leakage. We further provide
guidance on how to avoid common experimental design pitfalls that can hurt
model performance, trustworthiness, and reproducibility. Discussion is
accompanied by an interactive online tutorial that demonstrates foundational
principles of ML experimental design, tailored to the microbiomics community.
Formalizing community best practices for supervised ML in microbiomics is an
important step towards improving the success and efficiency of clinical
research, to the benefit of patients and other stakeholders.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Topological Representations with Bidirectional Graph Attention
  Network for Solving Job Shop Scheduling Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cong Zhang, Zhiguang Cao, Yaoxin Wu, Wen Song, Jing Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing learning-based methods for solving job shop scheduling problem
(JSSP) usually use off-the-shelf GNN models tailored to undirected graphs and
neglect the rich and meaningful topological structures of disjunctive graphs
(DGs). This paper proposes the topology-aware bidirectional graph attention
network (TBGAT), a novel GNN architecture based on the attention mechanism, to
embed the DG for solving JSSP in a local search framework. Specifically, TBGAT
embeds the DG from a forward and a backward view, respectively, where the
messages are propagated by following the different topologies of the views and
aggregated via graph attention. Then, we propose a novel operator based on the
message-passing mechanism to calculate the forward and backward topological
sorts of the DG, which are the features for characterizing the topological
structures and exploited by our model. In addition, we theoretically and
experimentally show that TBGAT has linear computational complexity to the
number of jobs and machines, respectively, which strengthens the practical
value of our method. Besides, extensive experiments on five synthetic datasets
and seven classic benchmarks show that TBGAT achieves new SOTA results by
outperforming a wide range of neural methods by a large margin.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing sleep detection by modelling weak label sets: A novel weakly
  supervised learning approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17601v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17601v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthias Boeker, Vajira Thambawita, Michael Riegler, Pål Halvorsen, Hugo L. Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding sleep and activity patterns plays a crucial role in physical
and mental health. This study introduces a novel approach for sleep detection
using weakly supervised learning for scenarios where reliable ground truth
labels are unavailable. The proposed method relies on a set of weak labels,
derived from the predictions generated by conventional sleep detection
algorithms. Introducing a novel approach, we suggest a novel generalised
non-linear statistical model in which the number of weak sleep labels is
modelled as outcome of a binomial distribution. The probability of sleep in the
binomial distribution is linked to the outcomes of neural networks trained to
detect sleep based on actigraphy. We show that maximizing the likelihood
function of the model, is equivalent to minimizing the soft cross-entropy loss.
Additionally, we explored the use of the Brier score as a loss function for
weak labels. The efficacy of the suggested modelling framework was demonstrated
using the Multi-Ethnic Study of Atherosclerosis dataset. A \gls{lstm} trained
on the soft cross-entropy outperformed conventional sleep detection algorithms,
other neural network architectures and loss functions in accuracy and model
calibration. This research not only advances sleep detection techniques in
scenarios where ground truth data is scarce but also contributes to the broader
field of weakly supervised learning by introducing innovative approach in
modelling sets of weak labels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Implicit Regularization via Spectral Neural Networks and Non-linear
  Matrix Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong T. M. Chu, Subhro Ghosh, Chi Thanh Lam, Soumendu Sundar Mukherjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The phenomenon of implicit regularization has attracted interest in recent
years as a fundamental aspect of the remarkable generalizing ability of neural
networks. In a nutshell, it entails that gradient descent dynamics in many
neural nets, even without any explicit regularizer in the loss function,
converges to the solution of a regularized learning problem. However, known
results attempting to theoretically explain this phenomenon focus
overwhelmingly on the setting of linear neural nets, and the simplicity of the
linear structure is particularly crucial to existing arguments. In this paper,
we explore this problem in the context of more realistic neural networks with a
general class of non-linear activation functions, and rigorously demonstrate
the implicit regularization phenomenon for such networks in the setting of
matrix sensing problems, together with rigorous rate guarantees that ensure
exponentially fast convergence of gradient descent.In this vein, we contribute
a network architecture called Spectral Neural Networks (abbrv. SNN) that is
particularly suitable for matrix learning problems. Conceptually, this entails
coordinatizing the space of matrices by their singular values and singular
vectors, as opposed to by their entries, a potentially fruitful perspective for
matrix learning. We demonstrate that the SNN architecture is inherently much
more amenable to theoretical analysis than vanilla neural nets and confirm its
effectiveness in the context of matrix sensing, via both mathematical
guarantees and empirical investigations. We believe that the SNN architecture
has the potential to be of wide applicability in a broad class of matrix
learning scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FaultProfIT: Hierarchical Fault Profiling of Incident Tickets in
  Large-scale Cloud Systems <span class="chip">ICSE</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Huang, Jinyang Liu, Zhuangbin Chen, Zhihan Jiang, Yichen LI, Jiazhen Gu, Cong Feng, Zengyin Yang, Yongqiang Yang, Michael R. Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Postmortem analysis is essential in the management of incidents within cloud
systems, which provides valuable insights to improve system's reliability and
robustness. At CloudA, fault pattern profiling is performed during the
postmortem phase, which involves the classification of incidents' faults into
unique categories, referred to as fault pattern. By aggregating and analyzing
these fault patterns, engineers can discern common faults, vulnerable
components and emerging fault trends. However, this process is currently
conducted by manual labeling, which has inherent drawbacks. On the one hand,
the sheer volume of incidents means only the most severe ones are analyzed,
causing a skewed overview of fault patterns. On the other hand, the complexity
of the task demands extensive domain knowledge, which leads to errors and
inconsistencies. To address these limitations, we propose an automated
approach, named FaultProfIT, for Fault pattern Profiling of Incident Tickets.
It leverages hierarchy-guided contrastive learning to train a hierarchy-aware
incident encoder and predicts fault patterns with enhanced incident
representations. We evaluate FaultProfIT using the production incidents from
CloudA. The results demonstrate that FaultProfIT outperforms state-of-the-art
methods. Our ablation study and analysis also verify the effectiveness of
hierarchy-guided contrastive learning. Additionally, we have deployed
FaultProfIT at CloudA for six months. To date, FaultProfIT has analyzed 10,000+
incidents from 30+ cloud services, successfully revealing several fault trends
that have informed system improvements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Proceedings of the 46th International Conference on
  Software Engineering: Software Engineering in Practice (ICSE SEIP 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hyperdimensional computing: a fast, robust and interpretable paradigm
  for biological data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michiel Stock, Dimitri Boeckaerts, Pieter Dewulf, Steff Taelman, Maxime Van Haeverbeke, Wim Van Criekinge, Bernard De Baets
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in bioinformatics are primarily due to new algorithms for processing
diverse biological data sources. While sophisticated alignment algorithms have
been pivotal in analyzing biological sequences, deep learning has substantially
transformed bioinformatics, addressing sequence, structure, and functional
analyses. However, these methods are incredibly data-hungry, compute-intensive
and hard to interpret. Hyperdimensional computing (HDC) has recently emerged as
an intriguing alternative. The key idea is that random vectors of high
dimensionality can represent concepts such as sequence identity or phylogeny.
These vectors can then be combined using simple operators for learning,
reasoning or querying by exploiting the peculiar properties of high-dimensional
spaces. Our work reviews and explores the potential of HDC for bioinformatics,
emphasizing its efficiency, interpretability, and adeptness in handling
multimodal and structured data. HDC holds a lot of potential for various omics
data searching, biosignal analysis and health applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse Variational Contaminated Noise Gaussian Process Regression for
  Forecasting Geomagnetic Perturbations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Iong, Matthew McAnear, Yuezhou Qu, Shasha Zou, Gabor Toth Yang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaussian Processes (GP) have become popular machine learning methods for
kernel based learning on datasets with complicated covariance structures. In
this paper, we present a novel extension to the GP framework using a
contaminated normal likelihood function to better account for heteroscedastic
variance and outlier noise. We propose a scalable inference algorithm based on
the Sparse Variational Gaussian Process (SVGP) method for fitting sparse
Gaussian process regression models with contaminated normal noise on large
datasets. We examine an application to geomagnetic ground perturbations, where
the state-of-art prediction model is based on neural networks. We show that our
approach yields shorter predictions intervals for similar coverage and accuracy
when compared to an artificial dense neural network baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluation of Predictive Reliability to Foster Trust in Artificial
  Intelligence. A case study in Multiple Sclerosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17554v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17554v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Peracchio, Giovanna Nicora, Enea Parimbelli, Tommaso Mario Buonocore, Roberto Bergamaschi, Eleonora Tavazzi, Arianna Dagliati, Riccardo Bellazzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Applying Artificial Intelligence (AI) and Machine Learning (ML) in critical
contexts, such as medicine, requires the implementation of safety measures to
reduce risks of harm in case of prediction errors. Spotting ML failures is of
paramount importance when ML predictions are used to drive clinical decisions.
ML predictive reliability measures the degree of trust of a ML prediction on a
new instance, thus allowing decision-makers to accept or reject it based on its
reliability. To assess reliability, we propose a method that implements two
principles. First, our approach evaluates whether an instance to be classified
is coming from the same distribution of the training set. To do this, we
leverage Autoencoders (AEs) ability to reconstruct the training set with low
error. An instance is considered Out-of-Distribution (OOD) if the AE
reconstructs it with a high error. Second, it is evaluated whether the ML
classifier has good performances on samples similar to the newly classified
instance by using a proxy model. We show that this approach is able to assess
reliability both in a simulated scenario and on a model trained to predict
disease progression of Multiple Sclerosis patients. We also developed a Python
package, named relAI, to embed reliability measures into ML pipelines. We
propose a simple approach that can be used in the deployment phase of any ML
model to suggest whether to trust predictions or not. Our method holds the
promise to provide effective support to clinicians by spotting potential ML
failures during deployment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adapting Learned Image Codecs to Screen Content via Adjustable
  Transformations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17544v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17544v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        H. Burak Dogaroglu, A. Burakhan Koyuncu, Atanas Boev, Elena Alshina, Eckehard Steinbach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As learned image codecs (LICs) become more prevalent, their low coding
efficiency for out-of-distribution data becomes a bottleneck for some
applications. To improve the performance of LICs for screen content (SC) images
without breaking backwards compatibility, we propose to introduce parameterized
and invertible linear transformations into the coding pipeline without changing
the underlying baseline codec's operation flow. We design two neural networks
to act as prefilters and postfilters in our setup to increase the coding
efficiency and help with the recovery from coding artifacts. Our end-to-end
trained solution achieves up to 10% bitrate savings on SC compression compared
to the baseline LICs while introducing only 1% extra parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Label-Noise Robust Diffusion Models <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17517v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17517v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Byeonghu Na, Yeongmin Kim, HeeSun Bae, Jung Hyun Lee, Se Jung Kwon, Wanmo Kang, Il-Chul Moon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conditional diffusion models have shown remarkable performance in various
generative tasks, but training them requires large-scale datasets that often
contain noise in conditional inputs, a.k.a. noisy labels. This noise leads to
condition mismatch and quality degradation of generated data. This paper
proposes Transition-aware weighted Denoising Score Matching (TDSM) for training
conditional diffusion models with noisy labels, which is the first study in the
line of diffusion models. The TDSM objective contains a weighted sum of score
networks, incorporating instance-wise and time-dependent label transition
probabilities. We introduce a transition-aware weight estimator, which
leverages a time-dependent noisy-label classifier distinctively customized to
the diffusion process. Through experiments across various datasets and noisy
label settings, TDSM improves the quality of generated samples aligned with
given conditions. Furthermore, our method improves generation performance even
on prevalent benchmark datasets, which implies the potential noisy labels and
their risk of generative model learning. Finally, we show the improved
performance of TDSM on top of conventional noisy label corrections, which
empirically proving its contribution as a part of label-noise robust generative
models. Our code is available at: https://github.com/byeonghu-na/tdsm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QUCE: The Minimisation and Quantification of Path-Based Uncertainty for
  Generative Counterfactual Explanations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jamie Duell, Hsuan Fu, Monika Seisenberger, Xiuyi Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Neural Networks (DNNs) stand out as one of the most prominent approaches
within the Machine Learning (ML) domain. The efficacy of DNNs has surged
alongside recent increases in computational capacity, allowing these approaches
to scale to significant complexities for addressing predictive challenges in
big data. However, as the complexity of DNN models rises, interpretability
diminishes. In response to this challenge, explainable models such as
Adversarial Gradient Integration (AGI) leverage path-based gradients provided
by DNNs to elucidate their decisions. Yet the performance of path-based
explainers can be compromised when gradients exhibit irregularities during
out-of-distribution path traversal. In this context, we introduce Quantified
Uncertainty Counterfactual Explanations (QUCE), a method designed to mitigate
out-of-distribution traversal by minimizing path uncertainty. QUCE not only
quantifies uncertainty when presenting explanations but also generates more
certain counterfactual examples. We showcase the performance of the QUCE method
by comparing it with competing methods for both path-based explanations and
generative counterfactual examples. The code repository for the QUCE method is
available at: https://github.com/jamie-duell/QUCE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Thermodynamics-informed super-resolution of scarce temporal dynamics
  data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Bermejo-Barbanoj, Beatriz Moya, Alberto Badías, Francisco Chinesta, Elías Cueto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a method to increase the resolution of measurements of a physical
system and subsequently predict its time evolution using thermodynamics-aware
neural networks. Our method uses adversarial autoencoders, which reduce the
dimensionality of the full order model to a set of latent variables that are
enforced to match a prior, for example a normal distribution. Adversarial
autoencoders are seen as generative models, and they can be trained to generate
high-resolution samples from low-resoution inputs, meaning they can address the
so-called super-resolution problem. Then, a second neural network is trained to
learn the physical structure of the latent variables and predict their temporal
evolution. This neural network is known as an structure-preserving neural
network. It learns the metriplectic-structure of the system and applies a
physical bias to ensure that the first and second principles of thermodynamics
are fulfilled. The integrated trajectories are decoded to their original
dimensionality, as well as to the higher dimensionality space produced by the
adversarial autoencoder and they are compared to the ground truth solution. The
method is tested with two examples of flow over a cylinder, where the fluid
properties are varied between both examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intensive Care as One Big Sequence Modeling Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vadim Liventsev, Tobias Fritz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning in Healthcare is typically concerned with narrow
self-contained tasks such as sepsis prediction or anesthesia control. However,
previous research has demonstrated the potential of generalist models (the
prime example being Large Language Models) to outperform task-specific
approaches due to their capability for implicit transfer learning. To enable
training of foundation models for Healthcare as well as leverage the
capabilities of state of the art Transformer architectures, we propose the
paradigm of Healthcare as Sequence Modeling, in which interaction between the
patient and the healthcare provider is represented as an event stream and tasks
like diagnosis and treatment selection are modeled as prediction of future
events in the stream. To explore this paradigm experimentally we develop
MIMIC-SEQ, a sequence modeling benchmark derived by translating heterogenous
clinical records from MIMIC-IV dataset into a uniform event stream format,
train a baseline model and explore its capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting Instability in Complex Oscillator Networks: Limitations and
  Potentials of Network Measures and Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17500v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17500v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Nauck, Michael Lindner, Nora Molkenthin, Jürgen Kurths, Eckehard Schöll, Jörg Raisch, Frank Hellmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A central question of network science is how functional properties of systems
arise from their structure. For networked dynamical systems, structure is
typically quantified with network measures. A functional property that is of
theoretical and practical interest for oscillatory systems is the stability of
synchrony to localized perturbations. Recently, Graph Neural Networks (GNNs)
have been shown to predict this stability successfully; at the same time,
network measures have struggled to paint a clear picture. Here we collect 46
relevant network measures and find that no small subset can reliably predict
stability. The performance of GNNs can only be matched by combining all network
measures and nodewise machine learning. However, unlike GNNs, this approach
fails to extrapolate from network ensembles to several real power grid
topologies. This suggests that correlations of network measures and function
may be misleading, and that GNNs capture the causal relationship between
structure and stability substantially better.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages (16 pages main section), 15 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ syren-halofit: A fast, interpretable, high-precision formula for the
  $Λ$CDM nonlinear matter power spectrum 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17492v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17492v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deaglan J. Bartlett, Benjamin D. Wandelt, Matteo Zennaro, Pedro G. Ferreira, Harry Desmond
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rapid and accurate evaluation of the nonlinear matter power spectrum, $P(k)$,
as a function of cosmological parameters and redshift is of fundamental
importance in cosmology. Analytic approximations provide an interpretable
solution, yet current approximations are neither fast nor accurate relative to
black-box numerical emulators. We use symbolic regression to obtain simple
analytic approximations to the nonlinear scale, $k_\sigma$, the effective
spectral index, $n_{\rm eff}$, and the curvature, $C$, which are required for
the halofit model. We then re-optimise the coefficients of halofit to fit a
wide range of cosmologies and redshifts. We then again exploit symbolic
regression to explore the space of analytic expressions to fit the residuals
between $P(k)$ and the optimised predictions of halofit. All methods are
validated against $N$-body simulations. Our symbolic expressions for
$k_\sigma$, $n_{\rm eff}$ and $C$ have root mean squared fractional errors of
0.8%, 0.2% and 0.3%, respectively, for redshifts below 3 and a wide range of
cosmologies. The re-optimised halofit parameters reduce the root mean squared
fractional error from 3% to below 2% for wavenumbers $k=9\times10^{-3}-9 \,
h{\rm Mpc^{-1}}$. We introduce syren-halofit (symbolic-regression-enhanced
halofit), an extension to halofit containing a short symbolic correction which
improves this error to 1%. Our method is 2350 and 3170 times faster than
current halofit and hmcode implementations, respectively, and 2680 and 64 times
faster than EuclidEmulator2 (which requires running class) and the BACCO
emulator. We obtain comparable accuracy to EuclidEmulator2 and the BACCO
emulator when tested on $N$-body simulations. Our work greatly increases the
speed and accuracy of symbolic approximations to $P(k)$, making them
significantly faster than their numerical counterparts without loss of
accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 figures. Submitted to A&A</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bit Rate Matching Algorithm Optimization in JPEG-AI Verification Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17487v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17487v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panqi Jia, A. Burakhan Koyuncu, Jue Mao, Ze Cui, Yi Ma, Tiansheng Guo, Timofey Solovyev, Alexander Karabutov, Yin Zhao, Jing Wang, Elena Alshina, Andre Kaup
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The research on neural network (NN) based image compression has shown
superior performance compared to classical compression frameworks. Unlike the
hand-engineered transforms in the classical frameworks, NN-based models learn
the non-linear transforms providing more compact bit representations, and
achieve faster coding speed on parallel devices over their classical
counterparts. Those properties evoked the attention of both scientific and
industrial communities, resulting in the standardization activity JPEG-AI. The
verification model for the standardization process of JPEG-AI is already in
development and has surpassed the advanced VVC intra codec. To generate
reconstructed images with the desired bits per pixel and assess the BD-rate
performance of both the JPEG-AI verification model and VVC intra, bit rate
matching is employed. However, the current state of the JPEG-AI verification
model experiences significant slowdowns during bit rate matching, resulting in
suboptimal performance due to an unsuitable model. The proposed methodology
offers a gradual algorithmic optimization for matching bit rates, resulting in
a fourfold acceleration and over 1% improvement in BD-rate at the base
operation point. At the high operation point, the acceleration increases up to
sixfold.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at (IEEE) PCS 2024; 6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fraud Detection with Binding Global and Local Relational Interaction <span class="chip">KDD 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17472v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17472v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haolin Li, Shuyang Jiang, Lifeng Zhang, Siyuan Du, Guangnan Ye, Hongfeng Chai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Network has been proved to be effective for fraud detection for
its capability to encode node interaction and aggregate features in a holistic
view. Recently, Transformer network with great sequence encoding ability, has
also outperformed other GNN-based methods in literatures. However, both
GNN-based and Transformer-based networks only encode one perspective of the
whole graph, while GNN encodes global features and Transformer network encodes
local ones. Furthermore, previous works ignored encoding global interaction
features of the heterogeneous graph with separate networks, thus leading to
suboptimal performance. In this work, we present a novel framework called
Relation-Aware GNN with transFormer (RAGFormer) which simultaneously embeds
local and global features into a target node. The simple yet effective network
applies a modified GAGA module where each transformer layer is followed by a
cross-relation aggregation layer, to encode local embeddings and node
interactions across different relations. Apart from the Transformer-based
network, we further introduce a Relation-Aware GNN module to learn global
embeddings, which is later merged into the local embeddings by an attention
fusion module and a skip connection. Extensive experiments on two popular
public datasets and an industrial dataset demonstrate that RAGFormer achieves
the state-of-the-art performance. Substantial analysis experiments validate the
effectiveness of each submodule of RAGFormer and its high efficiency in
utilizing small-scale data and low hyper-parameter sensitivity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review for SIGKDD 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bit Distribution Study and Implementation of Spatial Quality Map in the
  JPEG-AI Standardization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17470v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17470v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panqi Jia, Jue Mao, Esin Koyuncu, A. Burakhan Koyuncu, Timofey Solovyev, Alexander Karabutov, Yin Zhao, Elena Alshina, Andre Kaup
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Currently, there is a high demand for neural network-based image compression
codecs. These codecs employ non-linear transforms to create compact bit
representations and facilitate faster coding speeds on devices compared to the
hand-crafted transforms used in classical frameworks. The scientific and
industrial communities are highly interested in these properties, leading to
the standardization effort of JPEG-AI. The JPEG-AI verification model has been
released and is currently under development for standardization. Utilizing
neural networks, it can outperform the classic codec VVC intra by over 10%
BD-rate operating at base operation point. Researchers attribute this success
to the flexible bit distribution in the spatial domain, in contrast to VVC
intra's anchor that is generated with a constant quality point. However, our
study reveals that VVC intra displays a more adaptable bit distribution
structure through the implementation of various block sizes. As a result of our
observations, we have proposed a spatial bit allocation method to optimize the
JPEG-AI verification model's bit distribution and enhance the visual quality.
Furthermore, by applying the VVC bit distribution strategy, the objective
performance of JPEG-AI verification mode can be further improved, resulting in
a maximum gain of 0.45 dB in PSNR-Y.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Why do Learning Rates Transfer? Reconciling Optimization and Scaling
  Limits for Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Noci, Alexandru Meterez, Thomas Hofmann, Antonio Orvieto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there has been growing evidence that if the width and depth of a
neural network are scaled toward the so-called rich feature learning limit
($\mu$P and its depth extension), then some hyperparameters - such as the
learning rate - exhibit transfer from small to very large models, thus reducing
the cost of hyperparameter tuning. From an optimization perspective, this
phenomenon is puzzling, as it implies that the loss landscape is remarkably
consistent across very different model sizes. In this work, we find empirical
evidence that learning rate transfer can be attributed to the fact that under
$\mu$P and its depth extension, the largest eigenvalue of the training loss
Hessian (i.e. the sharpness) is largely independent of the width and depth of
the network for a sustained period of training time. On the other hand, we show
that under the neural tangent kernel (NTK) regime, the sharpness exhibits very
different dynamics at different scales, thus preventing learning rate transfer.
But what causes these differences in the sharpness dynamics? Through a
connection between the spectra of the Hessian and the NTK matrix, we argue that
the cause lies in the presence (for $\mu$P) or progressive absence (for the NTK
regime) of feature learning, which results in a different evolution of the NTK,
and thus of the sharpness. We corroborate our claims with a substantial suite
of experiments, covering a wide range of datasets and architectures: from
ResNets and Vision Transformers trained on benchmark vision datasets to
Transformers-based language models trained on WikiText
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DS-Agent: Automated Data Science by Empowering Large Language Models
  with Case-Based Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17453v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17453v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyuan Guo, Cheng Deng, Ying Wen, Hechang Chen, Yi Chang, Jun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we investigate the potential of large language models (LLMs)
based agents to automate data science tasks, with the goal of comprehending
task requirements, then building and training the best-fit machine learning
models. Despite their widespread success, existing LLM agents are hindered by
generating unreasonable experiment plans within this scenario. To this end, we
present DS-Agent, a novel automatic framework that harnesses LLM agent and
case-based reasoning (CBR). In the development stage, DS-Agent follows the CBR
framework to structure an automatic iteration pipeline, which can flexibly
capitalize on the expert knowledge from Kaggle, and facilitate consistent
performance improvement through the feedback mechanism. Moreover, DS-Agent
implements a low-resource deployment stage with a simplified CBR paradigm to
adapt past successful solutions from the development stage for direct code
generation, significantly reducing the demand on foundational capabilities of
LLMs. Empirically, DS-Agent with GPT-4 achieves an unprecedented 100% success
rate in the development stage, while attaining 36% improvement on average one
pass rate across alternative LLMs in the deployment stage. In both stages,
DS-Agent achieves the best rank in performance, costing \$1.60 and \$0.13 per
run with GPT-4, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Principled Architecture-aware Scaling of Hyperparameters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17440v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17440v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wuyang Chen, Junru Wu, Zhangyang Wang, Boris Hanin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training a high-quality deep neural network requires choosing suitable
hyperparameters, which is a non-trivial and expensive process. Current works
try to automatically optimize or design principles of hyperparameters, such
that they can generalize to diverse unseen scenarios. However, most designs or
optimization methods are agnostic to the choice of network structures, and thus
largely ignore the impact of neural architectures on hyperparameters. In this
work, we precisely characterize the dependence of initializations and maximal
learning rates on the network architecture, which includes the network depth,
width, convolutional kernel size, and connectivity patterns. By pursuing every
parameter to be maximally updated with the same mean squared change in
pre-activations, we can generalize our initialization and learning rates across
MLPs (multi-layer perception) and CNNs (convolutional neural network) with
sophisticated graph topologies. We verify our principles with comprehensive
experiments. More importantly, our strategy further sheds light on advancing
current benchmarks for architecture design. A fair comparison of AutoML
algorithms requires accurate network rankings. However, we demonstrate that
network rankings can be easily changed by better training networks in
benchmarks with our architecture-aware learning rates and initialization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The KANDY Benchmark: Incremental Neuro-Symbolic Learning and Reasoning
  with Kandinsky Patterns 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17431v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17431v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Salvatore Lorello, Marco Lippi, Stefano Melacci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence is continuously seeking novel challenges and
benchmarks to effectively measure performance and to advance the
state-of-the-art. In this paper we introduce KANDY, a benchmarking framework
that can be used to generate a variety of learning and reasoning tasks inspired
by Kandinsky patterns. By creating curricula of binary classification tasks
with increasing complexity and with sparse supervisions, KANDY can be used to
implement benchmarks for continual and semi-supervised learning, with a
specific focus on symbol compositionality. Classification rules are also
provided in the ground truth to enable analysis of interpretable solutions.
Together with the benchmark generation pipeline, we release two curricula, an
easier and a harder one, that we propose as new challenges for the research
community. With a thorough experimental evaluation, we show how both
state-of-the-art neural models and purely symbolic approaches struggle with
solving most of the tasks, thus calling for the application of advanced
neuro-symbolic methods trained over time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforced In-Context Black-Box Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17423v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17423v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Song, Chenxiao Gao, Ke Xue, Chenyang Wu, Dong Li, Jianye Hao, Zongzhang Zhang, Chao Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Black-Box Optimization (BBO) has found successful applications in many fields
of science and engineering. Recently, there has been a growing interest in
meta-learning particular components of BBO algorithms to speed up optimization
and get rid of tedious hand-crafted heuristics. As an extension, learning the
entire algorithm from data requires the least labor from experts and can
provide the most flexibility. In this paper, we propose RIBBO, a method to
reinforce-learn a BBO algorithm from offline data in an end-to-end fashion.
RIBBO employs expressive sequence models to learn the optimization histories
produced by multiple behavior algorithms and tasks, leveraging the in-context
learning ability of large models to extract task information and make decisions
accordingly. Central to our method is to augment the optimization histories
with regret-to-go tokens, which are designed to represent the performance of an
algorithm based on cumulative regret of the histories. The integration of
regret-to-go tokens enables RIBBO to automatically generate sequences of query
points that satisfy the user-desired regret, which is verified by its
universally good empirical performance on diverse problems, including BBOB
functions, hyper-parameter optimization and robot control problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A novel image space formalism of Fourier domain interpolation neural
  networks for noise propagation analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17410v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17410v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Dawood, Felix Breuer, Istvan Homolya, Jannik Stebani, Maximilian Gram, Peter M. Jakob, Moritz Zaiss, Martin Blaimer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: To develop an image space formalism of multi-layer convolutional
neural networks (CNNs) for Fourier domain interpolation in MRI reconstructions
and analytically estimate noise propagation during CNN inference. Theory and
Methods: Nonlinear activations in the Fourier domain (also known as k-space)
using complex-valued Rectifier Linear Units are expressed as elementwise
multiplication with activation masks. This operation is transformed into a
convolution in the image space. After network training in k-space, this
approach provides an algebraic expression for the derivative of the
reconstructed image with respect to the aliased coil images, which serve as the
input tensors to the network in the image space. This allows the variance in
the network inference to be estimated analytically and to be used to describe
noise characteristics. Monte-Carlo simulations and numerical approaches based
on auto-differentiation were used for validation. The framework was tested on
retrospectively undersampled invivo brain images. Results: Inferences conducted
in the image domain are quasi-identical to inferences in the k-space,
underlined by corresponding quantitative metrics. Noise variance maps obtained
from the analytical expression correspond with those obtained via Monte-Carlo
simulations, as well as via an auto-differentiation approach. The noise
resilience is well characterized, as in the case of classical Parallel Imaging.
Komolgorov-Smirnov tests demonstrate Gaussian distributions of voxel magnitudes
in variance maps obtained via Monte-Carlo simulations. Conclusion: The
quasi-equivalent image space formalism for neural networks for k-space
interpolation enables fast and accurate description of the noise
characteristics during CNN inference, analogous to geometry-factor maps in
traditional parallel imaging methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LSPT: Long-term Spatial <span class="highlight-title">Prompt</span> Tuning for Visual Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shentong Mo, Yansen Wang, Xufang Luo, Dongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Prompt Tuning (VPT) techniques have gained prominence for their
capacity to adapt pre-trained Vision Transformers (ViTs) to downstream visual
tasks using specialized learnable tokens termed as prompts. Contemporary VPT
methodologies, especially when employed with self-supervised vision
transformers, often default to the introduction of new learnable prompts or
gated prompt tokens predominantly sourced from the model's previous block. A
pivotal oversight in such approaches is their failure to harness the potential
of long-range previous blocks as sources of prompts within each self-supervised
ViT. To bridge this crucial gap, we introduce Long-term Spatial Prompt Tuning
(LSPT) - a revolutionary approach to visual representation learning. Drawing
inspiration from the intricacies of the human brain, LSPT ingeniously
incorporates long-term gated prompts. This feature serves as temporal coding,
curbing the risk of forgetting parameters acquired from earlier blocks. Further
enhancing its prowess, LSPT brings into play patch tokens, serving as spatial
coding. This is strategically designed to perpetually amass class-conscious
features, thereby fortifying the model's prowess in distinguishing and
identifying visual categories. To validate the efficacy of our proposed method,
we engaged in rigorous experimentation across 5 FGVC and 19 VTAB-1K benchmarks.
Our empirical findings underscore the superiority of LSPT, showcasing its
ability to set new benchmarks in visual prompt tuning performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beacon, a lightweight deep reinforcement learning benchmark library for
  flow control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17402v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17402v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Viquerat, Philippe Meliga, Pablo Jeken, Elie Hachem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the increasing use of deep reinforcement learning for flow control
problems has led to a new area of research, focused on the coupling and the
adaptation of the existing algorithms to the control of numerical fluid
dynamics environments. Although still in its infancy, the field has seen
multiple successes in a short time span, and its fast development pace can
certainly be partly imparted to the open-source effort that drives the
expansion of the community. Yet, this emerging domain still misses a common
ground to (i) ensure the reproducibility of the results, and (ii) offer a
proper ad-hoc benchmarking basis. To this end, we propose Beacon, an
open-source benchmark library composed of seven lightweight 1D and 2D flow
control problems with various characteristics, action and observation space
characteristics, and CPU requirements. In this contribution, the seven
considered problems are described, and reference control solutions are
provided. The sources for the following work are available at
https://github.com/jviquerat/beacon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Quantum Approach to Synthetic Minority Oversampling Technique (SMOTE) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nishikanta Mohanty, Bikash K. Behera, Christopher Ferrie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper proposes the Quantum-SMOTE method, a novel solution that uses
quantum computing techniques to solve the prevalent problem of class imbalance
in machine learning datasets. Quantum-SMOTE, inspired by the Synthetic Minority
Oversampling Technique (SMOTE), generates synthetic data points using quantum
processes such as swap tests and quantum rotation. The process varies from the
conventional SMOTE algorithm's usage of K-Nearest Neighbors (KNN) and Euclidean
distances, enabling synthetic instances to be generated from minority class
data points without relying on neighbor proximity. The algorithm asserts
greater control over the synthetic data generation process by introducing
hyperparameters such as rotation angle, minority percentage, and splitting
factor, which allow for customization to specific dataset requirements. The
approach is tested on a public dataset of TelecomChurn and evaluated alongside
two prominent classification algorithms, Random Forest and Logistic Regression,
to determine its impact along with varying proportions of synthetic data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 22 Figures, 2 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robustness-Congruent Adversarial Training for Secure Machine Learning
  Model Updates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17390v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17390v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniele Angioni, Luca Demetrio, Maura Pintor, Luca Oneto, Davide Anguita, Battista Biggio, Fabio Roli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine-learning models demand for periodic updates to improve their average
accuracy, exploiting novel architectures and additional data. However, a
newly-updated model may commit mistakes that the previous model did not make.
Such misclassifications are referred to as negative flips, and experienced by
users as a regression of performance. In this work, we show that this problem
also affects robustness to adversarial examples, thereby hindering the
development of secure model update practices. In particular, when updating a
model to improve its adversarial robustness, some previously-ineffective
adversarial examples may become misclassified, causing a regression in the
perceived security of the system. We propose a novel technique, named
robustness-congruent adversarial training, to address this issue. It amounts to
fine-tuning a model with adversarial training, while constraining it to retain
higher robustness on the adversarial examples that were correctly classified
before the update. We show that our algorithm and, more generally, learning
with non-regression constraints, provides a theoretically-grounded framework to
train consistent estimators. Our experiments on robust models for computer
vision confirm that (i) both accuracy and robustness, even if improved after
model update, can be affected by negative flips, and (ii) our
robustness-congruent adversarial training can mitigate the problem,
outperforming competing baseline methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerating Diffusion Sampling with Optimized Time Steps <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuchen Xue, Zhaoqiang Liu, Fei Chen, Shifeng Zhang, Tianyang Hu, Enze Xie, Zhenguo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion probabilistic models (DPMs) have shown remarkable performance in
high-resolution image synthesis, but their sampling efficiency is still to be
desired due to the typically large number of sampling steps. Recent
advancements in high-order numerical ODE solvers for DPMs have enabled the
generation of high-quality images with much fewer sampling steps. While this is
a significant development, most sampling methods still employ uniform time
steps, which is not optimal when using a small number of steps. To address this
issue, we propose a general framework for designing an optimization problem
that seeks more appropriate time steps for a specific numerical ODE solver for
DPMs. This optimization problem aims to minimize the distance between the
ground-truth solution to the ODE and an approximate solution corresponding to
the numerical solver. It can be efficiently solved using the constrained trust
region method, taking less than $15$ seconds. Our extensive experiments on both
unconditional and conditional sampling using pixel- and latent-space DPMs
demonstrate that, when combined with the state-of-the-art sampling method
UniPC, our optimized time steps significantly improve image generation
performance in terms of FID scores for datasets such as CIFAR-10 and ImageNet,
compared to using uniform time steps.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024. Under camera-ready revision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Impact of Computation in Integral Reinforcement Learning for
  Continuous-Time Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhan Cao, Wei Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integral reinforcement learning (IntRL) demands the precise computation of
the utility function's integral at its policy evaluation (PEV) stage. This is
achieved through quadrature rules, which are weighted sums of utility functions
evaluated from state samples obtained in discrete time. Our research reveals a
critical yet underexplored phenomenon: the choice of the computational method
-- in this case, the quadrature rule -- can significantly impact control
performance. This impact is traced back to the fact that computational errors
introduced in the PEV stage can affect the policy iteration's convergence
behavior, which in turn affects the learned controller. To elucidate how
computation impacts control, we draw a parallel between IntRL's policy
iteration and Newton's method applied to the Hamilton-Jacobi-Bellman equation.
In this light, computational error in PEV manifests as an extra error term in
each iteration of Newton's method, with its upper bound proportional to the
computational error. Further, we demonstrate that when the utility function
resides in a reproducing kernel Hilbert space (RKHS), the optimal quadrature is
achievable by employing Bayesian quadrature with the RKHS-inducing kernel
function. We prove that the local convergence rates for IntRL using the
trapezoidal rule and Bayesian quadrature with a Mat\'ern kernel to be
$O(N^{-2})$ and $O(N^{-b})$, where $N$ is the number of evenly-spaced samples
and $b$ is the Mat\'ern kernel's smoothness parameter. These theoretical
findings are finally validated by two canonical control tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CGGM: A conditional graph generation model with adaptive sparsity for
  node anomaly detection in IoT networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17363v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17363v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianshi Su, Munan Li, Tongbang Jiang, Hao Long
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic graphs are extensively employed for detecting anomalous behavior in
nodes within the Internet of Things (IoT). Generative models are often used to
address the issue of imbalanced node categories in dynamic graphs.
Nevertheless, the constraints it faces include the monotonicity of adjacency
relationships, the difficulty in constructing multi-dimensional features for
nodes, and the lack of a method for end-to-end generation of multiple
categories of nodes. This paper presents a novel graph generation model, called
CGGM, designed specifically to generate a larger number of nodes belonging to
the minority class. The mechanism for generating an adjacency matrix, through
adaptive sparsity, enhances flexibility in its structure. The feature
generation module, called multidimensional features generator (MFG) to generate
node features along with topological information. Labels are transformed into
embedding vectors, serving as conditional constraints to control the generation
of synthetic data across multiple categories. Using a multi-stage loss, the
distribution of synthetic data is adjusted to closely resemble that of real
data. In extensive experiments, we show that CGGM's synthetic data outperforms
state-of-the-art methods across various metrics. Our results demonstrate
efficient generation of diverse data categories, robustly enhancing
multi-category classification model performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 19 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding the training of PINNs for unsteady flow past a plunging
  foil through the lens of input subdomain level loss function gradients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17346v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17346v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rahul Sundar, Didier Lucor, Sunetra Sarkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently immersed boundary method-inspired physics-informed neural networks
(PINNs) including the moving boundary-enabled PINNs (MB-PINNs) have shown the
ability to accurately reconstruct velocity and recover pressure as a hidden
variable for unsteady flow past moving bodies. Considering flow past a plunging
foil, MB-PINNs were trained with global physics loss relaxation and also in
conjunction with a physics-based undersampling method, obtaining good accuracy.
The purpose of this study was to investigate which input spatial subdomain
contributes to the training under the effect of physics loss relaxation and
physics-based undersampling. In the context of MB-PINNs training, three spatial
zones: the moving body, wake, and outer zones were defined. To quantify which
spatial zone drives the training, two novel metrics are computed from the zonal
loss component gradient statistics and the proportion of sample points in each
zone. Results confirm that the learning indeed depends on the combined effect
of the zonal loss component gradients and the proportion of points in each
zone. Moreover, the dominant input zones are also the ones that have the
strongest solution gradients in some sense.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LocalGCL: Local-aware Contrastive Learning for Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17345v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17345v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haojun Jiang, Jiawei Sun, Jie Li, Chentao Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph representation learning (GRL) makes considerable progress recently,
which encodes graphs with topological structures into low-dimensional
embeddings. Meanwhile, the time-consuming and costly process of annotating
graph labels manually prompts the growth of self-supervised learning (SSL)
techniques. As a dominant approach of SSL, Contrastive learning (CL) learns
discriminative representations by differentiating between positive and negative
samples. However, when applied to graph data, it overemphasizes global patterns
while neglecting local structures. To tackle the above issue, we propose
\underline{Local}-aware \underline{G}raph \underline{C}ontrastive
\underline{L}earning (\textbf{\methnametrim}), a self-supervised learning
framework that supplementarily captures local graph information with
masking-based modeling compared with vanilla contrastive learning. Extensive
experiments validate the superiority of \methname against state-of-the-art
methods, demonstrating its promise as a comprehensive graph representation
learner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhanced Bayesian Optimization via Preferential Modeling of Abstract
  Properties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arun Kumar A V, Alistair Shilton, Sunil Gupta, Santu Rana, Stewart Greenhill, Svetha Venkatesh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Experimental (design) optimization is a key driver in designing and
discovering new products and processes. Bayesian Optimization (BO) is an
effective tool for optimizing expensive and black-box experimental design
processes. While Bayesian optimization is a principled data-driven approach to
experimental optimization, it learns everything from scratch and could greatly
benefit from the expertise of its human (domain) experts who often reason about
systems at different abstraction levels using physical properties that are not
necessarily directly measured (or measurable). In this paper, we propose a
human-AI collaborative Bayesian framework to incorporate expert preferences
about unmeasured abstract properties into the surrogate modeling to further
boost the performance of BO. We provide an efficient strategy that can also
handle any incorrect/misleading expert bias in preferential judgments. We
discuss the convergence behavior of our proposed framework. Our experimental
results involving synthetic functions and real-world datasets show the
superiority of our method against the baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 Pages, 6 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Outdoor Environment Reconstruction with Deep Learning on Radio
  Propagation Paths 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17336v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17336v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hrant Khachatrian, Rafayel Mkrtchyan, Theofanis P. Raptis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional methods for outdoor environment reconstruction rely
predominantly on vision-based techniques like photogrammetry and LiDAR, facing
limitations such as constrained coverage, susceptibility to environmental
conditions, and high computational and energy demands. These challenges are
particularly pronounced in applications like augmented reality navigation,
especially when integrated with wearable devices featuring constrained
computational resources and energy budgets. In response, this paper proposes a
novel approach harnessing ambient wireless signals for outdoor environment
reconstruction. By analyzing radio frequency (RF) data, the paper aims to
deduce the environmental characteristics and digitally reconstruct the outdoor
surroundings. Investigating the efficacy of selected deep learning (DL)
techniques on the synthetic RF dataset WAIR-D, the study endeavors to address
the research gap in this domain. Two DL-driven approaches are evaluated
(convolutional U-Net and CLIP+ based on vision transformers), with performance
assessed using metrics like intersection-over-union (IoU), Hausdorff distance,
and Chamfer distance. The results demonstrate promising performance of the
RF-based reconstruction method, paving the way towards lightweight and scalable
reconstruction solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible. Work partly supported by the RA Science Committee grant
  No. 22rl-052 (DISTAL) and the EU under Italian National Recovery and
  Resilience Plan of NextGenerationEU on "Telecommunications of the Future"
  (PE00000001 - program "RESTART")</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-Efficient Learning via Clustering-Based Sensitivity Sampling:
  Foundation Models and Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17327v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17327v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyriakos Axiotis, Vincent Cohen-Addad, Monika Henzinger, Sammy Jerome, Vahab Mirrokni, David Saulpic, David Woodruff, Michael Wunder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the data selection problem, whose aim is to select a small
representative subset of data that can be used to efficiently train a machine
learning model. We present a new data selection approach based on $k$-means
clustering and sensitivity sampling. Assuming access to an embedding
representation of the data with respect to which the model loss is H\"older
continuous, our approach provably allows selecting a set of ``typical'' $k +
1/\varepsilon^2$ elements whose average loss corresponds to the average loss of
the whole dataset, up to a multiplicative $(1\pm\varepsilon)$ factor and an
additive $\varepsilon \lambda \Phi_k$, where $\Phi_k$ represents the $k$-means
cost for the input embeddings and $\lambda$ is the H\"older constant.
  We furthermore demonstrate the performance and scalability of our approach on
fine-tuning foundation models and show that it outperforms state-of-the-art
methods. We also show how it can be applied on linear regression, leading to a
new sampling strategy that surprisingly matches the performances of leverage
score sampling, while being conceptually simpler and more scalable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Supervised Local Learning with Augmented Auxiliary Networks <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17318v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17318v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxiang Ma, Jibin Wu, Chenyang Si, Kay Chen Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks are typically trained using global error signals that
backpropagate (BP) end-to-end, which is not only biologically implausible but
also suffers from the update locking problem and requires huge memory
consumption. Local learning, which updates each layer independently with a
gradient-isolated auxiliary network, offers a promising alternative to address
the above problems. However, existing local learning methods are confronted
with a large accuracy gap with the BP counterpart, particularly for large-scale
networks. This is due to the weak coupling between local layers and their
subsequent network layers, as there is no gradient communication across layers.
To tackle this issue, we put forward an augmented local learning method, dubbed
AugLocal. AugLocal constructs each hidden layer's auxiliary network by
uniformly selecting a small subset of layers from its subsequent network layers
to enhance their synergy. We also propose to linearly reduce the depth of
auxiliary networks as the hidden layer goes deeper, ensuring sufficient network
capacity while reducing the computational cost of auxiliary networks. Our
extensive experiments on four image classification datasets (i.e., CIFAR-10,
SVHN, STL-10, and ImageNet) demonstrate that AugLocal can effectively scale up
to tens of local layers with a comparable accuracy to BP-trained networks while
reducing GPU memory usage by around 40%. The proposed AugLocal method,
therefore, opens up a myriad of opportunities for training high-performance
deep neural networks on resource-constrained platforms.Code is available at
https://github.com/ChenxiangMA/AugLocal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How we won BraTS 2023 Adult Glioma challenge? Just faking it! Enhanced
  Synthetic Data Augmentation and Model Ensemble for brain tumour segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        André Ferreira, Naida Solak, Jianning Li, Philipp Dammann, Jens Kleesiek, Victor Alves, Jan Egger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning is the state-of-the-art technology for segmenting brain
tumours. However, this requires a lot of high-quality data, which is difficult
to obtain, especially in the medical field. Therefore, our solutions address
this problem by using unconventional mechanisms for data augmentation.
Generative adversarial networks and registration are used to massively increase
the amount of available samples for training three different deep learning
models for brain tumour segmentation, the first task of the BraTS2023
challenge. The first model is the standard nnU-Net, the second is the Swin
UNETR and the third is the winning solution of the BraTS 2021 Challenge. The
entire pipeline is built on the nnU-Net implementation, except for the
generation of the synthetic data. The use of convolutional algorithms and
transformers is able to fill each other's knowledge gaps. Using the new metric,
our best solution achieves the dice results 0.9005, 0.8673, 0.8509 and HD95
14.940, 14.467, 17.699 (whole tumour, tumour core and enhancing tumour) in the
validation set.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantum Distance Approximation for Persistence Diagrams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bernardo Ameneyro, Rebekah Herrman, George Siopsis, Vasileios Maroulas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topological Data Analysis methods can be useful for classification and
clustering tasks in many different fields as they can provide two dimensional
persistence diagrams that summarize important information about the shape of
potentially complex and high dimensional data sets. The space of persistence
diagrams can be endowed with various metrics such as the Wasserstein distance
which admit a statistical structure and allow to use these summaries for
machine learning algorithms. However, computing the distance between two
persistence diagrams involves finding an optimal way to match the points of the
two diagrams and may not always be an easy task for classical computers. In
this work we explore the potential of quantum computers to estimate the
distance between persistence diagrams, in particular we propose variational
quantum algorithms for the Wasserstein distance as well as the $d^{c}_{p}$
distance. Our implementation is a weighted version of the Quantum Approximate
Optimization Algorithm that relies on control clauses to encode the constraints
of the optimization problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 11 figures, submitted to SIAM Journal on Computing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Interpretable Evaluation of Entropy-based Novelty of Generative
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17287v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17287v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingwei Zhang, Cheuk Ting Li, Farzan Farnia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The massive developments of generative model frameworks and architectures
require principled methods for the evaluation of a model's novelty compared to
a reference dataset or baseline generative models. While the recent literature
has extensively studied the evaluation of the quality, diversity, and
generalizability of generative models, the assessment of a model's novelty
compared to a baseline model has not been adequately studied in the machine
learning community. In this work, we focus on the novelty assessment under
multi-modal generative models and attempt to answer the following question:
Given the samples of a generative model $\mathcal{G}$ and a reference dataset
$\mathcal{S}$, how can we discover and count the modes expressed by
$\mathcal{G}$ more frequently than in $\mathcal{S}$. We introduce a spectral
approach to the described task and propose the Kernel-based Entropic Novelty
(KEN) score to quantify the mode-based novelty of distribution $P_\mathcal{G}$
with respect to distribution $P_\mathcal{S}$. We analytically interpret the
behavior of the KEN score under mixture distributions with sub-Gaussian
components. Next, we develop a method based on Cholesky decomposition to
compute the KEN score from observed samples. We support the KEN-based
quantification of novelty by presenting several numerical results on synthetic
and real image distributions. Our numerical results indicate the success of the
proposed approach in detecting the novel modes and the comparison of
state-of-the-art generative models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Agent, Human-Agent and Beyond: A <span class="highlight-title">Survey</span> on Cooperation in Social
  Dilemmas 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17270v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17270v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Guo, Chunjiang Mu, Yang Chen, Chen Shen, Shuyue Hu, Zhen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The study of cooperation within social dilemmas has long been a fundamental
topic across various disciplines, including computer science and social
science. Recent advancements in Artificial Intelligence (AI) have significantly
reshaped this field, offering fresh insights into understanding and enhancing
cooperation. This survey examines three key areas at the intersection of AI and
cooperation in social dilemmas. First, focusing on multi-agent cooperation, we
review the intrinsic and external motivations that support cooperation among
rational agents, and the methods employed to develop effective strategies
against diverse opponents. Second, looking into human-agent cooperation, we
discuss the current AI algorithms for cooperating with humans and the human
biases towards AI agents. Third, we review the emergent field of leveraging AI
agents to enhance cooperation among humans. We conclude by discussing future
research avenues, such as using large language models, establishing unified
theoretical frameworks, revisiting existing theories of human cooperation, and
exploring multiple real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Curriculum Learning Meets Directed Acyclic Graph for Multimodal Emotion
  Recognition <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cam-Van Thi Nguyen, Cao-Bach Nguyen, Quang-Thuy Ha, Duc-Trong Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotion recognition in conversation (ERC) is a crucial task in natural
language processing and affective computing. This paper proposes MultiDAG+CL, a
novel approach for Multimodal Emotion Recognition in Conversation (ERC) that
employs Directed Acyclic Graph (DAG) to integrate textual, acoustic, and visual
features within a unified framework. The model is enhanced by Curriculum
Learning (CL) to address challenges related to emotional shifts and data
imbalance. Curriculum learning facilitates the learning process by gradually
presenting training samples in a meaningful order, thereby improving the
model's performance in handling emotional variations and data imbalance.
Experimental results on the IEMOCAP and MELD datasets demonstrate that the
MultiDAG+CL models outperform baseline models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RIME: Robust Preference-based Reinforcement Learning with Noisy
  Preferences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17257v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17257v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Cheng, Gang Xiong, Xingyuan Dai, Qinghai Miao, Yisheng Lv, Fei-Yue Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Preference-based Reinforcement Learning (PbRL) avoids the need for reward
engineering by harnessing human preferences as the reward signal. However,
current PbRL algorithms over-reliance on high-quality feedback from domain
experts, which results in a lack of robustness. In this paper, we present RIME,
a robust PbRL algorithm for effective reward learning from noisy preferences.
Our method incorporates a sample selection-based discriminator to dynamically
filter denoised preferences for robust training. To mitigate the accumulated
error caused by incorrect selection, we propose to warm start the reward model,
which additionally bridges the performance gap during transition from
pre-training to online training in PbRL. Our experiments on robotic
manipulation and locomotion tasks demonstrate that RIME significantly enhances
the robustness of the current state-of-the-art PbRL method. Ablation studies
further demonstrate that the warm start is crucial for both robustness and
feedback-efficiency in limited-feedback cases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning-Based Speech and Vision Synthesis to Improve Phishing
  Attack Detection through a Multi-layer Adaptive Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17249v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17249v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tosin Ige, Christopher Kiekintveld, Aritran Piplai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ever-evolving ways attacker continues to im prove their phishing
techniques to bypass existing state-of-the-art phishing detection methods pose
a mountain of challenges to researchers in both industry and academia research
due to the inability of current approaches to detect complex phishing attack.
Thus, current anti-phishing methods remain vulnerable to complex phishing
because of the increasingly sophistication tactics adopted by attacker coupled
with the rate at which new tactics are being developed to evade detection. In
this research, we proposed an adaptable framework that combines Deep learning
and Randon Forest to read images, synthesize speech from deep-fake videos, and
natural language processing at various predictions layered to significantly
increase the performance of machine learning models for phishing attack
detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SDR-Former: A Siamese Dual-Resolution <span class="highlight-title">Transformer</span> for Liver Lesion
  Classification Using 3D Multi-Phase Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng Lou, Hanning Ying, Xiaoqing Liu, Hong-Yu Zhou, Yuqing Zhang, Yizhou Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated classification of liver lesions in multi-phase CT and MR scans is
of clinical significance but challenging. This study proposes a novel Siamese
Dual-Resolution Transformer (SDR-Former) framework, specifically designed for
liver lesion classification in 3D multi-phase CT and MR imaging with varying
phase counts. The proposed SDR-Former utilizes a streamlined Siamese Neural
Network (SNN) to process multi-phase imaging inputs, possessing robust feature
representations while maintaining computational efficiency. The weight-sharing
feature of the SNN is further enriched by a hybrid Dual-Resolution Transformer
(DR-Former), comprising a 3D Convolutional Neural Network (CNN) and a tailored
3D Transformer for processing high- and low-resolution images, respectively.
This hybrid sub-architecture excels in capturing detailed local features and
understanding global contextual information, thereby, boosting the SNN's
feature extraction capabilities. Additionally, a novel Adaptive Phase Selection
Module (APSM) is introduced, promoting phase-specific intercommunication and
dynamically adjusting each phase's influence on the diagnostic outcome. The
proposed SDR-Former framework has been validated through comprehensive
experiments on two clinical datasets: a three-phase CT dataset and an
eight-phase MR dataset. The experimental results affirm the efficacy of the
proposed framework. To support the scientific community, we are releasing our
extensive multi-phase MR dataset for liver lesion analysis to the public. This
pioneering dataset, being the first publicly available multi-phase MR dataset
in this field, also underpins the MICCAI LLD-MMRI Challenge. The dataset is
accessible at:https://bit.ly/3IyYlgN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Does Negative Sampling Matter? A <span class="highlight-title">Review</span> with Insights into its Theory
  and Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Yang, Ming Ding, Tinglin Huang, Yukuo Cen, Junshuai Song, Bin Xu, Yuxiao Dong, Jie Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Negative sampling has swiftly risen to prominence as a focal point of
research, with wide-ranging applications spanning machine learning, computer
vision, natural language processing, data mining, and recommender systems. This
growing interest raises several critical questions: Does negative sampling
really matter? Is there a general framework that can incorporate all existing
negative sampling methods? In what fields is it applied? Addressing these
questions, we propose a general framework that leverages negative sampling.
Delving into the history of negative sampling, we trace the development of
negative sampling through five evolutionary paths. We dissect and categorize
the strategies used to select negative sample candidates, detailing global,
local, mini-batch, hop, and memory-based approaches. Our review categorizes
current negative sampling methods into five types: static, hard, GAN-based,
Auxiliary-based, and In-batch methods, providing a clear structure for
understanding negative sampling. Beyond detailed categorization, we highlight
the application of negative sampling in various areas, offering insights into
its practical benefits. Finally, we briefly discuss open problems and future
directions for negative sampling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Review</span> of Data Mining in Personalized Education: Current Trends and
  Future Prospects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17236v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17236v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhang Xiong, Haoxuan Li, Zhuang Liu, Zhuofan Chen, Hao Zhou, Wenge Rong, Yuanxin Ouyang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized education, tailored to individual student needs, leverages
educational technology and artificial intelligence (AI) in the digital age to
enhance learning effectiveness. The integration of AI in educational platforms
provides insights into academic performance, learning preferences, and
behaviors, optimizing the personal learning process. Driven by data mining
techniques, it not only benefits students but also provides educators and
institutions with tools to craft customized learning experiences. To offer a
comprehensive review of recent advancements in personalized educational data
mining, this paper focuses on four primary scenarios: educational
recommendation, cognitive diagnosis, knowledge tracing, and learning analysis.
This paper presents a structured taxonomy for each area, compiles commonly used
datasets, and identifies future research directions, emphasizing the role of
data mining in enhancing personalized education and paving the way for future
exploration and innovation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stochastic Gradient Succeeds for Bandits <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jincheng Mei, Zixin Zhong, Bo Dai, Alekh Agarwal, Csaba Szepesvari, Dale Schuurmans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show that the \emph{stochastic gradient} bandit algorithm converges to a
\emph{globally optimal} policy at an $O(1/t)$ rate, even with a \emph{constant}
step size. Remarkably, global convergence of the stochastic gradient bandit
algorithm has not been previously established, even though it is an old
algorithm known to be applicable to bandits. The new result is achieved by
establishing two novel technical findings: first, the noise of the stochastic
updates in the gradient bandit algorithm satisfies a strong ``growth
condition'' property, where the variance diminishes whenever progress becomes
small, implying that additional noise control via diminishing step sizes is
unnecessary; second, a form of ``weak exploration'' is automatically achieved
through the stochastic gradient updates, since they prevent the action
probabilities from decaying faster than $O(1/t)$, thus ensuring that every
action is sampled infinitely often with probability $1$. These two findings can
be used to show that the stochastic gradient update is already ``sufficient''
for bandits in the sense that exploration versus exploitation is automatically
balanced in a manner that ensures almost sure convergence to a global optimum.
These novel theoretical findings are further verified by experimental results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages; Correction for a previous version published at ICML 2023
  conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid Square Neural ODE Causal Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bob Junyi Zou, Matthew E. Levine, Dessi P. Zaharieva, Ramesh Johari, Emily B. Fox
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hybrid models combine mechanistic ODE-based dynamics with flexible and
expressive neural network components. Such models have grown rapidly in
popularity, especially in scientific domains where such ODE-based modeling
offers important interpretability and validated causal grounding (e.g., for
counterfactual reasoning). The incorporation of mechanistic models also
provides inductive bias in standard blackbox modeling approaches, critical when
learning from small datasets or partially observed, complex systems.
Unfortunately, as hybrid models become more flexible, the causal grounding
provided by the mechanistic model can quickly be lost. We address this problem
by leveraging another common source of domain knowledge: ranking of treatment
effects for a set of interventions, even if the precise treatment effect is
unknown. We encode this information in a causal loss that we combine with the
standard predictive loss to arrive at a hybrid loss that biases our learning
towards causally valid hybrid models. We demonstrate our ability to achieve a
win-win -- state-of-the-art predictive performance and causal validity -- in
the challenging task of modeling glucose dynamics during exercise.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Two-scale Neural Networks for Partial Differential Equations with Small
  Parameters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17232v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17232v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiao Zhuang, Chris Ziyi Yao, Zhongqiang Zhang, George Em Karniadakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a two-scale neural network method for solving partial differential
equations (PDEs) with small parameters using physics-informed neural networks
(PINNs). We directly incorporate the small parameters into the architecture of
neural networks. The proposed method enables solving PDEs with small parameters
in a simple fashion, without adding Fourier features or other computationally
taxing searches of truncation parameters. Various numerical examples
demonstrate reasonable accuracy in capturing features of large derivatives in
the solutions caused by small parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Preserving Fairness Generalization in Deepfake Detection <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17229v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17229v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Lin, Xinan He, Yan Ju, Xin Wang, Feng Ding, Shu Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although effective deepfake detection models have been developed in recent
years, recent studies have revealed that these models can result in unfair
performance disparities among demographic groups, such as race and gender. This
can lead to particular groups facing unfair targeting or exclusion from
detection, potentially allowing misclassified deepfakes to manipulate public
opinion and undermine trust in the model. The existing method for addressing
this problem is providing a fair loss function. It shows good fairness
performance for intra-domain evaluation but does not maintain fairness for
cross-domain testing. This highlights the significance of fairness
generalization in the fight against deepfakes. In this work, we propose the
first method to address the fairness generalization problem in deepfake
detection by simultaneously considering features, loss, and optimization
aspects. Our method employs disentanglement learning to extract demographic and
domain-agnostic forgery features, fusing them to encourage fair learning across
a flattened loss landscape. Extensive experiments on prominent deepfake
datasets demonstrate our method's effectiveness, surpassing state-of-the-art
approaches in preserving fairness during cross-domain deepfake detection. The
code is available at https://github.com/Purdue-M2/Fairness-Generalization
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by The IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Backpropagation with Variance-Controlled Adaptive Sampling <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17227v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17227v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziteng Wang, Jianfei Chen, Jun Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sampling-based algorithms, which eliminate ''unimportant'' computations
during forward and/or back propagation (BP), offer potential solutions to
accelerate neural network training. However, since sampling introduces
approximations to training, such algorithms may not consistently maintain
accuracy across various tasks. In this work, we introduce a variance-controlled
adaptive sampling (VCAS) method designed to accelerate BP. VCAS computes an
unbiased stochastic gradient with fine-grained layerwise importance sampling in
data dimension for activation gradient calculation and leverage score sampling
in token dimension for weight gradient calculation. To preserve accuracy, we
control the additional variance by learning the sample ratio jointly with model
parameters during training. We assessed VCAS on multiple fine-tuning and
pre-training tasks in both vision and natural language domains. On all the
tasks, VCAS can preserve the original training loss trajectory and validation
accuracy with an up to 73.87% FLOPs reduction of BP and 49.58% FLOPs reduction
of the whole training process. The implementation is available at
https://github.com/thu-ml/VCAS .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Stochastic positional embeddings improve masked image modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.00566v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.00566v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Bar, Florian Bordes, Assaf Shocher, Mahmoud Assran, Pascal Vincent, Nicolas Ballas, Trevor Darrell, Amir Globerson, <span class="highlight-author">Yann LeCun</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Masked Image Modeling (MIM) is a promising self-supervised learning approach
that enables learning from unlabeled images. Despite its recent success,
learning good representations through MIM remains challenging because it
requires predicting the right semantic content in accurate locations. For
example, given an incomplete picture of a dog, we can guess that there is a
tail, but we cannot determine its exact location. In this work, we propose to
incorporate location uncertainty into MIM by using stochastic positional
embeddings (StoP). Specifically, we condition the model on stochastic masked
token positions drawn from a Gaussian distribution. StoP reduces overfitting to
location features and guides the model toward learning features that are more
robust to location uncertainties. Quantitatively, StoP improves downstream MIM
performance on a variety of downstream tasks, including $+1.7\%$ on ImageNet
linear probing using ViT-B, and $+2.5\%$ for ViT-H using $1\%$ of the data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and models available in https://github.com/amirbar/StoP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating Cutting-Plane Algorithms via Reinforcement Learning
  Surrogates <span class="chip">AAAI
  24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.08816v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.08816v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyle Mana, Fernando Acero, Stephen Mak, Parisa Zehtabi, Michael Cashmore, Daniele Magazzeni, Manuela Veloso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discrete optimization belongs to the set of $\mathcal{NP}$-hard problems,
spanning fields such as mixed-integer programming and combinatorial
optimization. A current standard approach to solving convex discrete
optimization problems is the use of cutting-plane algorithms, which reach
optimal solutions by iteratively adding inequalities known as \textit{cuts} to
refine a feasible set. Despite the existence of a number of general-purpose
cut-generating algorithms, large-scale discrete optimization problems continue
to suffer from intractability. In this work, we propose a method for
accelerating cutting-plane algorithms via reinforcement learning. Our approach
uses learned policies as surrogates for $\mathcal{NP}$-hard elements of the cut
generating procedure in a way that (i) accelerates convergence, and (ii)
retains guarantees of optimality. We apply our method on two types of problems
where cutting-plane algorithms are commonly used: stochastic optimization, and
mixed-integer quadratic programming. We observe the benefits of our method when
applied to Benders decomposition (stochastic optimization) and iterative loss
approximation (quadratic programming), achieving up to $45\%$ faster average
convergence when compared to modern alternative algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version (includes Supplementary Material). Accepted at AAAI
  24 Main Track with Oral Presentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feedback Efficient Online Fine-Tuning of Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16359v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16359v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masatoshi Uehara, Yulai Zhao, Kevin Black, Ehsan Hajiramezanali, Gabriele Scalia, Nathaniel Lee Diamant, Alex M Tseng, Sergey Levine, Tommaso Biancalani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models excel at modeling complex data distributions, including
those of images, proteins, and small molecules. However, in many cases, our
goal is to model parts of the distribution that maximize certain properties:
for example, we may want to generate images with high aesthetic quality, or
molecules with high bioactivity. It is natural to frame this as a reinforcement
learning (RL) problem, in which the objective is to fine-tune a diffusion model
to maximize a reward function that corresponds to some property. Even with
access to online queries of the ground-truth reward function, efficiently
discovering high-reward samples can be challenging: they might have a low
probability in the initial distribution, and there might be many infeasible
samples that do not even have a well-defined reward (e.g., unnatural images or
physically impossible molecules). In this work, we propose a novel
reinforcement learning procedure that efficiently explores on the manifold of
feasible samples. We present a theoretical analysis providing a regret
guarantee, as well as empirical validation across three domains: images,
biological sequences, and molecules.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review (codes will be released soon)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ricci flow-guided autoencoders in learning time-dependent dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.14591v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.14591v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Gracyk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a manifold-based autoencoder method for learning nonlinear
dynamics in time, notably partial differential equations (PDEs), in which the
manifold latent space evolves according to Ricci flow. This can be accomplished
by simulating Ricci flow in a physics-informed setting, and manifold quantities
can be matched so that Ricci flow is empirically achieved. With our
methodology, the manifold is learned as part of the training procedure, so
ideal geometries may be discerned, while the evolution simultaneously induces a
more accommodating latent representation over static methods. We present our
method on a range of numerical experiments consisting of PDEs that encompass
desirable characteristics such as periodicity and randomness, remarking error
on in-distribution and extrapolation scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Wisdom of Committee: Distilling from Foundation Model to Specialized
  Application Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14035v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14035v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichang Liu, Qingyun Liu, Yuening Li, Liang Liu, Anshumali Shrivastava, Shuchao Bi, Lichan Hong, Ed H. Chi, Zhe Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in foundation models have yielded impressive performance
across a wide range of tasks. Meanwhile, for specific applications,
practitioners have been developing specialized application models. To enjoy the
benefits of both kinds of models, one natural path is to transfer the knowledge
in foundation models into specialized application models, which are generally
more efficient for serving. Techniques from knowledge distillation may be
applied here, where the application model learns to mimic the foundation model.
However, specialized application models and foundation models have substantial
gaps in capacity, employing distinct architectures, using different input
features from different modalities, and being optimized on different
distributions. These differences in model characteristics lead to significant
challenges for distillation methods. In this work, we propose creating a
teaching committee comprising both foundation model teachers and complementary
teachers. Complementary teachers possess model characteristics akin to the
student's, aiming to bridge the gap between the foundation model and
specialized application models for a smoother knowledge transfer. Further, to
accommodate the dissimilarity among the teachers in the committee, we introduce
DiverseDistill, which allows the student to understand the expertise of each
teacher and extract task knowledge. Our evaluations demonstrate that adding
complementary teachers enhances student performance. Finally, DiverseDistill
consistently outperforms baseline distillation methods, regardless of the
teacher choices, resulting in significantly improved student performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Solving PDEs on Unknown Manifolds with Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.06682v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.06682v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Senwei Liang, Shixiao W. Jiang, John Harlim, Haizhao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a mesh-free computational framework and machine learning
theory for solving elliptic PDEs on unknown manifolds, identified with point
clouds, based on diffusion maps (DM) and deep learning. The PDE solver is
formulated as a supervised learning task to solve a least-squares regression
problem that imposes an algebraic equation approximating a PDE (and boundary
conditions if applicable). This algebraic equation involves a graph-Laplacian
type matrix obtained via DM asymptotic expansion, which is a consistent
estimator of second-order elliptic differential operators. The resulting
numerical method is to solve a highly non-convex empirical risk minimization
problem subjected to a solution from a hypothesis space of neural networks
(NNs). In a well-posed elliptic PDE setting, when the hypothesis space consists
of neural networks with either infinite width or depth, we show that the global
minimizer of the empirical loss function is a consistent solution in the limit
of large training data. When the hypothesis space is a two-layer neural
network, we show that for a sufficiently large width, gradient descent can
identify a global minimizer of the empirical loss function. Supporting
numerical examples demonstrate the convergence of the solutions, ranging from
simple manifolds with low and high co-dimensions, to rough surfaces with and
without boundaries. We also show that the proposed NN solver can robustly
generalize the PDE solution on new data points with generalization errors that
are almost identical to the training errors, superseding a Nystrom-based
interpolation method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributed Deep Joint Source-Channel Coding with Decoder-Only Side
  Information <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04311v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04311v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Selim F. Yilmaz, Ezgi Ozyilkan, Deniz Gunduz, Elza Erkip
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider low-latency image transmission over a noisy wireless channel when
correlated side information is present only at the receiver side (the Wyner-Ziv
scenario). In particular, we are interested in developing practical schemes
using a data-driven joint source-channel coding (JSCC) approach, which has been
previously shown to outperform conventional separation-based approaches in the
practical finite blocklength regimes, and to provide graceful degradation with
channel quality. We propose a novel neural network architecture that
incorporates the decoder-only side information at multiple stages at the
receiver side. Our results demonstrate that the proposed method succeeds in
integrating the side information, yielding improved performance at all channel
conditions in terms of the various quality measures considered here, especially
at low channel signal-to-noise ratios (SNRs) and small bandwidth ratios (BRs).
We have made the source code of the proposed method public to enable further
research, and the reproducibility of the results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in IEEE International Conference on Machine Learning for
  Communication and Networking (ICMLCN) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Asymmetry in Low-Rank Adapters of Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16842v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16842v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Zhu, Kristjan Greenewald, Kimia Nadjahi, Haitz Sáez de Ocáriz Borde, Rickard Brüel Gabrielsson, Leshem Choshen, Marzyeh Ghassemi, Mikhail Yurochkin, Justin Solomon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-efficient fine-tuning optimizes large, pre-trained foundation
models by updating a subset of parameters; in this class, Low-Rank Adaptation
(LoRA) is particularly effective. Inspired by an effort to investigate the
different roles of LoRA matrices during fine-tuning, this paper characterizes
and leverages unexpected asymmetry in the importance of low-rank adapter
matrices. Specifically, when updating the parameter matrices of a neural
network by adding a product $BA$, we observe that the $B$ and $A$ matrices have
distinct functions: $A$ extracts features from the input, while $B$ uses these
features to create the desired output. Based on this observation, we
demonstrate that fine-tuning $B$ is inherently more effective than fine-tuning
$A$, and that a random untrained $A$ should perform nearly as well as a
fine-tuned one. Using an information-theoretic lens, we also bound the
generalization of low-rank adapters, showing that the parameter savings of
exclusively training $B$ improves the bound. We support our conclusions with
experiments on RoBERTa, BART-Large, LLaMA-2, and ViTs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 2 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoDream: Exchanging dreams instead of models for federated aggregation
  with heterogeneous models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15968v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15968v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhishek Singh, Gauri Gupta, Ritvik Kapila, Yichuan Shi, Alex Dang, Sheshank Shankar, Mohammed Ehab, Ramesh Raskar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) enables collaborative optimization of machine
learning models across decentralized data by aggregating model parameters. Our
approach extends this concept by aggregating "knowledge" derived from models,
instead of model parameters. We present a novel framework called CoDream, where
clients collaboratively optimize randomly initialized data using federated
optimization in the input data space, similar to how randomly initialized model
parameters are optimized in FL. Our key insight is that jointly optimizing this
data can effectively capture the properties of the global data distribution.
Sharing knowledge in data space offers numerous benefits: (1) model-agnostic
collaborative learning, i.e., different clients can have different model
architectures; (2) communication that is independent of the model size,
eliminating scalability concerns with model parameters; (3) compatibility with
secure aggregation, thus preserving the privacy benefits of federated learning;
(4) allowing of adaptive optimization of knowledge shared for personalized
learning. We empirically validate CoDream on standard FL tasks, demonstrating
competitive performance despite not sharing model parameters. Our code:
https://mitmedialab.github.io/codream.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 12 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OCAtari: Object-Centric Atari 2600 Reinforcement Learning Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.08649v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.08649v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quentin Delfosse, Jannis Blüml, Bjarne Gregori, Sebastian Sztwiertnia, Kristian Kersting
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cognitive science and psychology suggest that object-centric representations
of complex scenes are a promising step towards enabling efficient abstract
reasoning from low-level perceptual features. Yet, most deep reinforcement
learning approaches only rely on pixel-based representations that do not
capture the compositional properties of natural scenes. For this, we need
environments and datasets that allow us to work and evaluate object-centric
approaches. In our work, we extend the Atari Learning Environments, the
most-used evaluation framework for deep RL approaches, by introducing OCAtari,
that performs resource-efficient extractions of the object-centric states for
these games. Our framework allows for object discovery, object representation
learning, as well as object-centric RL. We evaluate OCAtari's detection
capabilities and resource efficiency. Our source code is available at
github.com/k4ntz/OC_Atari.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 8 main paper pages, 36 appendix pages. In main paper: 4
  figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Federated Learning in Healthcare: a <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09650v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09650v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob Thrasher, Alina Devkota, Prasiddha Siwakotai, Rohit Chivukula, Pranav Poudel, Chaunbo Hu, Binod Bhattarai, Prashnna Gyawali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in multimodal machine learning have empowered the
development of accurate and robust AI systems in the medical domain, especially
within centralized database systems. Simultaneously, Federated Learning (FL)
has progressed, providing a decentralized mechanism where data need not be
consolidated, thereby enhancing the privacy and security of sensitive
healthcare data. The integration of these two concepts supports the ongoing
progress of multimodal learning in healthcare while ensuring the security and
privacy of patient records within local data-holding agencies. This paper
offers a concise overview of the significance of FL in healthcare and outlines
the current state-of-the-art approaches to Multimodal Federated Learning (MMFL)
within the healthcare domain. It comprehensively examines the existing
challenges in the field, shedding light on the limitations of present models.
Finally, the paper outlines potential directions for future advancements in the
field, aiming to bridge the gap between cutting-edge AI technology and the
imperative need for patient data privacy in healthcare applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advancing Translation Preference Modeling with RLHF: A Step Towards
  Cost-Effective Solution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11525v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11525v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nuo Xu, Jun Zhao, Can Zu, Sixian Li, Lu Chen, Zhihao Zhang, Rui Zheng, Shihan Dou, Wenjuan Qin, Tao Gui, Qi Zhang, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Faithfulness, expressiveness, and elegance is the constant pursuit in machine
translation. However, traditional metrics like \textit{BLEU} do not strictly
align with human preference of translation quality. In this paper, we explore
leveraging reinforcement learning with human feedback (\textit{RLHF}) to
improve translation quality. It is non-trivial to collect a large high-quality
dataset of human comparisons between translations, especially for low-resource
languages. To address this issue, we propose a cost-effective preference
learning strategy, optimizing reward models by distinguishing between human and
machine translations. In this manner, the reward model learns the deficiencies
of machine translation compared to human and guides subsequent improvements in
machine translation. Experimental results demonstrate that \textit{RLHF} can
effectively enhance translation quality and this improvement benefits other
translation directions not trained with \textit{RLHF}. Further analysis
indicates that the model's language capabilities play a crucial role in
preference learning. A reward model with strong language capabilities can more
sensitively learn the subtle differences in translation quality and align
better with real human translation preferences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detecting Heart Disease from Multi-View Ultrasound Images via Supervised
  Attention Multiple Instance Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.00003v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.00003v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Huang, Benjamin S. Wessler, Michael C. Hughes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aortic stenosis (AS) is a degenerative valve condition that causes
substantial morbidity and mortality. This condition is under-diagnosed and
under-treated. In clinical practice, AS is diagnosed with expert review of
transthoracic echocardiography, which produces dozens of ultrasound images of
the heart. Only some of these views show the aortic valve. To automate
screening for AS, deep networks must learn to mimic a human expert's ability to
identify views of the aortic valve then aggregate across these relevant images
to produce a study-level diagnosis. We find previous approaches to AS detection
yield insufficient accuracy due to relying on inflexible averages across
images. We further find that off-the-shelf attention-based multiple instance
learning (MIL) performs poorly. We contribute a new end-to-end MIL approach
with two key methodological innovations. First, a supervised attention
technique guides the learned attention mechanism to favor relevant views.
Second, a novel self-supervised pretraining strategy applies contrastive
learning on the representation of the whole study instead of individual images
as commonly done in prior literature. Experiments on an open-access dataset and
an external validation set show that our approach yields higher accuracy while
reducing model size.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Echocardiogram; multiple-instance learning; self-supervised learning;
  semi-supervised learning; medical imaging</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OneLog: Towards End-to-End Training in Software Log Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.07324v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.07324v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shayan Hashemi, Mika Mäntylä
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growth of online services, IoT devices, and DevOps-oriented software
development, software log anomaly detection is becoming increasingly important.
Prior works mainly follow a traditional four-staged architecture (Preprocessor,
Parser, Vectorizer, and Classifier). This paper proposes OneLog, which utilizes
a single Deep Neural Network (DNN) instead of multiple separate components.
OneLog harnesses Convolutional Neural Networks (CNN) at the character level to
take digits, numbers, and punctuations, which were removed in prior works, into
account alongside the main natural language text. We evaluate our approach in
six message- and sequence-based data sets: HDFS, Hadoop, BGL, Thunderbird,
Spirit, and Liberty. We experiment with Onelog with single-, multi-, and
cross-project setups. Onelog offers state-of-the-art performance in our
datasets. Onelog can utilize multi-project datasets simultaneously during
training, which suggests our model can generalize between datasets.
Multi-project training also improves Onelog performance making it ideal when
limited training data is available for an individual project. We also found
that cross-project anomaly detection is possible with a single project pair
(Liberty and Spirit). Analysis of model internals shows that one log has
multiple modes of detecting anomalies and that the model learns manually
validated parsing rules for the log messages. We conclude that character-based
CNNs are a promising approach toward end-to-end learning in log anomaly
detection. They offer good performance and generalization over multiple
datasets. We will make our scripts publicly available upon the acceptance of
this paper.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Killer Apps: Low-Speed, Large-Scale AI Weapons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01663v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01663v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Feldman, Aaron Dant, James R. Foulds
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The accelerating advancements in Artificial Intelligence (AI) and Machine
Learning (ML), highlighted by the development of cutting-edge Generative
Pre-trained Transformer (GPT) models by organizations such as OpenAI, Meta, and
Anthropic, present new challenges and opportunities in warfare and security.
Much of the current focus is on AI's integration within weapons systems and its
role in rapid decision-making in kinetic conflict. However, an equally
important but often overlooked aspect is the potential of AI-based
psychological manipulation at internet scales within the information domain.
These capabilities could pose significant threats to individuals,
organizations, and societies globally. This paper explores the concept of AI
weapons, their deployment, detection, and potential countermeasures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages with 10 pages of appendices. 3 Figures, 2 code listings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causal Fairness under Unobserved Confounding: A Neural Sensitivity
  Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.18460v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.18460v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maresa Schröder, Dennis Frauen, Stefan Feuerriegel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fairness for machine learning predictions is widely required in practice for
legal, ethical, and societal reasons. Existing work typically focuses on
settings without unobserved confounding, even though unobserved confounding can
lead to severe violations of causal fairness and, thus, unfair predictions. In
this work, we analyze the sensitivity of causal fairness to unobserved
confounding. Our contributions are three-fold. First, we derive bounds for
causal fairness metrics under different sources of unobserved confounding. This
enables practitioners to examine the sensitivity of their machine learning
models to unobserved confounding in fairness-critical applications. Second, we
propose a novel neural framework for learning fair predictions, which allows us
to offer worst-case guarantees of the extent to which causal fairness can be
violated due to unobserved confounding. Third, we demonstrate the effectiveness
of our framework in a series of experiments, including a real-world case study
about predicting prison sentences. To the best of our knowledge, ours is the
first work to study causal fairness under unobserved confounding. To this end,
our work is of direct practical value as a refutation strategy to ensure the
fairness of predictions in high-stakes applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Anatomy of Neural Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03797v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03797v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Majd Saleh, Stéphane Paquelet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The fields of generative AI and transfer learning have experienced remarkable
advancements in recent years especially in the domain of Natural Language
Processing (NLP). Transformers have been at the heart of these advancements
where the cutting-edge transformer-based Language Models (LMs) have led to new
state-of-the-art results in a wide spectrum of applications. While the number
of research works involving neural LMs is exponentially increasing, their vast
majority are high-level and far from self-contained. Consequently, a deep
understanding of the literature in this area is a tough task especially in the
absence of a unified mathematical framework explaining the main types of neural
LMs. We address the aforementioned problem in this tutorial where the objective
is to explain neural LMs in a detailed, simplified and unambiguous mathematical
framework accompanied by clear graphical illustrations. Concrete examples on
widely used models like BERT and GPT2 are explored. Finally, since transformers
pretrained on language-modeling-like tasks have been widely adopted in computer
vision and time series applications, we briefly explore some examples of such
solutions in order to enable readers to understand how transformers work in the
aforementioned domains and compare this use with the original one in NLP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 Pages; 25 Figures; some typos and notation errors are corrected in
  this version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Variational Gaussian Process Diffusion Processes <span class="chip">AISTATS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02066v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02066v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prakhar Verma, Vincent Adam, Arno Solin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion processes are a class of stochastic differential equations (SDEs)
providing a rich family of expressive models that arise naturally in dynamic
modelling tasks. Probabilistic inference and learning under generative models
with latent processes endowed with a non-linear diffusion process prior are
intractable problems. We build upon work within variational inference,
approximating the posterior process as a linear diffusion process, and point
out pathologies in the approach. We propose an alternative parameterization of
the Gaussian variational process using a site-based exponential family
description. This allows us to trade a slow inference algorithm with
fixed-point iterations for a fast algorithm for convex optimization akin to
natural gradient descent, which also provides a better objective for learning
model parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Artificial Intelligence and Statistics
  (AISTATS) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DoubleML -- An Object-Oriented Implementation of Double Machine Learning
  in R 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.09603v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.09603v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Bach, Victor Chernozhukov, Malte S. Kurz, Martin Spindler, Sven Klaassen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The R package DoubleML implements the double/debiased machine learning
framework of Chernozhukov et al. (2018). It provides functionalities to
estimate parameters in causal models based on machine learning methods. The
double machine learning framework consist of three key ingredients: Neyman
orthogonality, high-quality machine learning estimation and sample splitting.
Estimation of nuisance components can be performed by various state-of-the-art
machine learning methods that are available in the mlr3 ecosystem. DoubleML
makes it possible to perform inference in a variety of causal models, including
partially linear and interactive regression models and their extensions to
instrumental variable estimation. The object-oriented implementation of
DoubleML enables a high flexibility for the model specification and makes it
easily extendable. This paper serves as an introduction to the double machine
learning framework and the R package DoubleML. In reproducible code examples
with simulated and real data sets, we demonstrate how DoubleML users can
perform valid inference based on machine learning methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>56 pages, 8 Figures, 1 Table; Updated version for DoubleML 1.0.0</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ All the Feels: A dexterous hand with large-area tactile sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.15658v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.15658v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raunaq Bhirangi, Abigail DeFranco, Jacob Adkins, Carmel Majidi, Abhinav Gupta, Tess Hellebrekers, Vikash Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High cost and lack of reliability has precluded the widespread adoption of
dexterous hands in robotics. Furthermore, the lack of a viable tactile sensor
capable of sensing over the entire area of the hand impedes the rich, low-level
feedback that would improve learning of dexterous manipulation skills. This
paper introduces an inexpensive, modular, robust, and scalable platform -- the
DManus -- aimed at resolving these challenges while satisfying the large-scale
data collection capabilities demanded by deep robot learning paradigms. Studies
on human manipulation point to the criticality of low-level tactile feedback in
performing everyday dexterous tasks. The DManus comes with ReSkin sensing on
the entire surface of the palm as well as the fingertips. We demonstrate
effectiveness of the fully integrated system in a tactile aware task -- bin
picking and sorting. Code, documentation, design files, detailed assembly
instructions, trained models, task videos, and all supplementary materials
required to recreate the setup can be found on
https://sites.google.com/view/roboticsbenchmarks/platforms/dmanus.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NECO: NEural Collapse Based Out-of-distribution detection <span class="chip">ICLR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06823v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06823v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mouïn Ben Ammar, Nacim Belkhir, Sebastian Popescu, Antoine Manzanera, Gianni Franchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting out-of-distribution (OOD) data is a critical challenge in machine
learning due to model overconfidence, often without awareness of their
epistemological limits. We hypothesize that ``neural collapse'', a phenomenon
affecting in-distribution data for models trained beyond loss convergence, also
influences OOD data. To benefit from this interplay, we introduce NECO, a novel
post-hoc method for OOD detection, which leverages the geometric properties of
``neural collapse'' and of principal component spaces to identify OOD data. Our
extensive experiments demonstrate that NECO achieves state-of-the-art results
on both small and large-scale OOD detection tasks while exhibiting strong
generalization capabilities across different network architectures.
Furthermore, we provide a theoretical explanation for the effectiveness of our
method in OOD detection. Code is available at https://gitlab.com/drti/neco
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nemotron-4 15B Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16819v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16819v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jupinder Parmar, Shrimai Prabhumoye, Joseph Jennings, Mostofa Patwary, Sandeep Subramanian, Dan Su, Chen Zhu, Deepak Narayanan, Aastha Jhunjhunwala, Ayush Dattagupta, Vibhu Jawa, Jiwei Liu, Ameya Mahabaleshwarkar, Osvald Nitski, Annika Brundyn, James Maki, Miguel Martinez, Jiaxuan You, John Kamalu, Patrick LeGresley, Denys Fridman, Jared Casper, Ashwath Aithal, Oleksii Kuchaiev, Mohammad Shoeybi, Jonathan Cohen, Bryan Catanzaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Nemotron-4 15B, a 15-billion-parameter large multilingual
language model trained on 8 trillion text tokens. Nemotron-4 15B demonstrates
strong performance when assessed on English, multilingual, and coding tasks: it
outperforms all existing similarly-sized open models on 4 out of 7 downstream
evaluation areas and achieves competitive performance to the leading open
models in the remaining ones. Specifically, Nemotron-4 15B exhibits the best
multilingual capabilities of all similarly-sized models, even outperforming
models over four times larger and those explicitly specialized for multilingual
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning of Linear Dynamical Systems as a Non-Commutative Polynomial
  Optimization Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2002.01444v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2002.01444v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quan Zhou, Jakub Marecek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been much recent progress in forecasting the next observation of a
linear dynamical system (LDS), which is known as the improper learning, as well
as in the estimation of its system matrices, which is known as the proper
learning of LDS. We present an approach to proper learning of LDS, which in
spite of the non-convexity of the problem, guarantees global convergence of
numerical solutions to a least-squares estimator. We present promising
computational results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures; retitled to reflect the title of the the
  published version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ISCUTE: Instance Segmentation of Cables Using Text Embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11996v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11996v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shir Kozlovsky, Omkar Joglekar, Dotan Di Castro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of robotics and automation, conventional object recognition and
instance segmentation methods face a formidable challenge when it comes to
perceiving Deformable Linear Objects (DLOs) like wires, cables, and flexible
tubes. This challenge arises primarily from the lack of distinct attributes
such as shape, color, and texture, which calls for tailored solutions to
achieve precise identification. In this work, we propose a foundation
model-based DLO instance segmentation technique that is text-promptable and
user-friendly. Specifically, our approach combines the text-conditioned
semantic segmentation capabilities of CLIPSeg model with the zero-shot
generalization capabilities of Segment Anything Model (SAM). We show that our
method exceeds SOTA performance on DLO instance segmentation, achieving a mIoU
of $91.21\%$. We also introduce a rich and diverse DLO-specific dataset for
instance segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ask Again, Then Fail: Large Language Models' Vacillations in Judgement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02174v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02174v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiming Xie, Zengzhi Wang, Yi Feng, Rui Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the emergence of generative conversational large language models (LLMs)
like ChatGPT, serving as virtual assistants in various fields, the stability
and reliability of their responses have become crucial. However, during usage,
it has been observed that these models tend to waver in their judgements when
confronted with follow-up questions from users expressing skepticism or
disagreement. In this work, we draw inspiration from questioning strategies in
education and propose a \textsc{Follow-up Questioning Mechanism} along with two
evaluation metrics to assess the judgement consistency of LLMs before and after
exposure to disturbances. We evaluate the judgement consistency of ChatGPT,
PaLM2-Bison, and Vicuna-13B under this mechanism across eight reasoning
benchmarks. Empirical results show that even when the initial answers are
correct, judgement consistency sharply decreases when LLMs face disturbances
such as questioning, negation, or misleading. Additionally, we study these
models' judgement consistency under various settings (sampling temperature and
prompts) to validate this issue further, observing the impact of prompt tone
and conducting an in-depth error analysis for deeper behavioral insights.
Furthermore, we also explore several prompting methods to mitigate this issue
and demonstrate their
effectiveness\footnote{\url{https://github.com/NUSTM/LLMs-Waver-In-Judgements}}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Update mitigation results of fine-tuning the model on synthesized
  high-quality preference data with DPO algorithm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Series of Hessian-Vector Products for Tractable Saddle-Free Newton
  Optimisation of Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14901v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14901v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elre T. Oldewage, Ross M. Clarke, José Miguel Hernández-Lobato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their popularity in the field of continuous optimisation,
second-order quasi-Newton methods are challenging to apply in machine learning,
as the Hessian matrix is intractably large. This computational burden is
exacerbated by the need to address non-convexity, for instance by modifying the
Hessian's eigenvalues as in Saddle-Free Newton methods. We propose an
optimisation algorithm which addresses both of these concerns - to our
knowledge, the first efficiently-scalable optimisation algorithm to
asymptotically use the exact inverse Hessian with absolute-value eigenvalues.
Our method frames the problem as a series which principally square-roots and
inverts the squared Hessian, then uses it to precondition a gradient vector,
all without explicitly computing or eigendecomposing the Hessian. A truncation
of this infinite series provides a new optimisation algorithm which is scalable
and comparable to other first- and second-order optimisation methods in both
runtime and optimisation performance. We demonstrate this in a variety of
settings, including a ResNet-18 trained on CIFAR-10.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 10 figures, 5 tables. To appear in TMLR. First two authors'
  order randomised</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Contextformer: Spatio-Channel Window Attention for Fast
  Context Modeling in Learned Image Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.14287v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.14287v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        A. Burakhan Koyuncu, Panqi Jia, Atanas Boev, Elena Alshina, Eckehard Steinbach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entropy estimation is essential for the performance of learned image
compression. It has been demonstrated that a transformer-based entropy model is
of critical importance for achieving a high compression ratio, however, at the
expense of a significant computational effort. In this work, we introduce the
Efficient Contextformer (eContextformer) - a computationally efficient
transformer-based autoregressive context model for learned image compression.
The eContextformer efficiently fuses the patch-wise, checkered, and
channel-wise grouping techniques for parallel context modeling, and introduces
a shifted window spatio-channel attention mechanism. We explore better training
strategies and architectural designs and introduce additional complexity
optimizations. During decoding, the proposed optimization techniques
dynamically scale the attention span and cache the previous attention
computations, drastically reducing the model and runtime complexity. Compared
to the non-parallel approach, our proposal has ~145x lower model complexity and
~210x faster decoding speed, and achieves higher average bit savings on Kodak,
CLIC2020, and Tecnick datasets. Additionally, the low complexity of our context
model enables online rate-distortion algorithms, which further improve the
compression performance. We achieve up to 17% bitrate savings over the intra
coding of Versatile Video Coding (VVC) Test Model (VTM) 16.2 and surpass
various learning-based compression models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for IEEE TCSVT (14 pages, 10 figures, 9 tables)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Pretrain</span>ed Visual Uncertainties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16569v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16569v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Kirchhof, Mark Collier, Seong Joon Oh, Enkelejda Kasneci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate uncertainty estimation is vital to trustworthy machine learning, yet
uncertainties typically have to be learned for each task anew. This work
introduces the first pretrained uncertainty modules for vision models. Similar
to standard pretraining this enables the zero-shot transfer of uncertainties
learned on a large pretraining dataset to specialized downstream datasets. We
enable our large-scale pretraining on ImageNet-21k by solving a gradient
conflict in previous uncertainty modules and accelerating the training by up to
180x. We find that the pretrained uncertainties generalize to unseen datasets.
In scrutinizing the learned uncertainties, we find that they capture aleatoric
uncertainty, disentangled from epistemic components. We demonstrate that this
enables safe retrieval and uncertainty-aware dataset visualization. To
encourage applications to further problems and domains, we release all
pretrained checkpoints and code under https://github.com/mkirchhof/url .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pay Attention to the Atlas: Atlas-Guided Test-Time Adaptation Method for
  Robust 3D Medical Image Segmentation <span class="chip">MICCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.00676v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.00676v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingjie Guo, Weitong Zhang, Matthew Sinclair, Daniel Rueckert, Chen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks (CNNs) often suffer from poor performance when
tested on target data that differs from the training (source) data
distribution, particularly in medical imaging applications where variations in
imaging protocols across different clinical sites and scanners lead to
different imaging appearances. However, re-accessing source training data for
unsupervised domain adaptation or labeling additional test data for model
fine-tuning can be difficult due to privacy issues and high labeling costs,
respectively. To solve this problem, we propose a novel atlas-guided test-time
adaptation (TTA) method for robust 3D medical image segmentation, called
AdaAtlas. AdaAtlas only takes one single unlabeled test sample as input and
adapts the segmentation network by minimizing an atlas-based loss.
Specifically, the network is adapted so that its prediction after registration
is aligned with the learned atlas in the atlas space, which helps to reduce
anatomical segmentation errors at test time. In addition, different from most
existing TTA methods which restrict the adaptation to batch normalization
blocks in the segmentation network only, we further exploit the use of channel
and spatial attention blocks for improved adaptability at test time. Extensive
experiments on multiple datasets from different sites show that AdaAtlas with
attention blocks adapted (AdaAtlas-Attention) achieves superior performance
improvements, greatly outperforming other competitive TTA methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by MICCAI BTSD-1001AI workshop. (Oral
  presentation).https://btsdmiccai.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal Sparse Survival Trees <span class="chip">AISTATS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15330v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15330v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Zhang, Rui Xin, Margo Seltzer, Cynthia Rudin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interpretability is crucial for doctors, hospitals, pharmaceutical companies
and biotechnology corporations to analyze and make decisions for high stakes
problems that involve human health. Tree-based methods have been widely adopted
for survival analysis due to their appealing interpretablility and their
ability to capture complex relationships. However, most existing methods to
produce survival trees rely on heuristic (or greedy) algorithms, which risk
producing sub-optimal models. We present a dynamic-programming-with-bounds
approach that finds provably-optimal sparse survival tree models, frequently in
only a few seconds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AISTATS2024 camera ready version. arXiv admin note: text overlap with
  arXiv:2211.14980</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HRTF upsampling with a generative adversarial network using a gnomonic
  equiangular projection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05812v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05812v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aidan O. T. Hogg, Mads Jenkins, He Liu, Isaac Squires, Samuel J. Cooper, Lorenzo Picinali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An individualised head-related transfer function (HRTF) is very important for
creating realistic virtual reality (VR) and augmented reality (AR)
environments. However, acoustically measuring high-quality HRTFs requires
expensive equipment and an acoustic lab setting. To overcome these limitations
and to make this measurement more efficient HRTF upsampling has been exploited
in the past where a high-resolution HRTF is created from a low-resolution one.
This paper demonstrates how generative adversarial networks (GANs) can be
applied to HRTF upsampling. We propose a novel approach that transforms the
HRTF data for direct use with a convolutional super-resolution generative
adversarial network (SRGAN). This new approach is benchmarked against three
baselines: barycentric upsampling, spherical harmonic (SH) upsampling and an
HRTF selection approach. Experimental results show that the proposed method
outperforms all three baselines in terms of log-spectral distortion (LSD) and
localisation performance using perceptual models when the input HRTF is sparse
(less than 20 measured positions).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 9 figures, Preprint (Accepted to IEEE/ACM Transactions on
  Audio, Speech, and Language Processing on the 15 Feb 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.12499v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.12499v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuelong Dai, Kaisheng Liang, Bin Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unrestricted adversarial attacks present a serious threat to deep learning
models and adversarial defense techniques. They pose severe security problems
for deep learning applications because they can effectively bypass defense
mechanisms. However, previous attack methods often utilize Generative
Adversarial Networks (GANs), which are not theoretically provable and thus
generate unrealistic examples by incorporating adversarial objectives,
especially for large-scale datasets like ImageNet. In this paper, we propose a
new method, called AdvDiff, to generate unrestricted adversarial examples with
diffusion models. We design two novel adversarial guidance techniques to
conduct adversarial sampling in the reverse generation process of diffusion
models. These two techniques are effective and stable to generate high-quality,
realistic adversarial examples by integrating gradients of the target
classifier interpretably. Experimental results on MNIST and ImageNet datasets
demonstrate that AdvDiff is effective to generate unrestricted adversarial
examples, which outperforms GAN-based methods in terms of attack performance
and generation quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PolypNextLSTM: A lightweight and fast polyp video segmentation network
  using ConvNext and ConvLSTM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11585v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11585v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debayan Bhattacharya, Konrad Reuter, Finn Behrendnt, Lennart Maack, Sarah Grube, Alexander Schlaefer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Commonly employed in polyp segmentation, single image UNet architectures lack
the temporal insight clinicians gain from video data in diagnosing polyps. To
mirror clinical practices more faithfully, our proposed solution,
PolypNextLSTM, leverages video-based deep learning, harnessing temporal
information for superior segmentation performance with the least parameter
overhead, making it possibly suitable for edge devices. PolypNextLSTM employs a
UNet-like structure with ConvNext-Tiny as its backbone, strategically omitting
the last two layers to reduce parameter overhead. Our temporal fusion module, a
Convolutional Long Short Term Memory (ConvLSTM), effectively exploits temporal
features. Our primary novelty lies in PolypNextLSTM, which stands out as the
leanest in parameters and the fastest model, surpassing the performance of five
state-of-the-art image and video-based deep learning models. The evaluation of
the SUN-SEG dataset spans easy-to-detect and hard-to-detect polyp scenarios,
along with videos containing challenging artefacts like fast motion and
occlusion. Comparison against 5 image-based and 5 video-based models
demonstrates PolypNextLSTM's superiority, achieving a Dice score of 0.7898 on
the hard-to-detect polyp test set, surpassing image-based PraNet (0.7519) and
video-based PNSPlusNet (0.7486). Notably, our model excels in videos featuring
complex artefacts such as ghosting and occlusion. PolypNextLSTM, integrating
pruned ConvNext-Tiny with ConvLSTM for temporal fusion, not only exhibits
superior segmentation performance but also maintains the highest frames per
speed among evaluated models. Access code here
https://github.com/mtec-tuhh/PolypNextLSTM
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GmGM: a Fast Multi-Axis Gaussian Graphical Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.02920v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.02920v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bailey Andrew, David Westhead, Luisa Cutillo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the Gaussian multi-Graphical Model, a model to
construct sparse graph representations of matrix- and tensor-variate data. We
generalize prior work in this area by simultaneously learning this
representation across several tensors that share axes, which is necessary to
allow the analysis of multimodal datasets such as those encountered in
multi-omics. Our algorithm uses only a single eigendecomposition per axis,
achieving an order of magnitude speedup over prior work in the ungeneralized
case. This allows the use of our methodology on large multi-modal datasets such
as single-cell multi-omics data, which was challenging with previous
approaches. We validate our model on synthetic data and five real-world
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages (33 additional in supplementary material), 19 figures,
  accepted at AIStats 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The curse of dimensionality in operator learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.15924v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.15924v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Lanthaler, Andrew M. Stuart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural operator architectures employ neural networks to approximate operators
mapping between Banach spaces of functions; they may be used to accelerate
model evaluations via emulation, or to discover models from data. Consequently,
the methodology has received increasing attention over recent years, giving
rise to the rapidly growing field of operator learning. The first contribution
of this paper is to prove that for general classes of operators which are
characterized only by their $C^r$- or Lipschitz-regularity, operator learning
suffers from a curse of dimensionality, defined precisely here in terms of
representations of the infinite-dimensional input and output function spaces.
The result is applicable to a wide variety of existing neural operators,
including PCA-Net, DeepONet and the FNO. The second contribution of the paper
is to prove that the general curse of dimensionality can be overcome for
solution operators defined by the Hamilton-Jacobi equation; this is achieved by
leveraging additional structure in the underlying solution operator, going
beyond regularity. To this end, a novel neural operator architecture is
introduced, termed HJ-Net, which explicitly takes into account characteristic
information of the underlying Hamiltonian system. Error and complexity
estimates are derived for HJ-Net which show that this architecture can provably
beat the curse of dimensionality related to the infinite-dimensional input and
output function spaces.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-lingual Text-To-Speech with Flow-based Voice Conversion for
  Improved Pronunciation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.17264v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.17264v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Ellinas, Georgios Vamvoukakis, Konstantinos Markopoulos, Georgia Maniati, Panos Kakoulidis, June Sig Sung, Inchul Hwang, Spyros Raptis, Aimilios Chalamandaris, Pirros Tsiakoulis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a method for end-to-end cross-lingual text-to-speech
(TTS) which aims to preserve the target language's pronunciation regardless of
the original speaker's language. The model used is based on a non-attentive
Tacotron architecture, where the decoder has been replaced with a normalizing
flow network conditioned on the speaker identity, allowing both TTS and voice
conversion (VC) to be performed by the same model due to the inherent
linguistic content and speaker identity disentanglement. When used in a
cross-lingual setting, acoustic features are initially produced with a native
speaker of the target language and then voice conversion is applied by the same
model in order to convert these features to the target speaker's voice. We
verify through objective and subjective evaluations that our method can have
benefits compared to baseline cross-lingual synthesis. By including speakers
averaging 7.5 minutes of speech, we also present positive results on
low-resource scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Fundamental changes to the model described and experimental procedure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pyramidal Hidden Markov Model For Multivariate Time Series Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14341v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14341v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        YeXin Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Hidden Markov Model (HMM) can predict the future value of a time series
based on its current and previous values, making it a powerful algorithm for
handling various types of time series. Numerous studies have explored the
improvement of HMM using advanced techniques, leading to the development of
several variations of HMM. Despite these studies indicating the increased
competitiveness of HMM compared to other advanced algorithms, few have
recognized the significance and impact of incorporating multistep stochastic
states into its performance. In this work, we propose a Pyramidal Hidden Markov
Model (PHMM) that can capture multiple multistep stochastic states. Initially,
a multistep HMM is designed for extracting short multistep stochastic states.
Next, a novel time series forecasting structure is proposed based on PHMM,
which utilizes pyramid-like stacking to adaptively identify long multistep
stochastic states. By employing these two schemes, our model can effectively
handle non-stationary and noisy data, while also establishing long-term
dependencies for more accurate and comprehensive forecasting. The experimental
results on diverse multivariate time series datasets convincingly demonstrate
the superior performance of our proposed PHMM compared to its competitive peers
in time series forecasting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages,3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal and multicontrast image fusion via deep generative models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.15963v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.15963v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giovanna Maria Dimitri, Simeon Spasov, Andrea Duggento, Luca Passamonti, Pietro Li`o, Nicola Toschi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, it has become progressively more evident that classic diagnostic
labels are unable to reliably describe the complexity and variability of
several clinical phenotypes. This is particularly true for a broad range of
neuropsychiatric illnesses (e.g., depression, anxiety disorders, behavioral
phenotypes). Patient heterogeneity can be better described by grouping
individuals into novel categories based on empirically derived sections of
intersecting continua that span across and beyond traditional categorical
borders. In this context, neuroimaging data carry a wealth of spatiotemporally
resolved information about each patient's brain. However, they are usually
heavily collapsed a priori through procedures which are not learned as part of
model training, and consequently not optimized for the downstream prediction
task. This is because every individual participant usually comes with multiple
whole-brain 3D imaging modalities often accompanied by a deep genotypic and
phenotypic characterization, hence posing formidable computational challenges.
In this paper we design a deep learning architecture based on generative models
rooted in a modular approach and separable convolutional blocks to a) fuse
multiple 3D neuroimaging modalities on a voxel-wise level, b) convert them into
informative latent embeddings through heavy dimensionality reduction, c)
maintain good generalizability and minimal information loss. As proof of
concept, we test our architecture on the well characterized Human Connectome
Project database demonstrating that our latent embeddings can be clustered into
easily separable subject strata which, in turn, map to different phenotypical
information which was not included in the embedding creation process. This may
be of aid in predicting disease evolution as well as drug response, hence
supporting mechanistic disease understanding and empowering clinical trials.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Integrating Multimodal Data for Joint Generative Modeling of Complex
  Dynamics <span class="chip">AAAI
  2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07892v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07892v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Brenner, Florian Hess, Georgia Koppe, Daniel Durstewitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many, if not most, systems of interest in science are naturally described as
nonlinear dynamical systems. Empirically, we commonly access these systems
through time series measurements. Often such time series may consist of
discrete random variables rather than continuous measurements, or may be
composed of measurements from multiple data modalities observed simultaneously.
For instance, in neuroscience we may have behavioral labels in addition to
spike counts and continuous physiological recordings. While by now there is a
burgeoning literature on deep learning for dynamical systems reconstruction
(DSR), multimodal data integration has hardly been considered in this context.
Here we provide such an efficient and flexible algorithmic framework that rests
on a multimodal variational autoencoder for generating a sparse teacher signal
that guides training of a reconstruction model, exploiting recent advances in
DSR training techniques. It enables to combine various sources of information
for optimal reconstruction, even allows for reconstruction from symbolic data
(class labels) alone, and connects different types of observations within a
common latent dynamics space. In contrast to previous multimodal data
integration techniques for scientific applications, our framework is fully
\textit{generative}, producing, after training, trajectories with the same
geometrical and temporal structure as those of the ground truth system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A previous version was published as a workshop paper for the AAAI
  2023 Workshop MLmDS under the name "Multimodal Teacher Forcing for
  Reconstructing Nonlinear Dynamical Systems"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multiclass Learning from Noisy Labels for Non-decomposable Performance
  Measures <span class="chip">AISTATS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01055v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01055v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyuan Zhang, Shivani Agarwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been much interest in recent years in learning good classifiers
from data with noisy labels. Most work on learning from noisy labels has
focused on standard loss-based performance measures. However, many machine
learning problems require using non-decomposable performance measures which
cannot be expressed as the expectation or sum of a loss on individual examples;
these include for example the H-mean, Q-mean and G-mean in class imbalance
settings, and the Micro $F_1$ in information retrieval. In this paper, we
design algorithms to learn from noisy labels for two broad classes of
multiclass non-decomposable performance measures, namely, monotonic convex and
ratio-of-linear, which encompass all the above examples. Our work builds on the
Frank-Wolfe and Bisection based methods of Narasimhan et al. (2015). In both
cases, we develop noise-corrected versions of the algorithms under the widely
studied family of class-conditional noise models. We provide regret (excess
risk) bounds for our algorithms, establishing that even though they are trained
on noisy data, they are Bayes consistent in the sense that their performance
converges to the optimal performance w.r.t. the clean (non-noisy) distribution.
Our experiments demonstrate the effectiveness of our algorithms in handling
label noise.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AISTATS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simple, unified analysis of Johnson-Lindenstrauss with applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10232v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10232v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingru Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a simple and unified analysis of the Johnson-Lindenstrauss (JL)
lemma, a cornerstone in the field of dimensionality reduction critical for
managing high-dimensional data. Our approach not only simplifies the
understanding but also unifies various constructions under the JL framework,
including spherical, binary-coin, sparse JL, Gaussian and sub-Gaussian models.
This simplification and unification make significant strides in preserving the
intrinsic geometry of data, essential across diverse applications from
streaming algorithms to reinforcement learning. Notably, we deliver the first
rigorous proof of the spherical construction's effectiveness and provide a
general class of sub-Gaussian constructions within this simplified framework.
At the heart of our contribution is an innovative extension of the
Hanson-Wright inequality to high dimensions, complete with explicit constants.
By employing simple yet powerful probabilistic tools and analytical techniques,
such as an enhanced diagonalization process, our analysis not only solidifies
the JL lemma's theoretical foundation by removing an independence assumption
but also extends its practical reach, showcasing its adaptability and
importance in contemporary computational algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Composite Goodness-of-fit Tests with Kernels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.10275v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.10275v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oscar Key, Arthur Gretton, François-Xavier Briol, Tamara Fernandez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model misspecification can create significant challenges for the
implementation of probabilistic models, and this has led to development of a
range of robust methods which directly account for this issue. However, whether
these more involved methods are required will depend on whether the model is
really misspecified, and there is a lack of generally applicable methods to
answer this question. In this paper, we propose one such method. More
precisely, we propose kernel-based hypothesis tests for the challenging
composite testing problem, where we are interested in whether the data comes
from any distribution in some parametric family. Our tests make use of minimum
distance estimators based on the maximum mean discrepancy and the kernel Stein
discrepancy. They are widely applicable, including whenever the density of the
parametric model is known up to normalisation constant, or if the model takes
the form of a simulator. As our main result, we show that we are able to
estimate the parameter and conduct our test on the same data (without data
splitting), while maintaining a correct test level. Our approach is illustrated
on a range of problems, including testing for goodness-of-fit of an
unnormalised non-parametric density model, and an intractable generative model
of a biological cellular network.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Agents as Optimizable Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16823v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16823v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, Jürgen Schmidhuber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Various human-designed prompt engineering techniques have been proposed to
improve problem solvers based on Large Language Models (LLMs), yielding many
disparate code bases. We unify these approaches by describing LLM-based agents
as computational graphs. The nodes implement functions to process multimodal
data or query LLMs, and the edges describe the information flow between
operations. Graphs can be recursively combined into larger composite graphs
representing hierarchies of inter-agent collaboration (where edges connect
operations of different agents). Our novel automatic graph optimizers (1)
refine node-level LLM prompts (node optimization) and (2) improve agent
orchestration by changing graph connectivity (edge optimization). Experiments
demonstrate that our framework can be used to efficiently develop, integrate,
and automatically improve various LLM agents. The code can be found at
https://github.com/metauto-ai/gptswarm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Website: https://gptswarm.org ; Github Repo:
  https://github.com/metauto-ai/gptswarm ; Replace to fix typos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Good is Chat<span class="highlight-title">GPT</span> at Face Biometrics? A First Look into Recognition,
  Soft Biometrics, and Explainability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.13641v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.13641v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivan DeAndres-Tame, Ruben Tolosana, Ruben Vera-Rodriguez, Aythami Morales, Julian Fierrez, Javier Ortega-Garcia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) such as GPT developed by OpenAI, have already
shown astonishing results, introducing quick changes in our society. This has
been intensified by the release of ChatGPT which allows anyone to interact in a
simple conversational way with LLMs, without any experience in the field
needed. As a result, ChatGPT has been rapidly applied to many different tasks
such as code- and song-writer, education, virtual assistants, etc., showing
impressive results for tasks for which it was not trained (zero-shot learning).
  The present study aims to explore the ability of ChatGPT, based on the recent
GPT-4 multimodal LLM, for the task of face biometrics. In particular, we
analyze the ability of ChatGPT to perform tasks such as face verification,
soft-biometrics estimation, and explainability of the results. ChatGPT could be
very valuable to further increase the explainability and transparency of
automatic decisions in human scenarios. Experiments are carried out in order to
evaluate the performance and robustness of ChatGPT, using popular public
benchmarks and comparing the results with state-of-the-art methods in the
field. The results achieved in this study show the potential of LLMs such as
ChatGPT for face biometrics, especially to enhance explainability. For
reproducibility reasons, we release all the code in GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Snapture -- A Novel Neural Architecture for Combined Static and Dynamic
  Hand Gesture Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.15862v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.15862v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hassan Ali, Doreen Jirak, Stefan Wermter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As robots are expected to get more involved in people's everyday lives,
frameworks that enable intuitive user interfaces are in demand. Hand gesture
recognition systems provide a natural way of communication and, thus, are an
integral part of seamless Human-Robot Interaction (HRI). Recent years have
witnessed an immense evolution of computational models powered by deep
learning. However, state-of-the-art models fall short in expanding across
different gesture domains, such as emblems and co-speech. In this paper, we
propose a novel hybrid hand gesture recognition system. Our architecture
enables learning both static and dynamic gestures: by capturing a so-called
"snapshot" of the gesture performance at its peak, we integrate the hand pose
along with the dynamic movement. Moreover, we present a method for analyzing
the motion profile of a gesture to uncover its dynamic characteristics and
which allows regulating a static channel based on the amount of motion. Our
evaluation demonstrates the superiority of our approach on two gesture
benchmarks compared to a CNNLSTM baseline. We also provide an analysis on a
gesture class basis that unveils the potential of our Snapture architecture for
performance improvements. Thanks to its modular implementation, our framework
allows the integration of other multimodal data like facial expressions and
head tracking, which are important cues in HRI scenarios, into one
architecture. Thus, our work contributes both to gesture recognition research
and machine learning applications for non-verbal communication with robots.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Cognitive Computation(Accepted:30/06/2023,
  Published:17/07/2023),20 pages,20 figures,4 tables;Please find the published
  version/info to cite:
  https://doi.org/10.1007/s12559-023-10174-z;Repositories:
  https://zenodo.org/doi/10.5281/zenodo.10679196,
  https://zenodo.org/doi/10.5281/zenodo.10693816;This work was co-funded by
  Horizon Europe project TERAIS under Grant agreement number 101079338</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Point <span class="highlight-title">Transformer</span> with Federated Learning for Predicting Breast Cancer
  HER2 Status from Hematoxylin and Eosin-Stained Whole Slide Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06454v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06454v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bao Li, Zhenyu Liu, Lizhi Shao, Bensheng Qiu, Hong Bu, Jie Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Directly predicting human epidermal growth factor receptor 2 (HER2) status
from widely available hematoxylin and eosin (HE)-stained whole slide images
(WSIs) can reduce technical costs and expedite treatment selection. Accurately
predicting HER2 requires large collections of multi-site WSIs. Federated
learning enables collaborative training of these WSIs without gigabyte-size
WSIs transportation and data privacy concerns. However, federated learning
encounters challenges in addressing label imbalance in multi-site WSIs from the
real world. Moreover, existing WSI classification methods cannot simultaneously
exploit local context information and long-range dependencies in the site-end
feature representation of federated learning. To address these issues, we
present a point transformer with federated learning for multi-site HER2 status
prediction from HE-stained WSIs. Our approach incorporates two novel designs.
We propose a dynamic label distribution strategy and an auxiliary classifier,
which helps to establish a well-initialized model and mitigate label
distribution variations across sites. Additionally, we propose a farthest
cosine sampling based on cosine distance. It can sample the most distinctive
features and capture the long-range dependencies. Extensive experiments and
analysis show that our method achieves state-of-the-art performance at four
sites with a total of 2687 WSIs. Furthermore, we demonstrate that our model can
generalize to two unseen sites with 229 WSIs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Impact of Loss Functions and Scene Representations for 3D/2D
  Registration on Single-view Fluoroscopic X-ray Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.00214v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.00214v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaochao Zhou, Syed Hasib Akhter Faruqui, Abhinav Patel, Ramez N. Abdalla, Michael C. Hurley, Ali Shaibani, Matthew B. Potts, Babak S. Jahromi, Sameer A. Ansari, Donald R. Cantrell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many tasks performed in image-guided procedures can be cast as pose
estimation problems, where specific projections are chosen to reach a target in
3D space. In this study, we first develop a differentiable projection
(DiffProj) rendering framework for the efficient computation of Digitally
Reconstructed Radiographs (DRRs) with automatic differentiability from either
Cone-Beam Computerized Tomography (CBCT) or neural scene representations,
including two newly proposed methods, Neural Tuned Tomography (NeTT) and masked
Neural Radiance Fields (mNeRF). We then perform pose estimation by iterative
gradient descent using various candidate loss functions, that quantify the
image discrepancy of the synthesized DRR with respect to the ground-truth
fluoroscopic X-ray image. Compared to alternative loss functions, the Mutual
Information loss function can significantly improve pose estimation accuracy,
as it can effectively prevent entrapment in local optima. Using the Mutual
Information loss, a comprehensive evaluation of pose estimation performed on a
tomographic X-ray dataset of 50 patients$'$ skulls shows that utilizing either
discretized (CBCT) or neural (NeTT/mNeRF) scene representations in DiffProj
leads to comparable performance in DRR appearance and pose estimation (3D angle
errors: mean $\leq$ 3.2{\deg} and 90% quantile $\leq$ 3.4{\deg}), despite the
latter often incurring considerable training expenses and time. These findings
could be instrumental for selecting appropriate approaches to improve the
efficiency and effectiveness of fluoroscopic X-ray pose estimation in
widespread image-guided interventions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-level protein <span class="highlight-title">pre-train</span>ing with Vabs-Net 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01481v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01481v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiale Zhao, Wanru Zhuang, Jia Song, Yaqi Li, Shuqi Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, there has been a surge in the development of 3D
structure-based pre-trained protein models, representing a significant
advancement over pre-trained protein language models in various downstream
tasks. However, most existing structure-based pre-trained models primarily
focus on the residue level, i.e., alpha carbon atoms, while ignoring other
atoms like side chain atoms. We argue that modeling proteins at both residue
and atom levels is important since the side chain atoms can also be crucial for
numerous downstream tasks, for example, molecular docking. Nevertheless, we
find that naively combining residue and atom information during pre-training
typically fails. We identify a key reason is the information leakage caused by
the inclusion of atom structure in the input, which renders residue-level
pre-training tasks trivial and results in insufficiently expressive residue
representations. To address this issue, we introduce a span mask pre-training
strategy on 3D protein chains to learn meaningful representations of both
residues and atoms. This leads to a simple yet effective approach to learning
protein representation suitable for diverse downstream tasks. Extensive
experimental results on binding site prediction and function prediction tasks
demonstrate our proposed pre-training approach significantly outperforms other
methods. Our code will be made public.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Neighborhood Construction for Structured Large Discrete Action
  Spaces <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.19891v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.19891v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Akkerman, Julius Luy, Wouter van Heeswijk, Maximilian Schiffer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large discrete action spaces (LDAS) remain a central challenge in
reinforcement learning. Existing solution approaches can handle unstructured
LDAS with up to a few million actions. However, many real-world applications in
logistics, production, and transportation systems have combinatorial action
spaces, whose size grows well beyond millions of actions, even on small
instances. Fortunately, such action spaces exhibit structure, e.g., equally
spaced discrete resource units. With this work, we focus on handling structured
LDAS (SLDAS) with sizes that cannot be handled by current benchmarks: we
propose Dynamic Neighborhood Construction (DNC), a novel exploitation paradigm
for SLDAS. We present a scalable neighborhood exploration heuristic that
utilizes this paradigm and efficiently explores the discrete neighborhood
around the continuous proxy action in structured action spaces with up to
$10^{73}$ actions. We demonstrate the performance of our method by benchmarking
it against three state-of-the-art approaches designed for large discrete action
spaces across two distinct environments. Our results show that DNC matches or
outperforms state-of-the-art approaches while being computationally more
efficient. Furthermore, our method scales to action spaces that so far remained
computationally intractable for existing methodologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 Camera ready version.
  https://openreview.net/forum?id=80wh3jjCZf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Private, fair and accurate: Training large-scale, privacy-preserving AI
  models in medical imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01622v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01622v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soroosh Tayebi Arasteh, Alexander Ziller, Christiane Kuhl, Marcus Makowski, Sven Nebelung, Rickmer Braren, Daniel Rueckert, Daniel Truhn, Georgios Kaissis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI) models are increasingly used in the medical
domain. However, as medical data is highly sensitive, special precautions to
ensure its protection are required. The gold standard for privacy preservation
is the introduction of differential privacy (DP) to model training. Prior work
indicates that DP has negative implications on model accuracy and fairness,
which are unacceptable in medicine and represent a main barrier to the
widespread use of privacy-preserving techniques. In this work, we evaluated the
effect of privacy-preserving training of AI models regarding accuracy and
fairness compared to non-private training. For this, we used two datasets: (1)
A large dataset (N=193,311) of high quality clinical chest radiographs, and (2)
a dataset (N=1,625) of 3D abdominal computed tomography (CT) images, with the
task of classifying the presence of pancreatic ductal adenocarcinoma (PDAC).
Both were retrospectively collected and manually labeled by experienced
radiologists. We then compared non-private deep convolutional neural networks
(CNNs) and privacy-preserving (DP) models with respect to privacy-utility
trade-offs measured as area under the receiver-operator-characteristic curve
(AUROC), and privacy-fairness trade-offs, measured as Pearson's r or
Statistical Parity Difference. We found that, while the privacy-preserving
trainings yielded lower accuracy, they did largely not amplify discrimination
against age, sex or co-morbidity. Our study shows that -- under the challenging
realistic circumstances of a real-life clinical dataset -- the
privacy-preserving training of diagnostic deep learning models is possible with
excellent diagnostic accuracy and fairness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Communications Medicine. Nature Portfolio</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Feynman--Kac training of partial Bayesian neural networks <span class="chip">AISTATS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.19608v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.19608v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Zhao, Sebastian Mair, Thomas B. Schön, Jens Sjölund
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, partial Bayesian neural networks (pBNNs), which only consider a
subset of the parameters to be stochastic, were shown to perform competitively
with full Bayesian neural networks. However, pBNNs are often multi-modal in the
latent variable space and thus challenging to approximate with parametric
models. To address this problem, we propose an efficient sampling-based
training strategy, wherein the training of a pBNN is formulated as simulating a
Feynman--Kac model. We then describe variations of sequential Monte Carlo
samplers that allow us to simultaneously estimate the parameters and the latent
posterior distribution of this model at a tractable computational cost. Using
various synthetic and real-world datasets we show that our proposed training
scheme outperforms the state of the art in terms of predictive performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In AISTATS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Evaluation of Generative Models in Distributed Learning Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11714v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11714v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixiao Wang, Farzan Farnia, Zhenghao Lin, Yunheng Shen, Bei Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evaluation of deep generative models including generative adversarial
networks (GANs) and diffusion models has been extensively studied in the
literature. While the existing evaluation methods mainly target a centralized
learning problem with training data stored by a single client, many
applications of generative models concern distributed learning settings, e.g.
the federated learning scenario, where training data are collected by and
distributed among several clients. In this paper, we study the evaluation of
generative models in distributed learning tasks with heterogeneous data
distributions. First, we focus on the Fr\'echet inception distance (FID) and
consider the following FID-based aggregate scores over the clients: 1) FID-avg
as the mean of clients' individual FID scores, 2) FID-all as the FID distance
of the trained model to the collective dataset containing all clients' data. We
prove that the model rankings according to the FID-all and FID-avg scores could
be inconsistent, which can lead to different optimal generative models
according to the two aggregate scores. Next, we consider the kernel inception
distance (KID) and similarly define the KID-avg and KID-all aggregations.
Unlike the FID case, we prove that KID-all and KID-avg result in the same
rankings of generative models. We perform several numerical experiments on
standard image datasets and training schemes to support our theoretical
findings on the evaluation of generative models in distributed learning
problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 20 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Locally Stationary Graph Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.01657v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.01657v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdullah Canbolat, Elif Vural
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stationary graph process models are commonly used in the analysis and
inference of data sets collected on irregular network topologies. While most of
the existing methods represent graph signals with a single stationary process
model that is globally valid on the entire graph, in many practical problems,
the characteristics of the process may be subject to local variations in
different regions of the graph. In this work, we propose a locally stationary
graph process (LSGP) model that aims to extend the classical concept of local
stationarity to irregular graph domains. We characterize local stationarity by
expressing the overall process as the combination of a set of component
processes such that the extent to which the process adheres to each component
varies smoothly over the graph. We propose an algorithm for computing LSGP
models from realizations of the process, and also study the approximation of
LSGPs locally with WSS processes. Experiments on signal interpolation problems
show that the proposed process model provides accurate signal representations
competitive with the state of the art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COPR: Continual Human Preference Learning via Optimal Policy
  Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14228v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14228v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Zhang, Lin Gui, Yu Lei, Yuanzhao Zhai, Yehong Zhang, Yulan He, Hui Wang, Yue Yu, Kam-Fai Wong, Bin Liang, Ruifeng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning from Human Feedback (RLHF) is commonly utilized to
improve the alignment of Large Language Models (LLMs) with human preferences.
Given the evolving nature of human preferences, continual alignment becomes
more crucial and practical in comparison to traditional static alignment.
Nevertheless, making RLHF compatible with Continual Learning (CL) is
challenging due to its complex process. Meanwhile, directly learning new human
preferences may lead to Catastrophic Forgetting (CF) of historical preferences,
resulting in helpless or harmful outputs. To overcome these challenges, we
propose the Continual Optimal Policy Regularization (COPR) method, which draws
inspiration from the optimal policy theory. COPR utilizes a sampling
distribution as a demonstration and regularization constraints for CL. It
adopts the Lagrangian Duality (LD) method to dynamically regularize the current
policy based on the historically optimal policy, which prevents CF and avoids
over-emphasizing unbalanced objectives. We also provide formal proof for the
learnability of COPR. The experimental results show that COPR outperforms
strong CL baselines on our proposed benchmark, in terms of reward-based, GPT-4
evaluations and human assessment. Furthermore, we validate the robustness of
COPR under various CL settings, including different backbones, replay memory
sizes, and learning orders.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comparing the Robustness of Modern No-Reference Image- and Video-Quality
  Metrics to Adversarial Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06958v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06958v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anastasia Antsiferova, Khaled Abud, Aleksandr Gushchin, Ekaterina Shumitskaya, Sergey Lavrushkin, Dmitriy Vatolin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, neural-network-based image- and video-quality metrics perform
better than traditional methods. However, they also became more vulnerable to
adversarial attacks that increase metrics' scores without improving visual
quality. The existing benchmarks of quality metrics compare their performance
in terms of correlation with subjective quality and calculation time.
Nonetheless, the adversarial robustness of image-quality metrics is also an
area worth researching. This paper analyses modern metrics' robustness to
different adversarial attacks. We adapted adversarial attacks from computer
vision tasks and compared attacks' efficiency against 15 no-reference image-
and video-quality metrics. Some metrics showed high resistance to adversarial
attacks, which makes their usage in benchmarks safer than vulnerable metrics.
The benchmark accepts submissions of new metrics for researchers who want to
make their metrics more robust to attacks or to find such metrics for their
needs. The latest results can be found online:
https://videoprocessing.ai/benchmarks/metrics-robustness.html.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GraphEdit: Large Language Models for Graph Structure Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15183v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15183v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zirui Guo, Lianghao Xia, Yanhua Yu, Yuling Wang, Zixuan Yang, Wei Wei, Liang Pang, Tat-Seng Chua, Chao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Structure Learning (GSL) focuses on capturing intrinsic dependencies
and interactions among nodes in graph-structured data by generating novel graph
structures. Graph Neural Networks (GNNs) have emerged as promising GSL
solutions, utilizing recursive message passing to encode node-wise
inter-dependencies. However, many existing GSL methods heavily depend on
explicit graph structural information as supervision signals, leaving them
susceptible to challenges such as data noise and sparsity. In this work, we
propose GraphEdit, an approach that leverages large language models (LLMs) to
learn complex node relationships in graph-structured data. By enhancing the
reasoning capabilities of LLMs through instruction-tuning over graph
structures, we aim to overcome the limitations associated with explicit graph
structural information and enhance the reliability of graph structure learning.
Our approach not only effectively denoises noisy connections but also
identifies node-wise dependencies from a global perspective, providing a
comprehensive understanding of the graph structure. We conduct extensive
experiments on multiple benchmark datasets to demonstrate the effectiveness and
robustness of GraphEdit across various settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conditional Unscented Autoencoders for Trajectory Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.19944v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.19944v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Faris Janjoš, Marcel Hallgarten, Anthony Knittel, Maxim Dolgov, Andreas Zell, J. Marius Zöllner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The CVAE is one of the most widely-used models in trajectory prediction for
AD. It captures the interplay between a driving context and its ground-truth
future into a probabilistic latent space and uses it to produce predictions. In
this paper, we challenge key components of the CVAE. We leverage recent
advances in the space of the VAE, the foundation of the CVAE, which show that a
simple change in the sampling procedure can greatly benefit performance. We
find that unscented sampling, which draws samples from any learned distribution
in a deterministic manner, can naturally be better suited to trajectory
prediction than potentially dangerous random sampling. We go further and offer
additional improvements including a more structured Gaussian mixture latent
space, as well as a novel, potentially more expressive way to do inference with
CVAEs. We show wide applicability of our models by evaluating them on the
INTERACTION prediction dataset, outperforming the state of the art, as well as
at the task of image modeling on the CelebA dataset, outperforming the baseline
vanilla CVAE. Code is available at
https://github.com/boschresearch/cuae-prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reinforcement Learning with Maskable Stock Representation for Portfolio
  Management in Customizable Stock Pools 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10801v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10801v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Zhang, Yilei Zhao, Shuo Sun, Jie Ying, Yonggang Xie, Zitao Song, Xinrun Wang, Bo An
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Portfolio management (PM) is a fundamental financial trading task, which
explores the optimal periodical reallocation of capitals into different stocks
to pursue long-term profits. Reinforcement learning (RL) has recently shown its
potential to train profitable agents for PM through interacting with financial
markets. However, existing work mostly focuses on fixed stock pools, which is
inconsistent with investors' practical demand. Specifically, the target stock
pool of different investors varies dramatically due to their discrepancy on
market states and individual investors may temporally adjust stocks they desire
to trade (e.g., adding one popular stocks), which lead to customizable stock
pools (CSPs). Existing RL methods require to retrain RL agents even with a tiny
change of the stock pool, which leads to high computational cost and unstable
performance. To tackle this challenge, we propose EarnMore, a rEinforcement
leARNing framework with Maskable stOck REpresentation to handle PM with CSPs
through one-shot training in a global stock pool (GSP). Specifically, we first
introduce a mechanism to mask out the representation of the stocks outside the
target pool. Second, we learn meaningful stock representations through a
self-supervised masking and reconstruction process. Third, a re-weighting
mechanism is designed to make the portfolio concentrate on favorable stocks and
neglect the stocks outside the target pool. Through extensive experiments on 8
subset stock pools of the US stock market, we demonstrate that EarnMore
significantly outperforms 14 state-of-the-art baselines in terms of 6 popular
financial metrics with over 40% improvement on profit.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distinguishing the Knowable from the Unknowable with Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03563v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03563v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gustaf Ahdritz, Tian Qin, Nikhil Vyas, Boaz Barak, Benjamin L. Edelman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the feasibility of identifying epistemic uncertainty (reflecting a
lack of knowledge), as opposed to aleatoric uncertainty (reflecting entropy in
the underlying distribution), in the outputs of large language models (LLMs)
over free-form text. In the absence of ground-truth probabilities, we explore a
setting where, in order to (approximately) disentangle a given LLM's
uncertainty, a significantly larger model stands in as a proxy for the ground
truth. We show that small linear probes trained on the embeddings of frozen,
pretrained models accurately predict when larger models will be more confident
at the token level and that probes trained on one text domain generalize to
others. Going further, we propose a fully unsupervised method that achieves
non-trivial accuracy on the same task. Taken together, we interpret these
results as evidence that LLMs naturally contain internal representations of
different types of uncertainty that could potentially be leveraged to devise
more informative indicators of model confidence in diverse practical settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Empirical Risk Minimization with Relative Entropy Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.06617v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.06617v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samir M. Perlaza, Gaetan Bisson, Iñaki Esnaola, Alain Jean-Marie, Stefano Rini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The empirical risk minimization (ERM) problem with relative entropy
regularization (ERM-RER) is investigated under the assumption that the
reference measure is a $\sigma$-finite measure, and not necessarily a
probability measure. Under this assumption, which leads to a generalization of
the ERM-RER problem allowing a larger degree of flexibility for incorporating
prior knowledge, numerous relevant properties are stated. Among these
properties, the solution to this problem, if it exists, is shown to be a unique
probability measure, mutually absolutely continuous with the reference measure.
Such a solution exhibits a probably-approximately-correct guarantee for the ERM
problem independently of whether the latter possesses a solution. For a fixed
dataset and under a specific condition, the empirical risk is shown to be a
sub-Gaussian random variable when the models are sampled from the solution to
the ERM-RER problem. The generalization capabilities of the solution to the
ERM-RER problem (the Gibbs algorithm) are studied via the sensitivity of the
expected empirical risk to deviations from such a solution towards alternative
probability measures. Finally, an interesting connection between sensitivity,
generalization error, and lautum information is established.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the IEEE Transactions on Information Theory: Submitted
  June 2023. Revised in October 2023. Accepted January 2024. Also available as:
  Research Report, INRIA, No. RR-9454, Centre Inria d'Universit\'e C\^ote
  d'Azur, Sophia Antipolis, France, Feb., 2022. Last version: Version 7</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity
  within Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13516v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13516v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyang Song, Xu Han, Zhengyan Zhang, Shengding Hu, Xiyu Shi, Kuai Li, Chen Chen, Zhiyuan Liu, Guangli Li, Tao Yang, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Activation sparsity refers to the existence of considerable
weakly-contributed elements among activation outputs. As a prevalent property
of the models using the ReLU activation function, it has been proven a
promising paradigm to boost model inference efficiency. Nevertheless, most
large language models (LLMs) adopt activation functions without intrinsic
activation sparsity (e.g., GELU and Swish). Some recent efforts have explored
introducing ReLU or its variants as the substitutive activation function to
help LLMs achieve activation sparsity and inference acceleration, but few can
simultaneously obtain high sparsity and comparable model performance. This
paper introduces an effective sparsification method named "ProSparse" to push
LLMs for higher activation sparsity without decreasing model performance.
Specifically, after substituting the activation function of LLMs with ReLU,
ProSparse adopts progressive sparsity regularization with a factor smoothly
increasing along sine curves in multiple stages. This can enhance activation
sparsity and alleviate performance degradation by avoiding radical shifts in
activation distribution. With ProSparse, we obtain high sparsity of 89.32% and
88.80% for LLaMA2-7B and LLaMA2-13B, respectively, achieving comparable
performance to their original Swish-activated versions. Our inference
acceleration experiments further demonstrate the practical acceleration brought
by higher activation sparsity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 3 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Communication-Efficient Federated Bilevel Optimization with Local and
  Global Lower Level Problems <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.06701v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.06701v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyi Li, Feihu Huang, Heng Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bilevel Optimization has witnessed notable progress recently with new
emerging efficient algorithms. However, its application in the Federated
Learning setting remains relatively underexplored, and the impact of Federated
Learning's inherent challenges on the convergence of bilevel algorithms remain
obscure. In this work, we investigate Federated Bilevel Optimization problems
and propose a communication-efficient algorithm, named FedBiOAcc. The algorithm
leverages an efficient estimation of the hyper-gradient in the distributed
setting and utilizes the momentum-based variance-reduction acceleration.
Remarkably, FedBiOAcc achieves a communication complexity $O(\epsilon^{-1})$, a
sample complexity $O(\epsilon^{-1.5})$ and the linear speed up with respect to
the number of clients. We also analyze a special case of the Federated Bilevel
Optimization problems, where lower level problems are locally managed by
clients. We prove that FedBiOAcc-Local, a modified version of FedBiOAcc,
converges at the same rate for this type of problems. Finally, we validate the
proposed algorithms through two real-world tasks: Federated Data-cleaning and
Federated Hyper-representation Learning. Empirical results show superior
performance of our algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023 version (Algorithm 1 is updated to be more concise)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Goal-Space Planning with Subgoal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.02902v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.02902v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunlok Lo, Kevin Roice, Parham Mohammad Panahi, Scott Jordan, Adam White, Gabor Mihucz, Farzane Aminmansour, Martha White
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates a new approach to model-based reinforcement learning
using background planning: mixing (approximate) dynamic programming updates and
model-free updates, similar to the Dyna architecture. Background planning with
learned models is often worse than model-free alternatives, such as Double DQN,
even though the former uses significantly more memory and computation. The
fundamental problem is that learned models can be inaccurate and often generate
invalid states, especially when iterated many steps. In this paper, we avoid
this limitation by constraining background planning to a set of (abstract)
subgoals and learning only local, subgoal-conditioned models. This goal-space
planning (GSP) approach is more computationally efficient, naturally
incorporates temporal abstraction for faster long-horizon planning and avoids
learning the transition dynamics entirely. We show that our GSP algorithm can
propagate value from an abstract space in a manner that helps a variety of base
learners learn significantly faster in different domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Foundation Model for General Moving Object Segmentation in Medical
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.17264v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.17264v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongnuo Yan, Tong Han, Yuhao Huang, Lian Liu, Han Zhou, Jiongquan Chen, Wenlong Shi, Yan Cao, Xin Yang, Dong Ni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image segmentation aims to delineate the anatomical or pathological
structures of interest, playing a crucial role in clinical diagnosis. A
substantial amount of high-quality annotated data is crucial for constructing
high-precision deep segmentation models. However, medical annotation is highly
cumbersome and time-consuming, especially for medical videos or 3D volumes, due
to the huge labeling space and poor inter-frame consistency. Recently, a
fundamental task named Moving Object Segmentation (MOS) has made significant
advancements in natural images. Its objective is to delineate moving objects
from the background within image sequences, requiring only minimal annotations.
In this paper, we propose the first foundation model, named iMOS, for MOS in
medical images. Extensive experiments on a large multi-modal medical dataset
validate the effectiveness of the proposed iMOS. Specifically, with the
annotation of only a small number of images in the sequence, iMOS can achieve
satisfactory tracking and segmentation performance of moving objects throughout
the entire sequence in bi-directions. We hope that the proposed iMOS can help
accelerate the annotation speed of experts, and boost the development of
medical foundation models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 7 figures, 3 tables. This paper has been accepted by ISBI
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the dynamics of three-layer neural networks: initial condensation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15958v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15958v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng-An Chen, Tao Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Empirical and theoretical works show that the input weights of two-layer
neural networks, when initialized with small values, converge towards isolated
orientations. This phenomenon, referred to as condensation, indicates that the
gradient descent methods tend to spontaneously reduce the complexity of neural
networks during the training process. In this work, we elucidate the mechanisms
behind the condensation phenomena occurring in the training of three-layer
neural networks and distinguish it from the training of two-layer neural
networks. Through rigorous theoretical analysis, we establish the blow-up
property of effective dynamics and present a sufficient condition for the
occurrence of condensation, findings that are substantiated by experimental
results. Additionally, we explore the association between condensation and the
low-rank bias observed in deep matrix factorization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cascade Speculative Drafting for Even Faster LLM Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11462v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11462v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyi Chen, Xiaocong Yang, Jiacheng Lin, Chenkai Sun, Kevin Chen-Chuan Chang, Jie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Introduced to enhance the efficiency of large language model (LLM) inference,
speculative decoding operates by having a smaller model generate a draft. A
larger target model then reviews this draft to align with its output, and any
acceptance by the target model results in a reduction of the number of the
target model runs, ultimately improving efficiency. However, the drafting
process in speculative decoding includes slow autoregressive generation and
allocates equal time to generating tokens, irrespective of their importance.
These inefficiencies collectively contribute to the suboptimal performance of
speculative decoding. To further improve LLM inference, we introduce Cascade
Speculative Drafting (CS Drafting), a speculative execution algorithm that
incorporates two types of cascades. The Vertical Cascade eliminates
autoregressive generation from neural models, while the Horizontal Cascade
optimizes time allocation in drafting for improved efficiency. Combining both
cascades, CS Drafting achieves up to an 81 percent additional speedup over
speculative decoding in our experiments, while maintaining the same output
distribution as the target model. Our code is publicly available at
https://github.com/lfsszd/CS-Drafting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint in progress</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dance2MIDI: Dance-driven multi-instruments music generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09080v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09080v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Han, Yuheng Li, Yixuan Shen, Yi Ren, Feilin Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dance-driven music generation aims to generate musical pieces conditioned on
dance videos. Previous works focus on monophonic or raw audio generation, while
the multi-instruments scenario is under-explored. The challenges associated
with the dance-driven multi-instrument music (MIDI) generation are twofold: 1)
no publicly available multi-instruments MIDI and video paired dataset and 2)
the weak correlation between music and video. To tackle these challenges, we
build the first multi-instruments MIDI and dance paired dataset (D2MIDI). Based
on our proposed dataset, we introduce a multi-instruments MIDI generation
framework (Dance2MIDI) conditioned on dance video. Specifically, 1) to capture
the relationship between dance and music, we employ the Graph Convolutional
Network to encode the dance motion. This allows us to extract features related
to dance movement and dance style, 2) to generate a harmonious rhythm, we
utilize a Transformer model to decode the drum track sequence, leveraging a
cross-attention mechanism, and 3) we model the task of generating the remaining
tracks based on the drum track as a sequence understanding and completion task.
A BERT-like model is employed to comprehend the context of the entire music
piece through self-supervised learning. We evaluate the generated music of our
framework trained on the D2MIDI dataset and demonstrate that our method
achieves State-of-the-Art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>has been accepted by Computational Visual Media Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comparing the Robustness of Modern No-Reference Image- and Video-Quality
  Metrics to Adversarial Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06958v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06958v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anastasia Antsiferova, Khaled Abud, Aleksandr Gushchin, Ekaterina Shumitskaya, Sergey Lavrushkin, Dmitriy Vatolin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, neural-network-based image- and video-quality metrics perform
better than traditional methods. However, they also became more vulnerable to
adversarial attacks that increase metrics' scores without improving visual
quality. The existing benchmarks of quality metrics compare their performance
in terms of correlation with subjective quality and calculation time.
Nonetheless, the adversarial robustness of image-quality metrics is also an
area worth researching. This paper analyses modern metrics' robustness to
different adversarial attacks. We adapted adversarial attacks from computer
vision tasks and compared attacks' efficiency against 15 no-reference image-
and video-quality metrics. Some metrics showed high resistance to adversarial
attacks, which makes their usage in benchmarks safer than vulnerable metrics.
The benchmark accepts submissions of new metrics for researchers who want to
make their metrics more robust to attacks or to find such metrics for their
needs. The latest results can be found online:
https://videoprocessing.ai/benchmarks/metrics-robustness.html.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SPICA: Interactive Video Content Exploration through Augmented Audio
  Descriptions for Blind or Low-Vision Viewers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07300v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07300v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Ning, Brianna L. Wimer, Kaiwen Jiang, Keyi Chen, Jerrick Ban, Yapeng Tian, Yuhang Zhao, Toby Jia-Jun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Blind or Low-Vision (BLV) users often rely on audio descriptions (AD) to
access video content. However, conventional static ADs can leave out detailed
information in videos, impose a high mental load, neglect the diverse needs and
preferences of BLV users, and lack immersion. To tackle these challenges, we
introduce SPICA, an AI-powered system that enables BLV users to interactively
explore video content. Informed by prior empirical studies on BLV video
consumption, SPICA offers novel interactive mechanisms for supporting temporal
navigation of frame captions and spatial exploration of objects within key
frames. Leveraging an audio-visual machine learning pipeline, SPICA augments
existing ADs by adding interactivity, spatial sound effects, and individual
object descriptions without requiring additional human annotation. Through a
user study with 14 BLV participants, we evaluated the usability and usefulness
of SPICA and explored user behaviors, preferences, and mental models when
interacting with augmented ADs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InteractDiffusion: Interaction Control in Text-to-Image Diffusion Models <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05849v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05849v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiun Tian Hoe, Xudong Jiang, Chee Seng Chan, Yap-Peng Tan, Weipeng Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale text-to-image (T2I) diffusion models have showcased incredible
capabilities in generating coherent images based on textual descriptions,
enabling vast applications in content generation. While recent advancements
have introduced control over factors such as object localization, posture, and
image contours, a crucial gap remains in our ability to control the
interactions between objects in the generated content. Well-controlling
interactions in generated images could yield meaningful applications, such as
creating realistic scenes with interacting characters. In this work, we study
the problems of conditioning T2I diffusion models with Human-Object Interaction
(HOI) information, consisting of a triplet label (person, action, object) and
corresponding bounding boxes. We propose a pluggable interaction control model,
called InteractDiffusion that extends existing pre-trained T2I diffusion models
to enable them being better conditioned on interactions. Specifically, we
tokenize the HOI information and learn their relationships via interaction
embeddings. A conditioning self-attention layer is trained to map HOI tokens to
visual tokens, thereby conditioning the visual tokens better in existing T2I
diffusion models. Our model attains the ability to control the interaction and
location on existing T2I diffusion models, which outperforms existing baselines
by a large margin in HOI detection score, as well as fidelity in FID and KID.
Project page: https://jiuntian.github.io/interactdiffusion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: https://jiuntian.github.io/interactdiffusion. Accepted at
  CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-02-26T00:00:00Z">2024-02-26</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">44</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Large Language Models for Learning Complex Legal Concepts
  through Storytelling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17019v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17019v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Jiang, Xiajie Zhang, Robert Mahari, Daniel Kessler, Eric Ma, Tal August, Irene Li, Alex 'Sandy' Pentland, Yoon Kim, Jad Kabbara, Deb Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Making legal knowledge accessible to non-experts is crucial for enhancing
general legal literacy and encouraging civic participation in democracy.
However, legal documents are often challenging to understand for people without
legal backgrounds. In this paper, we present a novel application of large
language models (LLMs) in legal education to help non-experts learn intricate
legal concepts through storytelling, an effective pedagogical tool in conveying
complex and abstract concepts. We also introduce a new dataset LegalStories,
which consists of 295 complex legal doctrines, each accompanied by a story and
a set of multiple-choice questions generated by LLMs. To construct the dataset,
we experiment with various LLMs to generate legal stories explaining these
concepts. Furthermore, we use an expert-in-the-loop method to iteratively
design multiple-choice questions. Then, we evaluate the effectiveness of
storytelling with LLMs through an RCT experiment with legal novices on 10
samples from the dataset. We find that LLM-generated stories enhance
comprehension of legal concepts and interest in law among non-native speakers
compared to only definitions. Moreover, stories consistently help participants
relate legal concepts to their lives. Finally, we find that learning with
stories shows a higher retention rate for non-native speakers in the follow-up
assessment. Our work has strong implications for using LLMs in promoting
teaching and learning in the legal field and beyond.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Task Contrastive Learning for 8192-Token Bilingual Text Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17016v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17016v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isabelle Mohr, Markus Krimmel, Saba Sturua, Mohammad Kalim Akram, Andreas Koukounas, Michael Günther, Georgios Mastrapas, Vinit Ravishankar, Joan Fontanals Martínez, Feng Wang, Qi Liu, Ziniu Yu, Jie Fu, Saahil Ognawala, Susana Guzman, Bo Wang, Maximilian Werk, Nan Wang, Han Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel suite of state-of-the-art bilingual text embedding
models that are designed to support English and another target language. These
models are capable of processing lengthy text inputs with up to 8192 tokens,
making them highly versatile for a range of natural language processing tasks
such as text retrieval, clustering, and semantic textual similarity (STS)
calculations.
  By focusing on bilingual models and introducing a unique multi-task learning
objective, we have significantly improved the model performance on STS tasks,
which outperforms the capabilities of existing multilingual models in both
target language understanding and cross-lingual evaluation tasks. Moreover, our
bilingual models are more efficient, requiring fewer parameters and less memory
due to their smaller vocabulary needs. Furthermore, we have expanded the
Massive Text Embedding Benchmark (MTEB) to include benchmarks for German and
Spanish embedding models. This integration aims to stimulate further research
and advancement in text embedding technologies for these languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Z-AGI Labs at ClimateActivism 2024: Stance and Hate Event Detection on
  Social Media <span class="chip">EACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17014v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17014v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikhil Narayan, Mrutyunjay Biswal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the digital realm, rich data serves as a crucial source of insights into
the complexities of social, political, and economic landscapes. Addressing the
growing need for high-quality information on events and the imperative to
combat hate speech, this research led to the establishment of the Shared Task
on Climate Activism Stance and Hate Event Detection at CASE 2024. Focused on
climate activists contending with hate speech on social media, our study
contributes to hate speech identification from tweets. Analyzing three
sub-tasks - Hate Speech Detection (Sub-task A), Targets of Hate Speech
Identification (Sub-task B), and Stance Detection (Sub-task C) - Team Z-AGI
Labs evaluated various models, including LSTM, Xgboost, and LGBM based on
Tf-Idf. Results unveiled intriguing variations, with Catboost excelling in
Subtask-B (F1: 0.5604) and Subtask-C (F1: 0.7081), while LGBM emerged as the
top-performing model for Subtask-A (F1: 0.8684). This research provides
valuable insights into the suitability of classical machine learning models for
climate hate speech and stance detection, aiding informed model selection for
robust mechanisms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as Working Notes in CASE-EACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Explainability and Fairness in Swiss Judgement Prediction:
  Benchmarking on a Multilingual <span class="highlight-title">Dataset</span> <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17013v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17013v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Santosh T. Y. S. S, Nina Baumgartner, Matthias Stürmer, Matthias Grabmair, Joel Niklaus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The assessment of explainability in Legal Judgement Prediction (LJP) systems
is of paramount importance in building trustworthy and transparent systems,
particularly considering the reliance of these systems on factors that may lack
legal relevance or involve sensitive attributes. This study delves into the
realm of explainability and fairness in LJP models, utilizing Swiss Judgement
Prediction (SJP), the only available multilingual LJP dataset. We curate a
comprehensive collection of rationales that `support' and `oppose' judgement
from legal experts for 108 cases in German, French, and Italian. By employing
an occlusion-based explainability approach, we evaluate the explainability
performance of state-of-the-art monolingual and multilingual BERT-based LJP
models, as well as models developed with techniques such as data augmentation
and cross-lingual transfer, which demonstrated prediction performance
improvement. Notably, our findings reveal that improved prediction performance
does not necessarily correspond to enhanced explainability performance,
underscoring the significance of evaluating models from an explainability
perspective. Additionally, we introduce a novel evaluation framework, Lower
Court Insertion (LCI), which allows us to quantify the influence of lower court
information on model predictions, exposing current models' biases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffuCOMET: Contextual Commonsense Knowledge Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17011v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17011v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Silin Gao, Mete Ismayilzada, Mengjie Zhao, Hiromi Wakaki, Yuki Mitsufuji, Antoine Bosselut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inferring contextually-relevant and diverse commonsense to understand
narratives remains challenging for knowledge models. In this work, we develop a
series of knowledge models, DiffuCOMET, that leverage diffusion to learn to
reconstruct the implicit semantic connections between narrative contexts and
relevant commonsense knowledge. Across multiple diffusion steps, our method
progressively refines a representation of commonsense facts that is anchored to
a narrative, producing contextually-relevant and diverse commonsense inferences
for an input context. To evaluate DiffuCOMET, we introduce new metrics for
commonsense inference that more closely measure knowledge diversity and
contextual relevance. Our results on two different benchmarks, ComFact and
WebNLG+, show that knowledge generated by DiffuCOMET achieves a better
trade-off between commonsense diversity, contextual relevance and alignment to
known gold references, compared to baseline knowledge models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Large Language Models Recall Reference Location Like Humans? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye Wang, Xinrun Xu, Rui Xie, Wenxin Hu, Wei Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When completing knowledge-intensive tasks, humans sometimes need not just an
answer but also a corresponding reference passage for auxiliary reading.
Previous methods required obtaining pre-segmented article chunks through
additional retrieval models. This paper explores leveraging the parameterized
knowledge stored during the pre-training phase of large language models (LLMs)
to independently recall reference passage from any starting position. We
propose a two-stage framework that simulates the scenario of humans recalling
easily forgotten references. Initially, the LLM is prompted to recall document
title identifiers to obtain a coarse-grained document set. Then, based on the
acquired coarse-grained document set, it recalls fine-grained passage. In the
two-stage recall process, we use constrained decoding to ensure that content
outside of the stored documents is not generated. To increase speed, we only
recall a short prefix in the second stage, then locate its position to retrieve
a complete passage. Experiments on KILT knowledge-sensitive tasks have verified
that LLMs can independently recall reference passage location in various task
forms, and the obtained reference significantly assist downstream tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking LLMs on the Semantic Overlap Summarization Task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17008v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17008v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Salvador, Naman Bansal, Mousumi Akter, Souvika Sarkar, Anupam Das, Shubhra Kanti Karmaker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic Overlap Summarization (SOS) is a constrained multi-document
summarization task, where the constraint is to capture the common/overlapping
information between two alternative narratives. While recent advancements in
Large Language Models (LLMs) have achieved superior performance in numerous
summarization tasks, a benchmarking study of the SOS task using LLMs is yet to
be performed. As LLMs' responses are sensitive to slight variations in prompt
design, a major challenge in conducting such a benchmarking study is to
systematically explore a variety of prompts before drawing a reliable
conclusion. Fortunately, very recently, the TELeR taxonomy has been proposed
which can be used to design and explore various prompts for LLMs. Using this
TELeR taxonomy and 15 popular LLMs, this paper comprehensively evaluates LLMs
on the SOS Task, assessing their ability to summarize overlapping information
from multiple alternative narratives. For evaluation, we report
well-established metrics like ROUGE, BERTscore, and SEM-F1$ on two different
datasets of alternative narratives. We conclude the paper by analyzing the
strengths and limitations of various LLMs in terms of their capabilities in
capturing overlapping information The code and datasets used to conduct this
study are available at https://anonymous.4open.science/r/llm_eval-E16D.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Do Language Models Hear? Probing for Auditory Representations in
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16998v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16998v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jerry Ngo, Yoon Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work explores whether language models encode meaningfully grounded
representations of sounds of objects. We learn a linear probe that retrieves
the correct text representation of an object given a snippet of audio related
to that object, where the sound representation is given by a pretrained audio
model. This probe is trained via a contrastive loss that pushes the language
representations and sound representations of an object to be close to one
another. After training, the probe is tested on its ability to generalize to
objects that were not seen during training. Across different language models
and audio models, we find that the probe generalization is above chance in many
cases, indicating that despite being trained only on raw text, language models
encode grounded knowledge of sounds for some objects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long Dialog Summarization: An Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16986v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16986v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ankan Mullick, Ayan Kumar Bhowmick, Raghav R, Ravi Kokku, Prasenjit Dey, Pawan Goyal, Niloy Ganguly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dialog summarization has become increasingly important in managing and
comprehending large-scale conversations across various domains. This task
presents unique challenges in capturing the key points, context, and nuances of
multi-turn long conversations for summarization. It is worth noting that the
summarization techniques may vary based on specific requirements such as in a
shopping-chatbot scenario, the dialog summary helps to learn user preferences,
whereas in the case of a customer call center, the summary may involve the
problem attributes that a user specified, and the final resolution provided.
This work emphasizes the significance of creating coherent and contextually
rich summaries for effective communication in various applications. We explore
current state-of-the-art approaches for long dialog summarization in different
domains and benchmark metrics based evaluations show that one single model does
not perform well across various areas for distinct summarization tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dealing with Data for RE: Mitigating Challenges using NLP and Generative
  AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16977v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16977v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Smita Ghaisas, Anmol Singhal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Across the dynamic business landscape today, enterprises face an
ever-increasing range of challenges. These include the constantly evolving
regulatory environment, the growing demand for personalization within software
applications, and the heightened emphasis on governance. In response to these
multifaceted demands, large enterprises have been adopting automation that
spans from the optimization of core business processes to the enhancement of
customer experiences. Indeed, Artificial Intelligence (AI) has emerged as a
pivotal element of modern software systems. In this context, data plays an
indispensable role. AI-centric software systems based on supervised learning
and operating at an industrial scale require large volumes of training data to
perform effectively. Moreover, the incorporation of generative AI has led to a
growing demand for adequate evaluation benchmarks. Our experience in this field
has revealed that the requirement for large datasets for training and
evaluation introduces a host of intricate challenges. This book chapter
explores the evolving landscape of Software Engineering (SE) in general, and
Requirements Engineering (RE) in particular, in this era marked by AI
integration. We discuss challenges that arise while integrating Natural
Language Processing (NLP) and generative AI into enterprise-critical software
systems. The chapter provides practical insights, solutions, and examples to
equip readers with the knowledge and tools necessary for effectively building
solutions with NLP at their cores. We also reflect on how these text
data-centric tasks sit together with the traditional RE process. We also
highlight new RE tasks that may be necessary for handling the increasingly
important text data-centricity involved in developing software systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 2 figures, to be published in NLP for Requirements
  Engineering Book</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Successfully Guiding Humans with Imperfect Instructions by Highlighting
  Potential Errors and Suggesting Corrections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingjun Zhao, Khanh Nguyen, Hal Daumé III
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the challenge of leveraging imperfect language models to
guide human decision-making in the context of a grounded navigation task. We
show that an imperfect instruction generation model can be complemented with an
effective communication mechanism to become more successful at guiding humans.
The communication mechanism we build comprises models that can detect potential
hallucinations in instructions and suggest practical alternatives, and an
intuitive interface to present that information to users. We show that this
approach reduces the human navigation error by up to 29% with no additional
cognitive burden. This result underscores the potential of integrating diverse
communication channels into AI systems to compensate for their imperfections
and enhance their utility for humans.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GROUNDHOG: Grounding Large Language Models to Holistic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichi Zhang, Ziqiao Ma, Xiaofeng Gao, Suhaila Shakiah, Qiaozi Gao, Joyce Chai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most multimodal large language models (MLLMs) learn language-to-object
grounding through causal language modeling where grounded objects are captured
by bounding boxes as sequences of location tokens. This paradigm lacks
pixel-level representations that are important for fine-grained visual
understanding and diagnosis. In this work, we introduce GROUNDHOG, an MLLM
developed by grounding Large Language Models to holistic segmentation.
GROUNDHOG incorporates a masked feature extractor and converts extracted
features into visual entity tokens for the MLLM backbone, which then connects
groundable phrases to unified grounding masks by retrieving and merging the
entity masks. To train GROUNDHOG, we carefully curated M3G2, a grounded visual
instruction tuning dataset with Multi-Modal Multi-Grained Grounding, by
harvesting a collection of segmentation-grounded datasets with rich
annotations. Our experimental results show that GROUNDHOG achieves superior
performance on various language grounding tasks without task-specific
fine-tuning, and significantly reduces object hallucination. GROUNDHOG also
demonstrates better grounding towards complex forms of visual input and
provides easy-to-understand diagnosis in failure cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: https://groundhog-mllm.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Think Big, Generate Quick: LLM-to-SLM for Fast Autoregressive Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Bergner, Andrii Skliar, Amelie Royer, Tijmen Blankevoort, Yuki Asano, Babak Ehteshami Bejnordi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have become ubiquitous in practice and are
widely used for generation tasks such as translation, summarization and
instruction following. However, their enormous size and reliance on
autoregressive decoding increase deployment costs and complicate their use in
latency-critical applications. In this work, we propose a hybrid approach that
combines language models of different sizes to increase the efficiency of
autoregressive decoding while maintaining high performance. Our method utilizes
a pretrained frozen LLM that encodes all prompt tokens once in parallel, and
uses the resulting representations to condition and guide a small language
model (SLM), which then generates the response more efficiently. We investigate
the combination of encoder-decoder LLMs with both encoder-decoder and
decoder-only SLMs from different model families and only require fine-tuning of
the SLM. Experiments with various benchmarks show substantial speedups of up to
$4\times$, with minor performance penalties of $1-2\%$ for translation and
summarization tasks compared to the LLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-LoRA Composition for Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16843v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16843v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Zhong, Yelong Shen, Shuohang Wang, Yadong Lu, Yizhu Jiao, Siru Ouyang, Donghan Yu, Jiawei Han, Weizhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-Rank Adaptation (LoRA) is extensively utilized in text-to-image models
for the accurate rendition of specific elements like distinct characters or
unique styles in generated images. Nonetheless, existing methods face
challenges in effectively composing multiple LoRAs, especially as the number of
LoRAs to be integrated grows, thus hindering the creation of complex imagery.
In this paper, we study multi-LoRA composition through a decoding-centric
perspective. We present two training-free methods: LoRA Switch, which
alternates between different LoRAs at each denoising step, and LoRA Composite,
which simultaneously incorporates all LoRAs to guide more cohesive image
synthesis. To evaluate the proposed approaches, we establish ComposLoRA, a new
comprehensive testbed as part of this research. It features a diverse range of
LoRA categories with 480 composition sets. Utilizing an evaluation framework
based on GPT-4V, our findings demonstrate a clear improvement in performance
with our methods over the prevalent baseline, particularly evident when
increasing the number of LoRAs in a composition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Website:
  https://maszhongming.github.io/Multi-LoRA-Composition/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MobiLlama: Towards Accurate and Lightweight Fully Transparent <span class="highlight-title">GPT</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16840v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16840v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omkar Thawakar, Ashmal Vayani, Salman Khan, Hisham Cholakal, Rao M. Anwer, Michael Felsberg, Tim Baldwin, Eric P. Xing, Fahad Shahbaz Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  "Bigger the better" has been the predominant trend in recent Large Language
Models (LLMs) development. However, LLMs do not suit well for scenarios that
require on-device processing, energy efficiency, low memory footprint, and
response efficiency. These requisites are crucial for privacy, security, and
sustainable deployment. This paper explores the "less is more" paradigm by
addressing the challenge of designing accurate yet efficient Small Language
Models (SLMs) for resource constrained devices. Our primary contribution is the
introduction of an accurate and fully transparent open-source 0.5 billion
(0.5B) parameter SLM, named MobiLlama, catering to the specific needs of
resource-constrained computing with an emphasis on enhanced performance with
reduced resource demands. MobiLlama is a SLM design that initiates from a
larger model and applies a careful parameter sharing scheme to reduce both the
pre-training and the deployment cost. Our work strives to not only bridge the
gap in open-source SLMs but also ensures full transparency, where complete
training data pipeline, training code, model weights, and over 300 checkpoints
along with evaluation codes is available at :
https://github.com/mbzuai-oryx/MobiLlama.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at : https://github.com/mbzuai-oryx/MobiLlama</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do Large Language Models Latently Perform Multi-Hop Reasoning? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, Sebastian Riedel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study whether Large Language Models (LLMs) latently perform multi-hop
reasoning with complex prompts such as "The mother of the singer of
'Superstition' is". We look for evidence of a latent reasoning pathway where an
LLM (1) latently identifies "the singer of 'Superstition'" as Stevie Wonder,
the bridge entity, and (2) uses its knowledge of Stevie Wonder's mother to
complete the prompt. We analyze these two hops individually and consider their
co-occurrence as indicative of latent multi-hop reasoning. For the first hop,
we test if changing the prompt to indirectly mention the bridge entity instead
of any other entity increases the LLM's internal recall of the bridge entity.
For the second hop, we test if increasing this recall causes the LLM to better
utilize what it knows about the bridge entity. We find strong evidence of
latent multi-hop reasoning for the prompts of certain relation types, with the
reasoning pathway used in more than 80% of the prompts. However, the
utilization is highly contextual, varying across different types of prompts.
Also, on average, the evidence for the second hop and the full multi-hop
traversal is rather moderate and only substantial for the first hop. Moreover,
we find a clear scaling trend with increasing model size for the first hop of
reasoning but not for the second hop. Our experimental findings suggest
potential challenges and opportunities for future development and applications
of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PhyGrasp: Generalizing Robotic Grasping with Physics-informed Large
  Multimodal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16836v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16836v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingkun Guo, Yuqi Xiang, Shuqi Zhao, Xinghao Zhu, Masayoshi Tomizuka, Mingyu Ding, Wei Zhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic grasping is a fundamental aspect of robot functionality, defining how
robots interact with objects. Despite substantial progress, its
generalizability to counter-intuitive or long-tailed scenarios, such as objects
with uncommon materials or shapes, remains a challenge. In contrast, humans can
easily apply their intuitive physics to grasp skillfully and change grasps
efficiently, even for objects they have never seen before.
  This work delves into infusing such physical commonsense reasoning into
robotic manipulation. We introduce PhyGrasp, a multimodal large model that
leverages inputs from two modalities: natural language and 3D point clouds,
seamlessly integrated through a bridge module. The language modality exhibits
robust reasoning capabilities concerning the impacts of diverse physical
properties on grasping, while the 3D modality comprehends object shapes and
parts. With these two capabilities, PhyGrasp is able to accurately assess the
physical properties of object parts and determine optimal grasping poses.
Additionally, the model's language comprehension enables human instruction
interpretation, generating grasping poses that align with human preferences. To
train PhyGrasp, we construct a dataset PhyPartNet with 195K object instances
with varying physical properties and human preferences, alongside their
corresponding language descriptions. Extensive experiments conducted in the
simulation and on the real robots demonstrate that PhyGrasp achieves
state-of-the-art performance, particularly in long-tailed cases, e.g., about
10% improvement in success rate over GraspNet. Project page:
https://sites.google.com/view/phygrasp
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Eight Methods to Evaluate Robust Unlearning in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16835v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16835v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aengus Lynch, Phillip Guo, Aidan Ewart, Stephen Casper, Dylan Hadfield-Menell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine unlearning can be useful for removing harmful capabilities and
memorized text from large language models (LLMs), but there are not yet
standardized methods for rigorously evaluating it. In this paper, we first
survey techniques and limitations of existing unlearning evaluations. Second,
we apply a comprehensive set of tests for the robustness and competitiveness of
unlearning in the "Who's Harry Potter" (WHP) model from Eldan and Russinovich
(2023). While WHP's unlearning generalizes well when evaluated with the
"Familiarity" metric from Eldan and Russinovich, we find i)
higher-than-baseline amounts of knowledge can reliably be extracted, ii) WHP
performs on par with the original model on Harry Potter Q&A tasks, iii) it
represents latent knowledge comparably to the original model, and iv) there is
collateral unlearning in related domains. Overall, our results highlight the
importance of comprehensive unlearning evaluation that avoids ad-hoc metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mysterious Projections: Multimodal LLMs Gain Domain-Specific Visual
  Capabilities Without Richer Cross-Modal Projections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16832v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16832v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaurav Verma, Minje Choi, Kartik Sharma, Jamelle Watson-Daniels, Sejoon Oh, Srijan Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) like LLaVA and GPT-4(V) enable
general-purpose conversations about images with the language modality. As
off-the-shelf MLLMs may have limited capabilities on images from domains like
dermatology and agriculture, they must be fine-tuned to unlock domain-specific
applications. The prevalent architecture of current open-source MLLMs comprises
two major modules: an image-language (cross-modal) projection network and a
large language model. It is desirable to understand the roles of these two
modules in modeling domain-specific visual attributes to inform the design of
future models and streamline the interpretability efforts on the current
models. To this end, via experiments on 4 datasets and under 2 fine-tuning
settings, we find that as the MLLM is fine-tuned, it indeed gains
domain-specific visual capabilities, but the updates do not lead to the
projection extracting relevant domain-specific visual attributes. Our results
indicate that the domain-specific visual attributes are modeled by the LLM,
even when only the projection is fine-tuned. Through this study, we offer a
potential reinterpretation of the role of cross-modal projections in MLLM
architectures. Projection webpage:
https://claws-lab.github.io/projection-in-MLLMs/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SKILL: Similarity-aware Knowledge distILLation for Speech
  <span class="highlight-title">Self-Supervised</span> Learning <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16830v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16830v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Zampierin, Ghouthi Boukli Hacene, Bac Nguyen, Mirco Ravanelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) has achieved remarkable success across various
speech-processing tasks. To enhance its efficiency, previous works often
leverage the use of compression techniques. A notable recent attempt is
DPHuBERT, which applies joint knowledge distillation (KD) and structured
pruning to learn a significantly smaller SSL model. In this paper, we
contribute to this research domain by introducing SKILL, a novel method that
conducts distillation across groups of layers instead of distilling individual
arbitrarily selected layers within the teacher network. The identification of
the layers to distill is achieved through a hierarchical clustering procedure
applied to layer similarity measures. Extensive experiments demonstrate that
our distilled version of WavLM Base+ not only outperforms DPHuBERT but also
achieves state-of-the-art results in the 30M parameters model class across
several SUPERB tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the Self-supervision in Audio, Speech and Beyond (SASB)
  Workshop at ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GISTEmbed: Guided In-sample Selection of Training Negatives for Text
  Embedding Fine-tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16829v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16829v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aivin V. Solatorio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embedding models are integral to AI applications like semantic search,
personalized recommendations, and retrieval augmented generation for LLMs,
necessitating high-quality training data. However, the limited scalability of
manual data curation prompts the need for automated methods to ensure data
integrity. Traditional unsupervised triplet mining automates training data
generation, crucial for embedding model training, yet inadvertently injects
biases and noise, thereby degrading model performance. Addressing this, we
introduce GISTEmbed, a novel strategy that enhances in-batch negative selection
during contrastive training through a guide model. This approach departs from
reliance on random sampling and equal utility assumption of batch negatives,
significantly reducing noise from data quality issues and improving model
fine-tuning. Benchmarked against the Massive Text Embedding Benchmark (MTEB),
GISTEmbed showcases consistent performance improvements across various model
sizes and achieves state-of-the-art results in select categories. This
framework enables significant enhancements for smaller models by leveraging the
capabilities of powerful yet resource-intensive large models. GISTEmbed can
potentially revolutionize the creation of highly efficient, smaller models,
democratizing access to advanced AI technologies. Making these technologies
more accessible and cost-effective, especially for applications constrained by
resources, significantly expands the impact and accessibility of
state-of-the-art AI solutions across diverse sectors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>GISTEmbed GitHub repository at
  https://github.com/avsolatorio/GISTEmbed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Data Selection for Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, Colin Raffel, Shiyu Chang, Tatsunori Hashimoto, William Yang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A major factor in the recent success of large language models is the use of
enormous and ever-growing text datasets for unsupervised pre-training. However,
naively training a model on all available data may not be optimal (or
feasible), as the quality of available text data can vary. Filtering out data
can also decrease the carbon footprint and financial costs of training models
by reducing the amount of training required.
  Data selection methods aim to determine which candidate data points to
include in the training dataset and how to appropriately sample from the
selected data points. The promise of improved data selection methods has caused
the volume of research in the area to rapidly expand. However, because deep
learning is mostly driven by empirical evidence and experimentation on
large-scale data is expensive, few organizations have the resources for
extensive data selection research. Consequently, knowledge of effective data
selection practices has become concentrated within a few organizations, many of
which do not openly share their findings and methodologies.
  To narrow this gap in knowledge, we present a comprehensive review of
existing literature on data selection methods and related research areas,
providing a taxonomy of existing approaches. By describing the current
landscape of research, this work aims to accelerate progress in data selection
by establishing an entry point for new and established researchers.
Additionally, throughout this review we draw attention to noticeable holes in
the literature and conclude the paper by proposing promising avenues for future
research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rainbow Teaming: Open-Ended Generation of Diverse Adversarial <span class="highlight-title">Prompt</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikayel Samvelyan, Sharath Chandra Raparthy, Andrei Lupu, Eric Hambro, Aram H. Markosyan, Manish Bhatt, Yuning Mao, Minqi Jiang, Jack Parker-Holder, Jakob Foerster, Tim Rocktäschel, Roberta Raileanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) become increasingly prevalent across many
real-world applications, understanding and enhancing their robustness to user
inputs is of paramount importance. Existing methods for identifying adversarial
prompts tend to focus on specific domains, lack diversity, or require extensive
human annotations. To address these limitations, we present Rainbow Teaming, a
novel approach for producing a diverse collection of adversarial prompts.
Rainbow Teaming casts adversarial prompt generation as a quality-diversity
problem, and uses open-ended search to generate prompts that are both effective
and diverse. It can uncover a model's vulnerabilities across a broad range of
domains including, in this paper, safety, question answering, and
cybersecurity. We also demonstrate that fine-tuning on synthetic data generated
by Rainbow Teaming improves the safety of state-of-the-art LLMs without hurting
their general capabilities and helpfulness, paving the path to open-ended
self-improvement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating the Effectiveness of HyperTuning via Gisting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16817v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16817v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jason Phang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gisting (Mu et al., 2023) is a simple method for training models to compress
information into fewer token representations using a modified attention mask,
and can serve as an economical approach to training Transformer-based
hypernetworks. We introduce HyperLlama, a set of Gisting-based hypernetworks
built on Llama-2 models that generates task-specific soft prefixes based on
few-shot inputs. In experiments across P3, Super-NaturalInstructions and Symbol
Tuning datasets, we show that HyperLlama models can effectively compress
information from few-shot examples into soft prefixes. However, they still
underperform multi-task fine-tuned language models with full attention over
few-shot in-context examples. We also show that HyperLlama-generated soft
prefixes can serve as better initializations for further prefix tuning.
Overall, Gisting-based hypernetworks are economical and easy to implement, but
have mixed empirical performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Onco<span class="highlight-title">GPT</span>: A Medical Conversational Model Tailored with Oncology Domain
  Expertise on a Large Language Model Meta-AI (LLaMA) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16810v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16810v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fujian Jia, Xin Liu, Lixi Deng, Jiwen Gu, Chunchao Pu, Tunan Bai, Mengjiang Huang, Yuanzhi Lu, Kang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the past year, there has been a growing trend in applying Large Language
Models (LLMs) to the field of medicine, particularly with the advent of
advanced language models such as ChatGPT developed by OpenAI. However, there is
limited research on LLMs specifically addressing oncology-related queries. The
primary aim of this research was to develop a specialized language model that
demonstrates improved accuracy in providing advice related to oncology. We
performed an extensive data collection of online question-answer interactions
centered around oncology, sourced from reputable doctor-patient platforms.
Following data cleaning and anonymization, a dataset comprising over 180K+
oncology-related conversations was established. The conversations were
categorized and meticulously reviewed by field specialists and clinicians to
ensure precision. Employing the LLaMA model and other selected open-source
datasets, we conducted iterative fine-tuning to enhance the model's proficiency
in basic medical conversation and specialized oncology knowledge. We observed a
substantial enhancement in the model's understanding of genuine patient
inquiries and its reliability in offering oncology-related advice through the
utilization of real online question-answer interactions in the fine-tuning
process. We release database and models to the research community
(https://github.com/OncoGPT1).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Set the Clock: Temporal Alignment of <span class="highlight-title">Pretrain</span>ed Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16797v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16797v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Zhao, Zander Brumbaugh, Yizhong Wang, Hannaneh Hajishirzi, Noah A. Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs) are trained on web text originating from many points in
time and, in general, without any explicit temporal grounding. This work
investigates the temporal chaos of pretrained LMs and explores various methods
to align their internal knowledge to a target time, which we call "temporal
alignment." To do this, we first automatically construct a dataset containing
20K time-sensitive questions and their answers for each year from 2000 to 2023.
Based on this dataset, we empirically show that pretrained LMs (e.g., LLaMa2),
despite having a recent pretraining cutoff (e.g., 2022), mostly answer
questions using earlier knowledge (e.g., in 2019). We then develop several
methods, from prompting to finetuning, to align LMs to use their most recent
knowledge when answering questions, and investigate various factors in this
alignment. Our experiments show that aligning LLaMa2 to the year 2022 can boost
its performance by up to 62% relatively as measured by that year, even without
mentioning time information explicitly, indicating the possibility of aligning
models' internal sense of time after pretraining. Finally, we find that
alignment to a historical time is also possible, with up to 2.8$\times$ the
performance of the unaligned LM in 2010 if finetuning models to that year.
These findings hint at the sophistication of LMs' internal knowledge
organization and the necessity of tuning them properly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 7 figures. Our code and data will be available at
  https://github.com/yizhongw/llm-temporal-alignment</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Political Compass or Spinning Arrow? Towards More Meaningful Evaluations
  for Values and Opinions in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16786v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16786v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Röttger, Valentin Hofmann, Valentina Pyatkin, Musashi Hinck, Hannah Rose Kirk, Hinrich Schütze, Dirk Hovy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Much recent work seeks to evaluate values and opinions in large language
models (LLMs) using multiple-choice surveys and questionnaires. Most of this
work is motivated by concerns around real-world LLM applications. For example,
politically-biased LLMs may subtly influence society when they are used by
millions of people. Such real-world concerns, however, stand in stark contrast
to the artificiality of current evaluations: real users do not typically ask
LLMs survey questions. Motivated by this discrepancy, we challenge the
prevailing constrained evaluation paradigm for values and opinions in LLMs and
explore more realistic unconstrained evaluations. As a case study, we focus on
the popular Political Compass Test (PCT). In a systematic review, we find that
most prior work using the PCT forces models to comply with the PCT's
multiple-choice format. We show that models give substantively different
answers when not forced; that answers change depending on how models are
forced; and that answers lack paraphrase robustness. Then, we demonstrate that
models give different answers yet again in a more realistic open-ended answer
setting. We distill these findings into recommendations and open challenges in
evaluating values and opinions in LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v1 prepared for conference submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive Evaluation of Quantization Strategies for Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renren Jin, Jiangcun Du, Wuwei Huang, Wei Liu, Jian Luan, Bin Wang, Deyi Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Increasing the number of parameters in large language models (LLMs) usually
improves performance in downstream tasks but raises compute and memory costs,
making deployment difficult in resource-limited settings. Quantization
techniques, which reduce the bits needed for model weights or activations with
minimal performance loss, have become popular due to the rise of LLMs. However,
most quantization studies use pre-trained LLMs, and the impact of quantization
on instruction-tuned LLMs and the relationship between perplexity and benchmark
performance of quantized LLMs are not well understood. Evaluation of quantized
LLMs is often limited to language modeling and a few classification tasks,
leaving their performance on other benchmarks unclear. To address these gaps,
we propose a structured evaluation framework consisting of three critical
dimensions: (1) knowledge \& capacity, (2) alignment, and (3) efficiency, and
conduct extensive experiments across ten diverse benchmarks. Our experimental
results indicate that LLMs with 4-bit quantization can retain performance
comparable to their non-quantized counterparts, and perplexity can serve as a
proxy metric for quantized LLMs on most benchmarks. Furthermore, quantized LLMs
with larger parameter scales can outperform smaller LLMs. Despite the memory
savings achieved through quantization, it can also slow down the inference
speed of LLMs. Consequently, substantial engineering efforts and hardware
support are imperative to achieve a balanced optimization of decoding speed and
memory consumption in the context of quantized LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 16 figures, 16 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Far Can 100 Samples Go? Unlocking Overall Zero-Shot Multilingual
  Translation via Tiny Multi-Parallel Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.12413v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.12413v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Di Wu, Shaomu Tan, Yan Meng, David Stap, Christof Monz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot translation aims to translate between language pairs not seen
during training in Multilingual Machine Translation (MMT) and is largely
considered an open problem. A common, albeit resource-consuming, solution is to
add as many related translation directions as possible to the training corpus.
In this paper, we show that for an English-centric model, surprisingly large
zero-shot improvements can be achieved by simply fine-tuning with a very small
amount of multi-parallel data. For example, on the EC30 dataset, we obtain up
to +21.7 ChrF non-English overall improvements (870 directions) by using only
100 multi-parallel samples while preserving English-centric translation
quality. When investigating the size effect of fine-tuning data and its
transfer capabilities, we found that already a small, randomly sampled set of
fine-tuning directions is sufficient to achieve comparable improvements. The
resulting non-English performance is close to the complete translation upper
bound. Even in a minimal setting -- fine-tuning with only one single sample --
the well-known off-target issue is almost completely resolved, explaining
parts--but not all -- of the observed improvements in translation quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diagnosing <span class="highlight-title">Transformer</span>s: Illuminating Feature Spaces for Clinical
  Decision-Making 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17588v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17588v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aliyah R. Hsu, Yeshwanth Cherapanamjeri, Briton Park, Tristan Naumann, Anobel Y. Odisho, Bin Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained transformers are often fine-tuned to aid clinical decision-making
using limited clinical notes. Model interpretability is crucial, especially in
high-stakes domains like medicine, to establish trust and ensure safety, which
requires human engagement. We introduce SUFO, a systematic framework that
enhances interpretability of fine-tuned transformer feature spaces. SUFO
utilizes a range of analytic and visualization techniques, including Supervised
probing, Unsupervised similarity analysis, Feature dynamics, and Outlier
analysis to address key questions about model trust and interpretability. We
conduct a case study investigating the impact of pre-training data where we
focus on real-world pathology classification tasks, and validate our findings
on MedNLI. We evaluate five 110M-sized pre-trained transformer models,
categorized into general-domain (BERT, TNLR), mixed-domain (BioBERT, Clinical
BioBERT), and domain-specific (PubMedBERT) groups. Our SUFO analyses reveal
that: (1) while PubMedBERT, the domain-specific model, contains valuable
information for fine-tuning, it can overfit to minority classes when class
imbalances exist. In contrast, mixed-domain models exhibit greater resistance
to overfitting, suggesting potential improvements in domain-specific model
robustness; (2) in-domain pre-training accelerates feature disambiguation
during fine-tuning; and (3) feature spaces undergo significant sparsification
during this process, enabling clinicians to identify common outlier modes among
fine-tuned models as demonstrated in this paper. These findings showcase the
utility of SUFO in enhancing trust and safety when using transformers in
medicine, and we believe SUFO can aid practitioners in evaluating fine-tuned
language models for other applications in medicine and in more critical
domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PersonaLLM: Investigating the Ability of Large Language Models to
  Express Personality Traits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.02547v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.02547v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Jiang, Xiajie Zhang, Xubo Cao, Cynthia Breazeal, Jad Kabbara, Deb Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the many use cases for large language models (LLMs) in creating
personalized chatbots, there has been limited research on evaluating the extent
to which the behaviors of personalized LLMs accurately and consistently reflect
specific personality traits. We consider studying the behavior of LLM-based
agents which we refer to as LLM personas and present a case study with GPT-3.5
and GPT-4 to investigate whether LLMs can generate content that aligns with
their assigned personality profiles. To this end, we simulate distinct LLM
personas based on the Big Five personality model, have them complete the
44-item Big Five Inventory (BFI) personality test and a story writing task, and
then assess their essays with automatic and human evaluations. Results show
that LLM personas' self-reported BFI scores are consistent with their
designated personality types, with large effect sizes observed across five
traits. Additionally, LLM personas' writings have emerging representative
linguistic patterns for personality traits when compared with a human writing
corpus. Furthermore, human evaluation shows that humans can perceive some
personality traits with an accuracy of up to 80\%. Interestingly, the accuracy
drops significantly when the annotators were informed of the AI's authorship.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First version uploaded at IC2S2 in May 2023. Full paper submitted in
  Nov. 2023 and updated Feb. 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Abstract Meaning Representation-Based Logic-Driven Data Augmentation for
  Logical Reasoning <span class="chip">IJCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.12599v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.12599v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiming Bao, Alex Yuxuan Peng, Zhenyun Deng, Wanjun Zhong, Gael Gendron, Timothy Pistotti, Neset Tan, Nathan Young, Yang Chen, Yonghua Zhu, Paul Denny, Michael Witbrock, Jiamou Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combining large language models with logical reasoning enhances their
capacity to address problems in a robust and reliable manner. Nevertheless, the
intricate nature of logical reasoning poses challenges to gathering reliable
data from the web for building comprehensive training datasets, subsequently
affecting the performance on downstream tasks. To address this, we introduce a
novel logic-driven data augmentation approach, AMR-LDA. AMR-LDA converts the
original text into an Abstract Meaning Representation (AMR) graph, a structured
semantic representation that encapsulates the logic structure of the sentence,
upon which operations are performed to generate logically modified AMR graphs.
The modified AMR graphs are subsequently converted back into text to create
augmented data. Notably, our methodology is architecture-agnostic and enhances
both generative large language models, such as GPT-3.5 and GPT-4, through
prompt augmentation, and discriminative large language models through
contrastive learning with logic-driven data augmentation. Empirical evidence
underscores the efficacy of our proposed method with improvement in performance
across seven downstream tasks, such as reading comprehension requiring logical
reasoning, textual entailment, and natural language inference. Furthermore, our
method leads on the ReClor
leaderboard\footnote{\url{https://eval.ai/web/challenges/challenge-page/503/leaderboard/1347}}.
The source code and data are publicly
available\footnote{\url{https://bit.ly/3OWKe8r}}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The short version (v2) was accepted for oral presentation at the
  first LLM@IJCAI 2023 non-archival symposium; the full version is under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multistage Collaborative Knowledge Distillation from a Large Language
  Model for Semi-Supervised Sequence Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08640v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08640v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiachen Zhao, Wenlong Zhao, Andrew Drozdov, Benjamin Rozonoyer, Md Arafat Sultan, Jay-Yoon Lee, Mohit Iyyer, Andrew McCallum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study semi-supervised sequence generation tasks, where the few labeled
examples are too scarce to finetune a model, and meanwhile, few-shot prompted
large language models (LLMs) exhibit room for improvement. In this paper, we
present the discovery that a student model distilled from a few-shot prompted
LLM can commonly generalize better than its teacher to unseen examples on such
tasks. We find that the student is able to learn a general pattern from the
high-quality pseudolabels produced by the teacher during knowledge distillation
(KD), and favorably not a general pattern from the low-quality pseudolables.
Leveraging this discovery, we propose a new method, Multistage Collaborative
Knowledge Distillation from an LLM (MCKD), for these tasks. MCKD first few-shot
prompts an LLM to produce pseudolabels for unlabeled data. Then at each stage
of an iterative KD process, a new pair of students is trained on disjoint
partitions of the pseudolabeled data, and produces new and improved
pseudolabels for their unseen partitions. We conduct extensive experiments on
four syntactic and semantic parsing datasets and show the effectiveness of MCKD
for low-resource semi-supervised sequence generation. On CRAFT biomedical
parsing, for example, 3-stage MCKD with 50 labeled examples outperforms an LLM
teacher and vanilla KD by 7.5% and 3.7% parsing F1, respectively, and matches
the performance of supervised finetuning with 500 labeled examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When do Generative Query and Document Expansions Fail? A Comprehensive
  Study Across Methods, Retrievers, and <span class="highlight-title">Dataset</span>s <span class="chip">EACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08541v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08541v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Orion Weller, Kyle Lo, David Wadden, Dawn Lawrie, Benjamin Van Durme, Arman Cohan, Luca Soldaini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using large language models (LMs) for query or document expansion can improve
generalization in information retrieval. However, it is unknown whether these
techniques are universally beneficial or only effective in specific settings,
such as for particular retrieval models, dataset domains, or query types. To
answer this, we conduct the first comprehensive analysis of LM-based expansion.
We find that there exists a strong negative correlation between retriever
performance and gains from expansion: expansion improves scores for weaker
models, but generally harms stronger models. We show this trend holds across a
set of eleven expansion techniques, twelve datasets with diverse distribution
shifts, and twenty-four retrieval models. Through qualitative error analysis,
we hypothesize that although expansions provide extra information (potentially
improving recall), they add additional noise that makes it difficult to discern
between the top relevant documents (thus introducing false positives). Our
results suggest the following recipe: use expansions for weaker models or when
the target dataset significantly differs from training corpus in format;
otherwise, avoid expansions to keep the relevance signal clear.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EACL 2024 camera ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NevIR: Negation in Neural Information Retrieval <span class="chip">EACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.07614v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.07614v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Orion Weller, Dawn Lawrie, Benjamin Van Durme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Negation is a common everyday phenomena and has been a consistent area of
weakness for language models (LMs). Although the Information Retrieval (IR)
community has adopted LMs as the backbone of modern IR architectures, there has
been little to no research in understanding how negation impacts neural IR. We
therefore construct a straightforward benchmark on this theme: asking IR models
to rank two documents that differ only by negation. We show that the results
vary widely according to the type of IR architecture: cross-encoders perform
best, followed by late-interaction models, and in last place are bi-encoder and
sparse neural architectures. We find that most information retrieval models
(including SOTA ones) do not consider negation, performing the same or worse
than a random ranking. We show that although the obvious approach of continued
fine-tuning on a dataset of contrastive documents containing negations
increases performance (as does model size), there is still a large gap between
machine and human performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Defending Against Disinformation Attacks in Open-Domain Question
  Answering <span class="chip">EACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.10002v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.10002v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Orion Weller, Aleem Khan, Nathaniel Weir, Dawn Lawrie, Benjamin Van Durme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work in open-domain question answering (ODQA) has shown that
adversarial poisoning of the search collection can cause large drops in
accuracy for production systems. However, little to no work has proposed
methods to defend against these attacks. To do so, we rely on the intuition
that redundant information often exists in large corpora. To find it, we
introduce a method that uses query augmentation to search for a diverse set of
passages that could answer the original question but are less likely to have
been poisoned. We integrate these new passages into the model through the
design of a novel confidence method, comparing the predicted answer to its
appearance in the retrieved contexts (what we call Confidence from Answer
Redundancy, i.e. CAR). Together these methods allow for a simple but effective
way to defend against poisoning attacks that provides gains of nearly 20% exact
match across varying levels of data poisoning/knowledge conflicts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ "According to ...": <span class="highlight-title">Prompt</span>ing Language Models Improves Quoting from
  <span class="highlight-title">Pre-Train</span>ing Data <span class="chip">EACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.13252v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.13252v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Orion Weller, Marc Marone, Nathaniel Weir, Dawn Lawrie, Daniel Khashabi, Benjamin Van Durme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) may hallucinate and generate fake information,
despite pre-training on factual data. Inspired by the journalistic device of
"according to sources", we propose according-to prompting: directing LLMs to
ground responses against previously observed text. To quantify this grounding,
we propose a novel evaluation metric (QUIP-Score) that measures the extent to
which model-produced answers are directly found in underlying text corpora. We
illustrate with experiments on three corpora (Wikipedia, PubMed, and the U.S.
legal tax code) that these prompts improve grounding under our metrics, with
the additional benefit of often improving end-task performance. Furthermore,
prompts that ask the model to decrease grounding (or to ground to other
corpora) indeed decrease QUIP-Score, indicating the ability of LLMs to increase
or decrease grounded generations on request.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DecodingTrust: A Comprehensive Assessment of Trustworthiness in <span class="highlight-title">GPT</span>
  Models <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.11698v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.11698v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang T. Truong, Simran Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu Cheng, Sanmi Koyejo, Dawn Song, Bo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Pre-trained Transformer (GPT) models have exhibited exciting
progress in their capabilities, capturing the interest of practitioners and the
public alike. Yet, while the literature on the trustworthiness of GPT models
remains limited, practitioners have proposed employing capable GPT models for
sensitive applications such as healthcare and finance -- where mistakes can be
costly. To this end, this work proposes a comprehensive trustworthiness
evaluation for large language models with a focus on GPT-4 and GPT-3.5,
considering diverse perspectives -- including toxicity, stereotype bias,
adversarial robustness, out-of-distribution robustness, robustness on
adversarial demonstrations, privacy, machine ethics, and fairness. Based on our
evaluations, we discover previously unpublished vulnerabilities to
trustworthiness threats. For instance, we find that GPT models can be easily
misled to generate toxic and biased outputs and leak private information in
both training data and conversation history. We also find that although GPT-4
is usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is more
vulnerable given jailbreaking system or user prompts, potentially because GPT-4
follows (misleading) instructions more precisely. Our work illustrates a
comprehensive trustworthiness evaluation of GPT models and sheds light on the
trustworthiness gaps. Our benchmark is publicly available at
https://decodingtrust.github.io/ ; our dataset can be previewed at
https://huggingface.co/datasets/AI-Secure/DecodingTrust ; a concise version of
this work is at https://openreview.net/pdf?id=kaHpo8OZw2 .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023 Outstanding Paper (Datasets and Benchmarks Track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are Large Language Models Post Hoc Explainers? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05797v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05797v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Kroeger, Dan Ley, Satyapriya Krishna, Chirag Agarwal, Himabindu Lakkaraju
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing use of predictive models in high-stakes settings highlights
the need for ensuring that relevant stakeholders understand and trust the
decisions made by these models. To this end, several approaches have been
proposed in recent literature to explain the behavior of complex predictive
models in a post hoc fashion. However, despite the growing number of such post
hoc explanation techniques, many require white-box access to the model and/or
are computationally expensive, highlighting the need for next-generation post
hoc explainers. Recently, Large Language Models (LLMs) have emerged as powerful
tools that are effective at a wide variety of tasks. However, their potential
to explain the behavior of other complex predictive models remains relatively
unexplored. In this work, we carry out one of the initial explorations to
analyze the effectiveness of LLMs in explaining other complex predictive
models. To this end, we propose three novel approaches that exploit the
in-context learning (ICL) capabilities of LLMs to explain the predictions made
by other complex models. We conduct extensive experimentation with these
approaches on real-world datasets to demonstrate that LLMs perform on par with
state-of-the-art post hoc explainers, opening up promising avenues for future
research into LLM-based post hoc explanations of complex predictive models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Tool Use with Chain-of-Abstraction Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.17464v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.17464v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Silin Gao, Jane Dwivedi-Yu, Ping Yu, Xiaoqing Ellen Tan, Ramakanth Pasunuru, Olga Golovneva, Koustuv Sinha, Asli Celikyilmaz, Antoine Bosselut, Tianlu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To achieve faithful reasoning that aligns with human expectations, large
language models (LLMs) need to ground their reasoning to real-world knowledge
(e.g., web facts, math and physical rules). Tools help LLMs access this
external knowledge, but there remains challenges for fine-tuning LLM agents
(e.g., Toolformer) to invoke tools in multi-step reasoning problems, where
inter-connected tool calls require holistic and efficient tool usage planning.
  In this work, we propose a new method for LLMs to better leverage tools in
multi-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to
first decode reasoning chains with abstract placeholders, and then call domain
tools to reify each reasoning chain by filling in specific knowledge. This
planning with abstract chains enables LLMs to learn more general reasoning
strategies, which are robust to shifts of domain knowledge (e.g., math results)
relevant to different reasoning questions. It also allows LLMs to perform
decoding and calling of external tools in parallel, which avoids the inference
delay caused by waiting for tool responses. In mathematical reasoning and Wiki
QA domains, we show that our method consistently outperforms previous
chain-of-thought and tool-augmented baselines on both in-distribution and
out-of-distribution test sets, with an average ~6% absolute QA accuracy
improvement. LLM agents trained with our method also show more efficient tool
use, with inference speed being on average ~1.4x faster than baseline
tool-augmented LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Augmentation: <span class="highlight-title">Self-Supervised</span> Learning with Transformations in
  Activation Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.14537v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.14537v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rickard Brüel-Gabrielsson, Tongzhou Wang, Manel Baradad, Justin Solomon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Deep Augmentation, an approach to implicit data augmentation
using dropout or PCA to transform a targeted layer within a neural network to
improve performance and generalization. We demonstrate Deep Augmentation
through extensive experiments on contrastive learning tasks in NLP, computer
vision, and graph learning. We observe substantial performance gains with
Transformers, ResNets, and Graph Neural Networks as the underlying models in
contrastive learning, but observe inverse effects on the corresponding
supervised problems. Our analysis suggests that Deep Augmentation alleviates
co-adaption between layers, a form of "collapse." We use this observation to
formulate a method for selecting which layer to target; in particular, our
experimentation reveals that targeting deeper layers with Deep Augmentation
outperforms augmenting the input data. The simple network- and
modality-agnostic nature of this approach enables its integration into various
machine learning pipelines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Accuracy: Evaluating Self-Consistency of Code Large Language
  Models with IdentityChain <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14053v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14053v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcus J. Min, Yangruibo Ding, Luca Buratti, Saurabh Pujar, Gail Kaiser, Suman Jana, Baishakhi Ray
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code Large Language Models (Code LLMs) are being increasingly employed in
real-life applications, so evaluating them is critical. While the conventional
accuracy evaluates the performance of Code LLMs on a set of individual tasks,
their self-consistency across different tasks is overlooked. Intuitively, a
trustworthy model should be self-consistent when generating natural language
specifications for its own code and generating code for its own specifications.
Failure to preserve self-consistency reveals a lack of understanding of the
shared semantics underlying natural language and programming language, and
therefore undermines the trustworthiness of a model. In this paper, we first
formally define the self-consistency of Code LLMs and then design a framework,
IdentityChain, which effectively and efficiently evaluates the self-consistency
and conventional accuracy of a model at the same time. We study eleven Code
LLMs and show that they fail to preserve self-consistency, which is indeed a
distinct aspect from conventional accuracy. Furthermore, we show that
IdentityChain can be used as a model debugging tool to expose weaknesses of
Code LLMs by demonstrating three major weaknesses that we identify in current
models using IdentityChain. Our code is available at
https://github.com/marcusm117/IdentityChain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ O3D: Offline Data-driven Discovery and Distillation for Sequential
  Decision-Making with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14403v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14403v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Xiao, Yanchao Sun, Mengda Xu, Udari Madhushani, Jared Vann, Deepeka Garg, Sumitra Ganesh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have exhibited promising
performance in solving sequential decision-making problems. By imitating
few-shot examples provided in the prompts (i.e., in-context learning), an LLM
agent can interact with an external environment and complete given tasks
without additional training. However, such few-shot examples are often
insufficient to generate high-quality solutions for complex and long-horizon
tasks, while the limited context length cannot consume larger-scale
demonstrations with long interaction horizons. To this end, we propose an
offline learning framework that utilizes offline data at scale (e.g, logs of
human interactions) to improve LLM-powered policies without finetuning. The
proposed method O3D (Offline Data-driven Discovery and Distillation)
automatically discovers reusable skills and distills generalizable knowledge
across multiple tasks based on offline interaction data, advancing the
capability of solving downstream tasks. Empirical results under two interactive
decision-making benchmarks (ALFWorld and WebShop) verify that O3D can notably
enhance the decision-making capabilities of LLMs through the offline discovery
and distillation process, and consistently outperform baselines across various
LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AgentOhana: Design Unified Data and Training Pipeline for Effective
  Agent Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15506v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15506v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianguo Zhang, Tian Lan, Rithesh Murthy, Zhiwei Liu, Weiran Yao, Juntao Tan, Thai Hoang, Liangwei Yang, Yihao Feng, Zuxin Liu, Tulika Awalgaonkar, Juan Carlos Niebles, Silvio Savarese, Shelby Heinecke, Huan Wang, Caiming Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous agents powered by large language models (LLMs) have garnered
significant research attention. However, fully harnessing the potential of LLMs
for agent-based tasks presents inherent challenges due to the heterogeneous
nature of diverse data sources featuring multi-turn trajectories. In this
paper, we introduce \textbf{AgentOhana} as a comprehensive solution to address
these challenges. \textit{AgentOhana} aggregates agent trajectories from
distinct environments, spanning a wide array of scenarios. It meticulously
standardizes and unifies these trajectories into a consistent format,
streamlining the creation of a generic data loader optimized for agent
training. Leveraging the data unification, our training pipeline maintains
equilibrium across different data sources and preserves independent randomness
across devices during dataset partitioning and model training. Additionally, we
present \textbf{xLAM-v0.1}, a large action model tailored for AI agents, which
demonstrates exceptional performance across various benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Add testing results on ToolBench and correct typographical errors</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">29</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Fine-tuning Enhanced RAG System with Quantized Influence Measure as AI
  Judge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17081v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17081v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keshav Rangan, Yiqiao Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents an innovative enhancement to retrieval-augmented
generation (RAG) systems by seamlessly integrating fine-tuned large language
models (LLMs) with vector databases. This integration capitalizes on the
combined strengths of structured data retrieval and the nuanced comprehension
provided by advanced LLMs. Central to our approach are the LoRA and QLoRA
methodologies, which stand at the forefront of model refinement through
parameter-efficient fine-tuning and memory optimization. A novel feature of our
research is the incorporation of user feedback directly into the training
process, ensuring the model's continuous adaptation to user expectations and
thus, improving its performance and applicability. Additionally, we introduce a
Quantized Influence Measure (QIM) as an innovative "AI Judge" mechanism to
enhance the precision of result selection, further refining the system's
accuracy. Accompanied by an executive diagram and a detailed algorithm for
fine-tuning QLoRA, our work provides a comprehensive framework for implementing
these advancements within chatbot technologies. This research contributes
significant insights into LLM optimization for specific uses and heralds new
directions for further development in retrieval-augmented models. Through
extensive experimentation and analysis, our findings lay a robust foundation
for future advancements in chatbot technology and retrieval systems, marking a
significant step forward in the creation of more sophisticated, precise, and
user-centric conversational AI systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Task Contrastive Learning for 8192-Token Bilingual Text Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17016v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17016v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isabelle Mohr, Markus Krimmel, Saba Sturua, Mohammad Kalim Akram, Andreas Koukounas, Michael Günther, Georgios Mastrapas, Vinit Ravishankar, Joan Fontanals Martínez, Feng Wang, Qi Liu, Ziniu Yu, Jie Fu, Saahil Ognawala, Susana Guzman, Bo Wang, Maximilian Werk, Nan Wang, Han Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel suite of state-of-the-art bilingual text embedding
models that are designed to support English and another target language. These
models are capable of processing lengthy text inputs with up to 8192 tokens,
making them highly versatile for a range of natural language processing tasks
such as text retrieval, clustering, and semantic textual similarity (STS)
calculations.
  By focusing on bilingual models and introducing a unique multi-task learning
objective, we have significantly improved the model performance on STS tasks,
which outperforms the capabilities of existing multilingual models in both
target language understanding and cross-lingual evaluation tasks. Moreover, our
bilingual models are more efficient, requiring fewer parameters and less memory
due to their smaller vocabulary needs. Furthermore, we have expanded the
Massive Text Embedding Benchmark (MTEB) to include benchmarks for German and
Spanish embedding models. This integration aims to stimulate further research
and advancement in text embedding technologies for these languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long Dialog Summarization: An Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16986v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16986v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ankan Mullick, Ayan Kumar Bhowmick, Raghav R, Ravi Kokku, Prasenjit Dey, Pawan Goyal, Niloy Ganguly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dialog summarization has become increasingly important in managing and
comprehending large-scale conversations across various domains. This task
presents unique challenges in capturing the key points, context, and nuances of
multi-turn long conversations for summarization. It is worth noting that the
summarization techniques may vary based on specific requirements such as in a
shopping-chatbot scenario, the dialog summary helps to learn user preferences,
whereas in the case of a customer call center, the summary may involve the
problem attributes that a user specified, and the final resolution provided.
This work emphasizes the significance of creating coherent and contextually
rich summaries for effective communication in various applications. We explore
current state-of-the-art approaches for long dialog summarization in different
domains and benchmark metrics based evaluations show that one single model does
not perform well across various areas for distinct summarization tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CorpusBrain++: A Continual Generative <span class="highlight-title">Pre-Train</span>ing Framework for
  Knowledge-Intensive Language Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16767v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16767v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiafeng Guo, Changjiang Zhou, Ruqing Zhang, Jiangui Chen, Maarten de Rijke, Yixing Fan, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge-intensive language tasks (KILTs) typically require retrieving
relevant documents from trustworthy corpora, e.g., Wikipedia, to produce
specific answers. Very recently, a pre-trained generative retrieval model for
KILTs, named CorpusBrain, was proposed and reached new state-of-the-art
retrieval performance. However, most existing research on KILTs, including
CorpusBrain, has predominantly focused on a static document collection,
overlooking the dynamic nature of real-world scenarios, where new documents are
continuously being incorporated into the source corpus. To address this gap, it
is crucial to explore the capability of retrieval models to effectively handle
the dynamic retrieval scenario inherent in KILTs.
  In this work, we first introduce the continual document learning (CDL) task
for KILTs and build a novel benchmark dataset named KILT++ based on the
original KILT dataset for evaluation. Then, we conduct a comprehensive study
over the use of pre-trained CorpusBrain on KILT++. Unlike the promising results
in the stationary scenario, CorpusBrain is prone to catastrophic forgetting in
the dynamic scenario, hence hampering the retrieval performance. To alleviate
this issue, we propose CorpusBrain++, a continual generative pre-training
framework. Empirical results demonstrate the significant effectiveness and
remarkable efficiency of CorpusBrain++ in comparison to both traditional and
generative IR methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ACM Transactions on Information Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Avoiding Catastrophic Forgetting in Visual Classification Using Human
  Concept Formation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicki Barari, Xin Lian, Christopher J. MacLellan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have excelled in machine learning, particularly in
vision tasks, however, they often suffer from catastrophic forgetting when
learning new tasks sequentially. In this work, we propose Cobweb4V, a novel
visual classification approach that builds on Cobweb, a human like learning
system that is inspired by the way humans incrementally learn new concepts over
time. In this research, we conduct a comprehensive evaluation, showcasing the
proficiency of Cobweb4V in learning visual concepts, requiring less data to
achieve effective learning outcomes compared to traditional methods,
maintaining stable performance over time, and achieving commendable asymptotic
behavior, without catastrophic forgetting effects. These characteristics align
with learning strategies in human cognition, positioning Cobweb4V as a
promising alternative to neural network approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM-Assisted Multi-Teacher Continual Learning for Visual Question
  Answering in Robotic Surgery <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kexin Chen, Yuyang Du, Tao You, Mobarakol Islam, Ziyu Guo, Yueming Jin, Guangyong Chen, Pheng-Ann Heng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual question answering (VQA) can be fundamentally crucial for promoting
robotic-assisted surgical education. In practice, the needs of trainees are
constantly evolving, such as learning more surgical types, adapting to
different robots, and learning new surgical instruments and techniques for one
surgery. Therefore, continually updating the VQA system by a sequential data
stream from multiple resources is demanded in robotic surgery to address new
tasks. In surgical scenarios, the storage cost and patient data privacy often
restrict the availability of old data when updating the model, necessitating an
exemplar-free continual learning (CL) setup. However, prior studies overlooked
two vital problems of the surgical domain: i) large domain shifts from diverse
surgical operations collected from multiple departments or clinical centers,
and ii) severe data imbalance arising from the uneven presence of surgical
instruments or activities during surgical procedures. This paper proposes to
address these two problems with a multimodal large language model (LLM) and an
adaptive weight assignment methodology. We first develop a new multi-teacher CL
framework that leverages a multimodal LLM as the additional teacher. The strong
generalization ability of the LLM can bridge the knowledge gap when domain
shifts and data imbalances occur. We then put forth a novel data processing
method that transforms complex LLM embeddings into logits compatible with our
CL framework. We further design an adaptive weight assignment approach that
balances the generalization ability of the LLM and the domain expertise of the
old CL model. We construct a new dataset for surgical VQA tasks, providing
valuable data resources for future research. Extensive experimental results on
three datasets demonstrate the superiority of our method to other advanced CL
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accapted by 2024 IEEE International Conference on
  Robotics and Automation (ICRA)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BOXREC: Recommending a Box of Preferred Outfits in Online Shopping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debopriyo Banerjee, Krothapalli Sreenivasa Rao, Shamik Sural, Niloy Ganguly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the past few years, automation of outfit composition has gained much
attention from the research community. Most of the existing outfit
recommendation systems focus on pairwise item compatibility prediction (using
visual and text features) to score an outfit combination having several items,
followed by recommendation of top-n outfits or a capsule wardrobe having a
collection of outfits based on user's fashion taste. However, none of these
consider user's preference of price-range for individual clothing types or an
overall shopping budget for a set of items. In this paper, we propose a box
recommendation framework - BOXREC - which at first, collects user preferences
across different item types (namely, top-wear, bottom-wear and foot-wear)
including price-range of each type and a maximum shopping budget for a
particular shopping session. It then generates a set of preferred outfits by
retrieving all types of preferred items from the database (according to user
specified preferences including price-ranges), creates all possible
combinations of three preferred items (belonging to distinct item types) and
verifies each combination using an outfit scoring framework - BOXREC-OSF.
Finally, it provides a box full of fashion items, such that different
combinations of the items maximize the number of outfits suitable for an
occasion while satisfying maximum shopping budget. Empirical results show
superior performance of BOXREC-OSF over the baseline methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PAQA: Toward ProActive Open-Retrieval Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierre Erbacher, Jian-Yun Nie, Philippe Preux, Laure Soulier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational systems have made significant progress in generating natural
language responses. However, their potential as conversational search systems
is currently limited due to their passive role in the information-seeking
process. One major limitation is the scarcity of datasets that provide labelled
ambiguous questions along with a supporting corpus of documents and relevant
clarifying questions. This work aims to tackle the challenge of generating
relevant clarifying questions by taking into account the inherent ambiguities
present in both user queries and documents. To achieve this, we propose PAQA,
an extension to the existing AmbiNQ dataset, incorporating clarifying
questions. We then evaluate various models and assess how passage retrieval
impacts ambiguity detection and the generation of clarifying questions. By
addressing this gap in conversational search systems, we aim to provide
additional supervision to enhance their active participation in the
information-seeking process and provide users with more accurate results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrating Large Language Models with Graphical Session-Based
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naicheng Guo, Hongwei Cheng, Qianqiao Liang, Linxun Chen, Bing Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of Large Language Models (LLMs), various
explorations have arisen to utilize LLMs capability of context understanding on
recommender systems. While pioneering strategies have primarily transformed
traditional recommendation tasks into challenges of natural language
generation, there has been a relative scarcity of exploration in the domain of
session-based recommendation (SBR) due to its specificity. SBR has been
primarily dominated by Graph Neural Networks, which have achieved many
successful outcomes due to their ability to capture both the implicit and
explicit relationships between adjacent behaviors. The structural nature of
graphs contrasts with the essence of natural language, posing a significant
adaptation gap for LLMs. In this paper, we introduce large language models with
graphical Session-Based recommendation, named LLMGR, an effective framework
that bridges the aforementioned gap by harmoniously integrating LLMs with Graph
Neural Networks (GNNs) for SBR tasks. This integration seeks to leverage the
complementary strengths of LLMs in natural language understanding and GNNs in
relational data processing, leading to a more powerful session-based
recommender system that can understand and recommend items within a session.
Moreover, to endow the LLM with the capability to empower SBR tasks, we design
a series of prompts for both auxiliary and major instruction tuning tasks.
These prompts are crafted to assist the LLM in understanding graph-structured
data and align textual information with nodes, effectively translating nuanced
user interactions into a format that can be understood and utilized by LLM
architectures. Extensive experiments on three real-world datasets demonstrate
that LLMGR outperforms several competitive baselines, indicating its
effectiveness in enhancing SBR tasks and its potential as a research direction
for future exploration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Pre-train</span>ing Cross-lingual Open Domain Question Answering with
  Large-scale Synthetic Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16508v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16508v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Jiang, Tom Drummond, Trevor Cohn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-lingual question answering (CLQA) is a complex problem, comprising
cross-lingual retrieval from a multilingual knowledge base, followed by answer
generation either in English or the query language. Both steps are usually
tackled by separate models, requiring substantial annotated datasets, and
typically auxiliary resources, like machine translation systems to bridge
between languages. In this paper, we show that CLQA can be addressed using a
single encoder-decoder model. To effectively train this model, we propose a
self-supervised method based on exploiting the cross-lingual link structure
within Wikipedia. We demonstrate how linked Wikipedia pages can be used to
synthesise supervisory signals for cross-lingual retrieval, through a form of
cloze query, and generate more natural queries to supervise answer generation.
Together, we show our approach, \texttt{CLASS}, outperforms comparable methods
on both supervised and zero-shot language adaptation settings, including those
using machine translation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retrouver l'inventeur-auteur : la lev{é}e d'homonymies d'autorat entre
  les brevets et les publications scientifiques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16440v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16440v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Reymond, Heman Khouilla, Sandrine Wolff, Manuel Durand-Barthez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Patents and scientific papers provide an essential source for measuring
science and technology output, to be used as a basis for the most varied
scientometric analyzes. Authors' and inventors' names are the key identifiers
to carry out these analyses, which however, run up against the issue of
disambiguation. By extension identifying inventors who are also academic
authors is a non-trivial challenge. We propose a method using the International
Patent Classification (IPC) and the IPCCAT API to assess the degree of
similarity of patents and papers abstracts of a given inventor, in order to
match both types of documents. The method is developed and manually qualified
based on three corpora of patents extracted from the international EPO database
Espacenet. Among a set of 4679 patents and 7720 inventors, we obtain 2501
authors. The proposed algorithm solves the general problem of disambiguation
with an error rate lower than 5%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>in French language</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Effect of utterance duration and phonetic content on speaker
  identification using second-order statistical methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16429v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16429v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivan Magrin-Chagnolleau, Jean François Bonastre, Frédéric Bimbot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Second-order statistical methods show very good results for automatic speaker
identification in controlled recording conditions. These approaches are
generally used on the entire speech material available. In this paper, we study
the influence of the content of the test speech material on the performances of
such methods, i.e. under a more analytical approach. The goal is to investigate
on the kind of information which is used by these methods, and where it is
located in the speech signal. Liquids and glides together, vowels, and more
particularly nasal vowels and nasal consonants, are found to be particularly
speaker specific: test utterances of 1 second, composed in majority of acoustic
material from one of these classes provide better speaker identification
results than phonetically balanced test utterances, even though the training is
done, in both cases, with 15 seconds of phonetically balanced speech.
Nevertheless, results with other phoneme classes are never dramatically poor.
These results tend to show that the speaker-dependent information captured by
long-term second-order statistics is consistently common to all phonetic
classes, and that the homogeneity of the test material may improve the quality
of the estimates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Integrated Data Processing Framework for <span class="highlight-title">Pretrain</span>ing Foundation
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16358v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16358v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiding Sun, Feng Wang, Yutao Zhu, Wayne Xin Zhao, Jiaxin Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability of the foundation models heavily relies on large-scale, diverse,
and high-quality pretraining data. In order to improve data quality,
researchers and practitioners often have to manually curate datasets from
difference sources and develop dedicated data cleansing pipeline for each data
repository. Lacking a unified data processing framework, this process is
repetitive and cumbersome. To mitigate this issue, we propose a data processing
framework that integrates a Processing Module which consists of a series of
operators at different granularity levels, and an Analyzing Module which
supports probing and evaluation of the refined data. The proposed framework is
easy to use and highly flexible. In this demo paper, we first introduce how to
use this framework with some example use cases and then demonstrate its
effectiveness in improving the data quality with an automated evaluation with
ChatGPT and an end-to-end evaluation in pretraining the GPT-2 model. The code
and demonstration videos are accessible on GitHub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Rating Elicitation for New Users in Collaborative Filtering <span class="chip">WWW 2020</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16327v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16327v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonbin Kweon, SeongKu Kang, Junyoung Hwang, Hwanjo Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent recommender systems started to use rating elicitation, which asks new
users to rate a small seed itemset for inferring their preferences, to improve
the quality of initial recommendations. The key challenge of the rating
elicitation is to choose the seed items which can best infer the new users'
preference. This paper proposes a novel end-to-end Deep learning framework for
Rating Elicitation (DRE), that chooses all the seed items at a time with
consideration of the non-linear interactions. To this end, it first defines
categorical distributions to sample seed items from the entire itemset, then it
trains both the categorical distributions and a neural reconstruction network
to infer users' preferences on the remaining items from CF information of the
sampled seed items. Through the end-to-end training, the categorical
distributions are learned to select the most representative seed items while
reflecting the complex non-linear interactions. Experimental results show that
DRE outperforms the state-of-the-art approaches in the recommendation quality
by accurately inferring the new users' preferences and its seed itemset better
represents the latent space than the seed itemset obtained by the other
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WWW 2020</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Confidence Calibration for Recommender Systems and Its Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16325v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16325v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonbin Kweon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the importance of having a measure of confidence in recommendation
results, it has been surprisingly overlooked in the literature compared to the
accuracy of the recommendation. In this dissertation, I propose a model
calibration framework for recommender systems for estimating accurate
confidence in recommendation results based on the learned ranking scores.
Moreover, I subsequently introduce two real-world applications of confidence on
recommendations: (1) Training a small student model by treating the confidence
of a big teacher model as additional learning guidance, (2) Adjusting the
number of presented items based on the expected user utility estimated with
calibrated probability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Doctoral Dissertation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Top-Personalized-K Recommendation <span class="chip">WWW 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonbin Kweon, SeongKu Kang, Sanghwan Jang, Hwanjo Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The conventional top-K recommendation, which presents the top-K items with
the highest ranking scores, is a common practice for generating personalized
ranking lists. However, is this fixed-size top-K recommendation the optimal
approach for every user's satisfaction? Not necessarily. We point out that
providing fixed-size recommendations without taking into account user utility
can be suboptimal, as it may unavoidably include irrelevant items or limit the
exposure to relevant ones. To address this issue, we introduce
Top-Personalized-K Recommendation, a new recommendation task aimed at
generating a personalized-sized ranking list to maximize individual user
satisfaction. As a solution to the proposed task, we develop a model-agnostic
framework named PerK. PerK estimates the expected user utility by leveraging
calibrated interaction probabilities, subsequently selecting the recommendation
size that maximizes this expected utility. Through extensive experiments on
real-world datasets, we demonstrate the superiority of PerK in
Top-Personalized-K recommendation task. We expect that Top-Personalized-K
recommendation has the potential to offer enhanced solutions for various
real-world recommendation scenarios, based on its great compatibility with
existing models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WWW 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Against Filter Bubbles: Diversified Music Recommendation via Weighted
  Hypergraph Embedding Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16299v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16299v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoguang Luo, Liuying Wen, Yong Qin, Liangwei Yang, Zhineng Hu, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems serve a dual purpose for users: sifting out inappropriate
or mismatched information while accurately identifying items that align with
their preferences. Numerous recommendation algorithms are designed to provide
users with a personalized array of information tailored to their preferences.
Nevertheless, excessive personalization can confine users within a "filter
bubble". Consequently, achieving the right balance between accuracy and
diversity in recommendations is a pressing concern. To address this challenge,
exemplified by music recommendation, we introduce the Diversified Weighted
Hypergraph music Recommendation algorithm (DWHRec). In the DWHRec algorithm,
the initial connections between users and listened tracks are represented by a
weighted hypergraph. Simultaneously, associations between artists, albums and
tags with tracks are also appended to the hypergraph. To explore users' latent
preferences, a hypergraph-based random walk embedding method is applied to the
constructed hypergraph. In our investigation, accuracy is gauged by the
alignment between the user and the track, whereas the array of recommended
track types measures diversity. We rigorously compared DWHRec against seven
state-of-the-art recommendation algorithms using two real-world music datasets.
The experimental results validate DWHRec as a solution that adeptly harmonizes
accuracy and diversity, delivering a more enriched musical experience. Beyond
music recommendation, DWHRec can be extended to cater to other scenarios with
similar data structures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PerLTQA: A Personal Long-Term Memory <span class="highlight-title">Dataset</span> for Memory Classification,
  Retrieval, and Synthesis in Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16288v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16288v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Du, Hongru Wang, Zhengyi Zhao, Bin Liang, Baojun Wang, Wanjun Zhong, Zezhong Wang, Kam-Fai Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-term memory plays a critical role in personal interaction, considering
long-term memory can better leverage world knowledge, historical information,
and preferences in dialogues. Our research introduces PerLTQA, an innovative QA
dataset that combines semantic and episodic memories, including world
knowledge, profiles, social relationships, events, and dialogues. This dataset
is collected to investigate the use of personalized memories, focusing on
social interactions and events in the QA task. PerLTQA features two types of
memory and a comprehensive benchmark of 8,593 questions for 30 characters,
facilitating the exploration and application of personalized memories in Large
Language Models (LLMs). Based on PerLTQA, we propose a novel framework for
memory integration and generation, consisting of three main components: Memory
Classification, Memory Retrieval, and Memory Synthesis. We evaluate this
framework using five LLMs and three retrievers. Experimental results
demonstrate that BERT-based classification models significantly outperform LLMs
such as ChatGLM3 and ChatGPT in the memory classification task. Furthermore,
our study highlights the importance of effective memory integration in the QA
task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UniRetriever: Multi-task Candidates Selection for Various
  Context-Adaptive Conversational Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16261v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16261v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongru Wang, Boyang Xue, Baohang Zhou, Rui Wang, Fei Mi, Weichao Wang, Yasheng Wang, Kam-Fai Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational retrieval refers to an information retrieval system that
operates in an iterative and interactive manner, requiring the retrieval of
various external resources, such as persona, knowledge, and even response, to
effectively engage with the user and successfully complete the dialogue.
However, most previous work trained independent retrievers for each specific
resource, resulting in sub-optimal performance and low efficiency. Thus, we
propose a multi-task framework function as a universal retriever for three
dominant retrieval tasks during the conversation: persona selection, knowledge
selection, and response selection. To this end, we design a dual-encoder
architecture consisting of a context-adaptive dialogue encoder and a candidate
encoder, aiming to attention to the relevant context from the long dialogue and
retrieve suitable candidates by simply a dot product. Furthermore, we introduce
two loss constraints to capture the subtle relationship between dialogue
context and different candidates by regarding historically selected candidates
as hard negatives. Extensive experiments and analysis establish
state-of-the-art retrieval quality both within and outside its training domain,
revealing the promising potential and generalization capability of our model to
serve as a universal retriever for different candidate selection tasks
simultaneously.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High-Frequency-aware Hierarchical Contrastive Selective Coding for
  Representation Learning on Text-attributed Graphs <span class="chip">WWW 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiyan Zhang, Chaozhuo Li, Liying Kang, Feiran Huang, Senzhang Wang, Xing Xie, Sunghun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate node representation learning on text-attributed graphs (TAGs),
where nodes are associated with text information. Although recent studies on
graph neural networks (GNNs) and pretrained language models (PLMs) have
exhibited their power in encoding network and text signals, respectively, less
attention has been paid to delicately coupling these two types of models on
TAGs. Specifically, existing GNNs rarely model text in each node in a
contextualized way; existing PLMs can hardly be applied to characterize graph
structures due to their sequence architecture. To address these challenges, we
propose HASH-CODE, a High-frequency Aware Spectral Hierarchical Contrastive
Selective Coding method that integrates GNNs and PLMs into a unified model.
Different from previous "cascaded architectures" that directly add GNN layers
upon a PLM, our HASH-CODE relies on five self-supervised optimization
objectives to facilitate thorough mutual enhancement between network and text
signals in diverse granularities. Moreover, we show that existing contrastive
objective learns the low-frequency component of the augmentation graph and
propose a high-frequency component (HFC)-aware contrastive learning objective
that makes the learned embeddings more distinctive. Extensive experiments on
six real-world benchmarks substantiate the efficacy of our proposed approach.
In addition, theoretical analysis and item embedding visualization provide
insights into our model interoperability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW 2024. arXiv admin note: text overlap with
  arXiv:2009.10273 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When do Generative Query and Document Expansions Fail? A Comprehensive
  Study Across Methods, Retrievers, and <span class="highlight-title">Dataset</span>s <span class="chip">EACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08541v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08541v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Orion Weller, Kyle Lo, David Wadden, Dawn Lawrie, Benjamin Van Durme, Arman Cohan, Luca Soldaini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using large language models (LMs) for query or document expansion can improve
generalization in information retrieval. However, it is unknown whether these
techniques are universally beneficial or only effective in specific settings,
such as for particular retrieval models, dataset domains, or query types. To
answer this, we conduct the first comprehensive analysis of LM-based expansion.
We find that there exists a strong negative correlation between retriever
performance and gains from expansion: expansion improves scores for weaker
models, but generally harms stronger models. We show this trend holds across a
set of eleven expansion techniques, twelve datasets with diverse distribution
shifts, and twenty-four retrieval models. Through qualitative error analysis,
we hypothesize that although expansions provide extra information (potentially
improving recall), they add additional noise that makes it difficult to discern
between the top relevant documents (thus introducing false positives). Our
results suggest the following recipe: use expansions for weaker models or when
the target dataset significantly differs from training corpus in format;
otherwise, avoid expansions to keep the relevance signal clear.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EACL 2024 camera ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NevIR: Negation in Neural Information Retrieval <span class="chip">EACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.07614v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.07614v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Orion Weller, Dawn Lawrie, Benjamin Van Durme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Negation is a common everyday phenomena and has been a consistent area of
weakness for language models (LMs). Although the Information Retrieval (IR)
community has adopted LMs as the backbone of modern IR architectures, there has
been little to no research in understanding how negation impacts neural IR. We
therefore construct a straightforward benchmark on this theme: asking IR models
to rank two documents that differ only by negation. We show that the results
vary widely according to the type of IR architecture: cross-encoders perform
best, followed by late-interaction models, and in last place are bi-encoder and
sparse neural architectures. We find that most information retrieval models
(including SOTA ones) do not consider negation, performing the same or worse
than a random ranking. We show that although the obvious approach of continued
fine-tuning on a dataset of contrastive documents containing negations
increases performance (as does model size), there is still a large gap between
machine and human performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Defending Against Disinformation Attacks in Open-Domain Question
  Answering <span class="chip">EACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.10002v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.10002v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Orion Weller, Aleem Khan, Nathaniel Weir, Dawn Lawrie, Benjamin Van Durme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work in open-domain question answering (ODQA) has shown that
adversarial poisoning of the search collection can cause large drops in
accuracy for production systems. However, little to no work has proposed
methods to defend against these attacks. To do so, we rely on the intuition
that redundant information often exists in large corpora. To find it, we
introduce a method that uses query augmentation to search for a diverse set of
passages that could answer the original question but are less likely to have
been poisoned. We integrate these new passages into the model through the
design of a novel confidence method, comparing the predicted answer to its
appearance in the retrieved contexts (what we call Confidence from Answer
Redundancy, i.e. CAR). Together these methods allow for a simple but effective
way to defend against poisoning attacks that provides gains of nearly 20% exact
match across varying levels of data poisoning/knowledge conflicts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ITEm: Unsupervised Image-Text Embedding Learning for eCommerce 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.02084v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.02084v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baohao Liao, Michael Kozielski, Sanjika Hewavitharana, Jiangbo Yuan, Shahram Khadivi, Tomer Lancewicki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Product embedding serves as a cornerstone for a wide range of applications in
eCommerce. The product embedding learned from multiple modalities shows
significant improvement over that from a single modality, since different
modalities provide complementary information. However, some modalities are more
informatively dominant than others. How to teach a model to learn embedding
from different modalities without neglecting information from the less dominant
modality is challenging. We present an image-text embedding model (ITEm), an
unsupervised learning method that is designed to better attend to image and
text modalities. We extend BERT by (1) learning an embedding from text and
image without knowing the regions of interest; (2) training a global
representation to predict masked words and to construct masked image patches
without their individual representations. We evaluate the pre-trained ITEm on
two tasks: the search for extremely similar products and the prediction of
product categories, showing substantial gains compared to strong baseline
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge Sharing in Manufacturing using Large Language Models: User
  Evaluation and Model Benchmarking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.05200v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.05200v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Kernan Freire, Chaofan Wang, Mina Foosherian, Stefan Wellsandt, Santiago Ruiz-Arenas, Evangelos Niforatos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in natural language processing enable more intelligent ways
to support knowledge sharing in factories. In manufacturing, operating
production lines has become increasingly knowledge-intensive, putting strain on
a factory's capacity to train and support new operators. This paper introduces
a Large Language Model (LLM)-based system designed to retrieve information from
the extensive knowledge contained in factory documentation and knowledge shared
by expert operators. The system aims to efficiently answer queries from
operators and facilitate the sharing of new knowledge. We conducted a user
study at a factory to assess its potential impact and adoption, eliciting
several perceived benefits, namely, enabling quicker information retrieval and
more efficient resolution of issues. However, the study also highlighted a
preference for learning from a human expert when such an option is available.
Furthermore, we benchmarked several commercial and open-sourced LLMs for this
system. The current state-of-the-art model, GPT-4, consistently outperformed
its counterparts, with open-source models trailing closely, presenting an
attractive option given their data privacy and customization benefits. In
summary, this work offers preliminary insights and a system design for
factories considering using LLM tools for knowledge management.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 3 figures, and 1 table. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.05391v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.05391v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuo Chen, Yichi Zhang, Yin Fang, Yuxia Geng, Lingbing Guo, Xiang Chen, Qian Li, Wen Zhang, Jiaoyan Chen, Yushan Zhu, Jiaqi Li, Xiaoze Liu, Jeff Z. Pan, Ningyu Zhang, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Graphs (KGs) play a pivotal role in advancing various AI
applications, with the semantic web community's exploration into multi-modal
dimensions unlocking new avenues for innovation. In this survey, we carefully
review over 300 articles, focusing on KG-aware research in two principal
aspects: KG-driven Multi-Modal (KG4MM) learning, where KGs support multi-modal
tasks, and Multi-Modal Knowledge Graph (MM4KG), which extends KG studies into
the MMKG realm. We begin by defining KGs and MMKGs, then explore their
construction progress. Our review includes two primary task categories:
KG-aware multi-modal learning tasks, such as Image Classification and Visual
Question Answering, and intrinsic MMKG tasks like Multi-modal Knowledge Graph
Completion and Entity Alignment, highlighting specific research trajectories.
For most of these tasks, we provide definitions, evaluation benchmarks, and
additionally outline essential insights for conducting relevant research.
Finally, we discuss current challenges and identify emerging trends, such as
progress in Large Language Modeling and Multi-modal Pre-training strategies.
This survey aims to serve as a comprehensive reference for researchers already
involved in or considering delving into KG and multi-modal learning research,
offering insights into the evolving landscape of MMKG research and supporting
future work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing work; 41 pages (Main Text), 55 pages (Total), 11 Tables, 13
  Figures, 619 citations; Paper list is available at
  https://github.com/zjukg/KG-MM-Survey</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contextualizing Internet Memes Across Social Media Platforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11157v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11157v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saurav Joshi, Filip Ilievski, Luca Luceri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Internet memes have emerged as a novel format for communication and
expressing ideas on the web. Their fluidity and creative nature are reflected
in their widespread use, often across platforms and occasionally for unethical
or harmful purposes. While computational work has already analyzed their
high-level virality over time and developed specialized classifiers for hate
speech detection, there have been no efforts to date that aim to holistically
track, identify, and map internet memes posted on social media. To bridge this
gap, we investigate whether internet memes across social media platforms can be
contextualized by using a semantic repository of knowledge, namely, a knowledge
graph. We collect thousands of potential internet meme posts from two social
media platforms, namely Reddit and Discord, and develop an
extract-transform-load procedure to create a data lake with candidate meme
posts. By using vision transformer-based similarity, we match these candidates
against the memes cataloged in IMKG -- a recently released knowledge graph of
internet memes. We leverage this grounding to highlight the potential of our
proposed framework to study the prevalence of memes on different platforms, map
them to IMKG, and provide context about memes on social media.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transforming Norm-based To Graph-based Spatial Representation for
  Spatio-Temporal Epidemiological Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14539v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14539v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teddy Lazebnik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pandemics, with their profound societal and economic impacts, pose
significant threats to global health, mortality rates, economic stability, and
political landscapes. In response to these challenges, numerous studies have
employed spatio-temporal models to enhance our understanding and management of
these complex phenomena. These spatio-temporal models can be roughly divided
into two main spatial categories: norm-based and graph-based. Norm-based models
are usually more accurate and easier to model but are more computationally
intensive and require more data to fit. On the other hand, graph-based models
are less accurate and harder to model but are less computationally intensive
and require fewer data to fit. As such, ideally, one would like to use a
graph-based model while preserving the representation accuracy obtained by the
norm-based model. In this study, we explore the ability to transform from
norm-based to graph-based spatial representation for these models. We first
show no analytical mapping between the two exists, requiring one to use
approximation numerical methods instead. We introduce a novel framework for
this task together with twelve possible implementations using a wide range of
heuristic optimization approaches. Our findings show that by leveraging
agent-based simulations and heuristic algorithms for the graph node's location
and population's spatial walk dynamics approximation one can use graph-based
spatial representation without losing much of the model's accuracy and
expressiveness. We investigate our framework for three real-world cases,
achieving 94\% accuracy preservation, on average. Moreover, an analysis of
synthetic cases shows the proposed framework is relatively robust for changes
in both spatial and temporal properties.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Long-Term Value of Exploration: Measurements, Findings and Algorithms <span class="chip">WSDM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.07764v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.07764v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Su, Xiangyu Wang, Elaine Ya Le, Liang Liu, Yuening Li, Haokai Lu, Benjamin Lipshitz, Sriraj Badam, Lukasz Heldt, Shuchao Bi, Ed Chi, Cristos Goodrow, Su-Lin Wu, Lexi Baugher, Minmin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective exploration is believed to positively influence the long-term user
experience on recommendation platforms. Determining its exact benefits,
however, has been challenging. Regular A/B tests on exploration often measure
neutral or even negative engagement metrics while failing to capture its
long-term benefits. We here introduce new experiment designs to formally
quantify the long-term value of exploration by examining its effects on content
corpus, and connecting content corpus growth to the long-term user experience
from real-world experiments. Once established the values of exploration, we
investigate the Neural Linear Bandit algorithm as a general framework to
introduce exploration into any deep learning based ranking systems. We conduct
live experiments on one of the largest short-form video recommendation
platforms that serves billions of users to validate the new experiment designs,
quantify the long-term values of exploration, and to verify the effectiveness
of the adopted neural linear bandit algorithm for exploration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, WSDM 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Interaction Fidelity Model: A Taxonomy to Distinguish the Aspects of
  Fidelity in Virtual Reality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16665v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16665v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Bonfert, Thomas Muender, Ryan P. McMahan, Frank Steinicke, Doug Bowman, Rainer Malaka, Tanja Döring
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fidelity describes how closely a replication resembles the original. It can
be helpful to analyze how faithful interactions in virtual reality (VR) are to
a reference interaction. In prior research, fidelity has been restricted to the
simulation of reality - also called realism. Our definition includes other
reference interactions, such as superpowers or fiction. Interaction fidelity is
a multilayered concept. Unfortunately, different aspects of fidelity have
either not been distinguished in scientific discourse or referred to with
inconsistent terminology. Therefore, we present the Interaction Fidelity Model
(IntFi Model). Based on the human-computer interaction loop, it systematically
covers all stages of VR interactions. The conceptual model establishes a clear
structure and precise definitions of eight distinct components. It was reviewed
through interviews with fourteen VR experts. We provide guidelines, diverse
examples, and educational material to universally apply the IntFi Model to any
VR experience. We identify common patterns and propose foundational research
opportunities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages incl. references and appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPC-NeRF: Spatial Predictive Compression for Voxel Based Radiance Field 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16366v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16366v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zetian Song, Wenhong Duan, Yuhuai Zhang, Shiqi Wang, Siwei Ma, Wen Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representing the Neural Radiance Field (NeRF) with the explicit voxel grid
(EVG) is a promising direction for improving NeRFs. However, the EVG
representation is not efficient for storage and transmission because of the
terrific memory cost. Current methods for compressing EVG mainly inherit the
methods designed for neural network compression, such as pruning and
quantization, which do not take full advantage of the spatial correlation of
voxels. Inspired by prosperous digital image compression techniques, this paper
proposes SPC-NeRF, a novel framework applying spatial predictive coding in EVG
compression. The proposed framework can remove spatial redundancy efficiently
for better compression performance.Moreover, we model the bitrate and design a
novel form of the loss function, where we can jointly optimize compression
ratio and distortion to achieve higher coding efficiency. Extensive experiments
demonstrate that our method can achieve 32% bit saving compared to the
state-of-the-art method VQRF on multiple representative test datasets, with
comparable training time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Where Do We Go from Here? Multi-scale Allocentric Relational Inference
  from Natural Spatial Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tzuf Paz-Argaman, Sayali Kulkarni, John Palowitch, Jason Baldridge, Reut Tsarfaty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When communicating routes in natural language, the concept of {\em acquired
spatial knowledge} is crucial for geographic information retrieval (GIR) and in
spatial cognitive research. However, NLP navigation studies often overlook the
impact of such acquired knowledge on textual descriptions. Current navigation
studies concentrate on egocentric local descriptions (e.g., `it will be on your
right') that require reasoning over the agent's local perception. These
instructions are typically given as a sequence of steps, with each action-step
explicitly mentioning and being followed by a landmark that the agent can use
to verify they are on the right path (e.g., `turn right and then you will
see...'). In contrast, descriptions based on knowledge acquired through a map
provide a complete view of the environment and capture its overall structure.
These instructions (e.g., `it is south of Central Park and a block north of a
police station') are typically non-sequential, contain allocentric relations,
with multiple spatial relations and implicit actions, without any explicit
verification. This paper introduces the Rendezvous (RVS) task and dataset,
which includes 10,404 examples of English geospatial instructions for reaching
a target location using map-knowledge. Our analysis reveals that RVS exhibits a
richer use of spatial allocentric relations, and requires resolving more
spatial relations simultaneously compared to previous text-based navigation
benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gradient-Guided Modality Decoupling for Missing-Modality Robustness <span class="chip">AAAI24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16318v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16318v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Wang, Shengda Luo, Guosheng Hu, Jianguo Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal learning with incomplete input data (missing modality) is
practical and challenging. In this work, we conduct an in-depth analysis of
this challenge and find that modality dominance has a significant negative
impact on the model training, greatly degrading the missing modality
performance. Motivated by Grad-CAM, we introduce a novel indicator, gradients,
to monitor and reduce modality dominance which widely exists in the
missing-modality scenario. In aid of this indicator, we present a novel
Gradient-guided Modality Decoupling (GMD) method to decouple the dependency on
dominating modalities. Specifically, GMD removes the conflicted gradient
components from different modalities to achieve this decoupling, significantly
improving the performance. In addition, to flexibly handle modal-incomplete
data, we design a parameter-efficient Dynamic Sharing (DS) framework which can
adaptively switch on/off the network parameters based on whether one modality
is available. We conduct extensive experiments on three popular multimodal
benchmarks, including BraTS 2018 for medical segmentation, CMU-MOSI, and
CMU-MOSEI for sentiment analysis. The results show that our method can
significantly outperform the competitors, showing the effectiveness of the
proposed solutions. Our code is released here:
https://github.com/HaoWang420/Gradient-guided-Modality-Decoupling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI24</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-02-25T00:00:00Z">2024-02-25</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">11</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IR2: Information Regularization for Information Retrieval <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16200v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16200v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianyou Wang, Kaicheng Wang, Xiaoyue Wang, Weili Cao, Ramamohan Paturi, Leon Bergen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective information retrieval (IR) in settings with limited training data,
particularly for complex queries, remains a challenging task. This paper
introduces IR2, Information Regularization for Information Retrieval, a
technique for reducing overfitting during synthetic data generation. This
approach, representing a novel application of regularization techniques in
synthetic data creation for IR, is tested on three recent IR tasks
characterized by complex queries: DORIS-MAE, ArguAna, and WhatsThatBook.
Experimental results indicate that our regularization techniques not only
outperform previous synthetic query generation methods on the tasks considered
but also reduce cost by up to 50%. Furthermore, this paper categorizes and
explores three regularization methods at different stages of the query
synthesis pipeline-input, prompt, and output-each offering varying degrees of
performance improvement compared to models where no regularization is applied.
This provides a systematic approach for optimizing synthetic data generation in
data-limited, complex-query IR scenarios. All code, prompts and synthetic data
are available at
https://github.com/Info-Regularization/Information-Regularization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-COLING 2024 - The 2024 Joint International
  Conference on Computational Linguistics, Language Resources and Evaluation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disentangled Graph Variational Auto-Encoder for Multimodal
  Recommendation with Interpretability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16110v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16110v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Zhou, Chunyan Miao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal recommender systems amalgamate multimodal information (e.g.,
textual descriptions, images) into a collaborative filtering framework to
provide more accurate recommendations. While the incorporation of multimodal
information could enhance the interpretability of these systems, current
multimodal models represent users and items utilizing entangled numerical
vectors, rendering them arduous to interpret. To address this, we propose a
Disentangled Graph Variational Auto-Encoder (DGVAE) that aims to enhance both
model and recommendation interpretability. DGVAE initially projects multimodal
information into textual contents, such as converting images to text, by
harnessing state-of-the-art multimodal pre-training technologies. It then
constructs a frozen item-item graph and encodes the contents and interactions
into two sets of disentangled representations utilizing a simplified residual
graph convolutional network. DGVAE further regularizes these disentangled
representations through mutual information maximization, aligning the
representations derived from the interactions between users and items with
those learned from textual content. This alignment facilitates the
interpretation of user binary interactions via text. Our empirical analysis
conducted on three real-world datasets demonstrates that DGVAE significantly
surpasses the performance of state-of-the-art baselines by a margin of 10.02%.
We also furnish a case study from a real-world dataset to illustrate the
interpretability of DGVAE. Code is available at:
\url{https://github.com/enoche/DGVAE}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pfeed: Generating near real-time personalized feeds using precomputed
  embedding similarities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16073v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16073v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binyam Gebre, Karoliina Ranta, Stef van den Elzen, Ernst Kuiper, Thijs Baars, Tom Heskes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In personalized recommender systems, embeddings are often used to encode
customer actions and items, and retrieval is then performed in the embedding
space using approximate nearest neighbor search. However, this approach can
lead to two challenges: 1) user embeddings can restrict the diversity of
interests captured and 2) the need to keep them up-to-date requires an
expensive, real-time infrastructure. In this paper, we propose a method that
overcomes these challenges in a practical, industrial setting. The method
dynamically updates customer profiles and composes a feed every two minutes,
employing precomputed embeddings and their respective similarities. We tested
and deployed this method to personalise promotional items at Bol, one of the
largest e-commerce platforms of the Netherlands and Belgium. The method
enhanced customer engagement and experience, leading to a significant 4.9%
uplift in conversions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Bilingual Lexicon Induction with Large Language Models <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13995v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13995v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaoyiran Li, Anna Korhonen, Ivan Vulić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bilingual Lexicon Induction (BLI) is a core task in multilingual NLP that
still, to a large extent, relies on calculating cross-lingual word
representations. Inspired by the global paradigm shift in NLP towards Large
Language Models (LLMs), we examine the potential of the latest generation of
LLMs for the development of bilingual lexicons. We ask the following research
question: Is it possible to prompt and fine-tune multilingual LLMs (mLLMs) for
BLI, and how does this approach compare against and complement current BLI
approaches? To this end, we systematically study 1) zero-shot prompting for
unsupervised BLI and 2) few-shot in-context prompting with a set of seed
translation pairs, both without any LLM fine-tuning, as well as 3) standard
BLI-oriented fine-tuning of smaller LLMs. We experiment with 18 open-source
text-to-text mLLMs of different sizes (from 0.3B to 13B parameters) on two
standard BLI benchmarks covering a range of typologically diverse languages.
Our work is the first to demonstrate strong BLI capabilities of text-to-text
mLLMs. The results reveal that few-shot prompting with in-context examples from
nearest neighbours achieves the best performance, establishing new
state-of-the-art BLI scores for many language pairs. We also conduct a series
of in-depth analyses and ablation studies, providing more insights on BLI with
(m)LLMs, also along with their limitations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2023 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Title Reranker for Fast and Improved Knowledge-Intense NLP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12430v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12430v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyi Chen, Jize Jiang, Daqian Zuo, Heyi Tao, Jun Yang, Yuxiang Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent RAG approaches, rerankers play a pivotal role in refining retrieval
accuracy with the ability of revealing logical relations for each pair of query
and text. However, existing rerankers are required to repeatedly encode the
query and a large number of long retrieved text. This results in high
computational costs and limits the number of retrieved text, hindering
accuracy. As a remedy of the problem, we introduce the Efficient Title Reranker
via Broadcasting Query Encoder, a novel technique for title reranking that
achieves a 20x-40x speedup over the vanilla passage reranker. Furthermore, we
introduce Sigmoid Trick, a novel loss function customized for title reranking.
Combining both techniques, we empirically validated their effectiveness,
achieving state-of-the-art results on all four datasets we experimented with
from the KILT knowledge benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Recommendation: Towards Next-generation Recommender Paradigm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.03516v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.03516v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjie Wang, Xinyu Lin, Fuli Feng, Xiangnan He, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems typically retrieve items from an item corpus for
personalized recommendations. However, such a retrieval-based recommender
paradigm faces two limitations: 1) the human-generated items in the corpus
might fail to satisfy the users' diverse information needs, and 2) users
usually adjust the recommendations via inefficient passive feedback, e.g.,
clicks. Nowadays, AI-Generated Content (AIGC) has revealed significant success,
offering the potential to overcome these limitations: 1) generative AI can
produce personalized items to satisfy users' information needs, and 2) the
newly emerged large language models significantly reduce the efforts of users
to precisely express information needs via natural language instructions. In
this light, the boom of AIGC points the way towards the next-generation
recommender paradigm with two new objectives: 1) generating personalized
content through generative AI, and 2) integrating user instructions to guide
content generation.
  To this end, we propose a novel Generative Recommender paradigm named
GeneRec, which adopts an AI generator to personalize content generation and
leverages user instructions. Specifically, we pre-process users' instructions
and traditional feedback via an instructor to output the generation guidance.
Given the guidance, we instantiate the AI generator through an AI editor and an
AI creator to repurpose existing items and create new items. Eventually,
GeneRec can perform content retrieval, repurposing, and creation to satisfy
users' information needs. Besides, to ensure the trustworthiness of the
generated items, we emphasize various fidelity checks. Moreover, we provide a
roadmap to envision future developments of GeneRec and several domain-specific
applications of GeneRec with potential research tasks. Lastly, we study the
feasibility of implementing AI editor and AI creator on micro-video generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards A Unified View of Answer Calibration for Multi-Step Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09101v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09101v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shumin Deng, Ningyu Zhang, Nay Oo, Bryan Hooi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) employing Chain-of-Thought (CoT) prompting have
broadened the scope for improving multi-step reasoning capabilities. We
generally divide multi-step reasoning into two phases: path generation to
generate the reasoning path(s); and answer calibration post-processing the
reasoning path(s) to obtain a final answer. However, the existing literature
lacks systematic analysis on different answer calibration approaches. In this
paper, we summarize the taxonomy of recent answer calibration techniques and
break them down into step-level and path-level strategies. We then conduct a
thorough evaluation on these strategies from a unified view, systematically
scrutinizing step-level and path-level answer calibration across multiple
paths. Experimental results reveal that integrating the dominance of both
strategies tends to derive optimal outcomes. Our study holds the potential to
illuminate key insights for optimizing multi-step reasoning with answer
calibration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Working in Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Point-of-Interest Recommendations Leveraging Heterogeneous
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.07426v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.07426v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehui Wang, Wolfram Höpken, Dietmar Jannach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tourism is an important application domain for recommender systems. In this
domain, recommender systems are for example tasked with providing personalized
recommendations for transportation, accommodation, points-of-interest (POIs),
etc. Among these tasks, in particular the problem of recommending POIs that are
of likely interest to individual tourists has gained growing attention in
recent years. Providing POI recommendations to tourists can however be
especially challenging due to the variability of the user's context. With the
rapid development of the Web and today's multitude of online services, vast
amounts of data from various sources have become available, and these
heterogeneous data represent a huge potential to better address the challenges
of POI recommendation problems. In this work, we provide a survey of published
research on the problem of POI recommendation between 2021 and 2023. The
literature was surveyed to identify the information types, techniques and
evaluation methods employed. Based on the analysis, it was observed that the
current research tends to focus on a relatively narrow range of information
types and there is a significant potential in improving POI recommendation by
leveraging heterogeneous data. As the first information-centric survey on POI
recommendation research, this study serves as a reference for researchers
aiming to develop increasingly accurate, personalized and context-aware POI
recommender systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11916v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11916v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Geonmo Gu, Sanghyuk Chun, Wonjae Kim, HeeJae Jun, Yoohoon Kang, Sangdoo Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel diffusion-based model, CompoDiff, for solving
zero-shot Composed Image Retrieval (ZS-CIR) with latent diffusion. This paper
also introduces a new synthetic dataset, named SynthTriplets18M, with 18.8
million reference images, conditions, and corresponding target image triplets
to train CIR models. CompoDiff and SynthTriplets18M tackle the shortages of the
previous CIR approaches, such as poor generalizability due to the small dataset
scale and the limited types of conditions. CompoDiff not only achieves a new
state-of-the-art on four ZS-CIR benchmarks, including FashionIQ, CIRR, CIRCO,
and GeneCIS, but also enables a more versatile and controllable CIR by
accepting various conditions, such as negative text, and image mask conditions.
CompoDiff also shows the controllability of the condition strength between text
and image queries and the trade-off between inference speed and performance,
which are unavailable with existing CIR methods. The code and dataset are
available at https://github.com/navervision/CompoDiff
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First two authors contributed equally; 28 pages, 6.2MB</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Disentangled Contrastive Collaborative Filtering <span class="chip">SIGIR'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.02759v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.02759v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xubin Ren, Lianghao Xia, Jiashu Zhao, Dawei Yin, Chao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies show that graph neural networks (GNNs) are prevalent to model
high-order relationships for collaborative filtering (CF). Towards this
research line, graph contrastive learning (GCL) has exhibited powerful
performance in addressing the supervision label shortage issue by learning
augmented user and item representations. While many of them show their
effectiveness, two key questions still remain unexplored: i) Most existing
GCL-based CF models are still limited by ignoring the fact that user-item
interaction behaviors are often driven by diverse latent intent factors (e.g.,
shopping for family party, preferred color or brand of products); ii) Their
introduced non-adaptive augmentation techniques are vulnerable to noisy
information, which raises concerns about the model's robustness and the risk of
incorporating misleading self-supervised signals. In light of these
limitations, we propose a Disentangled Contrastive Collaborative Filtering
framework (DCCF) to realize intent disentanglement with self-supervised
augmentation in an adaptive fashion. With the learned disentangled
representations with global context, our DCCF is able to not only distill
finer-grained latent factors from the entangled self-supervision signals but
also alleviate the augmentation-induced noise. Finally, the cross-view
contrastive learning task is introduced to enable adaptive augmentation with
our parameterized interaction mask generator. Experiments on various public
datasets demonstrate the superiority of our method compared to existing
solutions. Our model implementation is released at the link
https://github.com/HKUDS/DCCF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a SIGIR'23 full paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Representation Learning with Large Language Models for Recommendation <span class="chip">WWW'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15950v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15950v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, Chao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems have seen significant advancements with the influence of
deep learning and graph neural networks, particularly in capturing complex
user-item relationships. However, these graph-based recommenders heavily depend
on ID-based data, potentially disregarding valuable textual information
associated with users and items, resulting in less informative learned
representations. Moreover, the utilization of implicit feedback data introduces
potential noise and bias, posing challenges for the effectiveness of user
preference learning. While the integration of large language models (LLMs) into
traditional ID-based recommenders has gained attention, challenges such as
scalability issues, limitations in text-only reliance, and prompt input
constraints need to be addressed for effective implementation in practical
recommender systems. To address these challenges, we propose a model-agnostic
framework RLMRec that aims to enhance existing recommenders with LLM-empowered
representation learning. It proposes a recommendation paradigm that integrates
representation learning with LLMs to capture intricate semantic aspects of user
behaviors and preferences. RLMRec incorporates auxiliary textual signals,
develops a user/item profiling paradigm empowered by LLMs, and aligns the
semantic space of LLMs with the representation space of collaborative
relational signals through a cross-view alignment framework. This work further
establish a theoretical foundation demonstrating that incorporating textual
signals through mutual information maximization enhances the quality of
representations. In our evaluation, we integrate RLMRec with state-of-the-art
recommender models, while also analyzing its efficiency and robustness to noise
data. Our implementation codes are available at
https://github.com/HKUDS/RLMRec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a WWW'24 full paper</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChatMusician: Understanding and Generating Music Intrinsically with LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16153v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16153v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruibin Yuan, Hanfeng Lin, Yi Wang, Zeyue Tian, Shangda Wu, Tianhao Shen, Ge Zhang, Yuhang Wu, Cong Liu, Ziya Zhou, Ziyang Ma, Liumeng Xue, Ziyu Wang, Qin Liu, Tianyu Zheng, Yizhi Li, Yinghao Ma, Yiming Liang, Xiaowei Chi, Ruibo Liu, Zili Wang, Pengfei Li, Jingcheng Wu, Chenghua Lin, Qifeng Liu, Tao Jiang, Wenhao Huang, Wenhu Chen, Emmanouil Benetos, Jie Fu, Gus Xia, Roger Dannenberg, Wei Xue, Shiyin Kang, Yike Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Large Language Models (LLMs) demonstrate impressive capabilities in
text generation, we find that their ability has yet to be generalized to music,
humanity's creative language. We introduce ChatMusician, an open-source LLM
that integrates intrinsic musical abilities. It is based on continual
pre-training and finetuning LLaMA2 on a text-compatible music representation,
ABC notation, and the music is treated as a second language. ChatMusician can
understand and generate music with a pure text tokenizer without any external
multi-modal neural structures or tokenizers. Interestingly, endowing musical
abilities does not harm language abilities, even achieving a slightly higher
MMLU score. Our model is capable of composing well-structured, full-length
music, conditioned on texts, chords, melodies, motifs, musical forms, etc,
surpassing GPT-4 baseline. On our meticulously curated college-level music
understanding benchmark, MusicTheoryBench, ChatMusician surpasses LLaMA2 and
GPT-3.5 on zero-shot setting by a noticeable margin. Our work reveals that LLMs
can be an excellent compressor for music, but there remains significant
territory to be conquered. We release our 4B token music-language corpora
MusicPile, the collected MusicTheoryBench, code, model and demo in GitHub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>GitHub: https://shanghaicannon.github.io/ChatMusician/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disentangled Graph Variational Auto-Encoder for Multimodal
  Recommendation with Interpretability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16110v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16110v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Zhou, Chunyan Miao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal recommender systems amalgamate multimodal information (e.g.,
textual descriptions, images) into a collaborative filtering framework to
provide more accurate recommendations. While the incorporation of multimodal
information could enhance the interpretability of these systems, current
multimodal models represent users and items utilizing entangled numerical
vectors, rendering them arduous to interpret. To address this, we propose a
Disentangled Graph Variational Auto-Encoder (DGVAE) that aims to enhance both
model and recommendation interpretability. DGVAE initially projects multimodal
information into textual contents, such as converting images to text, by
harnessing state-of-the-art multimodal pre-training technologies. It then
constructs a frozen item-item graph and encodes the contents and interactions
into two sets of disentangled representations utilizing a simplified residual
graph convolutional network. DGVAE further regularizes these disentangled
representations through mutual information maximization, aligning the
representations derived from the interactions between users and items with
those learned from textual content. This alignment facilitates the
interpretation of user binary interactions via text. Our empirical analysis
conducted on three real-world datasets demonstrates that DGVAE significantly
surpasses the performance of state-of-the-art baselines by a margin of 10.02%.
We also furnish a case study from a real-world dataset to illustrate the
interpretability of DGVAE. Code is available at:
\url{https://github.com/enoche/DGVAE}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Contrastive Self-Distillation for Ultra-Fine-Grained Visual
  Categorization Targeting Limited Samples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.06056v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.06056v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziye Fang, Xin Jiang, Hao Tang, Zechao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of intelligent multimedia analysis, ultra-fine-grained visual
categorization (Ultra-FGVC) plays a vital role in distinguishing intricate
subcategories within broader categories. However, this task is inherently
challenging due to the complex granularity of category subdivisions and the
limited availability of data for each category. To address these challenges,
this work proposes CSDNet, a pioneering framework that effectively explores
contrastive learning and self-distillation to learn discriminative
representations specifically designed for Ultra-FGVC tasks. CSDNet comprises
three main modules: Subcategory-Specific Discrepancy Parsing (SSDP), Dynamic
Discrepancy Learning (DDL), and Subcategory-Specific Discrepancy Transfer
(SSDT), which collectively enhance the generalization of deep models across
instance, feature, and logit prediction levels. To increase the diversity of
training samples, the SSDP module introduces adaptive augmented samples to
spotlight subcategory-specific discrepancies. Simultaneously, the proposed DDL
module stores historical intermediate features by a dynamic memory queue, which
optimizes the feature learning space through iterative contrastive learning.
Furthermore, the SSDT module effectively distills subcategory-specific
discrepancies knowledge from the inherent structure of limited training data
using a self-distillation paradigm at the logit prediction level. Experimental
results demonstrate that CSDNet outperforms current state-of-the-art Ultra-FGVC
methods, emphasizing its powerful efficacy and adaptability in addressing
Ultra-FGVC tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for Publication in TCSVT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MM-Point: Multi-View Information-Enhanced Multi-Modal <span class="highlight-title">Self-Supervised</span> 3D
  Point Cloud Understanding <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10002v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10002v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hai-Tao Yu, Mofei Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In perception, multiple sensory information is integrated to map visual
information from 2D views onto 3D objects, which is beneficial for
understanding in 3D environments. But in terms of a single 2D view rendered
from different angles, only limited partial information can be provided.The
richness and value of Multi-view 2D information can provide superior
self-supervised signals for 3D objects. In this paper, we propose a novel
self-supervised point cloud representation learning method, MM-Point, which is
driven by intra-modal and inter-modal similarity objectives. The core of
MM-Point lies in the Multi-modal interaction and transmission between 3D
objects and multiple 2D views at the same time. In order to more effectively
simultaneously perform the consistent cross-modal objective of 2D multi-view
information based on contrastive learning, we further propose Multi-MLP and
Multi-level Augmentation strategies. Through carefully designed transformation
strategies, we further learn Multi-level invariance in 2D Multi-views. MM-Point
demonstrates state-of-the-art (SOTA) performance in various downstream tasks.
For instance, it achieves a peak accuracy of 92.4% on the synthetic dataset
ModelNet40, and a top accuracy of 87.8% on the real-world dataset ScanObjectNN,
comparable to fully supervised methods. Additionally, we demonstrate its
effectiveness in tasks such as few-shot classification, 3D part segmentation
and 3D semantic segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Diffusion Models with State Space Backbone 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.05608v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.05608v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengcong Fei, Mingyuan Fan, Changqian Yu, Junshi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a new exploration into a category of diffusion models
built upon state space architecture. We endeavor to train diffusion models for
image data, wherein the traditional U-Net backbone is supplanted by a state
space backbone, functioning on raw patches or latent space. Given its notable
efficacy in accommodating long-range dependencies, Diffusion State Space Models
(DiS) are distinguished by treating all inputs including time, condition, and
noisy image patches as tokens. Our assessment of DiS encompasses both
unconditional and class-conditional image generation scenarios, revealing that
DiS exhibits comparable, if not superior, performance to CNN-based or
Transformer-based U-Net architectures of commensurate size. Furthermore, we
analyze the scalability of DiS, gauged by the forward pass complexity
quantified in Gflops. DiS models with higher Gflops, achieved through
augmentation of depth/width or augmentation of input tokens, consistently
demonstrate lower FID. In addition to demonstrating commendable scalability
characteristics, DiS-H/2 models in latent space achieve performance levels akin
to prior diffusion models on class-conditional ImageNet benchmarks at the
resolution of 256$\times$256 and 512$\times$512, while significantly reducing
the computational burden. The code and models are available at:
https://github.com/feizc/DiS.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-02-24T00:00:00Z">2024-02-24</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">10</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MultiContrievers: Analysis of Dense Retrieval Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15925v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15925v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seraphina Goldfarb-Tarrant, Pedro Rodriguez, Jane Dwivedi-Yu, Patrick Lewis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense retrievers compress source documents into (possibly lossy) vector
representations, yet there is little analysis of what information is lost
versus preserved, and how it affects downstream tasks. We conduct the first
analysis of the information captured by dense retrievers compared to the
language models they are based on (e.g., BERT versus Contriever). We use 25
MultiBert checkpoints as randomized initialisations to train MultiContrievers,
a set of 25 contriever models. We test whether specific pieces of information
-- such as gender and occupation -- can be extracted from contriever vectors of
wikipedia-like documents. We measure this extractability via information
theoretic probing. We then examine the relationship of extractability to
performance and gender bias, as well as the sensitivity of these results to
many random initialisations and data shuffles. We find that (1) contriever
models have significantly increased extractability, but extractability usually
correlates poorly with benchmark performance 2) gender bias is present, but is
not caused by the contriever representations 3) there is high sensitivity to
both random initialisation and to data shuffle, suggesting that future
retrieval research should test across a wider spread of both.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15838v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15838v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soyoung Yoon, Eunbi Lee, Jiyeon Kim, Yireun Kim, Hyeongu Yun, Seung-won Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose ListT5, a novel reranking approach based on Fusion-in-Decoder
(FiD) that handles multiple candidate passages at both train and inference
time. We also introduce an efficient inference framework for listwise ranking
based on m-ary tournament sort with output caching. We evaluate and compare our
model on the BEIR benchmark for zero-shot retrieval task, demonstrating that
ListT5 (1) outperforms the state-of-the-art RankT5 baseline with a notable +1.3
gain in the average NDCG@10 score, (2) has an efficiency comparable to
pointwise ranking models and surpasses the efficiency of previous listwise
ranking models, and (3) overcomes the lost-in-the-middle problem of previous
listwise rerankers. Our code, model checkpoints, and the evaluation framework
are fully open-sourced at \url{https://github.com/soyoung97/ListT5}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Debiased Model-based Interactive Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15819v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15819v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian Li, Ruichu Cai, Haiqin Huang, Sili Zhang, Yuguang Yan, Zhifeng Hao, Zhenghua Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing model-based interactive recommendation systems are trained by
querying a world model to capture the user preference, but learning the world
model from historical logged data will easily suffer from bias issues such as
popularity bias and sampling bias. This is why some debiased methods have been
proposed recently. However, two essential drawbacks still remain: 1) ignoring
the dynamics of the time-varying popularity results in a false reweighting of
items. 2) taking the unknown samples as negative samples in negative sampling
results in the sampling bias. To overcome these two drawbacks, we develop a
model called \textbf{i}dentifiable \textbf{D}ebiased \textbf{M}odel-based
\textbf{I}nteractive \textbf{R}ecommendation (\textbf{iDMIR} in short). In
iDMIR, for the first drawback, we devise a debiased causal world model based on
the causal mechanism of the time-varying recommendation generation process with
identification guarantees; for the second drawback, we devise a debiased
contrastive policy, which coincides with the debiased contrastive learning and
avoids sampling bias. Moreover, we demonstrate that the proposed method not
only outperforms several latest interactive recommendation algorithms but also
enjoys diverse recommendation performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cryptanalysis and improvement of multimodal data encryption by
  machine-learning-based system 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zakaria Tolba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rising popularity of the internet and the widespread use of networks
and information systems via the cloud and data centers, the privacy and
security of individuals and organizations have become extremely crucial. In
this perspective, encryption consolidates effective technologies that can
effectively fulfill these requirements by protecting public information
exchanges. To achieve these aims, the researchers used a wide assortment of
encryption algorithms to accommodate the varied requirements of this field, as
well as focusing on complex mathematical issues during their work to
substantially complicate the encrypted communication mechanism. as much as
possible to preserve personal information while significantly reducing the
possibility of attacks. Depending on how complex and distinct the requirements
established by these various applications are, the potential of trying to break
them continues to occur, and systems for evaluating and verifying the
cryptographic algorithms implemented continue to be necessary. The best
approach to analyzing an encryption algorithm is to identify a practical and
efficient technique to break it or to learn ways to detect and repair weak
aspects in algorithms, which is known as cryptanalysis. Experts in
cryptanalysis have discovered several methods for breaking the cipher, such as
discovering a critical vulnerability in mathematical equations to derive the
secret key or determining the plaintext from the ciphertext. There are various
attacks against secure cryptographic algorithms in the literature, and the
strategies and mathematical solutions widely employed empower cryptanalysts to
demonstrate their findings, identify weaknesses, and diagnose maintenance
failures in algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Doctoral thesis. Keywords: Cryptanalysis, Black-box, Deep learning,
  Machine learning, Ciphertext, Plaintext, Genetic algorithm, Permutation box,
  Substitution Box</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Query Augmentation by Decoding Semantics from Brain Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyi Ye, Jingtao Zhan, Qingyao Ai, Yiqun Liu, Maarten de Rijke, Christina Lioma, Tuukka Ruotsalo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Query augmentation is a crucial technique for refining semantically imprecise
queries. Traditionally, query augmentation relies on extracting information
from initially retrieved, potentially relevant documents. If the quality of the
initially retrieved documents is low, then the effectiveness of query
augmentation would be limited as well. We propose Brain-Aug, which enhances a
query by incorporating semantic information decoded from brain signals.
BrainAug generates the continuation of the original query with a prompt
constructed with brain signal information and a ranking-oriented inference
approach. Experimental results on fMRI (functional magnetic resonance imaging)
datasets show that Brain-Aug produces semantically more accurate queries,
leading to improved document ranking performance. Such improvement brought by
brain signals is particularly notable for ambiguous queries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Universal Model in Online Customer Service 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15666v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15666v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu-Ting Pi, Cheng-Ping Hsieh, Qun Liu, Yuying Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building machine learning models can be a time-consuming process that often
takes several months to implement in typical business scenarios. To ensure
consistent model performance and account for variations in data distribution,
regular retraining is necessary. This paper introduces a solution for improving
online customer service in e-commerce by presenting a universal model for
predict-ing labels based on customer questions, without requiring training. Our
novel approach involves using machine learning techniques to tag customer
questions in transcripts and create a repository of questions and corresponding
labels. When a customer requests assistance, an information retrieval model
searches the repository for similar questions, and statistical analysis is used
to predict the corresponding label. By eliminating the need for individual
model training and maintenance, our approach reduces both the model development
cycle and costs. The repository only requires periodic updating to maintain
accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal
  Contrastive EHR Modelling with Hierarchical Regularisation <span class="chip">EACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11648v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11648v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heejoon Koo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting next visit diagnosis using Electronic Health Records (EHR) is an
essential task in healthcare, critical for devising proactive future plans for
both healthcare providers and patients. Nonetheless, many preceding studies
have not sufficiently addressed the heterogeneous and hierarchical
characteristics inherent in EHR data, inevitably leading to sub-optimal
performance. To this end, we propose NECHO, a novel medical code-centric
multimodal contrastive EHR learning framework with hierarchical regularisation.
First, we integrate multifaceted information encompassing medical codes,
demographics, and clinical notes using a tailored network design and a pair of
bimodal contrastive losses, all of which pivot around a medical codes
representation. We also regularise modality-specific encoders using a parental
level information in medical ontology to learn hierarchical structure of EHR
data. A series of experiments on MIMIC-III data demonstrates effectiveness of
our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EACL 2024 (The 18th Conference of the European Chapter of
  the Association for Computational Linguistics)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Federated Recommendation Meets Cold-Start Problem: Separating Item
  Attributes and User Interactions <span class="chip">WWW'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.12650v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.12650v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunxu Zhang, Guodong Long, Tianyi Zhou, Zijian Zhang, Peng Yan, Bo Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated recommendation system usually trains a global model on the server
without direct access to users' private data on their own devices. However,
this separation of the recommendation model and users' private data poses a
challenge in providing quality service, particularly when it comes to new
items, namely cold-start recommendations in federated settings. This paper
introduces a novel method called Item-aligned Federated Aggregation (IFedRec)
to address this challenge. It is the first research work in federated
recommendation to specifically study the cold-start scenario. The proposed
method learns two sets of item representations by leveraging item attributes
and interaction records simultaneously. Additionally, an item representation
alignment mechanism is designed to align two item representations and learn the
meta attribute network at the server within a federated learning framework.
Experiments on four benchmark datasets demonstrate IFedRec's superior
performance for cold-start scenarios. Furthermore, we also verify IFedRec owns
good robustness when the system faces limited client participation and noise
injection, which brings promising practical application potential in
privacy-protection enhanced federated recommendation systems. The
implementation code is available
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a regular paper of WWW'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Broad Recommender System: An Efficient Nonlinear Collaborative Filtering
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.11602v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.11602v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ling Huang, Can-Rong Guan, Zhen-Wei Huang, Yuefang Gao, Yingjie Kuang, Chang-Dong Wang, C. L. Philip Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Deep Neural Networks (DNNs) have been widely introduced into
Collaborative Filtering (CF) to produce more accurate recommendation results
due to their capability of capturing the complex nonlinear relationships
between items and users.However, the DNNs-based models usually suffer from high
computational complexity, i.e., consuming very long training time and storing
huge amount of trainable parameters. To address these problems, we propose a
new broad recommender system called Broad Collaborative Filtering (BroadCF),
which is an efficient nonlinear collaborative filtering approach. Instead of
DNNs, Broad Learning System (BLS) is used as a mapping function to learn the
complex nonlinear relationships between users and items, which can avoid the
above issues while achieving very satisfactory recommendation performance.
However, it is not feasible to directly feed the original rating data into BLS.
To this end, we propose a user-item rating collaborative vector preprocessing
procedure to generate low-dimensional user-item input data, which is able to
harness quality judgments of the most similar users/items. Extensive
experiments conducted on seven benchmark datasets have confirmed the
effectiveness of the proposed BroadCF algorithm
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisit and Outstrip Entity Alignment: A Perspective of Generative
  Models <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14651v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14651v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingbing Guo, Zhuo Chen, Jiaoyan Chen, Yin Fang, Wen Zhang, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent embedding-based methods have achieved great successes in exploiting
entity alignment from knowledge graph (KG) embeddings of multiple modalities.
In this paper, we study embedding-based entity alignment (EEA) from a
perspective of generative models. We show that EEA shares similarities with
typical generative models and prove the effectiveness of the recently developed
generative adversarial network (GAN)-based EEA methods theoretically. We then
reveal that their incomplete objective limits the capacity on both entity
alignment and entity synthesis (i.e., generating new entities). We mitigate
this problem by introducing a generative EEA (GEEA) framework with the proposed
mutual variational autoencoder (M-VAE) as the generative model. M-VAE enables
entity conversion between KGs and generation of new entities from random noise
vectors. We demonstrate the power of GEEA with theoretical analysis and
empirical experiments on both entity alignment and entity synthesis tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting Outcomes in Video Games with Long Short Term Memory Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15923v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15923v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kittimate Chulajata, Sean Wu, Fabien Scalzo, Eun Sang Cha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Forecasting winners in E-sports with real-time analytics has the potential to
further engage audiences watching major tournament events. However, making such
real-time predictions is challenging due to unpredictable variables within the
game involving diverse player strategies and decision-making. Our work attempts
to enhance audience engagement within video game tournaments by introducing a
real-time method of predicting wins. Our Long Short Term Memory Network (LSTMs)
based approach enables efficient predictions of win-lose outcomes by only using
the health indicator of each player as a time series. As a proof of concept, we
evaluate our model's performance within a classic, two-player arcade game,
Super Street Fighter II Turbo. We also benchmark our method against state of
the art methods for time series forecasting; i.e. Transformer models found in
large language models (LLMs). Finally, we open-source our data set and code in
hopes of furthering work in predictive analysis for arcade games.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 Figures, 2 Tables. Kittimate Chulajata and Sean Wu are
  considered co-first authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intelligent Director: An Automatic Framework for Dynamic Visual
  Composition using Chat<span class="highlight-title">GPT</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sixiao Zheng, Jingyang Huo, Yu Wang, Yanwei Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rise of short video platforms represented by TikTok, the trend of
users expressing their creativity through photos and videos has increased
dramatically. However, ordinary users lack the professional skills to produce
high-quality videos using professional creation software. To meet the demand
for intelligent and user-friendly video creation tools, we propose the Dynamic
Visual Composition (DVC) task, an interesting and challenging task that aims to
automatically integrate various media elements based on user requirements and
create storytelling videos. We propose an Intelligent Director framework,
utilizing LENS to generate descriptions for images and video frames and
combining ChatGPT to generate coherent captions while recommending appropriate
music names. Then, the best-matched music is obtained through music retrieval.
Then, materials such as captions, images, videos, and music are integrated to
seamlessly synthesize the video. Finally, we apply AnimeGANv2 for style
transfer. We construct UCF101-DVC and Personal Album datasets and verified the
effectiveness of our framework in solving DVC through qualitative and
quantitative comparisons, along with user studies, demonstrating its
substantial potential.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://sixiaozheng.github.io/IntelligentDirector/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Applied User Research in Virtual Reality: Tools, Methods, and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonie Bensch, Andrea Casini, Aidan Cowley, Florian Dufresne, Enrico Guerra, Paul de Medeiros, Tommy Nilsson, Flavie Rometsch, Andreas Treuer, Anna Vock
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This chapter explores the practice of conducting user research studies and
design assessments in virtual reality (VR). An overview of key VR hardware and
software tools is provided, including game engines, such as Unity and Unreal
Engine. Qualitative and quantitative research methods, along with their various
synergies with VR, are likewise discussed, and some of the challenges
associated with VR, such as limited sensory stimulation, are reflected upon. VR
is proving particularly useful in the context of space systems development,
where its utilisation offers a cost-effective and secure method for simulating
extraterrestrial environments, allowing for rapid prototyping and evaluation of
innovative concepts under representative operational conditions. To illustrate
this, we present a case study detailing the application of VR to aid aerospace
engineers testing their ideas with end-users and stakeholders during early
design stages of the European Space Agency's (ESA) prospective Argonaut lunar
lander. This case study demonstrates the effectiveness of VR simulations in
gathering important feedback concerning the operability of the Argonaut lander
in poor lighting conditions as well as surfacing relevant ergonomics
considerations and constraints. The chapter concludes by discussing the
strengths and weaknesses associated with VR-based user studies and proposes
future research directions, emphasising the necessity for novel VR interfaces
to overcome existing technical limitations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COPA: Efficient Vision-Language <span class="highlight-title">Pre-train</span>ing Through Collaborative
  Object- and Patch-Text Alignment <span class="chip">ACM MM2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.03475v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.03475v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoya Jiang, Haiyang Xu, Wei Ye, Qinghao Ye, Chenliang Li, Ming Yan, Bin Bi, Shikun Zhang, Ji Zhang, Fei Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Pre-training (VLP) methods based on object detection enjoy
the rich knowledge of fine-grained object-text alignment but at the cost of
computationally expensive inference. Recent Visual-Transformer (ViT)-based
approaches circumvent this issue while struggling with long visual sequences
without detailed cross-modal alignment information. This paper introduces a
ViT-based VLP technique that efficiently incorporates object information
through a novel patch-text alignment mechanism. Specifically, we convert
object-level signals into patch-level ones and devise a Patch-Text Alignment
pre-training task (PTA) to learn a text-aware patch detector. By using
off-the-shelf delicate object annotations in 5\% training images, we jointly
train PTA with other conventional VLP objectives in an end-to-end manner,
bypassing the high computational cost of object detection and yielding an
effective patch detector that accurately detects text-relevant patches, thus
considerably reducing patch sequences and accelerating computation within the
ViT backbone. Our experiments on a variety of widely-used benchmarks reveal
that our method achieves a speedup of nearly 88\% compared to prior VLP models
while maintaining competitive or superior performance on downstream tasks with
similar model size and data scale.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted on ACM MM2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-02-23T00:00:00Z">2024-02-23</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">19</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Artful Path to Healing: Using Machine Learning for Visual Art
  Recommendation to Prevent and Reduce Post-Intensive Care 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bereket A. Yilma, Chan Mi Kim, Gerald C. Cupchik, Luis A. Leiva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Staying in the intensive care unit (ICU) is often traumatic, leading to
post-intensive care syndrome (PICS), which encompasses physical, psychological,
and cognitive impairments. Currently, there are limited interventions available
for PICS. Studies indicate that exposure to visual art may help address the
psychological aspects of PICS and be more effective if it is personalized. We
develop Machine Learning-based Visual Art Recommendation Systems (VA RecSys) to
enable personalized therapeutic visual art experiences for post-ICU patients.
We investigate four state-of-the-art VA RecSys engines, evaluating the
relevance of their recommendations for therapeutic purposes compared to
expert-curated recommendations. We conduct an expert pilot test and a
large-scale user study (n=150) to assess the appropriateness and effectiveness
of these recommendations. Our results suggest all recommendations enhance
temporal affective states. Visual and multimodal VA RecSys engines compare
favourably with expert-curated recommendations, indicating their potential to
support the delivery of personalized art therapy for PICS prevention and
treatment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the 2024 CHI Conference on Human Factors in Computing
  Systems (CHI 24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language-Based User Profiles for Recommendation <span class="chip">WSDM2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joyce Zhou, Yijia Dai, Thorsten Joachims
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most conventional recommendation methods (e.g., matrix factorization)
represent user profiles as high-dimensional vectors. Unfortunately, these
vectors lack interpretability and steerability, and often perform poorly in
cold-start settings. To address these shortcomings, we explore the use of user
profiles that are represented as human-readable text. We propose the
Language-based Factorization Model (LFM), which is essentially an
encoder/decoder model where both the encoder and the decoder are large language
models (LLMs). The encoder LLM generates a compact natural-language profile of
the user's interests from the user's rating history. The decoder LLM uses this
summary profile to complete predictive downstream tasks. We evaluate our LFM
approach on the MovieLens dataset, comparing it against matrix factorization
and an LLM model that directly predicts from the user's rating history. In
cold-start settings, we find that our method can have higher accuracy than
matrix factorization. Furthermore, we find that generating a compact and
human-readable summary often performs comparably with or better than direct LLM
prediction, while enjoying better interpretability and shorter model input
length. Our results motivate a number of future research directions and
potential improvements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages (4 in appendix), 22 tables/figures (16 in appendix). Accepted
  to LLM-IGS@WSDM2024 workshop, now sharing this slightly updated revision
  version with workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RecWizard: A Toolkit for Conversational Recommendation with Modular,
  Portable Models and Interactive User Interface <span class="chip">AAAI'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyuan Zhang, Tanmay Laud, Zihang He, Xiaojie Chen, Xinshuang Liu, Zhouhang Xie, Julian McAuley, Zhankui He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new Python toolkit called RecWizard for Conversational
Recommender Systems (CRS). RecWizard offers support for development of models
and interactive user interface, drawing from the best practices of the
Huggingface ecosystems. CRS with RecWizard are modular, portable, interactive
and Large Language Models (LLMs)-friendly, to streamline the learning process
and reduce the additional effort for CRS research. For more comprehensive
information about RecWizard, please check our GitHub
https://github.com/McAuley-Lab/RecWizard.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI'24 Demo Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Faithful Temporal Question Answering over Heterogeneous Sources <span class="chip">WWW 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15400v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15400v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Jia, Philipp Christmann, Gerhard Weikum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal question answering (QA) involves time constraints, with phrases such
as "... in 2019" or "... before COVID". In the former, time is an explicit
condition, in the latter it is implicit. State-of-the-art methods have
limitations along three dimensions. First, with neural inference, time
constraints are merely soft-matched, giving room to invalid or inexplicable
answers. Second, questions with implicit time are poorly supported. Third,
answers come from a single source: either a knowledge base (KB) or a text
corpus. We propose a temporal QA system that addresses these shortcomings.
First, it enforces temporal constraints for faithful answering with tangible
evidence. Second, it properly handles implicit questions. Third, it operates
over heterogeneous sources, covering KB, text and web tables in a unified
manner. The method has three stages: (i) understanding the question and its
temporal conditions, (ii) retrieving evidence from all sources, and (iii)
faithfully answering the question. As implicit questions are sparse in prior
benchmarks, we introduce a principled method for generating diverse questions.
Experiments show superior performance over a suite of baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WWW 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text2Pic Swift: Enhancing Long-Text to Image Retrieval for Large-Scale
  Libraries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijun Long, Xuri Ge, Richard Mccreadie, Joemon Jose
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image retrieval plays a crucial role across various applications,
including digital libraries, e-commerce platforms, and multimedia databases, by
enabling the search for images using text queries. Despite the advancements in
Multimodal Large Language Models (MLLMs), which offer leading-edge performance,
their applicability in large-scale, varied, and ambiguous retrieval scenarios
is constrained by significant computational demands and the generation of
injective embeddings. This paper introduces the Text2Pic Swift framework,
tailored for efficient and robust retrieval of images corresponding to
extensive textual descriptions in sizable datasets. The framework employs a
two-tier approach: the initial Entity-based Ranking (ER) stage addresses the
ambiguity inherent in lengthy text queries through a
multiple-queries-to-multiple-targets strategy, effectively narrowing down
potential candidates for subsequent analysis. Following this, the Summary-based
Re-ranking (SR) stage further refines these selections based on concise query
summaries. Additionally, we present a novel Decoupling-BEiT-3 encoder,
specifically designed to tackle the challenges of ambiguous queries and to
facilitate both stages of the retrieval process, thereby significantly
improving computational efficiency via vector-based similarity assessments. Our
evaluation, conducted on the AToMiC dataset, demonstrates that Text2Pic Swift
outperforms current MLLMs by achieving up to an 11.06% increase in Recall@1000,
alongside reductions in training and retrieval durations by 68.75% and 99.79%,
respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Countries pushing the boundaries of knowledge: the US dominance, China
  rise, and the EU stagnation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15263v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15263v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alonso Rodriguez-Navarro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowing which countries contribute the most to pushing the boundaries of
knowledge in science and technology has social and political importance.
However, common citation metrics do not adequately measure this contribution.
This measure requires more stringent metrics appropriate for the highly
influential breakthrough papers that push the boundaries of knowledge, which
are very highly cited but very rare. Here I used the recently described Rk
index, specifically designed to address this issue. I applied this index to 25
countries and the EU across 10 key research topics, five technological and five
biomedical, studying domestic and international collaborative papers
independently. In technological topics, the Rk indices of domestic papers show
that overall, the USA, China, and the EU are leaders; other countries are
clearly behind. The USA is notably ahead of China, and the EU is far behind
China. The same approach to biomedical topics shows an overwhelming dominance
of the USA and that the EU is ahead of China. The analysis of internationally
collaborative papers further demonstrates the US dominance. These results
conflict with current country rankings based on less stringent indicators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 1 figure, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Agent Collaboration Framework for Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhefan Wang, Yuanqing Yu, Wendi Zheng, Weizhi Ma, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM-based agents have gained considerable attention for their decision-making
skills and ability to handle complex tasks. Recognizing the current gap in
leveraging agent capabilities for multi-agent collaboration in recommendation
systems, we introduce MACRec, a novel framework designed to enhance
recommendation systems through multi-agent collaboration. Unlike existing work
on using agents for user/item simulation, we aim to deploy multi-agents to
tackle recommendation tasks directly. In our framework, recommendation tasks
are addressed through the collaborative efforts of various specialized agents,
including Manager, User/Item Analyst, Reflector, Searcher, and Task
Interpreter, with different working flows. Furthermore, we provide application
examples of how developers can easily use MACRec on various recommendation
tasks, including rating prediction, sequential recommendation, conversational
recommendation, and explanation generation of recommendation results. The
framework and demonstration video are publicly available at
https://github.com/wzf2000/MACRec.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Item-side Fairness of Large Language Model-based Recommendation System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15215v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15215v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng Jiang, Keqin Bao, Jizhi Zhang, Wenjie Wang, Zhengyi Yang, Fuli Feng, Xiangnan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommendation systems for Web content distribution intricately connect to
the information access and exposure opportunities for vulnerable populations.
The emergence of Large Language Models-based Recommendation System (LRS) may
introduce additional societal challenges to recommendation systems due to the
inherent biases in Large Language Models (LLMs). From the perspective of
item-side fairness, there remains a lack of comprehensive investigation into
the item-side fairness of LRS given the unique characteristics of LRS compared
to conventional recommendation systems. To bridge this gap, this study examines
the property of LRS with respect to item-side fairness and reveals the
influencing factors of both historical users' interactions and inherent
semantic biases of LLMs, shedding light on the need to extend conventional
item-side fairness methods for LRS. Towards this goal, we develop a concise and
effective framework called IFairLRS to enhance the item-side fairness of an
LRS. IFairLRS covers the main stages of building an LRS with specifically
adapted strategies to calibrate the recommendations of LRS. We utilize IFairLRS
to fine-tune LLaMA, a representative LLM, on \textit{MovieLens} and
\textit{Steam} datasets, and observe significant item-side fairness
improvements. The code can be found in
https://github.com/JiangM-C/IFairLRS.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the Proceedings of the ACM Web Conference 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EasyRL4Rec: A User-Friendly Code Library for Reinforcement Learning
  Based Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15164v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15164v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanqing Yu, Chongming Gao, Jiawei Chen, Heng Tang, Yuefeng Sun, Qian Chen, Weizhi Ma, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL)-Based Recommender Systems (RSs) are increasingly
recognized for their ability to improve long-term user engagement. Yet, the
field grapples with challenges such as the absence of accessible frameworks,
inconsistent evaluation standards, and the complexity of replicating prior
work. Addressing these obstacles, we present EasyRL4Rec, a user-friendly and
efficient library tailored for RL-based RSs. EasyRL4Rec features lightweight,
diverse RL environments built on five widely-used public datasets, and is
equipped with comprehensive core modules that offer rich options to ease the
development of models. It establishes consistent evaluation criteria with a
focus on long-term impacts and introduces customized solutions for state
modeling and action representation tailored to recommender systems.
Additionally, we share valuable insights gained from extensive experiments with
current methods. EasyRL4Rec aims to facilitate the model development and
experimental process in the domain of RL-based RSs. The library is openly
accessible at https://github.com/chongminggao/EasyRL4Rec.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trajectory-wise Iterative Reinforcement Learning Framework for
  Auto-bidding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15102v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15102v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoming Li, Yusen Huo, Shuai Dou, Zhenzhe Zheng, Zhilin Zhang, Chuan Yu, Jian Xu, Fan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In online advertising, advertisers participate in ad auctions to acquire ad
opportunities, often by utilizing auto-bidding tools provided by demand-side
platforms (DSPs). The current auto-bidding algorithms typically employ
reinforcement learning (RL). However, due to safety concerns, most RL-based
auto-bidding policies are trained in simulation, leading to a performance
degradation when deployed in online environments. To narrow this gap, we can
deploy multiple auto-bidding agents in parallel to collect a large interaction
dataset. Offline RL algorithms can then be utilized to train a new policy. The
trained policy can subsequently be deployed for further data collection,
resulting in an iterative training framework, which we refer to as iterative
offline RL. In this work, we identify the performance bottleneck of this
iterative offline RL framework, which originates from the ineffective
exploration and exploitation caused by the inherent conservatism of offline RL
algorithms. To overcome this bottleneck, we propose Trajectory-wise Exploration
and Exploitation (TEE), which introduces a novel data collecting and data
utilization method for iterative offline RL from a trajectory perspective.
Furthermore, to ensure the safety of online exploration while preserving the
dataset quality for TEE, we propose Safe Exploration by Adaptive Action
Selection (SEAS). Both offline experiments and real-world experiments on
Alibaba display advertising platform demonstrate the effectiveness of our
proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by The Web Conference 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Col<span class="highlight-title">BERT</span>-XM: A Modular Multi-Vector Representation Model for Zero-Shot
  Multilingual Information Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15059v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15059v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoine Louis, Vageesh Saxena, Gijs van Dijck, Gerasimos Spanakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art neural retrievers predominantly focus on high-resource
languages like English, which impedes their adoption in retrieval scenarios
involving other languages. Current approaches circumvent the lack of
high-quality labeled data in non-English languages by leveraging multilingual
pretrained language models capable of cross-lingual transfer. However, these
models require substantial task-specific fine-tuning across multiple languages,
often perform poorly in languages with minimal representation in the
pretraining corpus, and struggle to incorporate new languages after the
pretraining phase. In this work, we present a novel modular dense retrieval
model that learns from the rich data of a single high-resource language and
effectively zero-shot transfers to a wide array of languages, thereby
eliminating the need for language-specific labeled data. Our model, ColBERT-XM,
demonstrates competitive performance against existing state-of-the-art
multilingual retrievers trained on more extensive datasets in various
languages. Further analysis reveals that our modular approach is highly
data-efficient, effectively adapts to out-of-distribution data, and
significantly reduces energy consumption and carbon emissions. By demonstrating
its proficiency in zero-shot scenarios, ColBERT-XM marks a shift towards more
sustainable and inclusive retrieval systems, enabling effective information
accessibility in numerous languages. We publicly release our code and models
for the community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review. Code is available at
  https://github.com/ant-louis/xm-retrievers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RecMind: Large Language Model Powered Agent For Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.14296v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.14296v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yancheng Wang, Ziyan Jiang, Zheng Chen, Fan Yang, Yingxue Zhou, Eunah Cho, Xing Fan, Xiaojiang Huang, Yanbin Lu, Yingzhen Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the recommendation system (RS) has advanced significantly through deep
learning, current RS approaches usually train and fine-tune models on
task-specific datasets, limiting their generalizability to new recommendation
tasks and their ability to leverage external knowledge due to model scale and
data size constraints. Thus, we designed an LLM-powered autonomous recommender
agent, RecMind, which is capable of leveraging external knowledge, utilizing
tools with careful planning to provide zero-shot personalized recommendations.
We propose a Self-Inspiring algorithm to improve the planning ability. At each
intermediate step, the LLM self-inspires to consider all previously explored
states to plan for the next step. This mechanism greatly improves the model's
ability to comprehend and utilize historical information in planning for
recommendation. We evaluate RecMind's performance in various recommendation
scenarios. Our experiment shows that RecMind outperforms existing zero/few-shot
LLM-based recommendation baseline methods in various tasks and achieves
comparable performance to a fully trained recommendation model P5.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Klarna Product Page <span class="highlight-title">Dataset</span>: Web Element Nomination with Graph
  Neural Networks and Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.02168v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.02168v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandra Hotti, Riccardo Sven Risuleo, Stefan Magureanu, Aref Moradi, Jens Lagergren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Web automation holds the potential to revolutionize how users interact with
the digital world, offering unparalleled assistance and simplifying tasks via
sophisticated computational methods. Central to this evolution is the web
element nomination task, which entails identifying unique elements on webpages.
Unfortunately, the development of algorithmic designs for web automation is
hampered by the scarcity of comprehensive and realistic datasets that reflect
the complexity faced by real-world applications on the Web. To address this, we
introduce the Klarna Product Page Dataset, a comprehensive and diverse
collection of webpages that surpasses existing datasets in richness and
variety. The dataset features 51,701 manually labeled product pages from 8,175
e-commerce websites across eight geographic regions, accompanied by a dataset
of rendered page screenshots. To initiate research on the Klarna Product Page
Dataset, we empirically benchmark a range of Graph Neural Networks (GNNs) on
the web element nomination task. We make three important contributions. First,
we found that a simple Convolutional GNN (GCN) outperforms complex
state-of-the-art nomination methods. Second, we introduce a training refinement
procedure that involves identifying a small number of relevant elements from
each page using the aforementioned GCN. These elements are then passed to a
large language model for the final nomination. This procedure significantly
improves the nomination accuracy by 16.8 percentage points on our challenging
dataset, without any need for fine-tuning. Finally, in response to another
prevalent challenge in this field - the abundance of training methodologies
suitable for element nomination - we introduce the Challenge Nomination
Training Procedure, a novel training approach that further boosts nomination
accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures, 3 tables, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09727v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09727v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John Canny, Ian Fischer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current Large Language Models (LLMs) are not only limited to some maximum
context length, but also are not able to robustly consume long inputs. To
address these limitations, we propose ReadAgent, an LLM agent system that
increases effective context length up to 20x in our experiments. Inspired by
how humans interactively read long documents, we implement ReadAgent as a
simple prompting system that uses the advanced language capabilities of LLMs to
(1) decide what content to store together in a memory episode, (2) compress
those memory episodes into short episodic memories called gist memories, and
(3) take actions to look up passages in the original text if ReadAgent needs to
remind itself of relevant details to complete a task. We evaluate ReadAgent
against baselines using retrieval methods, using the original long contexts,
and using the gist memories. These evaluations are performed on three
long-document reading comprehension tasks: QuALITY, NarrativeQA, and QMSum.
ReadAgent outperforms the baselines on all three tasks while extending the
effective context window by 3-20x.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: https://read-agent.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unified Framework for Multi-Domain CTR Prediction via Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10743v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10743v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichuan Fu, Xiangyang Li, Chuhan Wu, Yichao Wang, Kuicai Dong, Xiangyu Zhao, Mengchen Zhao, Huifeng Guo, Ruiming Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Click-Through Rate (CTR) prediction is a crucial task in online
recommendation platforms as it involves estimating the probability of user
engagement with advertisements or items by clicking on them. Given the
availability of various services like online shopping, ride-sharing, food
delivery, and professional services on commercial platforms, recommendation
systems in these platforms are required to make CTR predictions across multiple
domains rather than just a single domain. However, multi-domain click-through
rate (MDCTR) prediction remains a challenging task in online recommendation due
to the complex mutual influence between domains. Traditional MDCTR models
typically encode domains as discrete identifiers, ignoring rich semantic
information underlying. Consequently, they can hardly generalize to new
domains. Besides, existing models can be easily dominated by some specific
domains, which results in significant performance drops in the other domains
(i.e. the "seesaw phenomenon"). In this paper, we propose a novel solution
Uni-CTR to address the above challenges. Uni-CTR leverages a backbone Large
Language Model (LLM) to learn layer-wise semantic representations that capture
commonalities between domains. Uni-CTR also uses several domain-specific
networks to capture the characteristics of each domain. Note that we design a
masked loss strategy so that these domain-specific networks are decoupled from
backbone LLM. This allows domain-specific networks to remain unchanged when
incorporating new or removing domains, thereby enhancing the flexibility and
scalability of the system significantly. Experimental results on three public
datasets show that Uni-CTR outperforms the state-of-the-art (SOTA) MDCTR models
significantly. Furthermore, Uni-CTR demonstrates remarkable effectiveness in
zero-shot prediction. We have applied Uni-CTR in industrial scenarios,
confirming its efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submited to TOIS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Cross-Domain Sequential Recommendation under Open-World
  Assumptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.04590v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.04590v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wujiang Xu, Qitian Wu, Runzhong Wang, Mingming Ha, Qiongxu Ma, Linxun Chen, Bing Han, Junchi Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-Domain Sequential Recommendation (CDSR) methods aim to tackle the data
sparsity and cold-start problems present in Single-Domain Sequential
Recommendation (SDSR). Existing CDSR works design their elaborate structures
relying on overlapping users to propagate the cross-domain information.
However, current CDSR methods make closed-world assumptions, assuming fully
overlapping users across multiple domains and that the data distribution
remains unchanged from the training environment to the test environment. As a
result, these methods typically result in lower performance on online
real-world platforms due to the data distribution shifts. To address these
challenges under open-world assumptions, we design an \textbf{A}daptive
\textbf{M}ulti-\textbf{I}nterest \textbf{D}ebiasing framework for cross-domain
sequential recommendation (\textbf{AMID}), which consists of a multi-interest
information module (\textbf{MIM}) and a doubly robust estimator (\textbf{DRE}).
Our framework is adaptive for open-world environments and can improve the model
of most off-the-shelf single-domain sequential backbone models for CDSR. Our
MIM establishes interest groups that consider both overlapping and
non-overlapping users, allowing us to effectively explore user intent and
explicit interest. To alleviate biases across multiple domains, we developed
the DRE for the CDSR methods. We also provide a theoretical analysis that
demonstrates the superiority of our proposed estimator in terms of bias and
tail bound, compared to the IPS estimator used in previous work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Re3val: Reinforced and Reranked Generative Retrieval <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.16979v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.16979v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        EuiYul Song, Sangryul Kim, Haeju Lee, Joonkee Kim, James Thorne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative retrieval models encode pointers to information in a corpus as an
index within the model's parameters. These models serve as part of a larger
pipeline, where retrieved information conditions generation for
knowledge-intensive NLP tasks. However, we identify two limitations: the
generative retrieval does not account for contextual information. Secondly, the
retrieval can't be tuned for the downstream readers as decoding the page title
is a non-differentiable operation. This paper introduces Re3val, trained with
generative reranking and reinforcement learning using limited data. Re3val
leverages context acquired via Dense Passage Retrieval to rerank the retrieved
page titles and utilizes REINFORCE to maximize rewards generated by constrained
decoding. Additionally, we generate questions from our pre-training dataset to
mitigate epistemic uncertainty and bridge the domain gap between the
pre-training and fine-tuning datasets. Subsequently, we extract and rerank
contexts from the KILT database using the rerank page titles. Upon grounding
the top five reranked contexts, Re3val demonstrates the Top 1 KILT scores
compared to all other generative retrieval models across five KILT datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 4 figures, Findings of the Association for Computational
  Linguistics: EACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dissecting users' needs for search result explanations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.16509v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.16509v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prerna Juneja, Wenjuan Zhang, Alison Marie Smith-Renner, Hemank Lamba, Joel Tetreault, Alex Jaimes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a growing demand for transparency in search engines to understand
how search results are curated and to enhance users' trust. Prior research has
introduced search result explanations with a focus on how to explain, assuming
explanations are beneficial. Our study takes a step back to examine if search
explanations are needed and when they are likely to provide benefits.
Additionally, we summarize key characteristics of helpful explanations and
share users' perspectives on explanation features provided by Google and Bing.
Interviews with non-technical individuals reveal that users do not always seek
or understand search explanations and mostly desire them for complex and
critical tasks. They find Google's search explanations too obvious but
appreciate the ability to contest search results. Based on our findings, we
offer design recommendations for search engines and explanations to help users
better evaluate search results and enhance their search experience.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fairness Rising from the Ranks: HITS and PageRank on Homophilic Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13787v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13787v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ana-Andreea Stoica, Nelly Litvak, Augustin Chaintreau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate the conditions under which link analysis
algorithms prevent minority groups from reaching high ranking slots. We find
that the most common link-based algorithms using centrality metrics, such as
PageRank and HITS, can reproduce and even amplify bias against minority groups
in networks. Yet, their behavior differs: one one hand, we empirically show
that PageRank mirrors the degree distribution for most of the ranking positions
and it can equalize representation of minorities among the top ranked nodes; on
the other hand, we find that HITS amplifies pre-existing bias in homophilic
networks through a novel theoretical analysis, supported by empirical results.
We find the root cause of bias amplification in HITS to be the level of
homophily present in the network, modeled through an evolving network model
with two communities. We illustrate our theoretical analysis on both synthetic
and real datasets and we present directions for future work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Proceedings of The Web Conference, 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Seeing is Believing: Mitigating Hallucination in Large Vision-Language
  Models via CLIP-Guided Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15300v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15300v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ailin Deng, Zhirui Chen, Bryan Hooi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) are susceptible to object
hallucinations, an issue in which their generated text contains non-existent
objects, greatly limiting their reliability and practicality. Current
approaches often rely on the model's token likelihoods or other internal
information, instruction tuning on additional datasets, or incorporating
complex external tools. We first perform empirical analysis on sentence-level
LVLM hallucination, finding that CLIP similarity to the image acts as a
stronger and more robust indicator of hallucination compared to token
likelihoods. Motivated by this, we introduce our CLIP-Guided Decoding (CGD)
approach, a straightforward but effective training-free approach to reduce
object hallucination at decoding time. CGD uses CLIP to guide the model's
decoding process by enhancing visual grounding of generated text with the
image. Experiments demonstrate that CGD effectively mitigates object
hallucination across multiple LVLM families while preserving the utility of
text generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal <span class="highlight-title">Transformer</span> With a Low-Computational-Cost Guarantee <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15096v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15096v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sungjin Park, Edward Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based models have significantly improved performance across a
range of multimodal understanding tasks, such as visual question answering and
action recognition. However, multimodal Transformers significantly suffer from
a quadratic complexity of the multi-head attention with the input sequence
length, especially as the number of modalities increases. To address this, we
introduce Low-Cost Multimodal Transformer (LoCoMT), a novel multimodal
attention mechanism that aims to reduce computational cost during training and
inference with minimal performance loss. Specifically, by assigning different
multimodal attention patterns to each attention head, LoCoMT can flexibly
control multimodal signals and theoretically ensures a reduced computational
cost compared to existing multimodal Transformer variants. Experimental results
on two multimodal datasets, namely Audioset and MedVidCL demonstrate that
LoCoMT not only reduces GFLOPs but also matches or even outperforms established
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2024 (5 pages)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DanceAnyWay: Synthesizing Beat-Guided 3D Dances with Randomized Temporal
  Contrastive Learning <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03870v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03870v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aneesh Bhattacharya, Manas Paranjape, Uttaran Bhattacharya, Aniket Bera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present DanceAnyWay, a generative learning method to synthesize
beat-guided dances of 3D human characters synchronized with music. Our method
learns to disentangle the dance movements at the beat frames from the dance
movements at all the remaining frames by operating at two hierarchical levels.
At the coarser "beat" level, it encodes the rhythm, pitch, and melody
information of the input music via dedicated feature representations only at
the beat frames. It leverages them to synthesize the beat poses of the target
dances using a sequence-to-sequence learning framework. At the finer
"repletion" level, our method encodes similar rhythm, pitch, and melody
information from all the frames of the input music via dedicated feature
representations. It generates the full dance sequences by combining the
synthesized beat and repletion poses and enforcing plausibility through an
adversarial learning framework. Our training paradigm also enforces
fine-grained diversity in the synthesized dances through a randomized temporal
contrastive loss, which ensures different segments of the dance sequences have
different movements and avoids motion freezing or collapsing to repetitive
movements. We evaluate the performance of our approach through extensive
experiments on the benchmark AIST++ dataset and observe improvements of about
7%-12% in motion quality metrics and 1.5%-4% in motion diversity metrics over
the current baselines, respectively. We also conducted a user study to evaluate
the visual quality of our synthesized dances. We note that, on average, the
samples generated by our method were about 9-48% more preferred by the
participants and had a 4-27% better five-point Likert-scale score over the best
available current baseline in terms of motion quality and synchronization. Our
source code and project page are available at
https://github.com/aneeshbhattacharya/DanceAnyWay.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures, 3 tables. To appear as part of the proceedings
  of the 38th Annual AAAI Conference on Artificial Intelligence, 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-02-22T00:00:00Z">2024-02-22</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">21</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Filter Bubble or Homogenization? Disentangling the Long-Term Effects of
  Recommendations on User Consumption Patterns <span class="chip">WWW '24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15013v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15013v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Sanzeed Anwar, Grant Schoenebeck, Paramveer S. Dhillon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommendation algorithms play a pivotal role in shaping our media choices,
which makes it crucial to comprehend their long-term impact on user behavior.
These algorithms are often linked to two critical outcomes: homogenization,
wherein users consume similar content despite disparate underlying preferences,
and the filter bubble effect, wherein individuals with differing preferences
only consume content aligned with their preferences (without much overlap with
other users). Prior research assumes a trade-off between homogenization and
filter bubble effects and then shows that personalized recommendations mitigate
filter bubbles by fostering homogenization. However, because of this assumption
of a tradeoff between these two effects, prior work cannot develop a more
nuanced view of how recommendation systems may independently impact
homogenization and filter bubble effects. We develop a more refined definition
of homogenization and the filter bubble effect by decomposing them into two key
metrics: how different the average consumption is between users (inter-user
diversity) and how varied an individual's consumption is (intra-user
diversity). We then use a novel agent-based simulation framework that enables a
holistic view of the impact of recommendation systems on homogenization and
filter bubble effects. Our simulations show that traditional recommendation
algorithms (based on past behavior) mainly reduce filter bubbles by affecting
inter-user diversity without significantly impacting intra-user diversity.
Building on these findings, we introduce two new recommendation algorithms that
take a more nuanced approach by accounting for both types of diversity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was accepted at the ACM Web Conference 2024 (WWW '24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Link Prediction under Heterophily: A Physics-Inspired Graph Neural
  Network Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Giuseppe Di Francesco, Francesco Caso, Maria Sofia Bucarelli, Fabrizio Silvestri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the past years, Graph Neural Networks (GNNs) have become the `de facto'
standard in various deep learning domains, thanks to their flexibility in
modeling real-world phenomena represented as graphs. However, the
message-passing mechanism of GNNs faces challenges in learnability and
expressivity, hindering high performance on heterophilic graphs, where adjacent
nodes frequently have different labels. Most existing solutions addressing
these challenges are primarily confined to specific benchmarks focused on node
classification tasks. This narrow focus restricts the potential impact that
link prediction under heterophily could offer in several applications,
including recommender systems. For example, in social networks, two users may
be connected for some latent reason, making it challenging to predict such
connections in advance. Physics-Inspired GNNs such as GRAFF provided a
significant contribution to enhance node classification performance under
heterophily, thanks to the adoption of physics biases in the message-passing.
Drawing inspiration from these findings, we advocate that the methodology
employed by GRAFF can improve link prediction performance as well. To further
explore this hypothesis, we introduce GRAFF-LP, an extension of GRAFF to link
prediction. We evaluate its efficacy within a recent collection of heterophilic
graphs, establishing a new benchmark for link prediction under heterophily. Our
approach surpasses previous methods, in most of the datasets, showcasing a
strong flexibility in different contexts, and achieving relative AUROC
improvements of up to 26.7%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IEPile: Unearthing Large-Scale Schema-Based Information Extraction
  Corpus 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14710v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14710v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Honghao Gui, Hongbin Ye, Lin Yuan, Ningyu Zhang, Mengshu Sun, Lei Liang, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) demonstrate remarkable potential across various
domains; however, they exhibit a significant performance gap in Information
Extraction (IE). Note that high-quality instruction data is the vital key for
enhancing the specific capabilities of LLMs, while current IE datasets tend to
be small in scale, fragmented, and lack standardized schema. To this end, we
introduce IEPile, a comprehensive bilingual (English and Chinese) IE
instruction corpus, which contains approximately 0.32B tokens. We construct
IEPile by collecting and cleaning 33 existing IE datasets, and introduce
schema-based instruction generation to unearth a large-scale corpus.
Experimental results on LLaMA and Baichuan demonstrate that using IEPile can
enhance the performance of LLMs for IE, especially the zero-shot
generalization. We open-source the resource and pre-trained models, hoping to
provide valuable support to the NLP community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing work; 18 pages; Github: https://github.com/zjunlp/IEPile</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Keywords to Structured Summaries: Streamlining Scholarly Knowledge
  Access 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14622v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14622v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahsa Shamsabadi, Jennifer D'Souza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This short paper highlights the growing importance of information retrieval
(IR) engines in the scientific community, addressing the inefficiency of
traditional keyword-based search engines due to the rising volume of
publications. The proposed solution involves structured records, underpinning
advanced information technology (IT) tools, including visualization dashboards,
to revolutionize how researchers access and filter articles, replacing the
traditional text-heavy approach. This vision is exemplified through a proof of
concept centered on the ``reproductive number estimate of infectious diseases''
research theme, using a fine-tuned large language model (LLM) to automate the
creation of structured records to populate a backend database that now goes
beyond keywords. The result is a next-generation IR method accessible at
https://orkg.org/usecases/r0-estimates.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personalized Behavior-Aware <span class="highlight-title">Transformer</span> for Multi-Behavior Sequential
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14473v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14473v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajie Su, Chaochao Chen, Zibin Lin, Xi Li, Weiming Liu, Xiaolin Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential Recommendation (SR) captures users' dynamic preferences by
modeling how users transit among items. However, SR models that utilize only
single type of behavior interaction data encounter performance degradation when
the sequences are short. To tackle this problem, we focus on Multi-Behavior
Sequential Recommendation (MBSR) in this paper, which aims to leverage
time-evolving heterogeneous behavioral dependencies for better exploring users'
potential intents on the target behavior. Solving MBSR is challenging. On the
one hand, users exhibit diverse multi-behavior patterns due to personal
characteristics. On the other hand, there exists comprehensive co-influence
between behavior correlations and item collaborations, the intensity of which
is deeply affected by temporal factors. To tackle these challenges, we propose
a Personalized Behavior-Aware Transformer framework (PBAT) for MBSR problem,
which models personalized patterns and multifaceted sequential collaborations
in a novel way to boost recommendation performance. First, PBAT develops a
personalized behavior pattern generator in the representation layer, which
extracts dynamic and discriminative behavior patterns for sequential learning.
Second, PBAT reforms the self-attention layer with a behavior-aware
collaboration extractor, which introduces a fused behavior-aware attention
mechanism for incorporating both behavioral and temporal impacts into
collaborative transitions. We conduct experiments on three benchmark datasets
and the results demonstrate the effectiveness and interpretability of our
framework. Our implementation code is released at
https://github.com/TiliaceaeSU/PBAT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recommender for Its Purpose: Repeat and Exploration in Food Delivery
  Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14440v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14440v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayu Li, Aixin Sun, Weizhi Ma, Peijie Sun, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems have been widely used for various scenarios, such as
e-commerce, news, and music, providing online contents to help and enrich
users' daily life. Different scenarios hold distinct and unique
characteristics, calling for domain-specific investigations and corresponding
designed recommender systems. Therefore, in this paper, we focus on food
delivery recommendations to unveil unique features in this domain, where users
order food online and enjoy their meals shortly after delivery. We first
conduct an in-depth analysis on food delivery datasets. The analysis shows that
repeat orders are prevalent for both users and stores, and situations'
differently influence repeat and exploration consumption in the food delivery
recommender systems. Moreover, we revisit the ability of existing
situation-aware methods for repeat and exploration recommendations
respectively, and find them unable to effectively solve both tasks
simultaneously. Based on the analysis and experiments, we have designed two
separate recommendation models -- ReRec for repeat orders and ExpRec for
exploration orders; both are simple in their design and computation. We conduct
experiments on three real-world food delivery datasets, and our proposed models
outperform various types of baselines on repeat, exploration, and combined
recommendation tasks. This paper emphasizes the importance of dedicated
analyses and methods for domain-specific characteristics for the recommender
system studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tug-of-War Between Knowledge: Exploring and Resolving Knowledge
  Conflicts in Retrieval-Augmented Language Models <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoran Jin, Pengfei Cao, Yubo Chen, Kang Liu, Xiaojian Jiang, Jiexin Xu, Qiuxia Li, Jun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented language models (RALMs) have demonstrated significant
potential in refining and expanding their internal memory by retrieving
evidence from external sources. However, RALMs will inevitably encounter
knowledge conflicts when integrating their internal memory with external
sources. Knowledge conflicts can ensnare RALMs in a tug-of-war between
knowledge, limiting their practical applicability. In this paper, we focus on
exploring and resolving knowledge conflicts in RALMs. First, we present an
evaluation framework for assessing knowledge conflicts across various
dimensions. Then, we investigate the behavior and preference of RALMs from the
following two perspectives: (1) Conflicts between internal memory and external
sources: We find that stronger RALMs emerge with the Dunning-Kruger effect,
persistently favoring their faulty internal memory even when correct evidence
is provided. Besides, RALMs exhibit an availability bias towards common
knowledge; (2) Conflicts between truthful, irrelevant and misleading evidence:
We reveal that RALMs follow the principle of majority rule, leaning towards
placing trust in evidence that appears more frequently. Moreover, we find that
RALMs exhibit confirmation bias, and are more willing to choose evidence that
is consistent with their internal memory. To solve the challenge of knowledge
conflicts, we propose a method called Conflict-Disentangle Contrastive Decoding
(CD2) to better calibrate the model's confidence. Experimental results
demonstrate that our CD2 can effectively resolve knowledge conflicts in RALMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ensure Timeliness and Accuracy: A Novel Sliding Window Data Stream
  Paradigm for Live Streaming Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14399v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14399v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengqi Liang, Baigong Zheng, Liqin Zhao, Guorui Zhou, Qian Wang, Yanan Niu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Live streaming recommender system is specifically designed to recommend
real-time live streaming of interest to users. Due to the dynamic changes of
live content, improving the timeliness of the live streaming recommender system
is a critical problem. Intuitively, the timeliness of the data determines the
upper bound of the timeliness that models can learn. However, none of the
previous works addresses the timeliness problem of the live streaming
recommender system from the perspective of data stream design. Employing the
conventional fixed window data stream paradigm introduces a trade-off dilemma
between labeling accuracy and timeliness. In this paper, we propose a new data
stream design paradigm, dubbed Sliver, that addresses the timeliness and
accuracy problem of labels by reducing the window size and implementing a
sliding window correspondingly. Meanwhile, we propose a time-sensitive re-reco
strategy reducing the latency between request and impression to improve the
timeliness of the recommendation service and features by periodically
requesting the recommendation service. To demonstrate the effectiveness of our
approach, we conduct offline experiments on a multi-task live streaming dataset
with labeling timestamps collected from the Kuaishou live streaming platform.
Experimental results demonstrate that Sliver outperforms two fixed-window data
streams with varying window sizes across all targets in four typical multi-task
recommendation models. Furthermore, we deployed Sliver on the Kuaishou live
streaming platform. Results of the online A/B test show a significant
improvement in click-through rate (CTR), and new follow number (NFN), further
validating the effectiveness of Sliver.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable and Provably Fair Exposure Control for Large-Scale Recommender
  Systems <span class="chip">WWW2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14369v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14369v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riku Togashi, Kenshi Abe, Yuta Saito
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Typical recommendation and ranking methods aim to optimize the satisfaction
of users, but they are often oblivious to their impact on the items (e.g.,
products, jobs, news, video) and their providers. However, there has been a
growing understanding that the latter is crucial to consider for a wide range
of applications, since it determines the utility of those being recommended.
Prior approaches to fairness-aware recommendation optimize a regularized
objective to balance user satisfaction and item fairness based on some notion
such as exposure fairness. These existing methods have been shown to be
effective in controlling fairness, however, most of them are computationally
inefficient, limiting their applications to only unrealistically small-scale
situations. This indeed implies that the literature does not yet provide a
solution to enable a flexible control of exposure in the industry-scale
recommender systems where millions of users and items exist. To enable a
computationally efficient exposure control even for such large-scale systems,
this work develops a scalable, fast, and fair method called
\emph{\textbf{ex}posure-aware \textbf{ADMM} (\textbf{exADMM})}. exADMM is based
on implicit alternating least squares (iALS), a conventional scalable algorithm
for collaborative filtering, but optimizes a regularized objective to achieve a
flexible control of accuracy-fairness tradeoff. A particular technical
challenge in developing exADMM is the fact that the fairness regularizer
destroys the separability of optimization subproblems for users and items,
which is an essential property to ensure the scalability of iALS. Therefore, we
develop a set of optimization tools to enable yet scalable fairness control
with provable convergence guarantees as a basis of our algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at WWW2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Efficient Pareto-optimal Utility-Fairness between Groups in
  Repeated Rankings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phuong Dinh Mai, Duc-Trong Le, Tuan-Anh Hoang, Dung D. Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we tackle the problem of computing a sequence of rankings with
the guarantee of the Pareto-optimal balance between (1) maximizing the utility
of the consumers and (2) minimizing unfairness between producers of the items.
Such a multi-objective optimization problem is typically solved using a
combination of a scalarization method and linear programming on bi-stochastic
matrices, representing the distribution of possible rankings of items. However,
the above-mentioned approach relies on Birkhoff-von Neumann (BvN)
decomposition, of which the computational complexity is $\mathcal{O}(n^5)$ with
$n$ being the number of items, making it impractical for large-scale systems.
To address this drawback, we introduce a novel approach to the above problem by
using the Expohedron - a permutahedron whose points represent all achievable
exposures of items. On the Expohedron, we profile the Pareto curve which
captures the trade-off between group fairness and user utility by identifying a
finite number of Pareto optimal solutions. We further propose an efficient
method by relaxing our optimization problem on the Expohedron's circumscribed
$n$-sphere, which significantly improve the running time. Moreover, the
approximate Pareto curve is asymptotically close to the real Pareto optimal
curve as the number of substantial solutions increases. Our methods are
applicable with different ranking merits that are non-decreasing functions of
item relevance. The effectiveness of our methods are validated through
experiments on both synthetic and real-world datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GenSERP: Large Language Models for Whole Page Presentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenning Zhang, Yunan Zhang, Suyu Ge, Guangwei Weng, Mridu Narang, Xia Song, Saurabh Tiwary
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of large language models (LLMs) brings an opportunity to minimize
the effort in search engine result page (SERP) organization. In this paper, we
propose GenSERP, a framework that leverages LLMs with vision in a few-shot
setting to dynamically organize intermediate search results, including
generated chat answers, website snippets, multimedia data, knowledge panels
into a coherent SERP layout based on a user's query. Our approach has three
main stages: (1) An information gathering phase where the LLM continuously
orchestrates API tools to retrieve different types of items, and proposes
candidate layouts based on the retrieved items, until it's confident enough to
generate the final result. (2) An answer generation phase where the LLM
populates the layouts with the retrieved content. In this phase, the LLM
adaptively optimize the ranking of items and UX configurations of the SERP.
Consequently, it assigns a location on the page to each item, along with the UX
display details. (3) A scoring phase where an LLM with vision scores all the
generated SERPs based on how likely it can satisfy the user. It then send the
one with highest score to rendering. GenSERP features two generation paradigms.
First, coarse-to-fine, which allow it to approach optimal layout in a more
manageable way, (2) beam search, which give it a better chance to hit the
optimal solution compared to greedy decoding. Offline experimental results on
real-world data demonstrate how LLMs can contextually organize heterogeneous
search results on-the-fly and provide a promising user experience.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MerRec: A Large-scale Multipurpose Mercari <span class="highlight-title">Dataset</span> for
  Consumer-to-Consumer Recommendation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14230v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14230v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lichi Li, Zainul Abi Din, Zhen Tan, Sam London, Tianlong Chen, Ajay Daptardar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the evolving e-commerce field, recommendation systems crucially shape user
experience and engagement. The rise of Consumer-to-Consumer (C2C)
recommendation systems, noted for their flexibility and ease of access for
customer vendors, marks a significant trend. However, the academic focus
remains largely on Business-to-Consumer (B2C) models, leaving a gap filled by
the limited C2C recommendation datasets that lack in item attributes, user
diversity, and scale. The intricacy of C2C recommendation systems is further
accentuated by the dual roles users assume as both sellers and buyers,
introducing a spectrum of less uniform and varied inputs. Addressing this, we
introduce MerRec, the first large-scale dataset specifically for C2C
recommendations, sourced from the Mercari e-commerce platform, covering
millions of users and products over 6 months in 2023. MerRec not only includes
standard features such as user_id, item_id, and session_id, but also unique
elements like timestamped action types, product taxonomy, and textual product
attributes, offering a comprehensive dataset for research. This dataset,
extensively evaluated across six recommendation tasks, establishes a new
benchmark for the development of advanced recommendation algorithms in
real-world scenarios, bridging the gap between academia and industry and
propelling the study of C2C recommendations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TREC iKAT 2023: The Interactive Knowledge Assistance Track <span class="highlight-title">Overview</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01330v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01330v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Aliannejadi, Zahra Abbasiantaeb, Shubham Chatterjee, Jeffery Dalton, Leif Azzopardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational Information Seeking has evolved rapidly in the last few years
with the development of Large Language Models providing the basis for
interpreting and responding in a naturalistic manner to user requests. iKAT
emphasizes the creation and research of conversational search agents that adapt
responses based on the user's prior interactions and present context. This
means that the same question might yield varied answers, contingent on the
user's profile and preferences. The challenge lies in enabling Conversational
Search Agents (CSA) to incorporate personalized context to effectively guide
users through the relevant information to them. iKAT's first year attracted
seven teams and a total of 24 runs. Most of the runs leveraged Large Language
Models (LLMs) in their pipelines, with a few focusing on a
generate-then-retrieve approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TREC iKAT 2023 Overview Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MuGI: Enhancing Information Retrieval through Multi-Text Generation
  Integration with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06311v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06311v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Le Zhang, Qian Yang, Yihong Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have emerged as a pivotal force in language
technology. Their robust reasoning capabilities and expansive knowledge
repositories have enabled exceptional zero-shot generalization abilities across
various facets of the natural language processing field, including information
retrieval (IR). In this paper, we conduct an in-depth investigation into the
utility of documents generated by LLMs for IR. We introduce a simple yet
effective framework, Multi-Text Generation Integration (MuGI), to augment
existing IR methodologies. Specifically, we prompt LLMs to generate multiple
pseudo references and integrate with query for retrieval. The training-free
MuGI model eclipses existing query expansion strategies, setting a new standard
in sparse retrieval. It outstrips supervised counterparts like ANCE and DPR,
achieving a notable over 18% enhancement in BM25 on the TREC DL dataset and a
7.5% increase on BEIR. Through MuGI, we have forged a rapid and high-fidelity
re-ranking pipeline. This allows a relatively small 110M parameter retriever to
surpass the performance of larger 3B models in in-domain evaluations, while
also bridging the gap in out-of-distribution situations. We release our code
and all generated references at https://github.com/lezhang7/Retrieval_MuGI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Silver Retriever: Advancing Neural Passage Retrieval for Polish Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08469v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08469v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piotr Rybak, Maciej Ogrodniczuk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern open-domain question answering systems often rely on accurate and
efficient retrieval components to find passages containing the facts necessary
to answer the question. Recently, neural retrievers have gained popularity over
lexical alternatives due to their superior performance. However, most of the
work concerns popular languages such as English or Chinese. For others, such as
Polish, few models are available. In this work, we present Silver Retriever, a
neural retriever for Polish trained on a diverse collection of manually or
weakly labeled datasets. Silver Retriever achieves much better results than
other Polish models and is competitive with larger multilingual models.
Together with the model, we open-source five new passage retrieval datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Trustworthy Reranking: A Simple yet Effective Abstention
  Mechanism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12997v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12997v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hippolyte Gisserot-Boukhlef, Manuel Faysse, Emmanuel Malherbe, Céline Hudelot, Pierre Colombo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Information Retrieval (NIR) has significantly improved upon
heuristic-based IR systems. Yet, failures remain frequent, the models used
often being unable to retrieve documents relevant to the user's query. We
address this challenge by proposing a lightweight abstention mechanism tailored
for real-world constraints, with particular emphasis placed on the reranking
phase. We introduce a protocol for evaluating abstention strategies in a
black-box scenario, demonstrating their efficacy, and propose a simple yet
effective data-driven mechanism. We provide open-source code for experiment
replication and abstention implementation, fostering wider adoption and
application in diverse contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities
  and Future Opportunities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.13168v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.13168v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqi Zhu, Xiaohan Wang, Jing Chen, Shuofei Qiao, Yixin Ou, Yunzhi Yao, Shumin Deng, Huajun Chen, Ningyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an exhaustive quantitative and qualitative evaluation of
Large Language Models (LLMs) for Knowledge Graph (KG) construction and
reasoning. We engage in experiments across eight diverse datasets, focusing on
four representative tasks encompassing entity and relation extraction, event
extraction, link prediction, and question-answering, thereby thoroughly
exploring LLMs' performance in the domain of construction and inference.
Empirically, our findings suggest that LLMs, represented by GPT-4, are more
suited as inference assistants rather than few-shot information extractors.
Specifically, while GPT-4 exhibits good performance in tasks related to KG
construction, it excels further in reasoning tasks, surpassing fine-tuned
models in certain cases. Moreover, our investigation extends to the potential
generalization ability of LLMs for information extraction, leading to the
proposition of a Virtual Knowledge Extraction task and the development of the
corresponding VINE dataset. Based on these empirical findings, we further
propose AutoKG, a multi-agent-based approach employing LLMs and external
sources for KG construction and reasoning. We anticipate that this research can
provide invaluable insights for future undertakings in the field of knowledge
graphs. The code and datasets are in https://github.com/zjunlp/AutoKG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Text Embeddings by Weakly-Supervised Contrastive <span class="highlight-title">Pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.03533v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.03533v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents E5, a family of state-of-the-art text embeddings that
transfer well to a wide range of tasks. The model is trained in a contrastive
manner with weak supervision signals from our curated large-scale text pair
dataset (called CCPairs). E5 can be readily used as a general-purpose embedding
model for any tasks requiring a single-vector representation of texts such as
retrieval, clustering, and classification, achieving strong performance in both
zero-shot and fine-tuned settings. We conduct extensive evaluations on 56
datasets from the BEIR and MTEB benchmarks. For zero-shot settings, E5 is the
first model that outperforms the strong BM25 baseline on the BEIR retrieval
benchmark without using any labeled data. When fine-tuned, E5 obtains the best
results on the MTEB benchmark, beating existing embedding models with 40x more
parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, v2 fixes the SummEval numbers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph-enhanced Optimizers for Structure-aware Recommendation Embedding
  Evolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03032v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03032v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cong Xu, Jun Wang, Jianyong Wang, Wei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embedding plays a critical role in modern recommender systems because they
are virtual representations of real-world entities and the foundation for
subsequent decision models. In this paper, we propose a novel embedding update
mechanism, Structure-aware Embedding Evolution (SEvo for short), to encourage
related nodes to evolve similarly at each step. Unlike GNN (Graph Neural
Network) that typically serves as an intermediate part, SEvo is able to
directly inject the graph structure information into embedding with negligible
computational overhead in training. The convergence properties of SEvo as well
as its possible variants are theoretically analyzed to justify the validity of
the designs. Moreover, SEvo can be seamlessly integrated into existing
optimizers for state-of-the-art performance. In particular, SEvo-enhanced AdamW
with moment estimate correction demonstrates consistent improvements across a
spectrum of models and datasets, suggesting a novel technical route to
effectively utilize graph structure information beyond explicit GNN modules.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding the Role of Cross-Entropy Loss in Fairly Evaluating Large
  Language Model-based Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.06216v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.06216v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cong Xu, Zhangchi Zhu, Jun Wang, Jianyong Wang, Wei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have gained much attention in the recommendation
community; some studies have observed that LLMs, fine-tuned by the
cross-entropy loss with a full softmax, could achieve state-of-the-art
performance already. However, these claims are drawn from unobjective and
unfair comparisons. In view of the substantial quantity of items in reality,
conventional recommenders typically adopt a pointwise/pairwise loss function
instead for training. This substitute however causes severe performance
degradation, leading to under-estimation of conventional methods and
over-confidence in the ranking capability of LLMs.
  In this work, we theoretically justify the superiority of cross-entropy, and
showcase that it can be adequately replaced by some elementary approximations
with certain necessary modifications. The remarkable results across three
public datasets corroborate that even in a practical sense, existing LLM-based
methods are not as effective as claimed for next-item recommendation. We hope
that these theoretical understandings in conjunction with the empirical results
will facilitate an objective evaluation of LLM-based recommendation in the
future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Trustworthy Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.12515v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.12515v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingqiang Ge, Shuchang Liu, Zuohui Fu, Juntao Tan, Zelong Li, Shuyuan Xu, Yunqi Li, Yikun Xian, Yongfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems (RS), serving at the forefront of Human-centered AI, are
widely deployed in almost every corner of the web and facilitate the human
decision-making process. However, despite their enormous capabilities and
potential, RS may also lead to undesired effects on users, items, producers,
platforms, or even the society at large, such as compromised user trust due to
non-transparency, unfair treatment of different consumers, or producers,
privacy concerns due to extensive use of user's private data for
personalization, just to name a few. All of these create an urgent need for
Trustworthy Recommender Systems (TRS) so as to mitigate or avoid such adverse
impacts and risks. In this survey, we will introduce techniques related to
trustworthy recommendation, including but not limited to explainable
recommendation, fairness in recommendation, privacy-aware recommendation,
robustness in recommendation, user-controllable recommendation, as well as the
relationship between these different perspectives in terms of trustworthy
recommendation. Through this survey, we hope to deliver readers with a
comprehensive view of the research area and raise attention to the community
about the importance, existing research achievements, and future research
directions on trustworthy recommendation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM Transactions on Recommender Systems (TORS)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Avalanche of Images on Telegram Preceded Russia's Full-Scale Invasion
  of Ukraine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14947v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14947v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Theisen, Michael Yankoski, Kristina Hook, Ernesto Verdeja, Walter Scheirer, Tim Weninger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Governments use propaganda, including through visual content -- or
Politically Salient Image Patterns (PSIP) -- on social media, to influence and
manipulate public opinion. In the present work, we collected Telegram
post-history of from 989 Russian milbloggers to better understand the social
and political narratives that circulated online in the months surrounding
Russia's 2022 full-scale invasion of Ukraine. Overall, we found an 8,925%
increase (p<0.001) in the number of posts and a 5,352% increase (p<0.001) in
the number of images posted by these accounts in the two weeks prior to the
invasion. We also observed a similar increase in the number and intensity of
politically salient manipulated images that circulated on Telegram. Although
this paper does not evaluate malice or coordination in these activities, we do
conclude with a call for further research into the role that manipulated visual
media has in the lead-up to instability events and armed conflict.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Think before You Leap: Content-Aware Low-Cost Edge-Assisted Video
  Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingxuan Yan, Yi Wang, Xuedou Xiao, Zhiqing Luo, Jianhua He, Wei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offloading computing to edge servers is a promising solution to support
growing video understanding applications at resource-constrained IoT devices.
Recent efforts have been made to enhance the scalability of such systems by
reducing inference costs on edge servers. However, existing research is not
directly applicable to pixel-level vision tasks such as video semantic
segmentation (VSS), partly due to the fluctuating VSS accuracy and segment
bitrate caused by the dynamic video content. In response, we present Penance, a
new edge inference cost reduction framework. By exploiting softmax outputs of
VSS models and the prediction mechanism of H.264/AVC codecs, Penance optimizes
model selection and compression settings to minimize the inference cost while
meeting the required accuracy within the available bandwidth constraints. We
implement Penance in a commercial IoT device with only CPUs. Experimental
results show that Penance consumes a negligible 6.8% more computation resources
than the optimal strategy while satisfying accuracy and bandwidth constraints
with a low failure rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM Multimedia 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unleashing the Power of Imbalanced Modality Information for Multi-modal
  Knowledge Graph Completion <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichi Zhang, Zhuo Chen, Lei Liang, Huajun Chen, Wen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal knowledge graph completion (MMKGC) aims to predict the missing
triples in the multi-modal knowledge graphs by incorporating structural,
visual, and textual information of entities into the discriminant models. The
information from different modalities will work together to measure the triple
plausibility. Existing MMKGC methods overlook the imbalance problem of modality
information among entities, resulting in inadequate modal fusion and
inefficient utilization of the raw modality information. To address the
mentioned problems, we propose Adaptive Multi-modal Fusion and Modality
Adversarial Training (AdaMF-MAT) to unleash the power of imbalanced modality
information for MMKGC. AdaMF-MAT achieves multi-modal fusion with adaptive
modality weights and further generates adversarial samples by
modality-adversarial training to enhance the imbalanced modality information.
Our approach is a co-design of the MMKGC model and training strategy which can
outperform 19 recent MMKGC methods and achieve new state-of-the-art results on
three public MMKGC benchmarks. Our code and data have been released at
https://github.com/zjukg/AdaMF-MAT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Modal Discussion <span class="highlight-title">Transformer</span>: Integrating Text, Images and Graph
  <span class="highlight-title">Transformer</span>s to Detect Hate Speech on Social Media <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09312v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09312v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liam Hebert, Gaurav Sahu, Yuxuan Guo, Nanda Kishore Sreenivas, Lukasz Golab, Robin Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the Multi-Modal Discussion Transformer (mDT), a novel methodfor
detecting hate speech in online social networks such as Reddit discussions. In
contrast to traditional comment-only methods, our approach to labelling a
comment as hate speech involves a holistic analysis of text and images grounded
in the discussion context. This is done by leveraging graph transformers to
capture the contextual relationships in the discussion surrounding a comment
and grounding the interwoven fusion layers that combine text and image
embeddings instead of processing modalities separately. To evaluate our work,
we present a new dataset, HatefulDiscussions, comprising complete multi-modal
discussions from multiple online communities on Reddit. We compare the
performance of our model to baselines that only process individual comments and
conduct extensive ablation studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2024 (AI for Social Impact Track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ V2Meow: Meowing to the Visual Beat via Video-to-Music Generation <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.06594v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.06594v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Su, Judith Yue Li, Qingqing Huang, Dima Kuzmin, Joonseok Lee, Chris Donahue, Fei Sha, Aren Jansen, Yu Wang, Mauro Verzetti, Timo I. Denk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video-to-music generation demands both a temporally localized high-quality
listening experience and globally aligned video-acoustic signatures. While
recent music generation models excel at the former through advanced audio
codecs, the exploration of video-acoustic signatures has been confined to
specific visual scenarios. In contrast, our research confronts the challenge of
learning globally aligned signatures between video and music directly from
paired music and videos, without explicitly modeling domain-specific rhythmic
or semantic relationships. We propose V2Meow, a video-to-music generation
system capable of producing high-quality music audio for a diverse range of
video input types using a multi-stage autoregressive model. Trained on 5k hours
of music audio clips paired with video frames mined from in-the-wild music
videos, V2Meow is competitive with previous domain-specific models when
evaluated in a zero-shot manner. It synthesizes high-fidelity music audio
waveforms solely by conditioning on pre-trained general-purpose visual features
extracted from video frames, with optional style control via text prompts.
Through both qualitative and quantitative evaluations, we demonstrate that our
model outperforms various existing music generation systems in terms of
visual-audio correspondence and audio quality. Music samples are available at
tinyurl.com/v2meow.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at AAAI 2024, music samples available at
  https://tinyurl.com/v2meow</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-02-21T00:00:00Z">2024-02-21</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">25</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BIRCO: A Benchmark of Information Retrieval Tasks with Complex
  Objectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyue Wang, Jianyou Wang, Weili Cao, Kaicheng Wang, Ramamohan Paturi, Leon Bergen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the Benchmark of Information Retrieval (IR) tasks with Complex
Objectives (BIRCO). BIRCO evaluates the ability of IR systems to retrieve
documents given multi-faceted user objectives. The benchmark's complexity and
compact size make it suitable for evaluating large language model (LLM)-based
information retrieval systems. We present a modular framework for investigating
factors that may influence LLM performance on retrieval tasks, and identify a
simple baseline model which matches or outperforms existing approaches and more
complex alternatives. No approach achieves satisfactory performance on all
benchmark tasks, suggesting that stronger models and new retrieval protocols
are necessary to address complex user needs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Combining Language and Graph Models for Semi-structured Information
  Extraction on the Web 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhi Hong, Kyle Chard, Ian Foster
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relation extraction is an efficient way of mining the extraordinary wealth of
human knowledge on the Web. Existing methods rely on domain-specific training
data or produce noisy outputs. We focus here on extracting targeted relations
from semi-structured web pages given only a short description of the relation.
We present GraphScholarBERT, an open-domain information extraction method based
on a joint graph and language model structure. GraphScholarBERT can generalize
to previously unseen domains without additional data or training and produces
only clean extraction results matched to the search keyword. Experiments show
that GraphScholarBERT can improve extraction F1 scores by as much as 34.8\%
compared to previous work in a zero-shot domain and zero-shot website setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Linear-Time Graph Neural Networks for Scalable Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Zhang, Rui Xue, Wenqi Fan, Xin Xu, Qing Li, Jian Pei, Xiaorui Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In an era of information explosion, recommender systems are vital tools to
deliver personalized recommendations for users. The key of recommender systems
is to forecast users' future behaviors based on previous user-item
interactions. Due to their strong expressive power of capturing high-order
connectivities in user-item interaction data, recent years have witnessed a
rising interest in leveraging Graph Neural Networks (GNNs) to boost the
prediction performance of recommender systems. Nonetheless, classic Matrix
Factorization (MF) and Deep Neural Network (DNN) approaches still play an
important role in real-world large-scale recommender systems due to their
scalability advantages. Despite the existence of GNN-acceleration solutions, it
remains an open question whether GNN-based recommender systems can scale as
efficiently as classic MF and DNN methods. In this paper, we propose a
Linear-Time Graph Neural Network (LTGNN) to scale up GNN-based recommender
systems to achieve comparable scalability as classic MF approaches while
maintaining GNNs' powerful expressiveness for superior prediction accuracy.
Extensive experiments and ablation studies are presented to validate the
effectiveness and scalability of the proposed algorithm. Our implementation
based on PyTorch is available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures, accepted by The Web Conference 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retention Induced Biases in a Recommendation System with Heterogeneous
  Users 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13959v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13959v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shichao Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  I examine a conceptual model of a recommendation system (RS) with user inflow
and churn dynamics. When inflow and churn balance out, the user distribution
reaches a steady state. Changing the recommendation algorithm alters the steady
state and creates a transition period. During this period, the RS behaves
differently from its new steady state. In particular, A/B experiment metrics
obtained in transition periods are biased indicators of the RS's long term
performance. Scholars and practitioners, however, often conduct A/B tests
shortly after introducing new algorithms to validate their effectiveness. This
A/B experiment paradigm, widely regarded as the gold standard for assessing RS
improvements, may consequently yield false conclusions. I also briefly discuss
the data bias caused by the user retention dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Science Checker Reloaded: A Bidirectional Paradigm for Transparency and
  Logical Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13897v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13897v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Loïc Rakotoson, Sylvain Massip, Fréjus A. A. Laleye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information retrieval is a rapidly evolving field. However it still faces
significant limitations in the scientific and industrial vast amounts of
information, such as semantic divergence and vocabulary gaps in sparse
retrieval, low precision and lack of interpretability in semantic search, or
hallucination and outdated information in generative models. In this paper, we
introduce a two-block approach to tackle these hurdles for long documents. The
first block enhances language understanding in sparse retrieval by query
expansion to retrieve relevant documents. The second block deepens the result
by providing comprehensive and informative answers to the complex question
using only the information spread in the long document, enabling bidirectional
engagement. At various stages of the pipeline, intermediate results are
presented to users to facilitate understanding of the system's reasoning. We
believe this bidirectional approach brings significant advancements in terms of
transparency, logical thinking, and comprehensive understanding in the field of
scientific information retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diversity-Aware $k$-Maximum Inner Product Search Revisited 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13858v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13858v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiang Huang, Yanhao Wang, Yiqun Sun, Anthony K. H. Tung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The $k$-Maximum Inner Product Search ($k$MIPS) serves as a foundational
component in recommender systems and various data mining tasks. However, while
most existing $k$MIPS approaches prioritize the efficient retrieval of highly
relevant items for users, they often neglect an equally pivotal facet of search
results: \emph{diversity}. To bridge this gap, we revisit and refine the
diversity-aware $k$MIPS (D$k$MIPS) problem by incorporating two well-known
diversity objectives -- minimizing the average and maximum pairwise item
similarities within the results -- into the original relevance objective. This
enhancement, inspired by Maximal Marginal Relevance (MMR), offers users a
controllable trade-off between relevance and diversity. We introduce
\textsc{Greedy} and \textsc{DualGreedy}, two linear scan-based algorithms
tailored for D$k$MIPS. They both achieve data-dependent approximations and,
when aiming to minimize the average pairwise similarity, \textsc{DualGreedy}
attains an approximation ratio of $1/4$ with an additive term for
regularization. To further improve query efficiency, we integrate a lightweight
Ball-Cone Tree (BC-Tree) index with the two algorithms. Finally, comprehensive
experiments on ten real-world data sets demonstrate the efficacy of our
proposed methods, showcasing their capability to efficiently deliver diverse
and relevant search results to users.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures, and 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM4SBR: A Lightweight and Effective Framework for Integrating Large
  Language Models in Session-based Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13840v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13840v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shutong Qiao, Chen Gao, Junhao Wen, Wei Zhou, Qun Luo, Peixuan Chen, Yong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional session-based recommendation (SBR) utilizes session behavior
sequences from anonymous users for recommendation. Although this strategy is
highly efficient, it sacrifices the inherent semantic information of the items,
making it difficult for the model to understand the true intent of the session
and resulting in a lack of interpretability in the recommended results.
Recently, large language models (LLMs) have flourished across various domains,
offering a glimpse of hope in addressing the aforementioned challenges.
Inspired by the impact of LLMs, research exploring the integration of LLMs with
the Recommender system (RS) has surged like mushrooms after rain. However,
constrained by high time and space costs, as well as the brief and anonymous
nature of session data, the first LLM recommendation framework suitable for
industrial deployment has yet to emerge in the field of SBR. To address the
aforementioned challenges, we have proposed the LLM Integration Framework for
SBR (LLM4SBR). Serving as a lightweight and plug-and-play framework, LLM4SBR
adopts a two-step strategy. Firstly, we transform session data into a bimodal
form of text and behavior. In the first step, leveraging the inferential
capabilities of LLMs, we conduct inference on session text data from different
perspectives and design the component for auxiliary enhancement. In the second
step, the SBR model is trained on behavior data, aligning and averaging two
modal session representations from different perspectives. Finally, we fuse
session representations from different perspectives and modalities as the
ultimate session representation for recommendation. We conducted experiments on
two real-world datasets, and the results demonstrate that LLM4SBR significantly
improves the performance of traditional SBR models and is highly lightweight
and efficient, making it suitable for industrial deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ General Debiasing for Graph-based Collaborative Filtering via
  Adversarial Graph Dropout <span class="chip">WWW 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        An Zhang, Wenchang Ma, Pengbo Wei, Leheng Sheng, Xiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks (GNNs) have shown impressive performance in recommender
systems, particularly in collaborative filtering (CF). The key lies in
aggregating neighborhood information on a user-item interaction graph to
enhance user/item representations. However, we have discovered that this
aggregation mechanism comes with a drawback, which amplifies biases present in
the interaction graph. For instance, a user's interactions with items can be
driven by both unbiased true interest and various biased factors like item
popularity or exposure. However, the current aggregation approach combines all
information, both biased and unbiased, leading to biased representation
learning. Consequently, graph-based recommenders can learn distorted views of
users/items, hindering the modeling of their true preferences and
generalizations. To address this issue, we introduce a novel framework called
Adversarial Graph Dropout (AdvDrop). It differentiates between unbiased and
biased interactions, enabling unbiased representation learning. For each
user/item, AdvDrop employs adversarial learning to split the neighborhood into
two views: one with bias-mitigated interactions and the other with bias-aware
interactions. After view-specific aggregation, AdvDrop ensures that the
bias-mitigated and bias-aware representations remain invariant, shielding them
from the influence of bias. We validate AdvDrop's effectiveness on five public
datasets that cover both general and specific biases, demonstrating significant
improvements. Furthermore, our method exhibits meaningful separation of
subgraphs and achieves unbiased representations for graph-based CF models, as
revealed by in-depth analysis. Our code is publicly available at
https://github.com/Arthurma71/AdvDrop.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to WWW 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Breaking the Barrier: Utilizing Large Language Models for Industrial
  Recommendation Systems through an Inferential Knowledge Graph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13750v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13750v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Zhao, Hao Qian, Ziqi Liu, Gong-Duo Zhang, Lihong Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommendation systems are widely used in e-commerce websites and online
platforms to address information overload. However, existing systems primarily
rely on historical data and user feedback, making it difficult to capture user
intent transitions. Recently, Knowledge Base (KB)-based models are proposed to
incorporate expert knowledge, but it struggle to adapt to new items and the
evolving e-commerce environment. To address these challenges, we propose a
novel Large Language Model based Complementary Knowledge Enhanced
Recommendation System (LLM-KERec). It introduces an entity extractor that
extracts unified concept terms from item and user information. To provide
cost-effective and reliable prior knowledge, entity pairs are generated based
on entity popularity and specific strategies. The large language model
determines complementary relationships in each entity pair, constructing a
complementary knowledge graph. Furthermore, a new complementary recall module
and an Entity-Entity-Item (E-E-I) weight decision model refine the scoring of
the ranking model using real complementary exposure-click samples. Extensive
experiments conducted on three industry datasets demonstrate the significant
performance improvement of our model compared to existing approaches.
Additionally, detailed analysis shows that LLM-KERec enhances users' enthusiasm
for consumption by recommending complementary items. In summary, LLM-KERec
addresses the limitations of traditional recommendation systems by
incorporating complementary knowledge and utilizing a large language model to
capture user intent transitions, adapt to new items, and enhance recommendation
efficiency in the evolving e-commerce landscape.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Video Corpus Moment Retrieval with Partial Relevance
  Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13576v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13576v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danyang Hou, Liang Pang, Huawei Shen, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video corpus moment retrieval~(VCMR) is a new video retrieval task aimed at
retrieving a relevant moment from a large corpus of untrimmed videos using a
natural language text as query. The relevance between the video and query is
partial, mainly evident in two aspects: (1) Scope: The untrimmed video contains
information-rich frames, and not all are relevant to the query. Strong
correlation is typically observed only within the relevant moment, emphasizing
the importance of capturing key content. (2) Modality: The relevance of query
to different modalities varies; action descriptions align more with the visual
elements, while character conversations are more related to textual
information. Recognizing and addressing these modality-specific nuances is
crucial for effective retrieval in VCMR. However, existing methods often treat
all video contents equally, leading to sub-optimal moment retrieval. We argue
that effectively capturing the partial relevance between the query and video is
essential for the VCMR task. To this end, we propose a Partial Relevance
Enhanced Model~(PREM) to improve VCMR. VCMR involves two sub-tasks: video
retrieval and moment localization. To align with their distinct objectives, we
implement specialized partial relevance enhancement strategies. For video
retrieval, we introduce a multi-modal collaborative video retriever, generating
distinct query representations tailored for different modalities by
modality-specific pooling, ensuring a more effective match. For moment
localization, we propose the focus-then-fuse moment localizer, utilizing
modality-specific gates to capture essential content, followed by fusing
multi-modal information for moment localization. Experimental results on TVR
and DiDeMo datasets show that the proposed model outperforms the baselines,
achieving a new state-of-the-art of VCMR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Event-aware Video Corpus Moment Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13566v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13566v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danyang Hou, Liang Pang, Huawei Shen, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video Corpus Moment Retrieval (VCMR) is a practical video retrieval task
focused on identifying a specific moment within a vast corpus of untrimmed
videos using the natural language query. Existing methods for VCMR typically
rely on frame-aware video retrieval, calculating similarities between the query
and video frames to rank videos based on maximum frame similarity.However, this
approach overlooks the semantic structure embedded within the information
between frames, namely, the event, a crucial element for human comprehension of
videos. Motivated by this, we propose EventFormer, a model that explicitly
utilizes events within videos as fundamental units for video retrieval. The
model extracts event representations through event reasoning and hierarchical
event encoding. The event reasoning module groups consecutive and visually
similar frame representations into events, while the hierarchical event
encoding encodes information at both the frame and event levels. We also
introduce anchor multi-head self-attenion to encourage Transformer to capture
the relevance of adjacent content in the video. The training of EventFormer is
conducted by two-branch contrastive learning and dual optimization for two
sub-tasks of VCMR. Extensive experiments on TVR, ANetCaps, and DiDeMo
benchmarks show the effectiveness and efficiency of EventFormer in VCMR,
achieving new state-of-the-art results. Additionally, the effectiveness of
EventFormer is also validated on partially relevant video retrieval task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ARL2: Aligning Retrievers for Black-box Large Language Models via
  Self-guided Adaptive Relevance Labeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingxi Zhang, Yue Yu, Kuan Wang, Chao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation enhances large language models (LLMs) by
incorporating relevant information from external knowledge sources. This
enables LLMs to adapt to specific domains and mitigate hallucinations in
knowledge-intensive tasks. However, existing retrievers are often misaligned
with LLMs due to their separate training processes and the black-box nature of
LLMs. To address this challenge, we propose ARL2, a retriever learning
technique that harnesses LLMs as labelers. ARL2 leverages LLMs to annotate and
score relevant evidence, enabling learning the retriever from robust LLM
supervision. Furthermore, ARL2 uses an adaptive self-training strategy for
curating high-quality and diverse relevance data, which can effectively reduce
the annotation cost. Extensive experiments demonstrate the effectiveness of
ARL2, achieving accuracy improvements of 5.4% on NQ and 4.6% on MMLU compared
to the state-of-the-art methods. Additionally, ARL2 exhibits robust transfer
learning capabilities and strong zero-shot generalization abilities. Our code
will be published at \url{https://github.com/zhanglingxi-cs/ARL2}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Translation For Optimal Recall: Tailoring LLM Personalization
  With User Profiles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13500v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13500v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karthik Ravichandran, Sarmistha Sarna Gomasta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores a novel technique for improving recall in cross-language
information retrieval (CLIR) systems using iterative query refinement grounded
in the user's lexical-semantic space. The proposed methodology combines
multi-level translation, semantic embedding-based expansion, and user
profile-centered augmentation to address the challenge of matching variance
between user queries and relevant documents. Through an initial BM25 retrieval,
translation into intermediate languages, embedding lookup of similar terms, and
iterative re-ranking, the technique aims to expand the scope of potentially
relevant results personalized to the individual user. Comparative experiments
on news and Twitter datasets demonstrate superior performance over baseline
BM25 ranking for the proposed approach across ROUGE metrics. The translation
methodology also showed maintained semantic accuracy through the multi-step
process. This personalized CLIR framework paves the path for improved
context-aware retrieval attentive to the nuances of user language.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is just an initial idea and it's implementation. The results are
  computed for the first 100 data points. Detailed results will be published
  with the actual paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can One Embedding Fit All? A Multi-Interest Learning Paradigm Towards
  Improving User Interest Diversity Fairness <span class="chip">WWW'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuying Zhao, Minghua Xu, Huiyuan Chen, Yuzhong Chen, Yiwei Cai, Rashidul Islam, Yu Wang, Tyler Derr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems (RSs) have gained widespread applications across various
domains owing to the superior ability to capture users' interests. However, the
complexity and nuanced nature of users' interests, which span a wide range of
diversity, pose a significant challenge in delivering fair recommendations. In
practice, user preferences vary significantly; some users show a clear
preference toward certain item categories, while others have a broad interest
in diverse ones. Even though it is expected that all users should receive
high-quality recommendations, the effectiveness of RSs in catering to this
disparate interest diversity remains under-explored.
  In this work, we investigate whether users with varied levels of interest
diversity are treated fairly. Our empirical experiments reveal an inherent
disparity: users with broader interests often receive lower-quality
recommendations. To mitigate this, we propose a multi-interest framework that
uses multiple (virtual) interest embeddings rather than single ones to
represent users. Specifically, the framework consists of stacked multi-interest
representation layers, which include an interest embedding generator that
derives virtual interests from shared parameters, and a center embedding
aggregator that facilitates multi-hop aggregation. Experiments demonstrate the
effectiveness of the framework in achieving better trade-off between fairness
and utility across various datasets and backbones.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Effectiveness of Graph Contrastive Learning on Mathematical
  Information Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pei-Syuan Wang, Hung-Hsuan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper details an empirical investigation into using Graph Contrastive
Learning (GCL) to generate mathematical equation representations, a critical
aspect of Mathematical Information Retrieval (MIR). Our findings reveal that
this simple approach consistently exceeds the performance of the current
leading formula retrieval model, TangentCFT. To support ongoing research and
development in this field, we have made our source code accessible to the
public at https://github.com/WangPeiSyuan/GCL-Formula-Retrieval/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Retrieve for Job Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13435v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13435v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianqiang Shen, Yuchin Juan, Shaobo Zhang, Ping Liu, Wen Pu, Sriram Vasudevan, Qingquan Song, Fedor Borisyuk, Kay Qianqi Shen, Haichao Wei, Yunxiang Ren, Yeou S. Chiou, Sicong Kuang, Yuan Yin, Ben Zheng, Muchen Wu, Shaghayegh Gharghabi, Xiaoqing Wang, Huichao Xue, Qi Guo, Daniel Hewlett, Luke Simon, Liangjie Hong, Wenjing Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Web-scale search systems typically tackle the scalability challenge with a
two-step paradigm: retrieval and ranking. The retrieval step, also known as
candidate selection, often involves extracting standardized entities, creating
an inverted index, and performing term matching for retrieval. Such traditional
methods require manual and time-consuming development of query models. In this
paper, we discuss applying learning-to-retrieve technology to enhance LinkedIns
job search and recommendation systems. In the realm of promoted jobs, the key
objective is to improve the quality of applicants, thereby delivering value to
recruiter customers. To achieve this, we leverage confirmed hire data to
construct a graph that evaluates a seeker's qualification for a job, and
utilize learned links for retrieval. Our learned model is easy to explain,
debug, and adjust. On the other hand, the focus for organic jobs is to optimize
seeker engagement. We accomplished this by training embeddings for personalized
retrieval, fortified by a set of rules derived from the categorization of
member feedback. In addition to a solution based on a conventional inverted
index, we developed an on-GPU solution capable of supporting both KNN and term
matching efficiently.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Re4: Learning to Re-contrast, Re-attend, Re-construct for Multi-interest
  Recommendation <span class="chip">WWW 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.08011v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.08011v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengyu Zhang, Lingxiao Yang, Dong Yao, Yujie Lu, Fuli Feng, Zhou Zhao, Tat-seng Chua, Fei Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effectively representing users lie at the core of modern recommender systems.
Since users' interests naturally exhibit multiple aspects, it is of increasing
interest to develop multi-interest frameworks for recommendation, rather than
represent each user with an overall embedding. Despite their effectiveness,
existing methods solely exploit the encoder (the forward flow) to represent
multiple aspects of interests. However, without explicit regularization, the
interest embeddings may not be distinct from each other nor semantically
reflect representative historical items. Towards this end, we propose the Re4
framework, which leverages the backward flow to reexamine each interest
embedding. Specifically, Re4 encapsulates three backward flows, i.e., 1)
Re-contrast, which drives each interest embedding to be distinct from other
interests using contrastive learning; 2) Re-attend, which ensures the
interest-item correlation estimation in the forward flow to be consistent with
the criterion used in final recommendation; and 3) Re-construct, which ensures
that each interest embedding can semantically reflect the information of
representative items that relate to the corresponding interest. We demonstrate
the novel forward-backward multi-interest paradigm on ComiRec, and perform
extensive experiments on three real-world datasets. Empirical studies validate
that Re4 helps to learn learning distinct and effective multi-interest
representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures, accepted by WWW 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Collaborative Large Language Model for Recommender Systems <span class="chip">WWW2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01343v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01343v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaochen Zhu, Liang Wu, Qi Guo, Liangjie Hong, Jundong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there has been growing interest in developing the next-generation
recommender systems (RSs) based on pretrained large language models (LLMs).
However, the semantic gap between natural language and recommendation tasks is
still not well addressed, leading to multiple issues such as spuriously
correlated user/item descriptors, ineffective language modeling on user/item
data, inefficient recommendations via auto-regression, etc. In this paper, we
propose CLLM4Rec, the first generative RS that tightly integrates the LLM
paradigm and ID paradigm of RSs, aiming to address the above challenges
simultaneously. We first extend the vocabulary of pretrained LLMs with
user/item ID tokens to faithfully model user/item collaborative and content
semantics. Accordingly, a novel soft+hard prompting strategy is proposed to
effectively learn user/item collaborative/content token embeddings via language
modeling on RS-specific corpora, where each document is split into a prompt
consisting of heterogeneous soft (user/item) tokens and hard (vocab) tokens and
a main text consisting of homogeneous item tokens or vocab tokens to facilitate
stable and effective language modeling. In addition, a novel mutual
regularization strategy is introduced to encourage CLLM4Rec to capture
recommendation-related information from noisy user/item content. Finally, we
propose a novel recommendation-oriented finetuning strategy for CLLM4Rec, where
an item prediction head with multinomial likelihood is added to the pretrained
CLLM4Rec backbone to predict hold-out items based on soft+hard prompts
established from masked user-item interaction history, where recommendations of
multiple items can be generated efficiently without hallucination. Codes are
released at https://github.com/yaochenzhu/llm4rec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RAG-Fusion: a New Take on Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03367v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03367v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zackary Rackauckas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Infineon has identified a need for engineers, account managers, and customers
to rapidly obtain product information. This problem is traditionally addressed
with retrieval-augmented generation (RAG) chatbots, but in this study, I
evaluated the use of the newly popularized RAG-Fusion method. RAG-Fusion
combines RAG and reciprocal rank fusion (RRF) by generating multiple queries,
reranking them with reciprocal scores and fusing the documents and scores.
Through manually evaluating answers on accuracy, relevance, and
comprehensiveness, I found that RAG-Fusion was able to provide accurate and
comprehensive answers due to the generated queries contextualizing the original
query from various perspectives. However, some answers strayed off topic when
the generated queries' relevance to the original query is insufficient. This
research marks significant progress in artificial intelligence (AI) and natural
language processing (NLP) applications and demonstrates transformations in a
global and multi-industry context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures, 8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InstructIE: A Bilingual Instruction-based Information Extraction <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.11527v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.11527v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Honghao Gui, Shuofei Qiao, Jintian Zhang, Hongbin Ye, Mengshu Sun, Lei Liang, Huajun Chen, Ningyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional information extraction (IE) methodologies, constrained by
pre-defined classes and static training paradigms, often falter in
adaptability, especially in the dynamic world. To bridge this gap, we explore
an instruction-based IE paradigm in this paper, leveraging the substantial
cross-task generalization capabilities of Large Language Models (LLMs). We
observe that most existing IE datasets tend to be overly redundant in their
label sets, which leads to the inclusion of numerous labels not directly
relevant to the extraction content when constructing instructions. To tackle
this issue, we introduce a bilingual theme-centric IE instruction dataset
(Chinese and English), InstructIE, and for the first time, incorporate a theme
scheme design that effectively simplifies the label structure. Furthermore, we
develop an innovative framework named KG2Instruction, which is specifically
designed for the automatic generation of such datasets. Experimental
evaluations based on InstructIE reveal that while current models show promise
in Instruction-based IE tasks, opportunities for their potential optimization
also emerge. The dataset is available at
https://huggingface.co/datasets/zjunlp/InstructIE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress; project homepage:
  https://www.zjukg.org/project/InstructIE/ dataset:
  https://huggingface.co/datasets/zjunlp/InstructIE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Debiasing Recommendation with Personal Popularity <span class="chip">WWW'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07425v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07425v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Ning, Reynold Cheng, Xiao Yan, Ben Kao, Nan Huo, Nur AI Hasan Haldar, Bo Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Global popularity (GP) bias is the phenomenon that popular items are
recommended much more frequently than they should be, which goes against the
goal of providing personalized recommendations and harms user experience and
recommendation accuracy. Many methods have been proposed to reduce GP bias but
they fail to notice the fundamental problem of GP, i.e., it considers
popularity from a \textit{global} perspective of \textit{all users} and uses a
single set of popular items, and thus cannot capture the interests of
individual users. As such, we propose a user-aware version of item popularity
named \textit{personal popularity} (PP), which identifies different popular
items for each user by considering the users that share similar interests. As
PP models the preferences of individual users, it naturally helps to produce
personalized recommendations and mitigate GP bias. To integrate PP into
recommendation, we design a general \textit{personal popularity aware
counterfactual} (PPAC) framework, which adapts easily to existing
recommendation models. In particular, PPAC recognizes that PP and GP have both
direct and indirect effects on recommendations and controls direct effects with
counterfactual inference techniques for unbiased recommendations. All codes and
datasets are available at \url{https://github.com/Stevenn9981/PPAC}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW'24 as a research full paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributionally Robust Graph-based Recommendation System <span class="chip">WWW2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12994v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12994v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohao Wang, Jiawei Chen, Changdong Li, Sheng Zhou, Qihao Shi, Yang Gao, Yan Feng, Chun Chen, Can Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the capacity to capture high-order collaborative signals, Graph Neural
Networks (GNNs) have emerged as powerful methods in Recommender Systems (RS).
However, their efficacy often hinges on the assumption that training and
testing data share the same distribution (a.k.a. IID assumption), and exhibits
significant declines under distribution shifts. Distribution shifts commonly
arises in RS, often attributed to the dynamic nature of user preferences or
ubiquitous biases during data collection in RS. Despite its significance,
researches on GNN-based recommendation against distribution shift are still
sparse. To bridge this gap, we propose Distributionally Robust GNN (DR-GNN)
that incorporates Distributional Robust Optimization (DRO) into the GNN-based
recommendation. DR-GNN addresses two core challenges: 1) To enable DRO to cater
to graph data intertwined with GNN, we reinterpret GNN as a graph smoothing
regularizer, thereby facilitating the nuanced application of DRO; 2) Given the
typically sparse nature of recommendation data, which might impede robust
optimization, we introduce slight perturbations in the training distribution to
expand its support. Notably, while DR-GNN involves complex optimization, it can
be implemented easily and efficiently. Our extensive experiments validate the
effectiveness of DR-GNN against three typical distribution shifts. The code is
available at https://github.com/WANGBohaO-jpg/DR-GNN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical Matrix Factorization for Interpretable Collaborative
  Filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13277v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13277v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Sugahara, Kazushi Okamoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Matrix factorization (MF) is a simple collaborative filtering technique that
achieves superior recommendation accuracy by decomposing the user-item
interaction matrix into user and item latent matrices. Because the model
typically learns each interaction independently, it may overlook the underlying
shared dependencies between users and items, resulting in less stable and
interpretable recommendations. Based on these insights, we propose
"Hierarchical Matrix Factorization" (HMF), which incorporates clustering
concepts to capture the hierarchy, where leaf nodes and other nodes correspond
to users/items and clusters, respectively. Central to our approach, called
hierarchical embeddings, is the additional decomposition of the latent matrices
(embeddings) into probabilistic connection matrices, which link the hierarchy,
and a root cluster latent matrix. The embeddings are differentiable, allowing
simultaneous learning of interactions and clustering using a single gradient
descent method. Furthermore, the obtained cluster-specific interactions
naturally summarize user-item interactions and provide interpretability.
Experimental results on ratings and ranking predictions show that HMF
outperforms existing MF methods, in particular achieving a 1.37 point
improvement in RMSE for sparse interactions. Additionally, it was confirmed
that the clustering integration of HMF has the potential for faster learning
convergence and mitigation of overfitting compared to MF, and also provides
interpretability through a cluster-centered case study.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discovering and exploring cases of educational source code plagiarism
  with Dolos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10853v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10853v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rien Maertens, Maarten Van Neyghem, Maxiem Geldhof, Charlotte Van Petegem, Niko Strijbol, Peter Dawyndt, Bart Mesuere
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Source code plagiarism is a significant issue in educational practice, and
educators need user-friendly tools to cope with such academic dishonesty. This
article introduces the latest version of Dolos, a state-of-the-art ecosystem of
tools for detecting and preventing plagiarism in educational source code. In
this new version, the primary focus has been on enhancing the user experience.
Educators can now run the entire plagiarism detection pipeline from a new web
app in their browser, eliminating the need for any installation or
configuration. Completely redesigned analytics dashboards provide an instant
assessment of whether a collection of source files contains suspected cases of
plagiarism and how widespread plagiarism is within the collection. The
dashboards support hierarchically structured navigation to facilitate zooming
in and out of suspect cases. Clusters are an essential new component of the
dashboard design, reflecting the observation that plagiarism can occur among
larger groups of students. To meet various user needs, the Dolos software stack
for source code plagiarism detections now includes a web interface, a JSON
application programming interface (API), a command line interface (CLI), a
JavaScript library and a preconfigured Docker container. Clear documentation
and a free-to-use instance of the web app can be found at
https://dolos.ugent.be. The source code is also available on GitHub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 5 figures; minor corrections, reference added for section 2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InPars-Light: Cost-Effective Unsupervised Training of Efficient Rankers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.02998v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.02998v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonid Boytsov, Preksha Patel, Vivek Sourabh, Riddhi Nisar, Sayani Kundu, Ramya Ramanathan, Eric Nyberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We carried out a reproducibility study of InPars, which is a method for
unsupervised training of neural rankers (Bonifacio et al., 2022). As a
by-product, we developed InPars-light, which is a simple-yet-effective
modification of InPars. Unlike InPars, InPars-light uses 7x-100x smaller
ranking models and only a freely available language model BLOOM, which -- as we
found out -- produced more accurate rankers compared to a proprietary GPT-3
model. On all five English retrieval collections (used in the original InPars
study) we obtained substantial (7%-30%) and statistically significant
improvements over BM25 (in nDCG and MRR) using only a 30M parameter six-layer
MiniLM-30M ranker and a single three-shot prompt. In contrast, in the InPars
study only a 100x larger monoT5-3B model consistently outperformed BM25,
whereas their smaller monoT5-220M model (which is still 7x larger than our
MiniLM ranker) outperformed BM25 only on MS MARCO and TREC DL 2020. In the same
three-shot prompting scenario, our 435M parameter DeBERTA v3 ranker was at par
with the 7x larger monoT5-3B (average gain over BM25 of 1.3 vs 1.32): In fact,
on three out of five datasets, DeBERTA slightly outperformed monoT5-3B.
Finally, these good results were achieved by re-ranking only 100 candidate
documents compared to 1000 used by Bonifacio et al. (2022). We believe that
InPars-light is the first truly cost-effective prompt-based unsupervised recipe
to train and deploy neural ranking models that outperform BM25. Our code and
data is publicly available. https://github.com/searchivarius/inpars_light/
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StreaMulT: Streaming Multimodal <span class="highlight-title">Transformer</span> for Heterogeneous and
  Arbitrary Long Sequential Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.08021v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.08021v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victor Pellegrain, Myriam Tami, Michel Batteux, Céline Hudelot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing complexity of Industry 4.0 systems brings new challenges
regarding predictive maintenance tasks such as fault detection and diagnosis. A
corresponding and realistic setting includes multi-source data streams from
different modalities, such as sensors measurements time series, machine images,
textual maintenance reports, etc. These heterogeneous multimodal streams also
differ in their acquisition frequency, may embed temporally unaligned
information and can be arbitrarily long, depending on the considered system and
task. Whereas multimodal fusion has been largely studied in a static setting,
to the best of our knowledge, there exists no previous work considering
arbitrarily long multimodal streams alongside with related tasks such as
prediction across time. Thus, in this paper, we first formalize this paradigm
of heterogeneous multimodal learning in a streaming setting as a new one. To
tackle this challenge, we propose StreaMulT, a Streaming Multimodal Transformer
relying on cross-modal attention and on a memory bank to process arbitrarily
long input sequences at training time and run in a streaming way at inference.
StreaMulT improves the state-of-the-art metrics on CMU-MOSEI dataset for
Multimodal Sentiment Analysis task, while being able to deal with much longer
inputs than other multimodal models. The conducted experiments eventually
highlight the importance of the textual embedding layer, questioning recent
improvements in Multimodal Sentiment Analysis benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoAVT: A Cognition-Inspired Unified Audio-Visual-Text <span class="highlight-title">Pre-Train</span>ing Model
  for Multimodal Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.12264v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.12264v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianghu Yue, Xiaohai Tian, Lu Lu, Malu Zhang, Zhizheng Wu, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been a long-standing quest for a unified audio-visual-text model to
enable various multimodal understanding tasks, which mimics the listening,
seeing and reading process of human beings. Humans tends to represent knowledge
using two separate systems: one for representing verbal (textual) information
and one for representing non-verbal (visual and auditory) information. These
two systems can operate independently but can also interact with each other.
Motivated by this understanding of human cognition, in this paper, we introduce
CoAVT -- a novel cognition-inspired Correlated Audio-Visual-Text pre-training
model to connect the three modalities. It contains a joint audio-visual encoder
that learns to encode audio-visual synchronization information together with
the audio and visual content for non-verbal information, and a text encoder to
handle textual input for verbal information. To bridge the gap between
modalities, CoAVT employs a query encoder, which contains a set of learnable
query embeddings, and extracts the most informative audiovisual features of the
corresponding text. Additionally, to leverage the correspondences between audio
and vision with language respectively, we also establish the audio-text and
visual-text bi-modal alignments upon the foundational audiovisual-text
tri-modal alignment to enhance the multimodal representation learning. Finally,
we jointly optimize CoAVT model with three multimodal objectives: contrastive
loss, matching loss and language modeling loss. Extensive experiments show that
CoAVT can learn strong multimodal correlations and be generalized to various
downstream tasks. CoAVT establishes new state-of-the-art performance on
text-video retrieval task on AudioCaps for both zero-shot and fine-tuning
settings, audio-visual event classification and audio-visual retrieval tasks on
AudioSet and VGGSound.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Test-Time Adaptation with CLIP Reward for Zero-Shot Generalization in
  Vision-Language Models <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18010v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18010v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Zhao, Xiaohan Wang, Linchao Zhu, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One fascinating aspect of pre-trained vision-language models~(VLMs) learning
under language supervision is their impressive zero-shot generalization
capability. However, this ability is hindered by distribution shifts between
the training and testing data. Previous test time adaptation~(TTA) methods for
VLMs in zero-shot classification rely on minimizing the entropy of model
outputs, tending to be stuck in incorrect model predictions. In this work, we
propose TTA with feedback to rectify the model output and prevent the model
from becoming blindly confident. Specifically, a CLIP model is adopted as the
reward model during TTA and provides feedback for the VLM. Given a single test
sample, the VLM is forced to maximize the CLIP reward between the input and
sampled results from the VLM output distribution. The proposed
\textit{reinforcement learning with CLIP feedback~(RLCF)} framework is highly
flexible and universal. Beyond the classification task, with task-specific
sampling strategies and a proper reward baseline choice, RLCF can be easily
extended to not only discrimination tasks like retrieval but also
generalization tasks like image captioning, improving the zero-shot
generalization capacity of VLMs. According to the characteristics of these VL
tasks, we build different fully TTA pipelines with RLCF to improve the
zero-shot generalization ability of various VLMs. Extensive experiments along
with promising empirical results demonstrate the effectiveness of RLCF. The
code is available at https://github.com/mzhaoshuai/RLCF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by ICLR 2024, project page at
  https://mzhaoshuai.github.io/RLCF/, code is at
  https://github.com/mzhaoshuai/RLCF</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-02-20T00:00:00Z">2024-02-20</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">19</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unlocking the `Why' of Buying: Introducing a New <span class="highlight-title">Dataset</span> and Benchmark
  for Purchase Reason and Post-Purchase Experience 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Chen, Siqi Zuo, Cheng Li, Mingyang Zhang, Qiaozhu Mei, Michael Bendersky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explanations are crucial for enhancing user trust and understanding within
modern recommendation systems. To build truly explainable systems, we need
high-quality datasets that elucidate why users make choices. While previous
efforts have focused on extracting users' post-purchase sentiment in reviews,
they ignore the reasons behind the decision to buy.
  In our work, we propose a novel purchase reason explanation task. To this
end, we introduce an LLM-based approach to generate a dataset that consists of
textual explanations of why real users make certain purchase decisions. We
induce LLMs to explicitly distinguish between the reasons behind purchasing a
product and the experience after the purchase in a user review. An automated,
LLM-driven evaluation, as well as a small scale human evaluation, confirms the
effectiveness of our approach to obtaining high-quality, personalized
explanations. We benchmark this dataset on two personalized explanation
generation tasks. We release the code and prompts to spur further research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Simple but Effective Approach to Improve Structured Language Model
  Output for Information Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinghao Li, Rampi Ramprasad, Chao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated impressive abilities in
generating unstructured natural language according to instructions. However,
their performance can be inconsistent when tasked with producing text that
adheres to specific structured formats, which is crucial in applications like
named entity recognition (NER) or relation extraction (RE). To address this
issue, this paper introduces an efficient method, G&O, to enhance their
structured text generation capabilities. It breaks the generation into a
two-step pipeline: initially, LLMs generate answers in natural language as
intermediate responses. Subsequently, LLMs are asked to organize the output
into the desired structure, using the intermediate responses as context. G&O
effectively separates the generation of content from the structuring process,
reducing the pressure of completing two orthogonal tasks simultaneously. Tested
on zero-shot NER and RE, the results indicate a significant improvement in LLM
performance with minimal additional efforts. This straightforward and adaptable
prompting technique can also be combined with other strategies, like
self-consistency, to further elevate LLM capabilities in various structured
text generation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 5 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unlocking Insights: Semantic Search in Jupyter Notebooks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13234v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13234v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lan Li, Jinpeng Lv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic search, a process aimed at delivering highly relevant search results
by comprehending the searcher's intent and the contextual meaning of terms
within a searchable dataspace, plays a pivotal role in information retrieval.
In this paper, we investigate the application of large language models to
enhance semantic search capabilities, specifically tailored for the domain of
Jupyter Notebooks. Our objective is to retrieve generated outputs, such as
figures or tables, associated functions and methods, and other pertinent
information.
  We demonstrate a semantic search framework that achieves a comprehensive
semantic understanding of the entire notebook's contents, enabling it to
effectively handle various types of user queries. Key components of this
framework include:
  1). A data preprocessor is designed to handle diverse types of cells within
Jupyter Notebooks, encompassing both markdown and code cells. 2). An innovative
methodology is devised to address token size limitations that arise with
code-type cells. We implement a finer-grained approach to data input,
transitioning from the cell level to the function level, effectively resolving
these issues.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mode Estimation with Partial Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13079v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13079v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charles Arnal, Vivien Cabannes, Vianney Perchet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The combination of lightly supervised pre-training and online fine-tuning has
played a key role in recent AI developments. These new learning pipelines call
for new theoretical frameworks. In this paper, we formalize core aspects of
weakly supervised and active learning with a simple problem: the estimation of
the mode of a distribution using partial feedback. We show how entropy coding
allows for optimal information acquisition from partial feedback, develop
coarse sufficient statistics for mode identification, and adapt bandit
algorithms to our new setting. Finally, we combine those contributions into a
statistically and computationally efficient solution to our problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Real-World Complex Network Representations with Hyperedge
  Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13033v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13033v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyu Zhao, Zehui Li, Mingzhu Shen, Guy-Bart Stan, Pietro Liò, Yiren Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph augmentation methods play a crucial role in improving the performance
and enhancing generalisation capabilities in Graph Neural Networks (GNNs).
Existing graph augmentation methods mainly perturb the graph structures and are
usually limited to pairwise node relations. These methods cannot fully address
the complexities of real-world large-scale networks that often involve
higher-order node relations beyond only being pairwise. Meanwhile, real-world
graph datasets are predominantly modelled as simple graphs, due to the scarcity
of data that can be used to form higher-order edges. Therefore, reconfiguring
the higher-order edges as an integration into graph augmentation strategies
lights up a promising research path to address the aforementioned issues. In
this paper, we present Hyperedge Augmentation (HyperAug), a novel graph
augmentation method that constructs virtual hyperedges directly form the raw
data, and produces auxiliary node features by extracting from the virtual
hyperedge information, which are used for enhancing GNN performances on
downstream tasks. We design three diverse virtual hyperedge construction
strategies to accompany the augmentation scheme: (1) via graph statistics, (2)
from multiple data perspectives, and (3) utilising multi-modality. Furthermore,
to facilitate HyperAug evaluation, we provide 23 novel real-world graph
datasets across various domains including social media, biology, and
e-commerce. Our empirical study shows that HyperAug consistently and
significantly outperforms GNN baselines and other graph augmentation methods,
across a variety of application contexts, which clearly indicates that it can
effectively incorporate higher-order node relations into graph augmentation
methods for real-world complex networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Under review. 17 pages, 4 figures, 14 tables. arXiv admin
  note: text overlap with arXiv:2306.05108</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Autonomous Large Language Model Agent for Chemical Literature Data
  Mining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12993v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12993v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kexin Chen, Hanqun Cao, Junyou Li, Yuyang Du, Menghao Guo, Xin Zeng, Lanqing Li, Jiezhong Qiu, Pheng Ann Heng, Guangyong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chemical synthesis, which is crucial for advancing material synthesis and
drug discovery, impacts various sectors including environmental science and
healthcare. The rise of technology in chemistry has generated extensive
chemical data, challenging researchers to discern patterns and refine synthesis
processes. Artificial intelligence (AI) helps by analyzing data to optimize
synthesis and increase yields. However, AI faces challenges in processing
literature data due to the unstructured format and diverse writing style of
chemical literature. To overcome these difficulties, we introduce an end-to-end
AI agent framework capable of high-fidelity extraction from extensive chemical
literature. This AI agent employs large language models (LLMs) for prompt
generation and iterative optimization. It functions as a chemistry assistant,
automating data collection and analysis, thereby saving manpower and enhancing
performance. Our framework's efficacy is evaluated using accuracy, recall, and
F1 score of reaction condition data, and we compared our method with human
experts in terms of content correctness and time efficiency. The proposed
approach marks a significant advancement in automating chemical literature
extraction and demonstrates the potential for AI to revolutionize data
management and utilization in chemistry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding and Mitigating the Threat of Vec2Text to Dense Retrieval
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengyao Zhuang, Bevan Koopman, Xiaoran Chu, Guido Zuccon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The introduction of Vec2Text, a technique for inverting text embeddings, has
raised serious privacy concerns within dense retrieval systems utilizing text
embeddings, including those provided by OpenAI and Cohere. This threat comes
from the ability for a malicious attacker with access to text embeddings to
reconstruct the original text.
  In this paper, we investigate various aspects of embedding models that could
influence the recoverability of text using Vec2Text. Our exploration involves
factors such as distance metrics, pooling functions, bottleneck pre-training,
training with noise addition, embedding quantization, and embedding dimensions
-- aspects not previously addressed in the original Vec2Text paper. Through a
thorough analysis of these factors, our aim is to gain a deeper understanding
of the critical elements impacting the trade-offs between text recoverability
and retrieval effectiveness in dense retrieval systems. This analysis provides
valuable insights for practitioners involved in designing privacy-aware dense
retrieval systems. Additionally, we propose a straightforward fix for embedding
transformation that ensures equal ranking effectiveness while mitigating the
risk of text recoverability.
  Furthermore, we extend the application of Vec2Text to the separate task of
corpus poisoning, where, theoretically, Vec2Text presents a more potent threat
compared to previous attack methods. Notably, Vec2Text does not require access
to the dense retriever's model parameters and can efficiently generate numerous
adversarial passages.
  In summary, this study highlights the potential threat posed by Vec2Text to
existing dense retrieval systems, while also presenting effective methods to
patch and strengthen such systems against such risks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpreting Conversational Dense Retrieval by Rewriting-Enhanced
  Inversion of Session Embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12774v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12774v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiruo Cheng, Kelong Mao, Zhicheng Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational dense retrieval has shown to be effective in conversational
search. However, a major limitation of conversational dense retrieval is their
lack of interpretability, hindering intuitive understanding of model behaviors
for targeted improvements. This paper presents CONVINV, a simple yet effective
approach to shed light on interpretable conversational dense retrieval models.
CONVINV transforms opaque conversational session embeddings into explicitly
interpretable text while faithfully maintaining their original retrieval
performance as much as possible. Such transformation is achieved by training a
recently proposed Vec2Text model based on the ad-hoc query encoder, leveraging
the fact that the session and query embeddings share the same space in existing
conversational dense retrieval. To further enhance interpretability, we propose
to incorporate external interpretable query rewrites into the transformation
process. Extensive evaluations on three conversational search benchmarks
demonstrate that CONVINV can yield more interpretable text and faithfully
preserve original retrieval performance than baselines. Our work connects
opaque session embeddings with transparent query rewriting, paving the way
toward trustworthy conversational search.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BMLP: Behavior-aware MLP for Heterogeneous Sequential Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixin Li, Yuhao Wu, Yang Liu, Weike Pan, Zhong Ming
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real recommendation scenarios, users often have different types of
behaviors, such as clicking and buying. Existing research methods show that it
is possible to capture the heterogeneous interests of users through different
types of behaviors. However, most multi-behavior approaches have limitations in
learning the relationship between different behaviors. In this paper, we
propose a novel multilayer perceptron (MLP)-based heterogeneous sequential
recommendation method, namely behavior-aware multilayer perceptron (BMLP).
Specifically, it has two main modules, including a heterogeneous interest
perception (HIP) module, which models behaviors at multiple granularities
through behavior types and transition relationships, and a purchase intent
perception (PIP) module, which adaptively fuses subsequences of auxiliary
behaviors to capture users' purchase intent. Compared with mainstream sequence
models, MLP is competitive in terms of accuracy and has unique advantages in
simplicity and efficiency. Extensive experiments show that BMLP achieves
significant improvement over state-of-the-art algorithms on four public
datasets. In addition, its pure MLP architecture leads to a linear time
complexity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modality-Aware Integration with Large Language Models for
  Knowledge-based Visual Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junnan Dong, Qinggang Zhang, Huachi Zhou, Daochen Zha, Pai Zheng, Xiao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge-based visual question answering (KVQA) has been extensively studied
to answer visual questions with external knowledge, e.g., knowledge graphs
(KGs). While several attempts have been proposed to leverage large language
models (LLMs) as an implicit knowledge source, it remains challenging since
LLMs may generate hallucinations. Moreover, multiple knowledge sources, e.g.,
images, KGs and LLMs, cannot be readily aligned for complex scenarios. To
tackle these, we present a novel modality-aware integration with LLMs for KVQA
(MAIL). It carefully leverages multimodal knowledge for both image
understanding and knowledge reasoning. Specifically, (i) we propose a two-stage
prompting strategy with LLMs to densely embody the image into a scene graph
with detailed visual features; (ii) We construct a coupled concept graph by
linking the mentioned entities with external facts. (iii) A tailored
pseudo-siamese graph medium fusion is designed for sufficient multimodal
fusion. We utilize the shared mentioned entities in two graphs as mediums to
bridge a tight inter-modal exchange, while maximally preserving insightful
intra-modal learning by constraining the fusion within mediums. Extensive
experiments on two benchmark datasets show the superiority of MAIL with 24x
less resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages,3 figures and 1 page appendix; The processed graphs and codes
  will be avalibale</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SoftQE: Learned Representations of Queries Expanded by LLMs <span class="chip">ECIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12663v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12663v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Varad Pimpalkhute, John Heyer, Xusen Yin, Sameer Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the integration of Large Language Models (LLMs) into query
encoders to improve dense retrieval without increasing latency and cost, by
circumventing the dependency on LLMs at inference time. SoftQE incorporates
knowledge from LLMs by mapping embeddings of input queries to those of the
LLM-expanded queries. While improvements over various strong baselines on
in-domain MS-MARCO metrics are marginal, SoftQE improves performance by 2.83
absolute percentage points on average on five out-of-domain BEIR tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in ECIR 2024 proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ User Modeling and User Profiling: A Comprehensive <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09660v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09660v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erasmo Purificato, Ludovico Boratto, Ernesto William De Luca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of artificial intelligence (AI) into daily life, particularly
through information retrieval and recommender systems, has necessitated
advanced user modeling and profiling techniques to deliver personalized
experiences. These techniques aim to construct accurate user representations
based on the rich amounts of data generated through interactions with these
systems. This paper presents a comprehensive survey of the current state,
evolution, and future directions of user modeling and profiling research. We
provide a historical overview, tracing the development from early stereotype
models to the latest deep learning techniques, and propose a novel taxonomy
that encompasses all active topics in this research area, including recent
trends. Our survey highlights the paradigm shifts towards more sophisticated
user profiling methods, emphasizing implicit data collection, multi-behavior
modeling, and the integration of graph data structures. We also address the
critical need for privacy-preserving techniques and the push towards
explainability and fairness in user modeling approaches. By examining the
definitions of core terminology, we aim to clarify ambiguities and foster a
clearer understanding of the field by proposing two novel encyclopedic
definitions of the main terms. Furthermore, we explore the application of user
modeling in various domains, such as fake news detection, cybersecurity, and
personalized education. This survey serves as a comprehensive resource for
researchers and practitioners, offering insights into the evolution of user
modeling and profiling and guiding the development of more personalized,
ethical, and effective AI systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>71 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fair Ranking under Disparate Uncertainty <span class="chip">UAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.01610v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.01610v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richa Rastogi, Thorsten Joachims
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ranking is a ubiquitous method for focusing the attention of human evaluators
on a manageable subset of options. Its use as part of human decision-making
processes ranges from surfacing potentially relevant products on an e-commerce
site to prioritizing college applications for human review. While ranking can
make human evaluation more effective by focusing attention on the most
promising options, we argue that it can introduce unfairness if the uncertainty
of the underlying relevance model differs between groups of options.
Unfortunately, such disparity in uncertainty appears widespread, often to the
detriment of minority groups for which relevance estimates can have higher
uncertainty due to a lack of data or appropriate features. To address this
fairness issue, we propose Equal-Opportunity Ranking (EOR) as a new fairness
criterion for ranking and show that it corresponds to a group-wise fair lottery
among the relevant options even in the presence of disparate uncertainty.
Furthermore, EOR optimizes for an even cost burden on all groups, unlike the
conventional Probability Ranking Principle. In contrast to affirmative action
interventions like proportional Rooney rule constraints, EOR does not require
the designation of a disadvantaged group. To make EOR ranking practical, we
present an efficient algorithm for computing it in time $O(n \log(n))$ and
prove its close approximation guarantee to the globally optimal solution. In a
comprehensive empirical evaluation on synthetic data, a US Census dataset, and
a real-world audit of Amazon search queries, we find that the algorithm
reliably guarantees EOR fairness while providing effective rankings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A version of this paper was accepted as Spotlight (Oral) at UAI
  workshop on Epistemic AI, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unified Hallucination Detection for Multimodal Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03190v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03190v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Chen, Chenxi Wang, Yida Xue, Ningyu Zhang, Xiaoyan Yang, Qiang Li, Yue Shen, Lei Liang, Jinjie Gu, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant strides in multimodal tasks, Multimodal Large Language
Models (MLLMs) are plagued by the critical issue of hallucination. The reliable
detection of such hallucinations in MLLMs has, therefore, become a vital aspect
of model evaluation and the safeguarding of practical application deployment.
Prior research in this domain has been constrained by a narrow focus on
singular tasks, an inadequate range of hallucination categories addressed, and
a lack of detailed granularity. In response to these challenges, our work
expands the investigative horizons of hallucination detection. We present a
novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate
the evaluation of advancements in hallucination detection methods.
Additionally, we unveil a novel unified multimodal hallucination detection
framework, UNIHD, which leverages a suite of auxiliary tools to validate the
occurrence of hallucinations robustly. We demonstrate the effectiveness of
UNIHD through meticulous evaluation and comprehensive analysis. We also provide
strategic insights on the application of specific tools for addressing various
categories of hallucinations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Click<span class="highlight-title">Prompt</span>: CTR Models are Strong <span class="highlight-title">Prompt</span> Generators for Adapting
  Language Models to CTR Prediction <span class="chip">WWW 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09234v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09234v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianghao Lin, Bo Chen, Hangyu Wang, Yunjia Xi, Yanru Qu, Xinyi Dai, Kangning Zhang, Ruiming Tang, Yong Yu, Weinan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Click-through rate (CTR) prediction has become increasingly indispensable for
various Internet applications. Traditional CTR models convert the multi-field
categorical data into ID features via one-hot encoding, and extract the
collaborative signals among features. Such a paradigm suffers from the problem
of semantic information loss. Another line of research explores the potential
of pretrained language models (PLMs) for CTR prediction by converting input
data into textual sentences through hard prompt templates. Although semantic
signals are preserved, they generally fail to capture the collaborative
information (e.g., feature interactions, pure ID features), not to mention the
unacceptable inference overhead brought by the huge model size. In this paper,
we aim to model both the semantic knowledge and collaborative knowledge for
accurate CTR estimation, and meanwhile address the inference inefficiency
issue. To benefit from both worlds and close their gaps, we propose a novel
model-agnostic framework (i.e., ClickPrompt), where we incorporate CTR models
to generate interaction-aware soft prompts for PLMs. We design a
prompt-augmented masked language modeling (PA-MLM) pretraining task, where PLM
has to recover the masked tokens based on the language context, as well as the
soft prompts generated by CTR model. The collaborative and semantic knowledge
from ID and textual features would be explicitly aligned and interacted via the
prompt interface. Then, we can either tune the CTR model with PLM for superior
performance, or solely tune the CTR model without PLM for inference efficiency.
Experiments on four real-world datasets validate the effectiveness of
ClickPrompt compared with existing baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model Editing at Scale leads to Gradual and Catastrophic Forgetting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.07453v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.07453v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshat Gupta, Anurag Rao, Gopala Anumanchipalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Editing knowledge in large language models is an attractive capability to
have which allows us to correct incorrectly learnt facts during pre-training,
as well as update the model with an ever-growing list of new facts. While
existing model editing techniques have shown promise, they are usually
evaluated using metrics for reliability, specificity and generalization over
one or few edits. We argue that for model editing to have practical utility, we
must be able to make multiple edits to the same model. With this in mind, we
evaluate the current model editing methods at scale, focusing on two state of
the art methods: ROME and MEMIT. We find that as the model is edited
sequentially with multiple facts, it continually forgets previously edited
facts and the ability to perform downstream tasks. This forgetting happens in
two phases -- an initial gradual but progressive forgetting phase followed by
abrupt or catastrophic forgetting phase. Both gradual and catastrophic
forgetting limit the usefulness of model editing methods at scale -- the former
making model editing less effective as multiple edits are made to the model
while the latter caps the scalability of such model editing methods. Our
analysis also highlights other key limitations of ROME and MEMIT at scale. With
our work, we push for the development and evaluation of model editing methods
keeping scalability in mind.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Added experiments with GPT-J and appendix for all samples and
  ablations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Challenging Low Homophily in Social Recommendation <span class="chip">WWW</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.14606v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.14606v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Jiang, Xinyi Gao, Guandong Xu, Tong Chen, Hongzhi Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social relations are leveraged to tackle the sparsity issue of user-item
interaction data in recommendation under the assumption of social homophily.
However, social recommendation paradigms predominantly focus on homophily based
on user preferences. While social information can enhance recommendations, its
alignment with user preferences is not guaranteed, thereby posing the risk of
introducing informational redundancy. We empirically discover that social
graphs in real recommendation data exhibit low preference-aware homophily,
which limits the effect of social recommendation models. To comprehensively
extract preference-aware homophily information latent in the social graph, we
propose Social Heterophily-alleviating Rewiring (SHaRe), a data-centric
framework for enhancing existing graph-based social recommendation models. We
adopt Graph Rewiring technique to capture and add highly homophilic social
relations, and cut low homophilic (or heterophilic) relations. To better refine
the user representations from reliable social relations, we integrate a
contrastive learning method into the training of SHaRe, aiming to calibrate the
user representations for enhancing the result of Graph Rewiring. Experiments on
real-world datasets show that the proposed framework not only exhibits enhanced
performances across varying homophily ratios but also improves the performance
of existing state-of-the-art (SOTA) social recommendation models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by The Web Conference (WWW) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Microstructures and Accuracy of Graph Recall by Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11821v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11821v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanbang Wang, Hejie Cui, Jon Kleinberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graphs data is crucial for many applications, and much of it exists in the
relations described in textual format. As a result, being able to accurately
recall and encode a graph described in earlier text is a basic yet pivotal
ability that LLMs need to demonstrate if they are to perform reasoning tasks
that involve graph-structured information. Human performance at graph recall
has been studied by cognitive scientists for decades, and has been found to
often exhibit certain structural patterns of bias that align with human
handling of social relationships. To date, however, we know little about how
LLMs behave in analogous graph recall tasks: do their recalled graphs also
exhibit certain biased patterns, and if so, how do they compare with humans and
affect other graph reasoning tasks? In this work, we perform the first
systematical study of graph recall by LLMs, investigating the accuracy and
biased microstructures (local structural patterns) in their recall. We find
that LLMs not only underperform often in graph recall, but also tend to favor
more triangles and alternating 2-paths. Moreover, we find that more advanced
LLMs have a striking dependence on the domain that a real-world graph comes
from -- by yielding the best recall accuracy when the graph is narrated in a
language style consistent with its original domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 7 tables, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive <span class="highlight-title">Survey</span> on Deep Learning Techniques in Educational Data
  Mining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.04761v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.04761v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanguo Lin, Hong Chen, Wei Xia, Fan Lin, Zongyue Wang, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Educational Data Mining (EDM) has emerged as a vital field of research, which
harnesses the power of computational techniques to analyze educational data.
With the increasing complexity and diversity of educational data, Deep Learning
techniques have shown significant advantages in addressing the challenges
associated with analyzing and modeling this data. This survey aims to
systematically review the state-of-the-art in EDM with Deep Learning. We begin
by providing a brief introduction to EDM and Deep Learning, highlighting their
relevance in the context of modern education. Next, we present a detailed
review of Deep Learning techniques applied in four typical educational
scenarios, including knowledge tracing, student behavior detection, performance
prediction, and personalized recommendation. Furthermore, a comprehensive
overview of public datasets and processing tools for EDM is provided. Finally,
we point out emerging trends and future directions in this research area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SoMeLVLM: A Large Vision Language Model for Social Media Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13022v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13022v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinnong Zhang, Haoyu Kuang, Xinyi Mou, Hanjia Lyu, Kun Wu, Siming Chen, Jiebo Luo, Xuanjing Huang, Zhongyu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growth of social media, characterized by its multimodal nature, has led
to the emergence of diverse phenomena and challenges, which calls for an
effective approach to uniformly solve automated tasks. The powerful Large
Vision Language Models make it possible to handle a variety of tasks
simultaneously, but even with carefully designed prompting methods, the general
domain models often fall short in aligning with the unique speaking style and
context of social media tasks. In this paper, we introduce a Large Vision
Language Model for Social Media Processing (SoMeLVLM), which is a cognitive
framework equipped with five key capabilities including knowledge &
comprehension, application, analysis, evaluation, and creation. SoMeLVLM is
designed to understand and generate realistic social media behavior. We have
developed a 654k multimodal social media instruction-tuning dataset to support
our cognitive framework and fine-tune our model. Our experiments demonstrate
that SoMeLVLM achieves state-of-the-art performance in multiple social media
tasks. Further analysis shows its significant advantages over baselines in
terms of cognitive abilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A User-Friendly Framework for Generating Model-Preferred <span class="highlight-title">Prompt</span>s in
  Text-to-Image Synthesis <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12760v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12760v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nailei Hei, Qianyu Guo, Zihao Wang, Yan Wang, Haofen Wang, Wenqiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Well-designed prompts have demonstrated the potential to guide text-to-image
models in generating amazing images. Although existing prompt engineering
methods can provide high-level guidance, it is challenging for novice users to
achieve the desired results by manually entering prompts due to a discrepancy
between novice-user-input prompts and the model-preferred prompts. To bridge
the distribution gap between user input behavior and model training datasets,
we first construct a novel Coarse-Fine Granularity Prompts dataset (CFP) and
propose a novel User-Friendly Fine-Grained Text Generation framework (UF-FGTG)
for automated prompt optimization. For CFP, we construct a novel dataset for
text-to-image tasks that combines coarse and fine-grained prompts to facilitate
the development of automated prompt generation methods. For UF-FGTG, we propose
a novel framework that automatically translates user-input prompts into
model-preferred prompts. Specifically, we propose a prompt refiner that
continually rewrites prompts to empower users to select results that align with
their unique needs. Meanwhile, we integrate image-related loss functions from
the text-to-image model into the training process of text generation to
generate model-preferred prompts. Additionally, we propose an adaptive feature
extraction module to ensure diversity in the generated results. Experiments
demonstrate that our approach is capable of generating more visually appealing
and diverse images than previous state-of-the-art methods, achieving an average
improvement of 5% across six quality and aesthetic metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by The 38th Annual AAAI Conference on Artificial
  Intelligence (AAAI 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Television Discourse Decoded: Comprehensive Multimodal Analytics at
  Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anmol Agarwal, Pratyush Priyadarshi, Shiven Sinha, Shrey Gupta, Hitkul Jangra, Kiran Garimella, Ponnurangam Kumaraguru
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we tackle the complex task of analyzing televised debates,
with a focus on a prime time news debate show from India. Previous methods,
which often relied solely on text, fall short in capturing the multimedia
essence of these debates. To address this gap, we introduce a comprehensive
automated toolkit that employs advanced computer vision and speech-to-text
techniques for large-scale multimedia analysis. Utilizing state-of-the-art
computer vision algorithms and speech-to-text methods, we transcribe, diarize,
and analyze thousands of YouTube videos of prime-time television debates in
India. These debates are a central part of Indian media but have been
criticized for compromised journalistic integrity and excessive dramatization.
Our toolkit provides concrete metrics to assess bias and incivility, capturing
a comprehensive multimedia perspective that includes text, audio utterances,
and video frames. Our findings reveal significant biases in topic selection and
panelist representation, along with alarming levels of incivility. This work
offers a scalable, automated approach for future research in multimedia
analysis, with profound implications for the quality of public discourse and
democratic debate. We will make our data analysis pipeline and collected data
publicly available to catalyze further research in this domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rip Current Detection in Nearshore Areas through UAV Video Analysis with
  Almost Local-Isometric Embedding Techniques on Sphere 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.11783v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.11783v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anchen Sun, Kaiqi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rip currents pose a significant danger to those who visit beaches, as they
can swiftly pull swimmers away from shore. Detecting these currents currently
relies on costly equipment and is challenging to implement on a larger scale.
The advent of unmanned aerial vehicles (UAVs) and camera technology, however,
has made monitoring near-shore regions more accessible and scalable. This paper
proposes a new framework for detecting rip currents using video-based methods
that leverage optical flow estimation, offshore direction calculation, earth
camera projection with almost local-isometric embedding on the sphere, and
temporal data fusion techniques. Through the analysis of videos from multiple
beaches, including Palm Beach, Haulover, Ocean Reef Park, and South Beach, as
well as YouTube footage, we demonstrate the efficacy of our approach, which
aligns with human experts' annotations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unified Hallucination Detection for Multimodal Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03190v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03190v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Chen, Chenxi Wang, Yida Xue, Ningyu Zhang, Xiaoyan Yang, Qiang Li, Yue Shen, Lei Liang, Jinjie Gu, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant strides in multimodal tasks, Multimodal Large Language
Models (MLLMs) are plagued by the critical issue of hallucination. The reliable
detection of such hallucinations in MLLMs has, therefore, become a vital aspect
of model evaluation and the safeguarding of practical application deployment.
Prior research in this domain has been constrained by a narrow focus on
singular tasks, an inadequate range of hallucination categories addressed, and
a lack of detailed granularity. In response to these challenges, our work
expands the investigative horizons of hallucination detection. We present a
novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate
the evaluation of advancements in hallucination detection methods.
Additionally, we unveil a novel unified multimodal hallucination detection
framework, UNIHD, which leverages a suite of auxiliary tools to validate the
occurrence of hallucinations robustly. We demonstrate the effectiveness of
UNIHD through meticulous evaluation and comprehensive analysis. We also provide
strategic insights on the application of specific tools for addressing various
categories of hallucinations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A multimodal dynamical variational autoencoder for audiovisual speech
  representation learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03582v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03582v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samir Sadok, Simon Leglaive, Laurent Girin, Xavier Alameda-Pineda, Renaud Séguier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a multimodal and dynamical VAE (MDVAE) applied to
unsupervised audio-visual speech representation learning. The latent space is
structured to dissociate the latent dynamical factors that are shared between
the modalities from those that are specific to each modality. A static latent
variable is also introduced to encode the information that is constant over
time within an audiovisual speech sequence. The model is trained in an
unsupervised manner on an audiovisual emotional speech dataset, in two stages.
In the first stage, a vector quantized VAE (VQ-VAE) is learned independently
for each modality, without temporal modeling. The second stage consists in
learning the MDVAE model on the intermediate representation of the VQ-VAEs
before quantization. The disentanglement between static versus dynamical and
modality-specific versus modality-common information occurs during this second
training stage. Extensive experiments are conducted to investigate how
audiovisual speech latent factors are encoded in the latent space of MDVAE.
These experiments include manipulating audiovisual speech, audiovisual facial
image denoising, and audiovisual speech emotion recognition. The results show
that MDVAE effectively combines the audio and visual information in its latent
space. They also show that the learned static representation of audiovisual
speech can be used for emotion recognition with few labeled data, and with
better accuracy compared with unimodal baselines and a state-of-the-art
supervised model based on an audiovisual transformer architecture.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 figures, https://samsad35.github.io/site-mdvae/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2024-02-28T12:10:40.300557481Z">
            2024-02-28 12:10:40 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
